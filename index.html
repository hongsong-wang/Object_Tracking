<!DOCTYPE html>
<html>
<head>
<title>arXiv Papers of Object Tracking</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 10px 20px 10px 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body><div id='title' style='font-size:1.3em; font-weight:bold;'>arXiv Papers of Object Tracking</div><br>
<div id='section'>PaperID: <span id='pid'>1, <a href='https://arxiv.org/pdf/2602.17231.pdf' target='_blank'>https://arxiv.org/pdf/2602.17231.pdf</a></span>   <span><a href='https://github.com/XuYiMing83/HiMAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiming Xu, Yi Yang, Hao Cheng, Monika Sester
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.17231">HiMAP: History-aware Map-occupancy Prediction with Fallback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate motion forecasting is critical for autonomous driving, yet most predictors rely on multi-object tracking (MOT) with identity association, assuming that objects are correctly and continuously tracked. When tracking fails due to, e.g., occlusion, identity switches, or missed detections, prediction quality degrades and safety risks increase. We present \textbf{HiMAP}, a tracking-free, trajectory prediction framework that remains reliable under MOT failures. HiMAP converts past detections into spatiotemporally invariant historical occupancy maps and introduces a historical query module that conditions on the current agent state to iteratively retrieve agent-specific history from unlabeled occupancy representations. The retrieved history is summarized by a temporal map embedding and, together with the final query and map context, drives a DETR-style decoder to produce multi-modal future trajectories. This design lifts identity reliance, supports streaming inference via reusable encodings, and serves as a robust fallback when tracking is unavailable. On Argoverse~2, HiMAP achieves performance comparable to tracking-based methods while operating without IDs, and it substantially outperforms strong baselines in the no-tracking setting, yielding relative gains of 11\% in FDE, 12\% in ADE, and a 4\% reduction in MR over a fine-tuned QCNet. Beyond aggregate metrics, HiMAP delivers stable forecasts for all agents simultaneously without waiting for tracking to recover, highlighting its practical value for safety-critical autonomy. The code is available under: https://github.com/XuYiMing83/HiMAP.
<div id='section'>PaperID: <span id='pid'>2, <a href='https://arxiv.org/pdf/2602.13636.pdf' target='_blank'>https://arxiv.org/pdf/2602.13636.pdf</a></span>   <span><a href='https://github.com/XiaoMoc/LGTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yang Zhou, Derui Ding, Ran Sun, Ying Sun, Haohua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.13636">Layer-Guided UAV Tracking: Enhancing Efficiency and Occlusion Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking (VOT) plays a pivotal role in unmanned aerial vehicle (UAV) applications. Addressing the trade-off between accuracy and efficiency, especially under challenging conditions like unpredictable occlusion, remains a significant challenge. This paper introduces LGTrack, a unified UAV tracking framework that integrates dynamic layer selection, efficient feature enhancement, and robust representation learning for occlusions. By employing a novel lightweight Global-Grouped Coordinate Attention (GGCA) module, LGTrack captures long-range dependencies and global contexts, enhancing feature discriminability with minimal computational overhead. Additionally, a lightweight Similarity-Guided Layer Adaptation (SGLA) module replaces knowledge distillation, achieving an optimal balance between tracking precision and inference efficiency. Experiments on three datasets demonstrate LGTrack's state-of-the-art real-time speed (258.7 FPS on UAVDT) while maintaining competitive tracking accuracy (82.8\% precision). Code is available at https://github.com/XiaoMoc/LGTrack
<div id='section'>PaperID: <span id='pid'>3, <a href='https://arxiv.org/pdf/2602.00570.pdf' target='_blank'>https://arxiv.org/pdf/2602.00570.pdf</a></span>   <span><a href='https://github.com/Confetti-lxy/GLAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xingyu Luo, Yidong Cai, Jie Liu, Jie Tang, Gangshan Wu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00570">GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD
<div id='section'>PaperID: <span id='pid'>4, <a href='https://arxiv.org/pdf/2602.00484.pdf' target='_blank'>https://arxiv.org/pdf/2602.00484.pdf</a></span>   <span><a href='https://github.com/ron941/GTATrack-STC2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Rong-Lin Jian, Ming-Chi Luo, Chen-Wei Huang, Chia-Ming Lee, Yu-Fan Lin, Chih-Chung Hsu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.00484">GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.
<div id='section'>PaperID: <span id='pid'>5, <a href='https://arxiv.org/pdf/2601.02905.pdf' target='_blank'>https://arxiv.org/pdf/2601.02905.pdf</a></span>   <span><a href='https://lab-rococo-sapienza.github.io/lost-3dsg/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Sara Micol Ferraina, Michele Brienza, Francesco Argenziano, Emanuele Musumeci, Vincenzo Suriani, Domenico D. Bloisi, Daniele Nardi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02905">LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.
<div id='section'>PaperID: <span id='pid'>6, <a href='https://arxiv.org/pdf/2601.01022.pdf' target='_blank'>https://arxiv.org/pdf/2601.01022.pdf</a></span>   <span><a href='https://github.com/Event-AHU/OpenEvTracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiao Wang, Xiao Wang, Haonan Zhao, Jiarui Xu, Bo Jiang, Lin Zhu, Xin Zhao, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01022">Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking
<div id='section'>PaperID: <span id='pid'>7, <a href='https://arxiv.org/pdf/2512.22799.pdf' target='_blank'>https://arxiv.org/pdf/2512.22799.pdf</a></span>   <span><a href='https://github.com/jcwang0602/VPTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jingchao Wang, Kaiwen Zhou, Zhijian Wu, Kunhua Ji, Dingjiang Huang, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22799">VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.
<div id='section'>PaperID: <span id='pid'>8, <a href='https://arxiv.org/pdf/2512.22624.pdf' target='_blank'>https://arxiv.org/pdf/2512.22624.pdf</a></span>   <span><a href='https://github.com/HamadYA/SAM3_Tracking_Zoo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohamad Alansari, Muzammal Naseer, Hasan Al Marzouqi, Naoufel Werghi, Sajid Javed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22624">Rethinking Memory Design in SAM-Based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>\noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}
<div id='section'>PaperID: <span id='pid'>9, <a href='https://arxiv.org/pdf/2512.22581.pdf' target='_blank'>https://arxiv.org/pdf/2512.22581.pdf</a></span>   <span><a href='https://marwan99.github.io/kv_tracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Marwan Taher, Ignacio Alzugaray, Kirill Mazur, Xin Kong, Andrew J. Davison
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22581">KV-Tracker: Real-Time Pose Tracking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $π^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\sim}27$ FPS.
<div id='section'>PaperID: <span id='pid'>10, <a href='https://arxiv.org/pdf/2512.22105.pdf' target='_blank'>https://arxiv.org/pdf/2512.22105.pdf</a></span>   <span><a href='https://github.com/Robotmurlock/TDLP' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Robotmurlock/TDLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Momir Adžemović
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22105">Learning Association via Track-Detection Matching for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking aims to maintain object identities over time by associating detections across video frames. Two dominant paradigms exist in literature: tracking-by-detection methods, which are computationally efficient but rely on handcrafted association heuristics, and end-to-end approaches, which learn association from data at the cost of higher computational complexity. We propose Track-Detection Link Prediction (TDLP), a tracking-by-detection method that performs per-frame association via link prediction between tracks and detections, i.e., by predicting the correct continuation of each track at every frame. TDLP is architecturally designed primarily for geometric features such as bounding boxes, while optionally incorporating additional cues, including pose and appearance. Unlike heuristic-based methods, TDLP learns association directly from data without handcrafted rules, while remaining modular and computationally efficient compared to end-to-end trackers. Extensive experiments on multiple benchmarks demonstrate that TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods. Finally, we provide a detailed analysis comparing link prediction with metric learning-based association and show that link prediction is more effective, particularly when handling heterogeneous features such as detection bounding boxes. Our code is available at \href{https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP}.
<div id='section'>PaperID: <span id='pid'>11, <a href='https://arxiv.org/pdf/2512.15066.pdf' target='_blank'>https://arxiv.org/pdf/2512.15066.pdf</a></span>   <span><a href='https://github.com/XiAooZ/MWNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenxiao Zhang, Runshi Zhang, Junchen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15066">Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.
<div id='section'>PaperID: <span id='pid'>12, <a href='https://arxiv.org/pdf/2512.13130.pdf' target='_blank'>https://arxiv.org/pdf/2512.13130.pdf</a></span>   <span><a href='https://github.com/shl-shawn/LeafTrackNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shanghua Liu, Majharulislam Babor, Christoph Verduyn, Breght Vandenberghe, Bruno Betoni Parodi, Cornelia Weltzien, Marina M. -C. Höhne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13130">LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.
<div id='section'>PaperID: <span id='pid'>13, <a href='https://arxiv.org/pdf/2512.13007.pdf' target='_blank'>https://arxiv.org/pdf/2512.13007.pdf</a></span>   <span><a href='https://github.com/nagonch/LiFT-6DoF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Nikolai Goncharov, James L. Gray, Donald G. Dansereau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13007">Light Field Based 6DoF Tracking of Previously Unobserved Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.
<div id='section'>PaperID: <span id='pid'>14, <a href='https://arxiv.org/pdf/2512.07599.pdf' target='_blank'>https://arxiv.org/pdf/2512.07599.pdf</a></span>   <span><a href='https://github.com/AutoLab-SAI-SJTU/AutoSeg3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanshi Wang, Zijian Cai, Jin Gao, Yiwei Zhang, Weiming Hu, Ke Wang, Zhipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07599">Online Segment Any 3D Thing as Instance Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.
<div id='section'>PaperID: <span id='pid'>15, <a href='https://arxiv.org/pdf/2512.07385.pdf' target='_blank'>https://arxiv.org/pdf/2512.07385.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chunhui Zhang, Li Liu, Zhipeng Zhang, Yong Wang, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07385">How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
<div id='section'>PaperID: <span id='pid'>16, <a href='https://arxiv.org/pdf/2512.02392.pdf' target='_blank'>https://arxiv.org/pdf/2512.02392.pdf</a></span>   <span><a href='https://github.com/Spongebobbbbbbbb/FDTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuqing Shao, Yuchen Yang, Rui Yu, Weilong Li, Xu Guo, Huaicheng Yan, Wei Wang, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02392">From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.
<div id='section'>PaperID: <span id='pid'>17, <a href='https://arxiv.org/pdf/2512.01885.pdf' target='_blank'>https://arxiv.org/pdf/2512.01885.pdf</a></span>   <span><a href='https://github.com/bozeklab/TransientTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Florian Bürger, Martim Dias Gomes, Nica Gutu, Adrián E. Granada, Noémie Moreau, Katarzyna Bozek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01885">TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.
<div id='section'>PaperID: <span id='pid'>18, <a href='https://arxiv.org/pdf/2511.22896.pdf' target='_blank'>https://arxiv.org/pdf/2511.22896.pdf</a></span>   <span><a href='https://vranlee.github.io/DM-3-T/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Weiran Li, Yeqiang Liu, Yijie Wei, Mina Han, Qiannan Guo, Zhenbo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22896">DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a fundamental task in computer vision with critical applications in autonomous driving and robotics. Multimodal MOT that integrates visible light and thermal infrared information is particularly essential for robust autonomous driving systems. However, effectively fusing these heterogeneous modalities is challenging. Simple strategies like concatenation or addition often fail to bridge the significant non-linear distribution gap between their feature representations, which can lead to modality conflicts and degrade tracking accuracy. Drawing inspiration from the connection between multimodal MOT and the iterative refinement in diffusion models, this paper proposes DM$^3$T, a novel framework that reformulates multimodal fusion as an iterative feature alignment process to generate accurate and temporally coherent object trajectories. Our approach performs iterative cross-modal harmonization through a proposed Cross-Modal Diffusion Fusion (C-MDF) module. In this process, features from both modalities provide mutual guidance, iteratively projecting them onto a shared, consistent feature manifold. This enables the learning of complementary information and achieves deeper fusion compared to conventional methods. Additionally, we introduce a plug-and-play Diffusion Refiner (DR) to enhance and refine the unified feature representation. To further improve tracking robustness, we design a Hierarchical Tracker that adaptively handles confidence estimation. DM$^3$T unifies object detection, state estimation, and data association into a comprehensive online tracking framework without complex post-processing. Extensive experiments on the VT-MOT benchmark demonstrate that our method achieves 41.7 HOTA, representing a 1.54% relative improvement over existing state-of-the-art methods. The code and models are available at https://vranlee.github.io/DM-3-T/.
<div id='section'>PaperID: <span id='pid'>19, <a href='https://arxiv.org/pdf/2511.18344.pdf' target='_blank'>https://arxiv.org/pdf/2511.18344.pdf</a></span>   <span><a href='https://xuefeng-zhu5.github.io/MM-UAV/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianyang Xu, Jinjie Gu, Xuefeng Zhu, XiaoJun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18344">A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.
<div id='section'>PaperID: <span id='pid'>20, <a href='https://arxiv.org/pdf/2511.17967.pdf' target='_blank'>https://arxiv.org/pdf/2511.17967.pdf</a></span>   <span><a href='https://github.com/IdolLab/CADTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hao Li, Yuhao Wang, Xiantao Hu, Wenning Hao, Pingping Zhang, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17967">CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.
<div id='section'>PaperID: <span id='pid'>21, <a href='https://arxiv.org/pdf/2511.17045.pdf' target='_blank'>https://arxiv.org/pdf/2511.17045.pdf</a></span>   <span><a href='https://github.com/OrcustD/RacketVision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Linfeng Dong, Yuchen Yang, Hao Wu, Wei Wang, Yuenan Hou, Zhihang Zhong, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17045">RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision
<div id='section'>PaperID: <span id='pid'>22, <a href='https://arxiv.org/pdf/2511.16227.pdf' target='_blank'>https://arxiv.org/pdf/2511.16227.pdf</a></span>   <span><a href='https://github.com/xuboyue1999/SwiTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Boyue Xu, Ruichao Hou, Tongwei Ren, Dongming Zhou, Gangshan Wu, Jinde Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16227">SwiTrack: Tri-State Switch for Cross-Modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\% and 4.3\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.
<div id='section'>PaperID: <span id='pid'>23, <a href='https://arxiv.org/pdf/2511.13105.pdf' target='_blank'>https://arxiv.org/pdf/2511.13105.pdf</a></span>   <span><a href='https://github.com/VisualScienceLab-KHU/PlugTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Seungjae Kim, SeungJoon Lee, MyeongAh Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13105">PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.
<div id='section'>PaperID: <span id='pid'>24, <a href='https://arxiv.org/pdf/2511.04128.pdf' target='_blank'>https://arxiv.org/pdf/2511.04128.pdf</a></span>   <span><a href='https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shengyu Tang, Zeyuan Lu, Jiazhi Dong, Changdong Yu, Xiaoyu Wang, Yaohui Lyu, Weihao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04128">DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.
<div id='section'>PaperID: <span id='pid'>25, <a href='https://arxiv.org/pdf/2511.01768.pdf' target='_blank'>https://arxiv.org/pdf/2511.01768.pdf</a></span>   <span><a href='https://github.com/happinesslz/UniLION' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhe Liu, Jinghua Hou, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01768">UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at https://github.com/happinesslz/UniLION
<div id='section'>PaperID: <span id='pid'>26, <a href='https://arxiv.org/pdf/2511.00510.pdf' target='_blank'>https://arxiv.org/pdf/2511.00510.pdf</a></span>   <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Kai Luo, Hao Shi, Kunyu Peng, Fei Teng, Sheng Wu, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00510">OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360° Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360° FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at https://github.com/xifen523/OmniTrack.
<div id='section'>PaperID: <span id='pid'>27, <a href='https://arxiv.org/pdf/2510.24410.pdf' target='_blank'>https://arxiv.org/pdf/2510.24410.pdf</a></span>   <span><a href='https://github.com/SDU-VelKoTek/GenTrack2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24410">A Hybrid Approach for Visual Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a visual multi-object tracking method that jointly employs stochastic and deterministic mechanisms to ensure identifier consistency for unknown and time-varying target numbers under nonlinear dynamics. A stochastic particle filter addresses nonlinear dynamics and non-Gaussian noise, with support from particle swarm optimization (PSO) to guide particles toward state distribution modes and mitigate divergence through proposed fitness measures incorporating motion consistency, appearance similarity, and social-interaction cues with neighboring targets. Deterministic association further enforces identifier consistency via a proposed cost matrix incorporating spatial consistency between particles and current detections, detection confidences, and track penalties. Subsequently, a novel scheme is proposed for the smooth updating of target states while preserving their identities, particularly for weak tracks during interactions with other targets and prolonged occlusions. Moreover, velocity regression over past states provides trend-seed velocities, enhancing particle sampling and state updates. The proposed tracker is designed to operate flexibly for both pre-recorded videos and camera live streams, where future frames are unavailable. Experimental results confirm superior performance compared to state-of-the-art trackers. The source-code reference implementations of both the proposed method and compared-trackers are provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack2
<div id='section'>PaperID: <span id='pid'>28, <a href='https://arxiv.org/pdf/2510.24399.pdf' target='_blank'>https://arxiv.org/pdf/2510.24399.pdf</a></span>   <span><a href='https://github.com/SDU-VelKoTek/GenTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24399">GenTrack: A New Generation of Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel multi-object tracking (MOT) method, dubbed GenTrack, whose main contributions include: a hybrid tracking approach employing both stochastic and deterministic manners to robustly handle unknown and time-varying numbers of targets, particularly in maintaining target identity (ID) consistency and managing nonlinear dynamics, leveraging particle swarm optimization (PSO) with some proposed fitness measures to guide stochastic particles toward their target distribution modes, enabling effective tracking even with weak and noisy object detectors, integration of social interactions among targets to enhance PSO-guided particles as well as improve continuous updates of both strong (matched) and weak (unmatched) tracks, thereby reducing ID switches and track loss, especially during occlusions, a GenTrack-based redefined visual MOT baseline incorporating a comprehensive state and observation model based on space consistency, appearance, detection confidence, track penalties, and social scores for systematic and efficient target updates, and the first-ever publicly available source-code reference implementation with minimal dependencies, featuring three variants, including GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation. Experimental results have shown that GenTrack provides superior performance on standard benchmarks and real-world scenarios compared to state-of-the-art trackers, with integrated implementations of baselines for fair comparison. Potential directions for future work are also discussed. The source-code reference implementations of both the proposed method and compared-trackers are provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack
<div id='section'>PaperID: <span id='pid'>29, <a href='https://arxiv.org/pdf/2510.23368.pdf' target='_blank'>https://arxiv.org/pdf/2510.23368.pdf</a></span>   <span><a href='https://github.com/HengLan/PlanarTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifan Jiao, Xinran Liu, Xiaoqiong Liu, Xiaohui Yuan, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23368">PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planar tracking has drawn increasing interest owing to its key roles in robotics and augmented reality. Despite recent great advancement, further development of planar tracking, particularly in the deep learning era, is largely limited compared to generic tracking due to the lack of large-scale platforms. To mitigate this, we propose PlanarTrack, a large-scale high-quality and challenging benchmark for planar tracking. Specifically, PlanarTrack consists of 1,150 sequences with over 733K frames, including 1,000 short-term and 150 new long-term videos, which enables comprehensive evaluation of short- and long-term tracking performance. All videos in PlanarTrack are recorded in unconstrained conditions from the wild, which makes PlanarTrack challenging but more realistic for real-world applications. To ensure high-quality annotations, each video frame is manually annotated by four corner points with multi-round meticulous inspection and refinement. To enhance target diversity of PlanarTrack, we only capture a unique target in one sequence, which is different from existing benchmarks. To our best knowledge, PlanarTrack is by far the largest and most diverse and challenging dataset dedicated to planar tracking. To understand performance of existing methods on PlanarTrack and to provide a comparison for future research, we evaluate 10 representative planar trackers with extensive comparison and in-depth analysis. Our evaluation reveals that, unsurprisingly, the top planar trackers heavily degrade on the challenging PlanarTrack, which indicates more efforts are required for improving planar tracking. Our data and results will be released at https://github.com/HengLan/PlanarTrack
<div id='section'>PaperID: <span id='pid'>30, <a href='https://arxiv.org/pdf/2510.20794.pdf' target='_blank'>https://arxiv.org/pdf/2510.20794.pdf</a></span>   <span><a href='https://github.com/radar-lab/Radar_Camera_MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lei Cheng, Siyang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20794">Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a Multi-Object Tracking (MOT) framework that fuses radar and camera data to enhance tracking efficiency while minimizing manual interventions. Contrary to many studies that underutilize radar and assign it a supplementary role--despite its capability to provide accurate range/depth information of targets in a world 3D coordinate system--our approach positions radar in a crucial role. Meanwhile, this paper utilizes common features to enable online calibration to autonomously associate detections from radar and camera. The main contributions of this work include: (1) the development of a radar-camera fusion MOT framework that exploits online radar-camera calibration to simplify the integration of detection results from these two sensors, (2) the utilization of common features between radar and camera data to accurately derive real-world positions of detected objects, and (3) the adoption of feature matching and category-consistency checking to surpass the limitations of mere position matching in enhancing sensor association accuracy. To the best of our knowledge, we are the first to investigate the integration of radar-camera common features and their use in online calibration for achieving MOT. The efficacy of our framework is demonstrated by its ability to streamline the radar-camera mapping process and improve tracking precision, as evidenced by real-world experiments conducted in both controlled environments and actual traffic scenarios. Code is available at https://github.com/radar-lab/Radar_Camera_MOT
<div id='section'>PaperID: <span id='pid'>31, <a href='https://arxiv.org/pdf/2510.12565.pdf' target='_blank'>https://arxiv.org/pdf/2510.12565.pdf</a></span>   <span><a href='https://github.com/Annzstbl/MMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianhao Li, Tingfa Xu, Ying Wang, Haolin Qin, Xu Lin, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12565">MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone-based multi-object tracking is essential yet highly challenging due to small targets, severe occlusions, and cluttered backgrounds. Existing RGB-based tracking algorithms heavily depend on spatial appearance cues such as color and texture, which often degrade in aerial views, compromising reliability. Multispectral imagery, capturing pixel-level spectral reflectance, provides crucial cues that enhance object discriminability under degraded spatial conditions. However, the lack of dedicated multispectral UAV datasets has hindered progress in this domain. To bridge this gap, we introduce MMOT, the first challenging benchmark for drone-based multispectral multi-object tracking. It features three key characteristics: (i) Large Scale - 125 video sequences with over 488.8K annotations across eight categories; (ii) Comprehensive Challenges - covering diverse conditions such as extreme small targets, high-density scenarios, severe occlusions, and complex motion; and (iii) Precise Oriented Annotations - enabling accurate localization and reduced ambiguity under aerial perspectives. To better extract spectral features and leverage oriented annotations, we further present a multispectral and orientation-aware MOT scheme adapting existing methods, featuring: (i) a lightweight Spectral 3D-Stem integrating spectral features while preserving compatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for precise state estimation; and (iii) an end-to-end orientation-adaptive transformer. Extensive experiments across representative trackers consistently show that multispectral input markedly improves tracking performance over RGB baselines, particularly for small and densely packed objects. We believe our work will advance drone-based multispectral multi-object tracking research. Our MMOT, code, and benchmarks are publicly available at https://github.com/Annzstbl/MMOT.
<div id='section'>PaperID: <span id='pid'>32, <a href='https://arxiv.org/pdf/2510.07134.pdf' target='_blank'>https://arxiv.org/pdf/2510.07134.pdf</a></span>   <span><a href='https://pku-epic.github.io/TrackVLA-plus-plus-Web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07134">TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.
<div id='section'>PaperID: <span id='pid'>33, <a href='https://arxiv.org/pdf/2510.06619.pdf' target='_blank'>https://arxiv.org/pdf/2510.06619.pdf</a></span>   <span><a href='https://github.com/Fengtao191/MSITrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tao Feng, Tingfa Xu, Haolin Qin, Tianhao Li, Shuaihao Han, Xuyang Zou, Zhan Lv, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06619">MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking in real-world scenarios presents numerous challenges including occlusion, interference from similar objects and complex backgrounds-all of which limit the effectiveness of RGB-based trackers. Multispectral imagery, which captures pixel-level spectral reflectance, enhances target discriminability. However, the availability of multispectral tracking datasets remains limited. To bridge this gap, we introduce MSITrack, the largest and most diverse multispectral single object tracking dataset to date. MSITrack offers the following key features: (i) More Challenging Attributes-including interference from similar objects and similarity in color and texture between targets and backgrounds in natural scenarios, along with a wide range of real-world tracking challenges; (ii) Richer and More Natural Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack far exceeds the scope of existing benchmarks. Many of these scenes and categories are introduced to the multispectral tracking domain for the first time; (iii) Larger Scale-300 videos comprising over 129k frames of multispectral imagery. To ensure annotation precision, each frame has undergone meticulous processing, manual labeling and multi-stage verification. Extensive evaluations using representative trackers demonstrate that the multispectral data in MSITrack significantly improves performance over RGB-only baselines, highlighting its potential to drive future advancements in the field. The MSITrack dataset is publicly available at: https://github.com/Fengtao191/MSITrack.
<div id='section'>PaperID: <span id='pid'>34, <a href='https://arxiv.org/pdf/2509.17323.pdf' target='_blank'>https://arxiv.org/pdf/2509.17323.pdf</a></span>   <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17323">DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Multi-Object Tracking (MOT) is a crucial component of robotic perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D cues, such as bounding boxes and motion modeling, which struggle under occlusions and close-proximity interactions. Trackers relying on these 2D cues are particularly unreliable in robotic environments, where dense targets and frequent occlusions are common. While depth information has the potential to alleviate these issues, most existing MOT datasets lack depth annotations, leading to its underexploited role in the domain. To unveil the potential of depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based detector enhanced with instance-level depth information. Specifically, we propose two key innovations: (i) foundation model-based instance-level soft depth label supervision, which refines depth prediction, and (ii) the distillation of dense depth maps to maintain global depth consistency. These strategies enable DepTR-MOT to output instance-level depth during inference, without requiring foundation models and without additional computational cost. By incorporating depth cues, our method enhances the robustness of the TBD paradigm, effectively resolving occlusion and close-proximity challenges. Experiments on both the QuadTrack and DanceTrack datasets demonstrate the effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47, respectively. In particular, results on QuadTrack, a robotic platform MOT dataset, highlight the advantages of our method in handling occlusion and close-proximity challenges in robotic tracking. The source code will be made publicly available at https://github.com/warriordby/DepTR-MOT.
<div id='section'>PaperID: <span id='pid'>35, <a href='https://arxiv.org/pdf/2509.16527.pdf' target='_blank'>https://arxiv.org/pdf/2509.16527.pdf</a></span>   <span><a href='https://george-zhuang.github.io/lbm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Guangze Zheng, Shijie Lin, Haobo Zuo, Si Si, Ming-Shan Wang, Changhong Fu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16527">Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes the Lattice Boltzmann Model (LBM) to learn real-world pixel dynamicity for visual tracking. LBM decomposes visual representations into dynamic pixel lattices and solves pixel motion states through collision-streaming processes. Specifically, the high-dimensional distribution of the target pixels is acquired through a multilayer predict-update network to estimate the pixel positions and visibility. The predict stage formulates lattice collisions among the spatial neighborhood of target pixels and develops lattice streaming within the temporal visual context. The update stage rectifies the pixel distributions with online visual representations. Compared with existing methods, LBM demonstrates practical applicability in an online and real-time manner, which can efficiently adapt to real-world visual tracking tasks. Comprehensive evaluations of real-world point tracking benchmarks such as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B further demonstrates LBM's real-world practicality.
<div id='section'>PaperID: <span id='pid'>36, <a href='https://arxiv.org/pdf/2509.13864.pdf' target='_blank'>https://arxiv.org/pdf/2509.13864.pdf</a></span>   <span><a href='https://github.com/jovanavidenovic/DAM4SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jovana Videnovic, Matej Kristan, Alan Lukezic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13864">Distractor-Aware Memory-Based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent emergence of memory-based video segmentation methods such as SAM2 has led to models with excellent performance in segmentation tasks, achieving leading results on numerous benchmarks. However, these modes are not fully adjusted for visual object tracking, where distractors (i.e., objects visually similar to the target) pose a key challenge. In this paper we propose a distractor-aware drop-in memory module and introspection-based management method for SAM2, leading to DAM4SAM. Our design effectively reduces the tracking drift toward distractors and improves redetection capability after object occlusion. To facilitate the analysis of tracking in the presence of distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results on ten. Furthermore, integrating the proposed distractor-aware memory into a real-time tracker EfficientTAM leads to 11% improvement and matches tracking quality of the non-real-time SAM2.1-L on multiple tracking and segmentation benchmarks, while integration with edge-based tracker EdgeTAM delivers 4% performance boost, demonstrating a very good generalization across architectures.
<div id='section'>PaperID: <span id='pid'>37, <a href='https://arxiv.org/pdf/2509.12913.pdf' target='_blank'>https://arxiv.org/pdf/2509.12913.pdf</a></span>   <span><a href='https://github.com/to/be/released' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hojat Ardi, Amir Jahanshahi, Ali Diba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12913">T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial object tracking remains a challenging task due to scale variations, dynamic backgrounds, clutter, and frequent occlusions. While most existing trackers emphasize spatial cues, they often overlook temporal dependencies, resulting in limited robustness in long-term tracking and under occlusion. Furthermore, correlation-based Siamese trackers are inherently constrained by the linear nature of correlation operations, making them ineffective against complex, non-linear appearance changes. To address these limitations, we introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends the SiamTPN architecture with explicit temporal modeling. Our approach incorporates temporal feature fusion and attention-based interactions, strengthening temporal consistency and enabling richer feature representations. These enhancements yield significant improvements over the baseline and achieve performance competitive with state-of-the-art trackers. Crucially, despite the added temporal modules, T-SiamTPN preserves computational efficiency. Deployed on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1 FPS, demonstrating its suitability for real-world embedded applications without notable runtime overhead. Experimental results highlight substantial gains: compared to the baseline, T-SiamTPN improves success rate by 13.7% and precision by 14.7%. These findings underscore the importance of temporal modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and efficient solution for aerial object tracking. Code is available at: https://github.com/to/be/released
<div id='section'>PaperID: <span id='pid'>38, <a href='https://arxiv.org/pdf/2509.11772.pdf' target='_blank'>https://arxiv.org/pdf/2509.11772.pdf</a></span>   <span><a href='https://github.com/hcmr-lab/Seg2Track-SAM2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Diogo MendonÃ§a, Tiago Barros, Cristiano Premebida, Urbano J. Nunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11772">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to operate reliably in dynamic environments. MOT ensures consistent object identity assignment and precise spatial delineation. Recent advances in foundation models, such as SAM2, have demonstrated strong zero-shot generalization for video segmentation, but their direct application to MOTS (MOT+Segmentation) remains limited by insufficient identity management and memory efficiency. This work introduces Seg2Track-SAM2, a framework that integrates pre-trained object detectors with SAM2 and a novel Seg2Track module to address track initialization, track management, and reinforcement. The proposed approach requires no fine-tuning and remains detector-agnostic. Experimental results on KITTI MOT and KITTI MOTS benchmarks show that Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth overall in both car and pedestrian classes on KITTI MOTS, while establishing a new benchmark in association accuracy (AssA). Furthermore, a sliding-window memory strategy reduces memory usage by up to 75% with negligible performance degradation, supporting deployment under resource constraints. These results confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot tracking, enhanced identity preservation, and efficient memory utilization. The code is available at https://github.com/hcmr-lab/Seg2Track-SAM2
<div id='section'>PaperID: <span id='pid'>39, <a href='https://arxiv.org/pdf/2509.11323.pdf' target='_blank'>https://arxiv.org/pdf/2509.11323.pdf</a></span>   <span><a href='https://github.com/SongJgit/TBDTracker' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SongJgit/filternet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jian Song, Wei Mei, Yunfeng Xu, Qiang Fu, Renke Kou, Lina Bu, Yucheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11323">Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion estimation is a crucial component in multi-object tracking (MOT). It predicts the trajectory of objects by analyzing the changes in their positions in consecutive frames of images, reducing tracking failures and identity switches. The Kalman filter (KF) based on the linear constant-velocity model is one of the most commonly used methods in MOT. However, it may yield unsatisfactory results when KF's parameters are mismatched and objects move in non-stationary. In this work, we utilize the learning-aided filter to handle the motion estimation of MOT. In particular, we propose a novel method named Semantic-Independent KalmanNet (SIKNet), which encodes the state vector (the input feature) using a Semantic-Independent Encoder (SIE) by two steps. First, the SIE uses a 1D convolution with a kernel size of 1, which convolves along the dimension of homogeneous-semantic elements across different state vectors to encode independent semantic information. Then it employs a fully-connected layer and a nonlinear activation layer to encode nonlinear and cross-dependency information between heterogeneous-semantic elements. To independently evaluate the performance of the motion estimation module in MOT, we constructed a large-scale semi-simulated dataset from several open-source MOT datasets. Experimental results demonstrate that the proposed SIKNet outperforms the traditional KF and achieves superior robustness and accuracy than existing learning-aided filters. The code is available at (https://github.com/SongJgit/filternet and https://github.com/SongJgit/TBDTracker).
<div id='section'>PaperID: <span id='pid'>40, <a href='https://arxiv.org/pdf/2509.09977.pdf' target='_blank'>https://arxiv.org/pdf/2509.09977.pdf</a></span>   <span><a href='https://github.com/lsying009/ISTASTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Siying Liu, Zikai Wang, Hanle Zheng, Yifan Hu, Xilin Wang, Qingkai Yang, Jibin Wu, Hao Guo, Lei Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09977">ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-Event tracking has become a promising trend in visual object tracking to leverage the complementary strengths of both RGB images and dynamic spike events for improved performance. However, existing artificial neural networks (ANNs) struggle to fully exploit the sparse and asynchronous nature of event streams. Recent efforts toward hybrid architectures combining ANNs and spiking neural networks (SNNs) have emerged as a promising solution in RGB-Event perception, yet effectively fusing features across heterogeneous paradigms remains a challenge. In this work, we propose ISTASTrack, the first transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model employs a vision transformer to extract spatial context from RGB inputs and a spiking transformer to capture spatio-temporal dynamics from event streams. To bridge the modality and paradigm gap between ANN and SNN features, we systematically design a model-based ISTA adapter for bidirectional feature interaction between the two branches, derived from sparse representation theory by unfolding the iterative shrinkage thresholding algorithm. Additionally, we incorporate a temporal downsampling attention module within the adapter to align multi-step SNN features with single-step ANN features in the latent space, improving temporal fusion. Experimental results on RGB-Event tracking benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that ISTASTrack achieves state-of-the-art performance while maintaining high energy efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking. The code is publicly available at https://github.com/lsying009/ISTASTrack.git.
<div id='section'>PaperID: <span id='pid'>41, <a href='https://arxiv.org/pdf/2509.09962.pdf' target='_blank'>https://arxiv.org/pdf/2509.09962.pdf</a></span>   <span><a href='https://github.com/ngobibibnbe/uncertain-identity-aware-tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Anne Marthe Sophie Ngo Bibinbe, Chiron Bang, Patrick Gagnon, Jamie Ahloy-Dallaire, Eric R. Paquet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09962">An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The need for long-term multi-object tracking (MOT) is growing due to the demand for analyzing individual behaviors in videos that span several minutes. Unfortunately, due to identity switches between objects, the tracking performance of existing MOT approaches decreases over time, making them difficult to apply for long-term tracking. However, in many real-world applications, such as in the livestock sector, it is possible to obtain sporadic identifications for some of the animals from sources like feeders. To address the challenges of long-term MOT, we propose a new framework that combines both uncertain identities and tracking using a Hidden Markov Model (HMM) formulation. In addition to providing real-world identities to animals, our HMM framework improves the F1 score of ByteTrack, a leading MOT approach even with re-identification, on a 10 minute pig tracking dataset with 21 identifications at the pen's feeding station. We also show that our approach is robust to the uncertainty of identifications, with performance increasing as identities are provided more frequently. The improved performance of our HMM framework was also validated on the MOT17 and MOT20 benchmark datasets using both ByteTrack and FairMOT. The code for this new HMM framework and the new 10-minute pig tracking video dataset are available at: https://github.com/ngobibibnbe/uncertain-identity-aware-tracking
<div id='section'>PaperID: <span id='pid'>42, <a href='https://arxiv.org/pdf/2509.08265.pdf' target='_blank'>https://arxiv.org/pdf/2509.08265.pdf</a></span>   <span><a href='https://github.com/lgao001/HyMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Long Gao, Yunhe Zhang, Yan Jiang, Weiying Xie, Yunsong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08265">Hyperspectral Mamba for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking holds great promise due to the rich spectral information and fine-grained material distinctions in hyperspectral images, which are beneficial in challenging scenarios. While existing hyperspectral trackers have made progress by either transforming hyperspectral data into false-color images or incorporating modality fusion strategies, they often fail to capture the intrinsic spectral information, temporal dependencies, and cross-depth interactions. To address these limitations, a new hyperspectral object tracking network equipped with Mamba (HyMamba), is proposed. It unifies spectral, cross-depth, and temporal modeling through state space modules (SSMs). The core of HyMamba lies in the Spectral State Integration (SSI) module, which enables progressive refinement and propagation of spectral features with cross-depth and temporal spectral information. Embedded within each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial and spectral information synchronously via three directional scanning SSMs. Based on SSI and HSM, HyMamba constructs joint features from false-color and hyperspectral inputs, and enhances them through interaction with original spectral features extracted from raw hyperspectral images. Extensive experiments conducted on seven benchmark datasets demonstrate that HyMamba achieves state-of-the-art performance. For instance, it achieves 73.0\% of the AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will be released at https://github.com/lgao001/HyMamba.
<div id='section'>PaperID: <span id='pid'>43, <a href='https://arxiv.org/pdf/2508.11531.pdf' target='_blank'>https://arxiv.org/pdf/2508.11531.pdf</a></span>   <span><a href='https://github.com/wsumel/MST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shilei Wang, Gong Cheng, Pujian Lai, Dong Gao, Junwei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11531">Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient trackers achieve faster runtime by reducing computational complexity and model parameters. However, this efficiency often compromises the expense of weakened feature representation capacity, thus limiting their ability to accurately capture target states using single-layer features. To overcome this limitation, we propose Multi-State Tracker (MST), which utilizes highly lightweight state-specific enhancement (SSE) to perform specialized enhancement on multi-state features produced by multi-state generation (MSG) and aggregates them in an interactive and adaptive manner using cross-state interaction (CSI). This design greatly enhances feature representation while incurring minimal computational overhead, leading to improved tracking robustness in complex environments. Specifically, the MSG generates multiple state representations at multiple stages during feature extraction, while SSE refines them to highlight target-specific features. The CSI module facilitates information exchange between these states and ensures the integration of complementary features. Notably, the introduced SSE and CSI modules adopt a highly lightweight hidden state adaptation-based state space duality (HSA-SSD) design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters. Experimental results demonstrate that MST outperforms all previous efficient trackers across multiple datasets, significantly improving tracking accuracy and robustness. In particular, it shows excellent runtime performance, with an AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.
<div id='section'>PaperID: <span id='pid'>44, <a href='https://arxiv.org/pdf/2508.10655.pdf' target='_blank'>https://arxiv.org/pdf/2508.10655.pdf</a></span>   <span><a href='https://github.com/Zhangyong-Tang/UniBench300' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Chunyang Cheng, Tao Zhou, Xiaojun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10655">Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \textit{inconsistency} between training and testing, thus leading to performance \textit{degradation}. To address these issues, this work advances in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\%. \ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.
<div id='section'>PaperID: <span id='pid'>45, <a href='https://arxiv.org/pdf/2508.10567.pdf' target='_blank'>https://arxiv.org/pdf/2508.10567.pdf</a></span>   <span><a href='https://phi-wol.github.io/sparcad/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Philipp Wolters, Johannes Gilg, Torben Teepe, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10567">SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/
<div id='section'>PaperID: <span id='pid'>46, <a href='https://arxiv.org/pdf/2508.10432.pdf' target='_blank'>https://arxiv.org/pdf/2508.10432.pdf</a></span>   <span><a href='https://github.com/01upup10/CRISP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Baichen Liu, Qi Lyu, Xudong Wang, Jiahua Dong, Lianqing Liu, Zhi Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10432">CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual video instance segmentation demands both the plasticity to absorb new object categories and the stability to retain previously learned ones, all while preserving temporal consistency across frames. In this work, we introduce Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier attempt tailored to address the instance-wise, category-wise, and task-wise confusion in continual video instance segmentation. For instance-wise learning, we model instance tracking and construct instance correlation loss, which emphasizes the correlation with the prior query space while strengthening the specificity of the current task query. For category-wise learning, we build an adaptive residual semantic prompt (ARSP) learning framework, which constructs a learnable semantic residual prompt pool generated by category text and uses an adjustive query-prompt matching mechanism to build a mapping relationship between the query of the current task and the semantic residual prompt. Meanwhile, a semantic consistency loss based on the contrastive learning is introduced to maintain semantic coherence between object queries and residual prompts during incremental training. For task-wise learning, to ensure the correlation at the inter-task level within the query space, we introduce a concise yet powerful initialization strategy for incremental prompts. Extensive experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that CRISP significantly outperforms existing continual segmentation methods in the long-term continual video instance segmentation task, avoiding catastrophic forgetting and effectively improving segmentation and classification performance. The code is available at https://github.com/01upup10/CRISP.
<div id='section'>PaperID: <span id='pid'>47, <a href='https://arxiv.org/pdf/2508.07250.pdf' target='_blank'>https://arxiv.org/pdf/2508.07250.pdf</a></span>   <span><a href='https://github.com/bearshng/suit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Fengchao Xiong, Zhenxing Wu, Sen Jia, Yuntao Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07250">SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal structure, offer distinct advantages in challenging tracking scenarios such as cluttered backgrounds and small objects. However, existing methods primarily focus on spatial interactions between the template and search regions, often overlooking spectral interactions, leading to suboptimal performance. To address this issue, this paper investigates spectral interactions from both the architectural and training perspectives. At the architectural level, we first establish band-wise long-range spatial relationships between the template and search regions using Transformers. We then model spectral interactions using the inclusion-exclusion principle from set theory, treating them as the union of spatial interactions across all bands. This enables the effective integration of both shared and band-specific spatial cues. At the training level, we introduce a spectral loss to enforce material distribution alignment between the template and predicted regions, enhancing robustness to shape deformation and appearance variations. Extensive experiments demonstrate that our tracker achieves state-of-the-art tracking performance. The source code, trained models and results will be publicly available via https://github.com/bearshng/suit to support reproducibility.
<div id='section'>PaperID: <span id='pid'>48, <a href='https://arxiv.org/pdf/2508.05630.pdf' target='_blank'>https://arxiv.org/pdf/2508.05630.pdf</a></span>   <span><a href='https://github.com/henghuiding/MOSE-api' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip H. S. Torr, Song Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05630">MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To bridge this gap, the coMplex video Object SEgmentation (MOSEv1) dataset was introduced to facilitate VOS research in complex scenes. Building on the foundations and insights of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces much greater scene complexity, including {more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), and scenarios requiring external knowledge.} We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops on MOSEv2. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and observe similar declines, demonstrating that MOSEv2 poses challenges across tasks. These results highlight that despite strong performance on existing datasets, current VOS methods still fall short under real-world complexities. Based on our analysis of the observed challenges, we further propose several practical tricks that enhance model performance. MOSEv2 is publicly available at https://MOSE.video.
<div id='section'>PaperID: <span id='pid'>49, <a href='https://arxiv.org/pdf/2508.05221.pdf' target='_blank'>https://arxiv.org/pdf/2508.05221.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Open_VLTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiao Wang, Liye Jin, Xufeng Lou, Shiao Wang, Lan Chen, Bo Jiang, Zhipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05221">ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the model's reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on https://github.com/Event-AHU/Open_VLTrack
<div id='section'>PaperID: <span id='pid'>50, <a href='https://arxiv.org/pdf/2508.02512.pdf' target='_blank'>https://arxiv.org/pdf/2508.02512.pdf</a></span>   <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Sheng Wu, Fei Teng, Hao Shi, Qi Jiang, Kai Luo, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02512">QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data-caused by inherent kinematic constraints and complex sensor calibration challenges-fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer-the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available at https://github.com/losehu/QuaDreamer.
<div id='section'>PaperID: <span id='pid'>51, <a href='https://arxiv.org/pdf/2508.01802.pdf' target='_blank'>https://arxiv.org/pdf/2508.01802.pdf</a></span>   <span><a href='https://github.com/AtomScott/SoccerTrack-v2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Atom Scott, Ikuma Uchida, Kento Kuroda, Yufi Kim, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01802">SoccerTrack v2: A Full-Pitch Multi-View Soccer Dataset for Game State Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SoccerTrack v2 is a new public dataset for advancing multi-object tracking (MOT), game state reconstruction (GSR), and ball action spotting (BAS) in soccer analytics. Unlike prior datasets that use broadcast views or limited scenarios, SoccerTrack v2 provides 10 full-length, panoramic 4K recordings of university-level matches, captured with BePro cameras for complete player visibility. Each video is annotated with GSR labels (2D pitch coordinates, jersey-based player IDs, roles, teams) and BAS labels for 12 action classes (e.g., Pass, Drive, Shot). This technical report outlines the datasets structure, collection pipeline, and annotation process. SoccerTrack v2 is designed to advance research in computer vision and soccer analytics, enabling new benchmarks and practical applications in tactical analysis and automated tools.
<div id='section'>PaperID: <span id='pid'>52, <a href='https://arxiv.org/pdf/2507.21732.pdf' target='_blank'>https://arxiv.org/pdf/2507.21732.pdf</a></span>   <span><a href='https://github.com/Sam1224/SAMITE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qianxiong Xu, Lanyun Zhu, Chenxi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21732">SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is widely used in applications like autonomous driving to continuously track targets in videos. Existing methods can be roughly categorized into template matching and autoregressive methods, where the former usually neglects the temporal dependencies across frames and the latter tends to get biased towards the object categories during training, showing weak generalizability to unseen classes. To address these issues, some methods propose to adapt the video foundation model SAM2 for VOT, where the tracking results of each frame would be encoded as memory for conditioning the rest of frames in an autoregressive manner. Nevertheless, existing methods fail to overcome the challenges of object occlusions and distractions, and do not have any measures to intercept the propagation of tracking errors. To tackle them, we present a SAMITE model, built upon SAM2 with additional modules, including: (1) Prototypical Memory Bank: We propose to quantify the feature-wise and position-wise correctness of each frame's tracking results, and select the best frames to condition subsequent frames. As the features of occluded and distracting objects are feature-wise and position-wise inaccurate, their scores would naturally be lower and thus can be filtered to intercept error propagation; (2) Positional Prompt Generator: To further reduce the impacts of distractors, we propose to generate positional mask prompts to provide explicit positional clues for the target, leading to more accurate tracking. Extensive experiments have been conducted on six benchmarks, showing the superiority of SAMITE. The code is available at https://github.com/Sam1224/SAMITE.
<div id='section'>PaperID: <span id='pid'>53, <a href='https://arxiv.org/pdf/2507.21606.pdf' target='_blank'>https://arxiv.org/pdf/2507.21606.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/SSTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yaozong Zheng, Bineng Zhong, Qihua Liang, Ning Li, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21606">Decoupled Spatio-Temporal Consistency Learning for Self-Supervised Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of visual tracking has been largely driven by datasets with manual box annotations. However, these box annotations require tremendous human effort, limiting the scale and diversity of existing tracking datasets. In this work, we present a novel Self-Supervised Tracking framework named \textbf{\tracker}, designed to eliminate the need of box annotations. Specifically, a decoupled spatio-temporal consistency training framework is proposed to learn rich target information across timestamps through global spatial localization and local temporal association. This allows for the simulation of appearance and motion variations of instances in real-world scenarios. Furthermore, an instance contrastive loss is designed to learn instance-level correspondences from a multi-view perspective, offering robust instance supervision without additional labels. This new design paradigm enables {\tracker} to effectively learn generic tracking representations in a self-supervised manner, while reducing reliance on extensive box annotations. Extensive experiments on nine benchmark datasets demonstrate that {\tracker} surpasses \textit{SOTA} self-supervised tracking methods, achieving an improvement of more than 25.3\%, 20.4\%, and 14.8\% in AUC (AO) score on the GOT10K, LaSOT, TrackingNet datasets, respectively. Code: https://github.com/GXNU-ZhongLab/SSTrack.
<div id='section'>PaperID: <span id='pid'>54, <a href='https://arxiv.org/pdf/2507.19754.pdf' target='_blank'>https://arxiv.org/pdf/2507.19754.pdf</a></span>   <span><a href='https://github.com/Seung-Hun-Lee/LOMM' target='_blank'>  GitHub</a></span> <span><a href='https://seung-hun-lee.github.io/projects/LOMM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Seunghun Lee, Jiwan Seo, Minwoo Choi, Kiljoon Han, Jaehoon Jeong, Zane Durante, Ehsan Adeli, Sang Hyun Park, Sunghoon Im
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19754">Latest Object Memory Management for Temporally Consistent Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present Latest Object Memory Management (LOMM) for temporally consistent video instance segmentation that significantly improves long-term instance tracking. At the core of our method is Latest Object Memory (LOM), which robustly tracks and continuously updates the latest states of objects by explicitly modeling their presence in each frame. This enables consistent tracking and accurate identity management across frames, enhancing both performance and reliability through the VIS process. Moreover, we introduce Decoupled Object Association (DOA), a strategy that separately handles newly appearing and already existing objects. By leveraging our memory system, DOA accurately assigns object indices, improving matching accuracy and ensuring stable identity consistency, even in dynamic scenes where objects frequently appear and disappear. Extensive experiments and ablation studies demonstrate the superiority of our method over traditional approaches, setting a new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of 54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos. Project page: https://seung-hun-lee.github.io/projects/LOMM/
<div id='section'>PaperID: <span id='pid'>55, <a href='https://arxiv.org/pdf/2507.19239.pdf' target='_blank'>https://arxiv.org/pdf/2507.19239.pdf</a></span>   <span><a href='https://github.com/zhongjiaru/CoopTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiaru Zhong, Jiahao Wang, Jiahui Xu, Xiaofan Li, Zaiqing Nie, Haibao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19239">CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\% mAP and 32.8\% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.
<div id='section'>PaperID: <span id='pid'>56, <a href='https://arxiv.org/pdf/2507.16191.pdf' target='_blank'>https://arxiv.org/pdf/2507.16191.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/RSTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Fansheng Zeng, Bineng Zhong, Haiying Xia, Yufei Tan, Xiantao Hu, Liangtao Shi, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16191">Explicit Context Reasoning with Supervision for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contextual reasoning with constraints is crucial for enhancing temporal consistency in cross-frame modeling for visual tracking. However, mainstream tracking algorithms typically associate context by merely stacking historical information without explicitly supervising the association process, making it difficult to effectively model the target's evolving dynamics. To alleviate this problem, we propose RSTrack, which explicitly models and supervises context reasoning via three core mechanisms. \textit{1) Context Reasoning Mechanism}: Constructs a target state reasoning pipeline, converting unconstrained contextual associations into a temporal reasoning process that predicts the current representation based on historical target states, thereby enhancing temporal consistency. \textit{2) Forward Supervision Strategy}: Utilizes true target features as anchors to constrain the reasoning pipeline, guiding the predicted output toward the true target distribution and suppressing drift in the context reasoning process. \textit{3) Efficient State Modeling}: Employs a compression-reconstruction mechanism to extract the core features of the target, removing redundant information across frames and preventing ineffective contextual associations. These three mechanisms collaborate to effectively alleviate the issue of contextual association divergence in traditional temporal modeling. Experimental results show that RSTrack achieves state-of-the-art performance on multiple benchmark datasets while maintaining real-time running speeds. Our code is available at https://github.com/GXNU-ZhongLab/RSTrack.
<div id='section'>PaperID: <span id='pid'>57, <a href='https://arxiv.org/pdf/2507.15292.pdf' target='_blank'>https://arxiv.org/pdf/2507.15292.pdf</a></span>   <span><a href='https://szupc.github.io/EndoControlMag/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>An Wang, Rulin Zhou, Mengya Xu, Yiru Ye, Longfei Gou, Yiting Chang, Hao Chen, Chwee Ming Lim, Jiankun Wang, Hongliang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15292">EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visualizing subtle vascular motions in endoscopic surgery is crucial for surgical precision and decision-making, yet remains challenging due to the complex and dynamic nature of surgical scenes. To address this, we introduce EndoControlMag, a training-free, Lagrangian-based framework with mask-conditioned vascular motion magnification tailored to endoscopic environments. Our approach features two key modules: a Periodic Reference Resetting (PRR) scheme that divides videos into short overlapping clips with dynamically updated reference frames to prevent error accumulation while maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification (HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores using a pretrained visual tracking model to maintain accurate localization despite occlusions and view changes. It then applies one of two adaptive softening strategies to surrounding tissues: motion-based softening that modulates magnification strength proportional to observed tissue displacement, or distance-based exponential decay that simulates biomechanical force attenuation. This dual-mode approach accommodates diverse surgical scenarios-motion-based softening excels with complex tissue deformations while distance-based softening provides stability during unreliable optical flow conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four different surgery types and various challenging scenarios, including occlusions, instrument disturbance, view changes, and vessel deformations. Quantitative metrics, visual assessments, and expert surgeon evaluations demonstrate that EndoControlMag significantly outperforms existing methods in both magnification accuracy and visual quality while maintaining robustness across challenging surgical conditions. The code, dataset, and video results are available at https://szupc.github.io/EndoControlMag/.
<div id='section'>PaperID: <span id='pid'>58, <a href='https://arxiv.org/pdf/2507.14613.pdf' target='_blank'>https://arxiv.org/pdf/2507.14613.pdf</a></span>   <span><a href='https://github.com/apple1986/DD-SAM2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Guoping Xu, Christopher Kabat, You Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14613">Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in medical image segmentation have been driven by deep learning; however, most existing methods remain limited by modality-specific designs and exhibit poor adaptability to dynamic medical imaging scenarios. The Segment Anything Model 2 (SAM2) and its related variants, which introduce a streaming memory mechanism for real-time video segmentation, present new opportunities for prompt-based, generalizable solutions. Nevertheless, adapting these models to medical video scenarios typically requires large-scale datasets for retraining or transfer learning, leading to high computational costs and the risk of catastrophic forgetting. To address these challenges, we propose DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction with minimal parameter overhead. This design enables effective fine-tuning of SAM2 on medical videos with limited training data. Unlike existing adapter-based methods focused solely on static images, DD-SAM2 fully exploits SAM2's streaming memory for medical video object tracking and segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation) and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best of our knowledge, this work provides an initial attempt at systematically exploring adapter-based SAM2 fine-tuning for medical video segmentation and tracking. Code, datasets, and models will be publicly available at https://github.com/apple1986/DD-SAM2.
<div id='section'>PaperID: <span id='pid'>59, <a href='https://arxiv.org/pdf/2507.12087.pdf' target='_blank'>https://arxiv.org/pdf/2507.12087.pdf</a></span>   <span><a href='https://github.com/Salvatore-Love/YOLOv8-SMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiang Yu, Xinyao Liu, Guang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12087">YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned Aerial Vehicle (UAV) perspective is a highly challenging computer vision task. The difficulty stems from three main sources: the extreme scarcity of target appearance features, the complex motion entanglement caused by the combined dynamics of the camera and the targets themselves, and the frequent occlusions and identity ambiguity arising from dense flocking behavior. This paper details our championship-winning solution in the MVA 2025 "Finding Birds" Small Multi-Object Tracking Challenge (SMOT4SB), which adopts the tracking-by-detection paradigm with targeted innovations at both the detection and association levels. On the detection side, we propose a systematic training enhancement framework named \textbf{SliceTrain}. This framework, through the synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic augmentation, effectively addresses the problem of insufficient learning for small objects in high-resolution image training. On the tracking side, we designed a robust tracker that is completely independent of appearance information. By integrating a \textbf{motion direction maintenance (EMA)} mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding box expansion and distance penalty} into the OC-SORT framework, our tracker can stably handle irregular motion and maintain target identities. Our method achieves state-of-the-art performance on the SMOT4SB public test set, reaching an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness and advancement of our framework in solving complex real-world SMOT problems. The source code will be made available at https://github.com/Salvatore-Love/YOLOv8-SMOT.
<div id='section'>PaperID: <span id='pid'>60, <a href='https://arxiv.org/pdf/2507.07603.pdf' target='_blank'>https://arxiv.org/pdf/2507.07603.pdf</a></span>   <span><a href='https://github.com/LouisFinner/HiM2SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruixiang Chen, Guolei Sun, Yawei Li, Jie Qin, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07603">HiM2SAM: Enhancing SAM2 with Hierarchical Motion Estimation and Memory Optimization towards Long-term Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents enhancements to the SAM2 framework for video object tracking task, addressing challenges such as occlusions, background clutter, and target reappearance. We introduce a hierarchical motion estimation strategy, combining lightweight linear prediction with selective non-linear refinement to improve tracking accuracy without requiring additional training. In addition, we optimize the memory bank by distinguishing long-term and short-term memory frames, enabling more reliable tracking under long-term occlusions and appearance changes. Experimental results show consistent improvements across different model scales. Our method achieves state-of-the-art performance on LaSOT and LaSOText with the large model, achieving 9.6% and 7.2% relative improvements in AUC over the original SAM2, and demonstrates even larger relative gains on smaller models, highlighting the effectiveness of our trainless, low-overhead improvements for boosting long-term tracking performance. The code is available at https://github.com/LouisFinner/HiM2SAM.
<div id='section'>PaperID: <span id='pid'>61, <a href='https://arxiv.org/pdf/2507.06543.pdf' target='_blank'>https://arxiv.org/pdf/2507.06543.pdf</a></span>   <span><a href='https://github.com/naver-ai/tobo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Taekyung Kim, Dongyoon Han, Byeongho Heo, Jeongeun Park, Sangdoo Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06543">Token Bottleneck: One Token to Remember Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales.
<div id='section'>PaperID: <span id='pid'>62, <a href='https://arxiv.org/pdf/2507.06400.pdf' target='_blank'>https://arxiv.org/pdf/2507.06400.pdf</a></span>   <span><a href='https://vranlee.github.io/SU-T/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Weiran Li, Yeqiang Liu, Qiannan Guo, Yijie Wei, Hwa Liang Leo, Zhenbo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06400">When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) technology has made significant progress in terrestrial applications, but underwater tracking scenarios remain underexplored despite their importance to marine ecology and aquaculture. In this paper, we present Multiple Fish Tracking Dataset 2025 (MFT25), a comprehensive dataset specifically designed for underwater multiple fish tracking, featuring 15 diverse video sequences with 408,578 meticulously annotated bounding boxes across 48,066 frames. Our dataset captures various underwater environments, fish species, and challenging conditions including occlusions, similar appearances, and erratic motion patterns. Additionally, we introduce Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework featuring an Unscented Kalman Filter (UKF) optimized for non-linear swimming patterns of fish and a novel Fish-Intersection-over-Union (FishIoU) matching that accounts for the unique morphological characteristics of aquatic species. Extensive experiments demonstrate that our SU-T baseline achieves state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while revealing fundamental differences between fish tracking and terrestrial object tracking scenarios. The dataset and codes are released at https://vranlee.github.io/SU-T/.
<div id='section'>PaperID: <span id='pid'>63, <a href='https://arxiv.org/pdf/2507.05899.pdf' target='_blank'>https://arxiv.org/pdf/2507.05899.pdf</a></span>   <span><a href='https://github.com/supertyd/FlexTrack/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuedong Tan, Jiawei Shao, Eduard Zamfir, Ruanjun Li, Zhaochong An, Chao Ma, Danda Paudel, Luc Van Gool, Radu Timofte, Zongwei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05899">What You Have is What You Track: Adaptive and Robust Multimodal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal data is known to be helpful for visual tracking by improving robustness to appearance variations. However, sensor synchronization challenges often compromise data availability, particularly in video settings where shortages can be temporal. Despite its importance, this area remains underexplored. In this paper, we present the first comprehensive study on tracker performance with temporally incomplete multimodal data. Unsurprisingly, under such a circumstance, existing trackers exhibit significant performance degradation, as their rigid architectures lack the adaptability needed to effectively handle missing modalities. To address these limitations, we propose a flexible framework for robust multimodal tracking. We venture that a tracker should dynamically activate computational units based on missing data rates. This is achieved through a novel Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity, coupled with a video-level masking strategy that ensures both temporal consistency and spatial completeness which is critical for effective video tracking. Surprisingly, our model not only adapts to varying missing rates but also adjusts to scene complexity. Extensive experiments show that our model achieves SOTA performance across 9 benchmarks, excelling in both conventional complete and missing modality settings. The code and benchmark will be publicly available at https://github.com/supertyd/FlexTrack/tree/main.
<div id='section'>PaperID: <span id='pid'>64, <a href='https://arxiv.org/pdf/2507.02479.pdf' target='_blank'>https://arxiv.org/pdf/2507.02479.pdf</a></span>   <span><a href='https://github.com/loseevaya/CrowdTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Teng Fu, Yuwen Chen, Zhuofan Chen, Mengyang Zhao, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02479">CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .
<div id='section'>PaperID: <span id='pid'>65, <a href='https://arxiv.org/pdf/2507.00648.pdf' target='_blank'>https://arxiv.org/pdf/2507.00648.pdf</a></span>   <span><a href='https://github.com/Z-Z188/UMDATrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Siyuan Yao, Rui Zhu, Ziqi Wang, Wenqi Ren, Yanyang Yan, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00648">UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has gained promising progress in past decades. Most of the existing approaches focus on learning target representation in well-conditioned daytime data, while for the unconstrained real-world scenarios with adverse weather conditions, e.g. nighttime or foggy environment, the tremendous domain shift leads to significant performance degradation. In this paper, we propose UMDATrack, which is capable of maintaining high-quality target state prediction under various adverse weather conditions within a unified domain adaptation framework. Specifically, we first use a controllable scenario generator to synthesize a small amount of unlabeled videos (less than 2% frames in source daytime datasets) in multiple weather conditions under the guidance of different text prompts. Afterwards, we design a simple yet effective domain-customized adapter (DCA), allowing the target objects' representation to rapidly adapt to various weather conditions without redundant model updating. Furthermore, to enhance the localization consistency between source and target domains, we propose a target-aware confidence alignment module (TCA) following optimal transport theorem. Extensive experiments demonstrate that UMDATrack can surpass existing advanced visual trackers and lead new state-of-the-art performance by a significant margin. Our code is available at https://github.com/Z-Z188/UMDATrack.
<div id='section'>PaperID: <span id='pid'>66, <a href='https://arxiv.org/pdf/2506.23972.pdf' target='_blank'>https://arxiv.org/pdf/2506.23972.pdf</a></span>   <span><a href='https://github.com/xuboyue1999/mmtrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Boyue Xu, Ruichao Hou, Tongwei Ren, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23972">Visual and Memory Dual Adapter for Multi-Modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt-learning-based multi-modal trackers have achieved promising progress by employing lightweight visual adapters to incorporate auxiliary modality features into frozen foundation models. However, existing approaches often struggle to learn reliable prompts due to limited exploitation of critical cues across frequency and temporal domains. In this paper, we propose a novel visual and memory dual adapter (VMDA) to construct more robust and discriminative representations for multi-modal tracking. Specifically, we develop a simple but effective visual adapter that adaptively transfers discriminative cues from auxiliary modality to dominant modality by jointly modeling the frequency, spatial, and channel-wise features. Additionally, we design the memory adapter inspired by the human memory mechanism, which stores global temporal cues and performs dynamic update and retrieval operations to ensure the consistent propagation of reliable temporal information across video sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth, and RGB-Event tracking. Code and models are available at https://github.com/xuboyue1999/mmtrack.git.
<div id='section'>PaperID: <span id='pid'>67, <a href='https://arxiv.org/pdf/2506.23783.pdf' target='_blank'>https://arxiv.org/pdf/2506.23783.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Mamba_FETrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiao Wang, Ju Huang, Qingchuan Ma, Jinfeng Gao, Chunyi Xu, Xiao Wang, Lan Chen, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23783">Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining traditional RGB cameras with bio-inspired event cameras for robust object tracking has garnered increasing attention in recent years. However, most existing multimodal tracking algorithms depend heavily on high-complexity Vision Transformer architectures for feature extraction and fusion across modalities. This not only leads to substantial computational overhead but also limits the effectiveness of cross-modal interactions. In this paper, we propose an efficient RGB-Event object tracking framework based on the linear-complexity Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a lightweight Prompt Generator that utilizes embedded features from each modality, together with a shared prompt pool, to dynamically generate modality-specific learnable prompt vectors. These prompts, along with the modality-specific embedded features, are then fed into a Vision Mamba-based FEMamba backbone, which facilitates prompt-guided feature extraction, cross-modal interaction, and fusion in a unified manner. Finally, the fused representations are passed to the tracking head for accurate target localization. Extensive experimental evaluations on multiple RGB-Event tracking benchmarks, including short-term COESOT dataset and long-term datasets, i.e., FE108 and FELT V2, demonstrate the superior performance and efficiency of the proposed tracking framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/Mamba_FETrack
<div id='section'>PaperID: <span id='pid'>68, <a href='https://arxiv.org/pdf/2506.17119.pdf' target='_blank'>https://arxiv.org/pdf/2506.17119.pdf</a></span>   <span><a href='https://github.com/GreatenAnoymous/RGBTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Teng Guo, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17119">RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a robust framework, RGBTrack, for real-time 6D pose estimation and tracking that operates solely on RGB data, thereby eliminating the need for depth input for such dynamic and precise object pose tracking tasks. Building on the FoundationPose architecture, we devise a novel binary search strategy combined with a render-and-compare mechanism to efficiently infer depth and generate robust pose hypotheses from true-scale CAD models. To maintain stable tracking in dynamic scenarios, including rapid movements and occlusions, RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman filter and a state machine for proactive object pose recovery. In addition, RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale using an initial depth estimate, enabling seamless integration with modern generative reconstruction techniques. Extensive evaluations on benchmark datasets demonstrate that RGBTrack's novel depth-free approach achieves competitive accuracy and real-time performance, making it a promising practical solution candidate for application areas including robotics, augmented reality, and computer vision.
  The source code for our implementation will be made publicly available at https://github.com/GreatenAnoymous/RGBTrack.git.
<div id='section'>PaperID: <span id='pid'>69, <a href='https://arxiv.org/pdf/2506.15945.pdf' target='_blank'>https://arxiv.org/pdf/2506.15945.pdf</a></span>   <span><a href='https://github.com/arc-l/karl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Kowndinya Boyalakuntla, Abdeslam Boularias, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15945">KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic object tracking and grasping over eye-on-hand (EoH) systems, significantly expanding such systems capabilities in challenging, realistic environments. In comparison to the previous state-of-the-art, KARL (1) incorporates a novel six-stage RL curriculum that doubles the system's motion range, thereby greatly enhancing the system's grasping performance, (2) integrates a robust Kalman filter layer between the perception and reinforcement learning (RL) control modules, enabling the system to maintain an uncertain but continuous 6D pose estimate even when the target object temporarily exits the camera's field-of-view or undergoes rapid, unpredictable motion, and (3) introduces mechanisms to allow retries to gracefully recover from unavoidable policy execution failures. Extensive evaluations conducted in both simulation and real-world experiments qualitatively and quantitatively corroborate KARL's advantage over earlier systems, achieving higher grasp success rates and faster robot execution speed. Source code and supplementary materials for KARL will be made available at: https://github.com/arc-l/karl.
<div id='section'>PaperID: <span id='pid'>70, <a href='https://arxiv.org/pdf/2506.12105.pdf' target='_blank'>https://arxiv.org/pdf/2506.12105.pdf</a></span>   <span><a href='https://github.com/softwarePupil/VSMB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoxiang Chen, Wei Zhao, Rufei Zhang, Nannan Li, Dongjin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12105">Multiple Object Tracking in Video SAR: A Benchmark and Tracking Baseline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of multi-object tracking using video synthetic aperture radar (Video SAR), Doppler shifts induced by target motion result in artifacts that are easily mistaken for shadows caused by static occlusions. Moreover, appearance changes of the target caused by Doppler mismatch may lead to association failures and disrupt trajectory continuity. A major limitation in this field is the lack of public benchmark datasets for standardized algorithm evaluation. To address the above challenges, we collected and annotated 45 video SAR sequences containing moving targets, and named the Video SAR MOT Benchmark (VSMB). Specifically, to mitigate the effects of trailing and defocusing in moving targets, we introduce a line feature enhancement mechanism that emphasizes the positive role of motion shadows and reduces false alarms induced by static occlusions. In addition, to mitigate the adverse effects of target appearance variations, we propose a motion-aware clue discarding mechanism that substantially improves tracking robustness in Video SAR. The proposed model achieves state-of-the-art performance on the VSMB, and the dataset and model are released at https://github.com/softwarePupil/VSMB.
<div id='section'>PaperID: <span id='pid'>71, <a href='https://arxiv.org/pdf/2506.02866.pdf' target='_blank'>https://arxiv.org/pdf/2506.02866.pdf</a></span>   <span><a href='https://github.com/AhsanBaidar/MVTD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ahsan Baidar Bakht, Muhayy Ud Din, Sajid Javed, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02866">MVTD: A Benchmark Dataset for Maritime Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is a fundamental task with widespread applications in autonomous navigation, surveillance, and maritime robotics. Despite significant advances in generic object tracking, maritime environments continue to present unique challenges, including specular water reflections, low-contrast targets, dynamically changing backgrounds, and frequent occlusions. These complexities significantly degrade the performance of state-of-the-art tracking algorithms, highlighting the need for domain-specific datasets. To address this gap, we introduce the Maritime Visual Tracking Dataset (MVTD), a comprehensive and publicly available benchmark specifically designed for maritime VOT. MVTD comprises 182 high-resolution video sequences, totaling approximately 150,000 frames, and includes four representative object classes: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset captures a diverse range of operational conditions and maritime scenarios, reflecting the real-world complexities of maritime environments. We evaluated 14 recent SOTA tracking algorithms on the MVTD benchmark and observed substantial performance degradation compared to their performance on general-purpose datasets. However, when fine-tuned on MVTD, these models demonstrate significant performance gains, underscoring the effectiveness of domain adaptation and the importance of transfer learning in specialized tracking contexts. The MVTD dataset fills a critical gap in the visual tracking community by providing a realistic and challenging benchmark for maritime scenarios. Dataset and Source Code can be accessed here "https://github.com/AhsanBaidar/MVTD".
<div id='section'>PaperID: <span id='pid'>72, <a href='https://arxiv.org/pdf/2506.01373.pdf' target='_blank'>https://arxiv.org/pdf/2506.01373.pdf</a></span>   <span><a href='https://github.com/tstanczyk95/McByte' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tomasz Stanczyk, Seongro Yoon, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01373">No Train Yet Gain: Towards Generic Multi-Object Tracking in Sports and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is essential for sports analytics, enabling performance evaluation and tactical insights. However, tracking in sports is challenging due to fast movements, occlusions, and camera shifts. Traditional tracking-by-detection methods require extensive tuning, while segmentation-based approaches struggle with track processing. We propose McByte, a tracking-by-detection framework that integrates temporally propagated segmentation mask as an association cue to improve robustness without per-video tuning. Unlike many existing methods, McByte does not require training, relying solely on pre-trained models and object detectors commonly used in the community. Evaluated on SportsMOT, DanceTrack, SoccerNet-tracking 2022 and MOT17, McByte demonstrates strong performance across sports and general pedestrian tracking. Our results highlight the benefits of mask propagation for a more adaptable and generalizable MOT approach. Code will be made available at https://github.com/tstanczyk95/McByte.
<div id='section'>PaperID: <span id='pid'>73, <a href='https://arxiv.org/pdf/2506.00774.pdf' target='_blank'>https://arxiv.org/pdf/2506.00774.pdf</a></span>   <span><a href='https://github.com/Milad-Khanchi/DepthMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Milad Khanchi, Maria Amer, Charalambos Poullis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00774">Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current motion-based multiple object tracking (MOT) approaches rely heavily on Intersection-over-Union (IoU) for object association. Without using 3D features, they are ineffective in scenarios with occlusions or visually similar objects. To address this, our paper presents a novel depth-aware framework for MOT. We estimate depth using a zero-shot approach and incorporate it as an independent feature in the association process. Additionally, we introduce a Hierarchical Alignment Score that refines IoU by integrating both coarse bounding box overlap and fine-grained (pixel-level) alignment to improve association accuracy without requiring additional learnable parameters. To our knowledge, this is the first MOT framework to incorporate 3D features (monocular depth) as an independent decision matrix in the association step. Our framework achieves state-of-the-art results on challenging benchmarks without any training nor fine-tuning. The code is available at https://github.com/Milad-Khanchi/DepthMOT
<div id='section'>PaperID: <span id='pid'>74, <a href='https://arxiv.org/pdf/2506.00325.pdf' target='_blank'>https://arxiv.org/pdf/2506.00325.pdf</a></span>   <span><a href='https://github.com/pgao-lab/DiffDf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Long Xu, Peng Gao, Wen-Jia Tang, Fei Wang, Ru-Yue Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00325">Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although deep learning-based visual tracking methods have made significant progress, they exhibit vulnerabilities when facing carefully designed adversarial attacks, which can lead to a sharp decline in tracking performance. To address this issue, this paper proposes for the first time a novel adversarial defense method based on denoise diffusion probabilistic models, termed DiffDf, aimed at effectively improving the robustness of existing visual tracking methods against adversarial attacks. DiffDf establishes a multi-scale defense mechanism by combining pixel-level reconstruction loss, semantic consistency loss, and structural similarity loss, effectively suppressing adversarial perturbations through a gradual denoising process. Extensive experimental results on several mainstream datasets show that the DiffDf method demonstrates excellent generalization performance for trackers with different architectures, significantly improving various evaluation metrics while achieving real-time inference speeds of over 30 FPS, showcasing outstanding defense performance and efficiency. Codes are available at https://github.com/pgao-lab/DiffDf.
<div id='section'>PaperID: <span id='pid'>75, <a href='https://arxiv.org/pdf/2505.23704.pdf' target='_blank'>https://arxiv.org/pdf/2505.23704.pdf</a></span>   <span><a href='https://github.com/HamadYA/CLDTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohamad Alansari, Sajid Javed, Iyyakutti Iyappan Ganapathi, Sara Alansari, Muzammal Naseer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23704">CLDTracker: A Comprehensive Language Description for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VOT remains a fundamental yet challenging task in computer vision due to dynamic appearance changes, occlusions, and background clutter. Traditional trackers, relying primarily on visual cues, often struggle in such complex scenarios. Recent advancements in VLMs have shown promise in semantic understanding for tasks like open-vocabulary detection and image captioning, suggesting their potential for VOT. However, the direct application of VLMs to VOT is hindered by critical limitations: the absence of a rich and comprehensive textual representation that semantically captures the target object's nuances, limiting the effective use of language information; inefficient fusion mechanisms that fail to optimally integrate visual and textual features, preventing a holistic understanding of the target; and a lack of temporal modeling of the target's evolving appearance in the language domain, leading to a disconnect between the initial description and the object's subsequent visual changes. To bridge these gaps and unlock the full potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive Language Description framework for robust visual Tracking. Our tracker introduces a dual-branch architecture consisting of a textual and a visual branch. In the textual branch, we construct a rich bag of textual descriptions derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with semantic and contextual cues to address the lack of rich textual representation. Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves SOTA performance, validating the effectiveness of leveraging robust and temporally-adaptive vision-language representations for tracking. Code and models are publicly available at: https://github.com/HamadYA/CLDTracker
<div id='section'>PaperID: <span id='pid'>76, <a href='https://arxiv.org/pdf/2505.23189.pdf' target='_blank'>https://arxiv.org/pdf/2505.23189.pdf</a></span>   <span><a href='https://pku-epic.github.io/TrackVLA-web' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaoan Wang, Jiazhao Zhang, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23189">TrackVLA: Embodied Visual Tracking in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed. Our project page is: https://pku-epic.github.io/TrackVLA-web.
<div id='section'>PaperID: <span id='pid'>77, <a href='https://arxiv.org/pdf/2505.21795.pdf' target='_blank'>https://arxiv.org/pdf/2505.21795.pdf</a></span>   <span><a href='https://github.com/ClaudiaCuttano/SANSA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ClaudiaCuttano/SANSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21795">SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at https://github.com/ClaudiaCuttano/SANSA.
<div id='section'>PaperID: <span id='pid'>78, <a href='https://arxiv.org/pdf/2505.16321.pdf' target='_blank'>https://arxiv.org/pdf/2505.16321.pdf</a></span>   <span><a href='https://github.com/zj5559/Motion-Prompt-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16321">Efficient Motion Prompt Learning for Robust Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the challenges of processing temporal information, most trackers depend solely on visual discriminability and overlook the unique temporal coherence of video data. In this paper, we propose a lightweight and plug-and-play motion prompt tracking method. It can be easily integrated into existing vision-based trackers to build a joint tracking framework leveraging both motion and vision cues, thereby achieving robust tracking through efficient prompt learning. A motion encoder with three different positional encodings is proposed to encode the long-term motion trajectory into the visual embedding space, while a fusion decoder and an adaptive weight mechanism are designed to dynamically fuse visual and motion features. We integrate our motion module into three different trackers with five models in total. Experiments on seven challenging tracking benchmarks demonstrate that the proposed motion module significantly improves the robustness of vision-based trackers, with minimal training costs and negligible speed sacrifice. Code is available at https://github.com/zj5559/Motion-Prompt-Tracking.
<div id='section'>PaperID: <span id='pid'>79, <a href='https://arxiv.org/pdf/2505.15928.pdf' target='_blank'>https://arxiv.org/pdf/2505.15928.pdf</a></span>   <span><a href='https://github.com/t-montes/viqagent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tony Montes, Fernando Lozano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15928">ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Video Question Answering (VideoQA) have introduced LLM-based agents, modular frameworks, and procedural solutions, yielding promising results. These systems use dynamic agents and memory-based mechanisms to break down complex tasks and refine answers. However, significant improvements remain in tracking objects for grounding over time and decision-making based on reasoning to better align object references with language model outputs, as newer models get better at both tasks. This work presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA) that combines a Chain-of-Thought framework with grounding reasoning alongside YOLO-World to enhance object tracking and alignment. This approach establishes a new state-of-the-art in VideoQA and Video Understanding, showing enhanced performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also enables cross-checking of grounding timeframes, improving accuracy and providing valuable support for verification and increased output reliability across multiple video domains. The code is available at https://github.com/t-montes/viqagent.
<div id='section'>PaperID: <span id='pid'>80, <a href='https://arxiv.org/pdf/2505.12903.pdf' target='_blank'>https://arxiv.org/pdf/2505.12903.pdf</a></span>   <span><a href='https://github.com/Event-AHU/SlowFast_Event_Track' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiao Wang, Xiao Wang, Liye Jin, Bo Jiang, Lin Zhu, Lan Chen, Yonghong Tian, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12903">Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing tracking algorithms typically rely on low-frame-rate RGB cameras coupled with computationally intensive deep neural network architectures to achieve effective tracking. However, such frame-based methods inherently face challenges in achieving low-latency performance and often fail in resource-constrained environments. Visual object tracking using bio-inspired event cameras has emerged as a promising research direction in recent years, offering distinct advantages for low-latency applications. In this paper, we propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different operational requirements, termed SFTrack. The proposed framework supports two complementary modes, i.e., a high-precision slow tracker for scenarios with sufficient computational resources, and an efficient fast tracker tailored for latency-aware, resource-constrained environments. Specifically, our framework first performs graph-based representation learning from high-temporal-resolution event streams, and then integrates the learned graph-structured information into two FlashAttention-based vision backbones, yielding the slow and fast trackers, respectively. The fast tracker achieves low latency through a lightweight network design and by producing multiple bounding box outputs in a single forward pass. Finally, we seamlessly combine both trackers via supervised fine-tuning and further enhance the fast tracker's performance through a knowledge distillation strategy. Extensive experiments on public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the effectiveness and efficiency of our proposed method across different real-world scenarios. The source code has been released on https://github.com/Event-AHU/SlowFast_Event_Track.
<div id='section'>PaperID: <span id='pid'>81, <a href='https://arxiv.org/pdf/2505.08999.pdf' target='_blank'>https://arxiv.org/pdf/2505.08999.pdf</a></span>   <span><a href='https://github.com/pgao-lab/AMGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Wei-Long Tian, Peng Gao, Xiao Liu, Long Xu, Hamido Fujita, Hanan Aljuai, Mao-Li Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08999">Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, visual tracking methods based on convolutional neural networks and Transformers have achieved remarkable performance and have been successfully applied in fields such as autonomous driving. However, the numerous security issues exposed by deep learning models have gradually affected the reliable application of visual tracking methods in real-world scenarios. Therefore, how to reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks has become a critical problem that needs to be addressed. To this end, we propose an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking. This method integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing, which can significantly enhance the transferability and attack effectiveness of adversarial examples. AMGA randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model. This paradigm minimizes the gap between white- and black-box adversarial attacks, thus achieving excellent attack performance in black-box scenarios. Extensive experimental results on large-scale datasets such as OTB2015, LaSOT, and GOT-10k demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples. Codes and data are available at https://github.com/pgao-lab/AMGA.
<div id='section'>PaperID: <span id='pid'>82, <a href='https://arxiv.org/pdf/2505.05936.pdf' target='_blank'>https://arxiv.org/pdf/2505.05936.pdf</a></span>   <span><a href='https://github.com/Nightwatch-Fox11/CGTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Weihong Li, Xiaoqiong Liu, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05936">CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in visual object tracking have markedly improved the capabilities of unmanned aerial vehicle (UAV) tracking, which is a critical component in real-world robotics applications. While the integration of hierarchical lightweight networks has become a prevalent strategy for enhancing efficiency in UAV tracking, it often results in a significant drop in network capacity, which further exacerbates challenges in UAV scenarios, such as frequent occlusions and extreme changes in viewing angles. To address these issues, we introduce a novel family of UAV trackers, termed CGTrack, which combines explicit and implicit techniques to expand network capacity within a coarse-to-fine framework. Specifically, we first introduce a Hierarchical Feature Cascade (HFC) module that leverages the spirit of feature reuse to increase network capacity by integrating the deep semantic cues with the rich spatial information, incurring minimal computational costs while enhancing feature representation. Based on this, we design a novel Lightweight Gated Center Head (LGCH) that utilizes gating mechanisms to decouple target-oriented coordinates from previously expanded features, which contain dense local discriminative information. Extensive experiments on three challenging UAV tracking benchmarks demonstrate that CGTrack achieves state-of-the-art performance while running fast. Code will be available at https://github.com/Nightwatch-Fox11/CGTrack.
<div id='section'>PaperID: <span id='pid'>83, <a href='https://arxiv.org/pdf/2505.01257.pdf' target='_blank'>https://arxiv.org/pdf/2505.01257.pdf</a></span>   <span><a href='https://github.com/TrackingLaboratory/CAMELTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Vladimir Somers, Baptiste Standaert, Victor Joos, Alexandre Alahi, Christophe De Vleeschouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01257">CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online multi-object tracking has been recently dominated by tracking-by-detection (TbD) methods, where recent advances rely on increasingly sophisticated heuristics for tracklet representation, feature fusion, and multi-stage matching. The key strength of TbD lies in its modular design, enabling the integration of specialized off-the-shelf models like motion predictors and re-identification. However, the extensive usage of human-crafted rules for temporal associations makes these methods inherently limited in their ability to capture the complex interplay between various tracking cues. In this work, we introduce CAMEL, a novel association module for Context-Aware Multi-Cue ExpLoitation, that learns resilient association strategies directly from data, breaking free from hand-crafted heuristics while maintaining TbD's valuable modularity. At its core, CAMEL employs two transformer-based modules and relies on a novel association-centric training scheme to effectively model the complex interactions between tracked targets and their various association cues. Unlike end-to-end detection-by-tracking approaches, our method remains lightweight and fast to train while being able to leverage external off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack, achieves state-of-the-art performance on multiple tracking benchmarks. Our code is available at https://github.com/TrackingLaboratory/CAMELTrack.
<div id='section'>PaperID: <span id='pid'>84, <a href='https://arxiv.org/pdf/2504.21716.pdf' target='_blank'>https://arxiv.org/pdf/2504.21716.pdf</a></span>   <span><a href='https://github.com/marc1198/chat-hsr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Marc Glocker, Peter HÃ¶nig, Matthias Hirschmanner, Markus Vincze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21716">LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.
<div id='section'>PaperID: <span id='pid'>85, <a href='https://arxiv.org/pdf/2504.18068.pdf' target='_blank'>https://arxiv.org/pdf/2504.18068.pdf</a></span>   <span><a href='https://github.com/bytepioneerX/s3mot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhuohao Yan, Shaoquan Feng, Xingxing Li, Yuxuan Zhou, Chunxi Xia, Shengyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18068">S3MOT: Monocular 3D Object Tracking with Selective State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at https://github.com/bytepioneerX/s3mot.
<div id='section'>PaperID: <span id='pid'>86, <a href='https://arxiv.org/pdf/2504.15609.pdf' target='_blank'>https://arxiv.org/pdf/2504.15609.pdf</a></span>   <span><a href='https://github.com/LiYunfengLYF/SonarT165' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunfeng Li, Bo Wang, Jiahao Wan, Xueyi Wu, Ye Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15609">SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater observation systems typically integrate optical cameras and imaging sonar systems. When underwater visibility is insufficient, only sonar systems can provide stable data, which necessitates exploration of the underwater acoustic object tracking (UAOT) task. Previous studies have explored traditional methods and Siamese networks for UAOT. However, the absence of a unified evaluation benchmark has significantly constrained the value of these methods. To alleviate this limitation, we propose the first large-scale UAOT benchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and 205K high-quality annotations. Experimental results demonstrate that SonarT165 reveals limitations in current state-of-the-art SOT trackers. To address these limitations, we propose STFTrack, an efficient framework for acoustic object tracking. It includes two novel modules, a multi-view template fusion module (MTFM) and an optimal trajectory correction module (OTCM). The MTFM module integrates multi-view feature of both the original image and the binary image of the dynamic template, and introduces a cross-attention-like layer to fuse the spatio-temporal target representations. The OTCM module introduces the acoustic-response-equivalent pixel property and proposes normalized pixel brightness response scores, thereby suppressing suboptimal matches caused by inaccurate Kalman filter prediction boxes. To further improve the model feature, STFTrack introduces a acoustic image enhancement method and a Frequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive experiments show the proposed STFTrack achieves state-of-the-art performance on the proposed benchmark. The code is available at https://github.com/LiYunfengLYF/SonarT165.
<div id='section'>PaperID: <span id='pid'>87, <a href='https://arxiv.org/pdf/2504.14423.pdf' target='_blank'>https://arxiv.org/pdf/2504.14423.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Adversarial_Attack_Defense' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiang Chen, Xiao Wang, Haowen Wang, Bo Jiang, Lin Zhu, Dawei Zhang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14423">Adversarial Attack for RGB-Event based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking is a crucial research topic in the fields of computer vision and multi-modal fusion. Among various approaches, robust visual tracking that combines RGB frames with Event streams has attracted increasing attention from researchers. While striving for high accuracy and efficiency in tracking, it is also important to explore how to effectively conduct adversarial attacks and defenses on RGB-Event stream tracking algorithms, yet research in this area remains relatively scarce. To bridge this gap, in this paper, we propose a cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because of the diverse representations of Event streams, and given that Event voxels and frames are more commonly used, this paper will focus on these two representations for an in-depth study. Specifically, for the RGB-Event voxel, we first optimize the perturbation by adversarial loss to generate RGB frame adversarial examples. For discrete Event voxel representations, we propose a two-step attack strategy, more in detail, we first inject Event voxels into the target region as initialized adversarial examples, then, conduct a gradient-guided optimization by perturbing the spatial location of the Event voxels. For the RGB-Event frame based tracking, we optimize the cross-modal universal perturbation by integrating the gradient information from multimodal data. We evaluate the proposed approach against attacks on three widely used RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive experiments show that our method significantly reduces the performance of the tracker across numerous datasets in both unimodal and multimodal scenarios. The source code will be released on https://github.com/Event-AHU/Adversarial_Attack_Defense
<div id='section'>PaperID: <span id='pid'>88, <a href='https://arxiv.org/pdf/2504.10165.pdf' target='_blank'>https://arxiv.org/pdf/2504.10165.pdf</a></span>   <span><a href='https://dat-nguyenvn.github.io/WildLive/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Nguyen Ngoc Dat, Tom Richardson, Matthew Watson, Kilian Meier, Jenna Kline, Sid Reid, Guy Maalouf, Duncan Hine, Majid Mirmehdi, Tilo Burghardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10165">WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Live tracking of wildlife via high-resolution video processing directly onboard drones is widely unexplored and most existing solutions rely on streaming video to ground stations to support navigation. Yet, both autonomous animal-reactive flight control beyond visual line of sight and/or mission-specific individual and behaviour recognition tasks rely to some degree on this capability. In response, we introduce WildLive - a near real-time animal detection and tracking framework for high-resolution imagery running directly onboard uncrewed aerial vehicles (UAVs). The system performs multi-animal detection and tracking at 17.81fps for HD and 7.53fps on 4K video streams suitable for operation during higher altitude flights to minimise animal disturbance. Our system is optimised for Jetson Orin AGX onboard hardware. It integrates the efficiency of sparse optical flow tracking and mission-specific sampling with device-optimised and proven YOLO-driven object detection and segmentation techniques. Essentially, computational resource is focused onto spatio-temporal regions of high uncertainty to significantly improve UAV processing speeds. Alongside, we introduce our WildLive dataset, which comprises 200K+ annotated animal instances across 19K+ frames from 4K UAV videos collected at the Ol Pejeta Conservancy in Kenya. All frames contain ground truth bounding boxes, segmentation masks, as well as individual tracklets and tracking point trajectories. We compare our system against current object tracking approaches including OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments with onboard hardware confirm that near real-time high-resolution wildlife tracking is possible on UAVs whilst maintaining high accuracy levels as needed for future navigational and mission-specific animal-centric operational autonomy. Our materials are available at: https://dat-nguyenvn.github.io/WildLive/
<div id='section'>PaperID: <span id='pid'>89, <a href='https://arxiv.org/pdf/2504.09195.pdf' target='_blank'>https://arxiv.org/pdf/2504.09195.pdf</a></span>   <span><a href='https://github.com/Tzoulio/ReferGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Tzoulio Chamiti, Leandro Di Bella, Adrian Munteanu, Nikos Deligiannis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09195">ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple objects based on textual queries is a challenging task that requires linking language understanding with object association across frames. Previous works typically train the whole process end-to-end or integrate an additional referring text module into a multi-object tracker, but they both require supervised training and potentially struggle with generalization to open-set queries. In this work, we introduce ReferGPT, a novel zero-shot referring multi-object tracking framework. We provide a multi-modal large language model (MLLM) with spatial knowledge enabling it to generate 3D-aware captions. This enhances its descriptive capabilities and supports a more flexible referring vocabulary without training. We also propose a robust query-matching strategy, leveraging CLIP-based semantic encoding and fuzzy matching to associate MLLM generated captions with user queries. Extensive experiments on Refer-KITTI, Refer-KITTIv2 and Refer-KITTI+ demonstrate that ReferGPT achieves competitive performance against trained methods, showcasing its robustness and zero-shot capabilities in autonomous driving. The codes are available on https://github.com/Tzoulio/ReferGPT
<div id='section'>PaperID: <span id='pid'>90, <a href='https://arxiv.org/pdf/2504.07962.pdf' target='_blank'>https://arxiv.org/pdf/2504.07962.pdf</a></span>   <span><a href='https://glus-video.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07962">GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between "Ref" and "VOS": they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse "context frames" provides global information, while a stream of continuous "query frames" conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at https://glus-video.github.io/.
<div id='section'>PaperID: <span id='pid'>91, <a href='https://arxiv.org/pdf/2504.04519.pdf' target='_blank'>https://arxiv.org/pdf/2504.04519.pdf</a></span>   <span><a href='https://github.com/TripleJoy/SAM2MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Junjie Jiang, Zelin Wang, Manqi Zhao, Yin Li, DongSheng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04519">SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segment Anything 2 (SAM2) enables robust single-object tracking using segmentation. To extend this to multi-object tracking (MOT), we propose SAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking by Detection or Tracking by Query, SAM2MOT directly generates tracking boxes from segmentation masks, reducing reliance on detection accuracy. SAM2MOT has two key advantages: zero-shot generalization, allowing it to work across datasets without fine-tuning, and strong object association, inherited from SAM2. To further improve performance, we integrate a trajectory manager system for precise object addition and removal, and a cross-object interaction module to handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show state-of-the-art results. Notably, SAM2MOT outperforms existing methods on DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT. Code is available at https://github.com/TripleJoy/SAM2MOT.
<div id='section'>PaperID: <span id='pid'>92, <a href='https://arxiv.org/pdf/2504.01321.pdf' target='_blank'>https://arxiv.org/pdf/2504.01321.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chunhui Zhang, Li Liu, Jialin Gao, Xin Sun, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01321">COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer has recently demonstrated great potential in improving vision-language (VL) tracking algorithms. However, most of the existing VL trackers rely on carefully designed mechanisms to perform the multi-stage multi-modal fusion. Additionally, direct multi-modal fusion without alignment ignores distribution discrepancy between modalities in feature space, potentially leading to suboptimal representations. In this work, we propose COST, a contrastive one-stage transformer fusion framework for VL tracking, aiming to learn semantically consistent and unified VL representations. Specifically, we introduce a contrastive alignment strategy that maximizes mutual information (MI) between a video and its corresponding language description. This enables effective cross-modal alignment, yielding semantically consistent features in the representation space. By leveraging a visual-linguistic transformer, we establish an efficient multi-modal fusion and reasoning mechanism, empirically demonstrating that a simple stack of transformer encoders effectively enables unified VL representations. Moreover, we contribute a newly collected VL tracking benchmark dataset for small object tracking, named VL-SOT500, with bounding boxes and language descriptions. Our dataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated to evaluating generic and high-speed small object tracking, respectively. Small object tracking is notoriously challenging due to weak appearance and limited features, and this dataset is, to the best of our knowledge, the first to explore the usage of language cues to enhance visual representation for small object tracking. Extensive experiments demonstrate that COST achieves state-of-the-art performance on five existing VL tracking datasets, as well as on our proposed VL-SOT500 dataset. Source codes and dataset will be made publicly available.
<div id='section'>PaperID: <span id='pid'>93, <a href='https://arxiv.org/pdf/2504.00954.pdf' target='_blank'>https://arxiv.org/pdf/2504.00954.pdf</a></span>   <span><a href='https://github.com/BwLiu01/IDMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Bangwei Liu, Yicheng Bao, Shaohui Lin, Xuhong Wang, Xin Tan, Yingchun Wang, Yuan Xie, Chaochao Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00954">IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal retrieval systems are becoming increasingly vital for cutting-edge AI technologies, such as embodied AI and AI-driven digital content industries. However, current multimodal retrieval tasks lack sufficient complexity and demonstrate limited practical application value. It spires us to design Instance-Driven Multimodal Image Retrieval (IDMR), a novel task that requires models to retrieve images containing the same instance as a query image while matching a text-described scenario. Unlike existing retrieval tasks focused on global image similarity or category-level matching, IDMR demands fine-grained instance-level consistency across diverse contexts. To benchmark this capability, we develop IDMR-bench using real-world object tracking and first-person video data. Addressing the scarcity of training data, we propose a cross-domain synthesis method that creates 557K training samples by cropping objects from standard detection datasets. Our Multimodal Large Language Model (MLLM) based retrieval model, trained on 1.2M samples, outperforms state-of-the-art approaches on both traditional benchmarks and our zero-shot IDMR-bench. Experimental results demonstrate previous models' limitations in instance-aware retrieval and highlight the potential of MLLM for advanced retrieval applications. The whole training dataset, codes and models, with wide ranges of sizes, are available at https://github.com/BwLiu01/IDMR.
<div id='section'>PaperID: <span id='pid'>94, <a href='https://arxiv.org/pdf/2503.18338.pdf' target='_blank'>https://arxiv.org/pdf/2503.18338.pdf</a></span>   <span><a href='https://github.com/WenRuiCai/SPMTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenrui Cai, Qingjie Liu, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18338">SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture of Experts for Scalable Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most state-of-the-art trackers adopt one-stream paradigm, using a single Vision Transformer for joint feature extraction and relation modeling of template and search region images. However, relation modeling between different image patches exhibits significant variations. For instance, background regions dominated by target-irrelevant information require reduced attention allocation, while foreground, particularly boundary areas, need to be be emphasized. A single model may not effectively handle all kinds of relation modeling simultaneously. In this paper, we propose a novel tracker called SPMTrack based on mixture-of-experts tailored for visual tracking task (TMoE), combining the capability of multiple experts to handle diverse relation modeling more flexibly. Benefiting from TMoE, we extend relation modeling from image pairs to spatio-temporal context, further improving tracking accuracy with minimal increase in model parameters. Moreover, we employ TMoE as a parameter-efficient fine-tuning method, substantially reducing trainable parameters, which enables us to train SPMTrack of varying scales efficiently and preserve the generalization ability of pretrained models to achieve superior performance. We conduct experiments on seven datasets, and experimental results demonstrate that our method significantly outperforms current state-of-the-art trackers. The source code is available at https://github.com/WenRuiCai/SPMTrack.
<div id='section'>PaperID: <span id='pid'>95, <a href='https://arxiv.org/pdf/2503.18282.pdf' target='_blank'>https://arxiv.org/pdf/2503.18282.pdf</a></span>   <span><a href='https://github.com/open-starlab/TrackID3x3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Kazuhiro Yamada, Li Yin, Qingrui Hu, Ning Ding, Shunsuke Iwashita, Jun Ichikawa, Kiwamu Kotani, Calvin Yeung, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18282">TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with Identification and Pose Estimation in 3x3 Basketball Full-court Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking, player identification, and pose estimation are fundamental components of sports analytics, essential for analyzing player movements, performance, and tactical strategies. However, existing datasets and methodologies primarily target mainstream team sports such as soccer and conventional 5-on-5 basketball, often overlooking scenarios involving fixed-camera setups commonly used at amateur levels, less mainstream sports, or datasets that explicitly incorporate pose annotations. In this paper, we propose the TrackID3x3 dataset, the first publicly available comprehensive dataset specifically designed for multi-player tracking, player identification, and pose estimation in 3x3 basketball scenarios. The dataset comprises three distinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera footage), capturing diverse full-court camera perspectives and environments. We also introduce the Track-ID task, a simplified variant of the game state reconstruction task that excludes field detection and focuses exclusively on fixed-camera scenarios. To evaluate performance, we propose a baseline algorithm called Track-ID algorithm, tailored to assess tracking and identification quality. Furthermore, our benchmark experiments, utilizing recent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose estimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results and highlight remaining challenges. Our dataset and evaluation benchmarks provide a solid foundation for advancing automated analytics in 3x3 basketball. Dataset and code will be available at https://github.com/open-starlab/TrackID3x3.
<div id='section'>PaperID: <span id='pid'>96, <a href='https://arxiv.org/pdf/2503.17699.pdf' target='_blank'>https://arxiv.org/pdf/2503.17699.pdf</a></span>   <span><a href='https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haolin Qin, Tingfa Xu, Tianhao Li, Zhenxiang Chen, Tao Feng, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17699">MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>UAV tracking faces significant challenges in real-world scenarios, such as small-size targets and occlusions, which limit the performance of RGB-based trackers. Multispectral images (MSI), which capture additional spectral information, offer a promising solution to these challenges. However, progress in this field has been hindered by the lack of relevant datasets. To address this gap, we introduce the first large-scale Multispectral UAV Single Object Tracking dataset (MUST), which includes 250 video sequences spanning diverse environments and challenges, providing a comprehensive data foundation for multispectral UAV tracking. We also propose a novel tracking framework, UNTrack, which encodes unified spectral, spatial, and temporal features from spectrum prompts, initial templates, and sequential searches. UNTrack employs an asymmetric transformer with a spectral background eliminate mechanism for optimal relationship modeling and an encoder that continuously updates the spectrum prompt to refine tracking, improving both accuracy and efficiency. Extensive experiments show that our proposed UNTrack outperforms state-of-the-art UAV trackers. We believe our dataset and framework will drive future research in this area. The dataset is available on https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking.
<div id='section'>PaperID: <span id='pid'>97, <a href='https://arxiv.org/pdf/2503.12888.pdf' target='_blank'>https://arxiv.org/pdf/2503.12888.pdf</a></span>   <span><a href='https://github.com/ManOfStory/UncTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Siyuan Yao, Yang Guo, Yanyang Yan, Wenqi Ren, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12888">UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based trackers have achieved promising success and become the dominant tracking paradigm due to their accuracy and efficiency. Despite the substantial progress, most of the existing approaches tackle object tracking as a deterministic coordinate regression problem, while the target localization uncertainty has been greatly overlooked, which hampers trackers' ability to maintain reliable target state prediction in challenging scenarios. To address this issue, we propose UncTrack, a novel uncertainty-aware transformer tracker that predicts the target localization uncertainty and incorporates this uncertainty information for accurate target state inference. Specifically, UncTrack utilizes a transformer encoder to perform feature interaction between template and search images. The output features are passed into an uncertainty-aware localization decoder (ULD) to coarsely predict the corner-based localization and the corresponding localization uncertainty. Then the localization uncertainty is sent into a prototype memory network (PMN) to excavate valuable historical information to identify whether the target state prediction is reliable or not. To enhance the template representation, the samples with high confidence are fed back into the prototype memory bank for memory updating, making the tracker more robust to challenging appearance variations. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods. Our code is available at https://github.com/ManOfStory/UncTrack.
<div id='section'>PaperID: <span id='pid'>98, <a href='https://arxiv.org/pdf/2503.12562.pdf' target='_blank'>https://arxiv.org/pdf/2503.12562.pdf</a></span>   <span><a href='https://github.com/HELLORPG/HATReID-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruopeng Gao, Yuyao Wang, Chunxu Liu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12562">History-Aware Transformation of ReID Features for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of multiple object tracking (MOT) is to detect all objects in a video and bind them into multiple trajectories. Generally, this process is carried out in two steps: detecting objects and associating them across frames based on various cues and metrics. Many studies and applications adopt object appearance, also known as re-identification (ReID) features, for target matching through straightforward similarity calculation. However, we argue that this practice is overly naive and thus overlooks the unique characteristics of MOT tasks. Unlike regular re-identification tasks that strive to distinguish all potential targets in a general representation, multi-object tracking typically immerses itself in differentiating similar targets within the same video sequence. Therefore, we believe that seeking a more suitable feature representation space based on the different sample distributions of each sequence will enhance tracking performance. In this paper, we propose using history-aware transformations on ReID features to achieve more discriminative appearance representations. Specifically, we treat historical trajectory features as conditions and employ a tailored Fisher Linear Discriminant (FLD) to find a spatial projection matrix that maximizes the differentiation between different trajectories. Our extensive experiments reveal that this training-free projection can significantly boost feature-only trackers to achieve competitive, even superior tracking performance compared to state-of-the-art methods while also demonstrating impressive zero-shot transfer capabilities. This demonstrates the effectiveness of our proposal and further encourages future investigation into the importance and customization of ReID models in multiple object tracking. The code will be released at https://github.com/HELLORPG/HATReID-MOT.
<div id='section'>PaperID: <span id='pid'>99, <a href='https://arxiv.org/pdf/2503.12527.pdf' target='_blank'>https://arxiv.org/pdf/2503.12527.pdf</a></span>   <span><a href='https://github.com/yiyscut/VIO-IPNet.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yang Yi, Kunqing Wang, Jinpu Zhang, Zhen Tan, Xiangke Wang, Hui Shen, Dewen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12527">A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The bias of low-cost Inertial Measurement Units (IMU) is a critical factor affecting the performance of Visual-Inertial Odometry (VIO). In particular, when visual tracking encounters errors, the optimized bias results may deviate significantly from the true values, adversely impacting the system's stability and localization precision. In this paper, we propose a novel plug-and-play framework featuring the Inertial Prior Network (IPNet), which is designed to accurately estimate IMU bias. Recognizing the substantial impact of initial bias errors in low-cost inertial devices on system performance, our network directly leverages raw IMU data to estimate the mean bias, eliminating the dependency on historical estimates in traditional recursive predictions and effectively preventing error propagation. Furthermore, we introduce an iterative approach to calculate the mean value of the bias for network training, addressing the lack of bias labels in many visual-inertial datasets. The framework is evaluated on two public datasets and one self-collected dataset. Extensive experiments demonstrate that our method significantly enhances both localization precision and robustness, with the ATE-RMSE metric improving on average by 46\%. The source code and video will be available at \textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.
<div id='section'>PaperID: <span id='pid'>100, <a href='https://arxiv.org/pdf/2503.12006.pdf' target='_blank'>https://arxiv.org/pdf/2503.12006.pdf</a></span>   <span><a href='https://github.com/ShanZard/ROS-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhe Shan, Yang Liu, Lei Zhou, Cheng Yan, Heng Wang, Xia Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12006">ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of large-scale remote sensing video data underscores the importance of high-quality interactive segmentation. However, challenges such as small object sizes, ambiguous features, and limited generalization make it difficult for current methods to achieve this goal. In this work, we propose ROS-SAM, a method designed to achieve high-quality interactive segmentation while preserving generalization across diverse remote sensing data. The ROS-SAM is built upon three key innovations: 1) LoRA-based fine-tuning, which enables efficient domain adaptation while maintaining SAM's generalization ability, 2) Enhancement of deep network layers to improve the discriminability of extracted features, thereby reducing misclassifications, and 3) Integration of global context with local boundary details in the mask decoder to generate high-quality segmentation masks. Additionally, we design the data pipeline to ensure the model learns to better handle objects at varying scales during training while focusing on high-quality predictions during inference. Experiments on remote sensing video datasets show that the redesigned data pipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally, when evaluated on existing remote sensing object tracking datasets, ROS-SAM demonstrates impressive zero-shot capabilities, generating masks that closely resemble manual annotations. These results confirm ROS-SAM as a powerful tool for fine-grained segmentation in remote sensing applications. Code is available at https://github.com/ShanZard/ROS-SAM.
<div id='section'>PaperID: <span id='pid'>101, <a href='https://arxiv.org/pdf/2503.10616.pdf' target='_blank'>https://arxiv.org/pdf/2503.10616.pdf</a></span>   <span><a href='https://github.com/jinyanglii/OVTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jinyang Li, En Yu, Sijia Chen, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10616">OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary multiple object tracking aims to generalize trackers to unseen categories during training, enabling their application across a variety of real-world scenarios. However, the existing open-vocabulary tracker is constrained by its framework structure, isolated frame-level perception, and insufficient modal interactions, which hinder its performance in open-vocabulary classification and tracking. In this paper, we propose OVTR (End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the first end-to-end open-vocabulary tracker that models motion, appearance, and category simultaneously. To achieve stable classification and continuous tracking, we design the CIP (Category Information Propagation) strategy, which establishes multiple high-level category information priors for subsequent frames. Additionally, we introduce a dual-branch structure for generalization capability and deep multimodal interaction, and incorporate protective strategies in the decoder to enhance performance. Experimental results show that our method surpasses previous trackers on the open-vocabulary MOT benchmark while also achieving faster inference speeds and significantly reducing preprocessing requirements. Moreover, the experiment transferring the model to another dataset demonstrates its strong adaptability. Models and code are released at https://github.com/jinyanglii/OVTR.
<div id='section'>PaperID: <span id='pid'>102, <a href='https://arxiv.org/pdf/2503.08471.pdf' target='_blank'>https://arxiv.org/pdf/2503.08471.pdf</a></span>   <span><a href='https://github.com/Tsinghua-MARS-Lab/TrackOcc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhuoguang Chen, Kenan Li, Xiuyu Yang, Tao Jiang, Yiming Li, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08471">TrackOcc: Camera-based 4D Panoptic Occupancy Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehensive and consistent dynamic scene understanding from camera input is essential for advanced autonomous systems. Traditional camera-based perception tasks like 3D object tracking and semantic occupancy prediction lack either spatial comprehensiveness or temporal consistency. In this work, we introduce a brand-new task, Camera-based 4D Panoptic Occupancy Tracking, which simultaneously addresses panoptic occupancy segmentation and object tracking from camera-only input. Furthermore, we propose TrackOcc, a cutting-edge approach that processes image inputs in a streaming, end-to-end manner with 4D panoptic queries to address the proposed task. Leveraging the localization-aware loss, TrackOcc enhances the accuracy of 4D panoptic occupancy tracking without bells and whistles. Experimental results demonstrate that our method achieves state-of-the-art performance on the Waymo dataset. The source code will be released at https://github.com/Tsinghua-MARS-Lab/TrackOcc.
<div id='section'>PaperID: <span id='pid'>103, <a href='https://arxiv.org/pdf/2503.06625.pdf' target='_blank'>https://arxiv.org/pdf/2503.06625.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/SGLATrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chaocan Xue, Bineng Zhong, Qihua Liang, Yaozong Zheng, Ning Li, Yuanliang Xue, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06625">Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision transformers (ViTs) have emerged as a popular backbone for visual tracking. However, complete ViT architectures are too cumbersome to deploy for unmanned aerial vehicle (UAV) tracking which extremely emphasizes efficiency. In this study, we discover that many layers within lightweight ViT-based trackers tend to learn relatively redundant and repetitive target representations. Based on this observation, we propose a similarity-guided layer adaptation approach to optimize the structure of ViTs. Our approach dynamically disables a large number of representation-similar layers and selectively retains only a single optimal layer among them, aiming to achieve a better accuracy-speed trade-off. By incorporating this approach into existing ViTs, we tailor previously complete ViT architectures into an efficient similarity-guided layer-adaptive framework, namely SGLATrack, for real-time UAV tracking. Extensive experiments on six tracking benchmarks verify the effectiveness of the proposed approach, and show that our SGLATrack achieves a state-of-the-art real-time speed while maintaining competitive tracking precision. Codes and models are available at https://github.com/GXNU-ZhongLab/SGLATrack.
<div id='section'>PaperID: <span id='pid'>104, <a href='https://arxiv.org/pdf/2503.04565.pdf' target='_blank'>https://arxiv.org/pdf/2503.04565.pdf</a></span>   <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04565">Omnidirectional Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic imagery, with its 360Â° field of view, offers comprehensive information to support Multi-Object Tracking (MOT) in capturing spatial and temporal relationships of surrounding objects. However, most MOT algorithms are tailored for pinhole images with limited views, impairing their effectiveness in panoramic settings. Additionally, panoramic image distortions, such as resolution loss, geometric deformation, and uneven lighting, hinder direct adaptation of existing MOT methods, leading to significant performance degradation. To address these challenges, we propose OmniTrack, an omnidirectional MOT framework that incorporates Tracklet Management to introduce temporal cues, FlexiTrack Instances for object localization and association, and the CircularStatE Module to alleviate image and geometric distortions. This integration enables tracking in panoramic field-of-view scenarios, even under rapid sensor motion. To mitigate the lack of panoramic MOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as panoramic fields of view, intense motion, and complex environments. Extensive experiments on the public JRDB dataset and the newly introduced QuadTrack benchmark demonstrate the state-of-the-art performance of the proposed framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the baseline by 6.81%. The established dataset and source code are available at https://github.com/xifen523/OmniTrack.
<div id='section'>PaperID: <span id='pid'>105, <a href='https://arxiv.org/pdf/2503.04322.pdf' target='_blank'>https://arxiv.org/pdf/2503.04322.pdf</a></span>   <span><a href='https://github.com/LarsBredereke/object_tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lars Bredereke, Yale Hartmann, Tanja Schultz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04322">A Modular Pipeline for 3D Object Tracking Using RGB Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a key challenge of computer vision with various applications that all require different architectures. Most tracking systems have limitations such as constraining all movement to a 2D plane and they often track only one object. In this paper, we present a new modular pipeline that calculates 3D trajectories of multiple objects. It is adaptable to various settings where multiple time-synced and stationary cameras record moving objects, using off the shelf webcams. Our pipeline was tested on the Table Setting Dataset, where participants are recorded with various sensors as they set a table with tableware objects. We need to track these manipulated objects, using 6 rgb webcams. Challenges include: Detecting small objects in 9.874.699 camera frames, determining camera poses, discriminating between nearby and overlapping objects, temporary occlusions, and finally calculating a 3D trajectory using the right subset of an average of 11.12.456 pixel coordinates per 3-minute trial. We implement a robust pipeline that results in accurate trajectories with covariance of x,y,z-position as a confidence metric. It deals dynamically with appearing and disappearing objects, instantiating new Extended Kalman Filters. It scales to hundreds of table-setting trials with very little human annotation input, even with the camera poses of each trial unknown. The code is available at https://github.com/LarsBredereke/object_tracking
<div id='section'>PaperID: <span id='pid'>106, <a href='https://arxiv.org/pdf/2503.00516.pdf' target='_blank'>https://arxiv.org/pdf/2503.00516.pdf</a></span>   <span><a href='https://github.com/jiawen-zhu/AsymTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiawen Zhu, Huayi Tang, Xin Chen, Xinying Wang, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00516">Two-stream Beats One-stream: Asymmetric Siamese Network for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient tracking has garnered attention for its ability to operate on resource-constrained platforms for real-world deployment beyond desktop GPUs. Current efficient trackers mainly follow precision-oriented trackers, adopting a one-stream framework with lightweight modules. However, blindly adhering to the one-stream paradigm may not be optimal, as incorporating template computation in every frame leads to redundancy, and pervasive semantic interaction between template and search region places stress on edge devices. In this work, we propose a novel asymmetric Siamese tracker named \textbf{AsymTrack} for efficient tracking. AsymTrack disentangles template and search streams into separate branches, with template computing only once during initialization to generate modulation signals. Building on this architecture, we devise an efficient template modulation mechanism to unidirectional inject crucial cues into the search features, and design an object perception enhancement module that integrates abstract semantics and local details to overcome the limited representation in lightweight tracker. Extensive experiments demonstrate that AsymTrack offers superior speed-precision trade-offs across different platforms compared to the current state-of-the-arts. For instance, AsymTrack-T achieves 60.8\% AUC on LaSOT and 224/81/84 FPS on GPU/CPU/AGX, surpassing HiT-Tiny by 6.0\% AUC with higher speeds. The code is available at https://github.com/jiawen-zhu/AsymTrack.
<div id='section'>PaperID: <span id='pid'>107, <a href='https://arxiv.org/pdf/2502.20111.pdf' target='_blank'>https://arxiv.org/pdf/2502.20111.pdf</a></span>   <span><a href='https://mii-laboratory.github.io/MITracker/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20111">MITracker: Multi-View Integration for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a bird's eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance. The code and the new dataset will be available at https://mii-laboratory.github.io/MITracker/.
<div id='section'>PaperID: <span id='pid'>108, <a href='https://arxiv.org/pdf/2502.18220.pdf' target='_blank'>https://arxiv.org/pdf/2502.18220.pdf</a></span>   <span><a href='https://github.com/wanghe/UASTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>He Wang, Tianyang Xu, Zhangyong Tang, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18220">UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal tracking is essential in single-object tracking (SOT), as different sensor types contribute unique capabilities to overcome challenges caused by variations in object appearance. However, existing unified RGB-X trackers (X represents depth, event, or thermal modality) either rely on the task-specific training strategy for individual RGB-X image pairs or fail to address the critical importance of modality-adaptive perception in real-world applications. In this work, we propose UASTrack, a unified adaptive selection framework that facilitates both model and parameter unification, as well as adaptive modality discrimination across various multi-modal tracking tasks. To achieve modality-adaptive perception in joint RGB-X pairs, we design a Discriminative Auto-Selector (DAS) capable of identifying modality labels, thereby distinguishing the data distributions of auxiliary modalities. Furthermore, we propose a Task-Customized Optimization Adapter (TCOA) tailored to various modalities in the latent space. This strategy effectively filters noise redundancy and mitigates background interference based on the specific characteristics of each modality. Extensive comparisons conducted on five benchmarks including LasHeR, GTOT, RGBT234, VisEvent, and DepthTrack, covering RGB-T, RGB-E, and RGB-D tracking scenarios, demonstrate our innovative approach achieves comparative performance by introducing only additional training parameters of 1.87M and flops of 1.95G. The code will be available at https://github.com/wanghe/UASTrack.
<div id='section'>PaperID: <span id='pid'>109, <a href='https://arxiv.org/pdf/2502.16809.pdf' target='_blank'>https://arxiv.org/pdf/2502.16809.pdf</a></span>   <span><a href='https://github.com/ZJZhao123/CRTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zijing Zhao, Jianlong Yu, Lin Zhang, Shunli Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16809">CRTrack: Low-Light Semi-Supervised Multi-object Tracking Based on Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking under low-light environments is prevalent in real life. Recent years have seen rapid development in the field of multi-object tracking. However, due to the lack of datasets and the high cost of annotations, multi-object tracking under low-light environments remains a persistent challenge. In this paper, we focus on multi-object tracking under low-light conditions. To address the issues of limited data and the lack of dataset, we first constructed a low-light multi-object tracking dataset (LLMOT). This dataset comprises data from MOT17 that has been enhanced for nighttime conditions as well as multiple unannotated low-light videos. Subsequently, to tackle the high annotation costs and address the issue of image quality degradation, we propose a semi-supervised multi-object tracking method based on consistency regularization named CRTrack. First, we calibrate a consistent adaptive sampling assignment to replace the static IoU-based strategy, enabling the semi-supervised tracking method to resist noisy pseudo-bounding boxes. Then, we design a adaptive semi-supervised network update method, which effectively leverages unannotated data to enhance model performance. Dataset and Code: https://github.com/ZJZhao123/CRTrack.
<div id='section'>PaperID: <span id='pid'>110, <a href='https://arxiv.org/pdf/2502.09672.pdf' target='_blank'>https://arxiv.org/pdf/2502.09672.pdf</a></span>   <span><a href='https://github.com/Ap01lo/IMM-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaohong Liu, Xulong Zhao, Gang Liu, Zili Wu, Tao Wang, Lei Meng, Yuhan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09672">IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT) provides the trajectories of surrounding objects, assisting robots or vehicles in smarter path planning and obstacle avoidance. Existing 3D MOT methods based on the Tracking-by-Detection framework typically use a single motion model to track an object throughout its entire tracking process. However, objects may change their motion patterns due to variations in the surrounding environment. In this paper, we introduce the Interacting Multiple Model filter in IMM-MOT, which accurately fits the complex motion patterns of individual objects, overcoming the limitation of single-model tracking in existing approaches. In addition, we incorporate a Damping Window mechanism into the trajectory lifecycle management, leveraging the continuous association status of trajectories to control their creation and termination, reducing the occurrence of overlooked low-confidence true targets. Furthermore, we propose the Distance-Based Score Enhancement module, which enhances the differentiation between false positives and true positives by adjusting detection scores, thereby improving the effectiveness of the Score Filter. On the NuScenes Val dataset, IMM-MOT outperforms most other single-modal models using 3D point clouds, achieving an AMOTA of 73.8%. Our project is available at https://github.com/Ap01lo/IMM-MOT.
<div id='section'>PaperID: <span id='pid'>111, <a href='https://arxiv.org/pdf/2502.05574.pdf' target='_blank'>https://arxiv.org/pdf/2502.05574.pdf</a></span>   <span><a href='https://github.com/Event-AHU/EventVOT_Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiao Wang, Xiao Wang, Chao Wang, Liye Jin, Lin Zhu, Bo Jiang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05574">Event Stream-based Visual Object Tracking: HDETrack V2 and A High-Definition Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We then introduce a novel hierarchical knowledge distillation strategy that incorporates the similarity matrix, feature representation, and response map-based distillation to guide the learning of the student Transformer network. We also enhance the model's ability to capture temporal dependencies by applying the temporal Fourier transform to establish temporal relationships between video frames. We adapt the network model to specific target objects during testing via a newly proposed test-time tuning strategy to achieve high performance and flexibility in target tracking. Recognizing the limitations of existing event-based tracking datasets, which are predominantly low-resolution, we propose EventVOT, the first large-scale high-resolution event-based tracking dataset. It comprises 1141 videos spanning diverse categories such as pedestrians, vehicles, UAVs, ping pong, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, FELT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. Both the benchmark dataset and source code have been released on https://github.com/Event-AHU/EventVOT_Benchmark
<div id='section'>PaperID: <span id='pid'>112, <a href='https://arxiv.org/pdf/2501.12386.pdf' target='_blank'>https://arxiv.org/pdf/2501.12386.pdf</a></span>   <span><a href='https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12386">InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to improve the performance of video multimodal large language models (MLLM) via long and rich context (LRC) modeling. As a result, we develop a new version of InternVideo2.5 with a focus on enhancing the original MLLMs' ability to perceive fine-grained details and capture long-form temporal structure in videos. Specifically, our approach incorporates dense vision task annotations into MLLMs using direct preference optimization and develops compact spatiotemporal representations through adaptive hierarchical token compression. Experimental results demonstrate this unique design of LRC greatly improves the results of video MLLM in mainstream video understanding benchmarks (short & long), enabling the MLLM to memorize significantly longer video inputs (at least 6x longer than the original), and master specialized vision capabilities like object tracking and segmentation. Our work highlights the importance of multimodal context richness (length and fineness) in empowering MLLM's innate abilites (focus and memory), providing new insights for future research on video MLLM. Code and models are available at https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5
<div id='section'>PaperID: <span id='pid'>113, <a href='https://arxiv.org/pdf/2501.11288.pdf' target='_blank'>https://arxiv.org/pdf/2501.11288.pdf</a></span>   <span><a href='https://github.com/Wangyc2000/PD_SORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanchao Wang, Dawei Zhang, Run Li, Zhonglong Zheng, Minglu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11288">PD-SORT: Occlusion-Robust Multi-Object Tracking Using Pseudo-Depth Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a rising topic in video processing technologies and has important application value in consumer electronics. Currently, tracking-by-detection (TBD) is the dominant paradigm for MOT, which performs target detection and association frame by frame. However, the association performance of TBD methods degrades in complex scenes with heavy occlusions, which hinders the application of such methods in real-world scenarios.To this end, we incorporate pseudo-depth cues to enhance the association performance and propose Pseudo-Depth SORT (PD-SORT). First, we extend the Kalman filter state vector with pseudo-depth states. Second, we introduce a novel depth volume IoU (DVIoU) by combining the conventional 2D IoU with pseudo-depth. Furthermore, we develop a quantized pseudo-depth measurement (QPDM) strategy for more robust data association. Besides, we also integrate camera motion compensation (CMC) to handle dynamic camera situations. With the above designs, PD-SORT significantly alleviates the occlusion-induced ambiguous associations and achieves leading performances on DanceTrack, MOT17, and MOT20. Note that the improvement is especially obvious on DanceTrack, where objects show complex motions, similar appearances, and frequent occlusions. The code is available at https://github.com/Wangyc2000/PD_SORT.
<div id='section'>PaperID: <span id='pid'>114, <a href='https://arxiv.org/pdf/2501.07554.pdf' target='_blank'>https://arxiv.org/pdf/2501.07554.pdf</a></span>   <span><a href='https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Varun Biyyala, Bharat Chanderprakash Kathuria, Jialu Li, Youshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07554">SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video editing models have advanced significantly, but evaluating their performance remains challenging. Traditional metrics, such as CLIP text and image scores, often fall short: text scores are limited by inadequate training data and hierarchical dependencies, while image scores fail to assess temporal consistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation Metric), a novel evaluation framework that leverages modern Vision-Language Models (VLMs), Object Detection, and Temporal Consistency checks. SST-EM comprises four components: (1) semantic extraction from frames using a VLM, (2) primary object tracking with Object Detection, (3) focused object refinement via an LLM agent, and (4) temporal consistency assessment using a Vision Transformer (ViT). These components are integrated into a unified metric with weights derived from human evaluations and regression analysis. The name SST-EM reflects its focus on Semantic, Spatial, and Temporal aspects of video evaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and temporal smoothness in video editing. The source code is available in the \textbf{\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub Repository}}.
<div id='section'>PaperID: <span id='pid'>115, <a href='https://arxiv.org/pdf/2501.07360.pdf' target='_blank'>https://arxiv.org/pdf/2501.07360.pdf</a></span>   <span><a href='https://github.com/timbervision/timbervision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Daniel Steininger, Julia Simon, Andreas Trondl, Markus Murschitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07360">TimberVision: A Multi-Task Dataset and Framework for Log-Component Segmentation and Tracking in Autonomous Forestry Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Timber represents an increasingly valuable and versatile resource. However, forestry operations such as harvesting, handling and measuring logs still require substantial human labor in remote environments posing significant safety risks. Progressively automating these tasks has the potential of increasing their efficiency as well as safety, but requires an accurate detection of individual logs as well as live trees and their context. Although initial approaches have been proposed for this challenging application domain, specialized data and algorithms are still too scarce to develop robust solutions. To mitigate this gap, we introduce the TimberVision dataset, consisting of more than 2k annotated RGB images containing a total of 51k trunk components including cut and lateral surfaces, thereby surpassing any existing dataset in this domain in terms of both quantity and detail by a large margin. Based on this data, we conduct a series of ablation experiments for oriented object detection and instance segmentation and evaluate the influence of multiple scene parameters on model performance. We introduce a generic framework to fuse the components detected by our models for both tasks into unified trunk representations. Furthermore, we automatically derive geometric properties and apply multi-object tracking to further enhance robustness. Our detection and tracking approach provides highly descriptive and accurate trunk representations solely from RGB image data, even under challenging environmental conditions. Our solution is suitable for a wide range of application scenarios and can be readily combined with other sensor modalities.
<div id='section'>PaperID: <span id='pid'>116, <a href='https://arxiv.org/pdf/2501.05453.pdf' target='_blank'>https://arxiv.org/pdf/2501.05453.pdf</a></span>   <span><a href='https://brjathu.github.io/toto/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jathushan Rajasegaran, Ilija Radosavovic, Rahul Ravishankar, Yossi Gandelsman, Christoph Feichtenhofer, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05453">An Empirical Study of Autoregressive Pre-training from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/
<div id='section'>PaperID: <span id='pid'>117, <a href='https://arxiv.org/pdf/2501.01275.pdf' target='_blank'>https://arxiv.org/pdf/2501.01275.pdf</a></span>   <span><a href='https://github.com/leandro-svg/HybridTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Leandro Di Bella, Yangxintong Lyu, Bruno Cornelis, Adrian Munteanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01275">HybridTrack: A Hybrid Approach for Robust Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of Advanced Driver Assistance Systems (ADAS) has increased the need for robust and generalizable algorithms for multi-object tracking. Traditional statistical model-based tracking methods rely on predefined motion models and assumptions about system noise distributions. Although computationally efficient, they often lack adaptability to varying traffic scenarios and require extensive manual design and parameter tuning. To address these issues, we propose a novel 3D multi-object tracking approach for vehicles, HybridTrack, which integrates a data-driven Kalman Filter (KF) within a tracking-by-detection paradigm. In particular, it learns the transition residual and Kalman gain directly from data, which eliminates the need for manual motion and stochastic parameter modeling. Validated on the real-world KITTI dataset, HybridTrack achieves 82.72% HOTA accuracy, significantly outperforming state-of-the-art methods. We also evaluate our method under different configurations, achieving the fastest processing speed of 112 FPS. Consequently, HybridTrack eliminates the dependency on scene-specific designs while improving performance and maintaining real-time efficiency. The code is publicly available at: https://github.com/leandro-svg/HybridTrack.
<div id='section'>PaperID: <span id='pid'>118, <a href='https://arxiv.org/pdf/2412.20002.pdf' target='_blank'>https://arxiv.org/pdf/2412.20002.pdf</a></span>   <span><a href='https://github.com/wuyou3474/AVTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>You Wu, Yongxin Li, Mengyuan Liu, Xucheng Wang, Xiangyang Yang, Hengzhou Ye, Dan Zeng, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20002">Learning an Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based models have improved visual tracking, but most still cannot run in real time on resource-limited devices, especially for unmanned aerial vehicle (UAV) tracking. To achieve a better balance between performance and efficiency, we propose AVTrack, an adaptive computation tracking framework that adaptively activates transformer blocks through an Activation Module (AM), which dynamically optimizes the ViT architecture by selectively engaging relevant components. To address extreme viewpoint variations, we propose to learn view-invariant representations via mutual information (MI) maximization. In addition, we propose AVTrack-MD, an enhanced tracker incorporating a novel MI maximization-based multi-teacher knowledge distillation framework. Leveraging multiple off-the-shelf AVTrack models as teachers, we maximize the MI between their aggregated softened features and the corresponding softened feature of the student model, improving the generalization and performance of the student, especially under noisy conditions. Extensive experiments show that AVTrack-MD achieves performance comparable to AVTrack's performance while reducing model complexity and boosting average tracking speed by over 17\%. Codes is available at: https://github.com/wuyou3474/AVTrack.
<div id='section'>PaperID: <span id='pid'>119, <a href='https://arxiv.org/pdf/2412.17807.pdf' target='_blank'>https://arxiv.org/pdf/2412.17807.pdf</a></span>   <span><a href='https://github.com/chen-si-jia/CRMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Sijia Chen, En Yu, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17807">Cross-View Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) is an important topic in the current tracking field. Its task form is to guide the tracker to track objects that match the language description. Current research mainly focuses on referring multi-object tracking under single-view, which refers to a view sequence or multiple unrelated view sequences. However, in the single-view, some appearances of objects are easily invisible, resulting in incorrect matching of objects with the language description. In this work, we propose a new task, called Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the cross-view to obtain the appearances of objects from multiple views, avoiding the problem of the invisible appearances of objects in RMOT task. CRMOT is a more challenging task of accurately tracking the objects that match the language description and maintaining the identity consistency of objects in each cross-view. To advance CRMOT task, we construct a cross-view referring multi-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named CRTrack. Specifically, it provides 13 different scenes and 221 language descriptions. Furthermore, we propose an end-to-end cross-view referring multi-object tracking method, named CRTracker. Extensive experiments on the CRTrack benchmark verify the effectiveness of our method. The dataset and code are available at https://github.com/chen-si-jia/CRMOT.
<div id='section'>PaperID: <span id='pid'>120, <a href='https://arxiv.org/pdf/2412.15691.pdf' target='_blank'>https://arxiv.org/pdf/2412.15691.pdf</a></span>   <span><a href='https://github.com/NJU-PCALab/STTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiantao Hu, Ying Tai, Xu Zhao, Chen Zhao, Zhenyu Zhang, Jun Li, Bineng Zhong, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15691">Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal tracking has garnered widespread attention as a result of its ability to effectively address the inherent limitations of traditional RGB tracking. However, existing multimodal trackers mainly focus on the fusion and enhancement of spatial features or merely leverage the sparse temporal relationships between video frames. These approaches do not fully exploit the temporal correlations in multimodal videos, making it difficult to capture the dynamic changes and motion information of targets in complex scenarios. To alleviate this problem, we propose a unified multimodal spatial-temporal tracking approach named STTrack. In contrast to previous paradigms that solely relied on updating reference information, we introduced a temporal state generator (TSG) that continuously generates a sequence of tokens containing multimodal temporal information. These temporal information tokens are used to guide the localization of the target in the next time state, establish long-range contextual relationships between video frames, and capture the temporal trajectory of the target. Furthermore, at the spatial level, we introduced the mamba fusion and background suppression interactive (BSI) modules. These modules establish a dual-stage mechanism for coordinating information interaction and fusion between modalities. Extensive comparisons on five benchmark datasets illustrate that STTrack achieves state-of-the-art performance across various multimodal tracking scenarios. Code is available at: https://github.com/NJU-PCALab/STTrack.
<div id='section'>PaperID: <span id='pid'>121, <a href='https://arxiv.org/pdf/2412.15212.pdf' target='_blank'>https://arxiv.org/pdf/2412.15212.pdf</a></span>   <span><a href='https://github.com/google-deepmind/representations4d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>JoÃ£o Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro VÃ©lez, Luisa PolanÃ­a, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica PÄtrÄucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15212">Scaling 4D Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling has not yet been convincingly demonstrated for pure self-supervised learning from video. However, prior work has focused evaluations on semantic-related tasks $\unicode{x2013}$ action classification, ImageNet classification, etc. In this paper we focus on evaluating self-supervised learning on non-semantic vision tasks that are more spatial (3D) and temporal (+1D = 4D), such as camera pose estimation, point and object tracking, and depth estimation. We show that by learning from very large video datasets, masked auto-encoding (MAE) with transformer video models actually scales, consistently improving performance on these 4D tasks, as model size increases from 20M all the way to the largest by far reported self-supervised video model $\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with many recent image and video models demonstrates the benefits of scaling 4D representations. Pretrained models are available at https://github.com/google-deepmind/representations4d .
<div id='section'>PaperID: <span id='pid'>122, <a href='https://arxiv.org/pdf/2412.11023.pdf' target='_blank'>https://arxiv.org/pdf/2412.11023.pdf</a></span>   <span><a href='https://github.com/kangben258/MCITrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ben Kang, Xin Chen, Simiao Lai, Yang Liu, Yi Liu, Dong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11023">Exploring Enhanced Contextual Information for Video-Level Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contextual information at the video level has become increasingly crucial for visual object tracking. However, existing methods typically use only a few tokens to convey this information, which can lead to information loss and limit their ability to fully capture the context. To address this issue, we propose a new video-level visual object tracking framework called MCITrack. It leverages Mamba's hidden states to continuously record and transmit extensive contextual information throughout the video stream, resulting in more robust object tracking. The core component of MCITrack is the Contextual Information Fusion module, which consists of the mamba layer and the cross-attention layer. The mamba layer stores historical contextual information, while the cross-attention layer integrates this information into the current visual features of each backbone block. This module enhances the model's ability to capture and utilize contextual information at multiple levels through deep integration with the backbone. Experiments demonstrate that MCITrack achieves competitive performance across numerous benchmarks. For instance, it gets 76.6% AUC on LaSOT and 80.0% AO on GOT-10k, establishing a new state-of-the-art performance. Code and models are available at https://github.com/kangben258/MCITrack.
<div id='section'>PaperID: <span id='pid'>123, <a href='https://arxiv.org/pdf/2412.10861.pdf' target='_blank'>https://arxiv.org/pdf/2412.10861.pdf</a></span>   <span><a href='https://github.com/xuqingyu26/HGTMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Qingyu Xu, Longguang Wang, Weidong Sheng, Yingqian Wang, Chao Xiao, Chao Ma, Wei An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10861">Heterogeneous Graph Transformer for Multiple Tiny Object Tracking in RGB-T Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple tiny objects is highly challenging due to their weak appearance and limited features. Existing multi-object tracking algorithms generally focus on single-modality scenes, and overlook the complementary characteristics of tiny objects captured by multiple remote sensors. To enhance tracking performance by integrating complementary information from multiple sources, we propose a novel framework called {HGT-Track (Heterogeneous Graph Transformer based Multi-Tiny-Object Tracking)}. Specifically, we first employ a Transformer-based encoder to embed images from different modalities. Subsequently, we utilize Heterogeneous Graph Transformer to aggregate spatial and temporal information from multiple modalities to generate detection and tracking features. Additionally, we introduce a target re-detection module (ReDet) to ensure tracklet continuity by maintaining consistency across different modalities. Furthermore, this paper introduces the first benchmark VT-Tiny-MOT (Visible-Thermal Tiny Multi-Object Tracking) for RGB-T fused multiple tiny object tracking. Extensive experiments are conducted on VT-Tiny-MOT, and the results have demonstrated the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of MOTA (Multiple-Object Tracking Accuracy) and ID-F1 score. The code and dataset will be made available at https://github.com/xuqingyu26/HGTMT.
<div id='section'>PaperID: <span id='pid'>124, <a href='https://arxiv.org/pdf/2412.09617.pdf' target='_blank'>https://arxiv.org/pdf/2412.09617.pdf</a></span>   <span><a href='https://joehjhuang.github.io/normalflow' target='_blank'>  GitHub</a></span> <span><a href='https://joehjhuang.github.io/normalflow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hung-Jui Huang, Michael Kaess, Wenzhen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09617">NormalFlow: Fast, Robust, and Accurate Contact-based Object 6DoF Pose Tracking with Vision-based Tactile Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing is crucial for robots aiming to achieve human-level dexterity. Among tactile-dependent skills, tactile-based object tracking serves as the cornerstone for many tasks, including manipulation, in-hand manipulation, and 3D reconstruction. In this work, we introduce NormalFlow, a fast, robust, and real-time tactile-based 6DoF tracking algorithm. Leveraging the precise surface normal estimation of vision-based tactile sensors, NormalFlow determines object movements by minimizing discrepancies between the tactile-derived surface normals. Our results show that NormalFlow consistently outperforms competitive baselines and can track low-texture objects like table surfaces. For long-horizon tracking, we demonstrate when rolling the sensor around a bead for 360 degrees, NormalFlow maintains a rotational tracking error of 2.5 degrees. Additionally, we present state-of-the-art tactile-based 3D reconstruction results, showcasing the high accuracy of NormalFlow. We believe NormalFlow unlocks new possibilities for high-precision perception and manipulation tasks that involve interacting with objects using hands. The video demo, code, and dataset are available on our website: https://joehjhuang.github.io/normalflow.
<div id='section'>PaperID: <span id='pid'>125, <a href='https://arxiv.org/pdf/2412.05548.pdf' target='_blank'>https://arxiv.org/pdf/2412.05548.pdf</a></span>   <span><a href='https://lolrudy.github.io/No3DTrackSG/' target='_blank'>  GitHub</a></span> <span><a href='https://lolrudy.github.io/No3DTrackSG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruida Zhang, Chengxi Li, Chenyangguang Zhang, Xingyu Liu, Haili Yuan, Yanyan Li, Xiangyang Ji, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05548">Street Gaussians without 3D Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic scene reconstruction in driving scenarios poses significant challenges due to fast-moving objects. Most existing methods rely on labor-intensive manual labeling of object poses to reconstruct dynamic objects in canonical space and move them based on these poses during rendering. While some approaches attempt to use 3D object trackers to replace manual annotations, the limited generalization of 3D trackers -- caused by the scarcity of large-scale 3D datasets -- results in inferior reconstructions in real-world settings. In contrast, 2D foundation models demonstrate strong generalization capabilities. To eliminate the reliance on 3D trackers and enhance robustness across diverse environments, we propose a stable object tracking module by leveraging associations from 2D deep trackers within a 3D object fusion strategy. We address inevitable tracking errors by further introducing a motion learning strategy in an implicit feature space that autonomously corrects trajectory errors and recovers missed detections. Experimental results on Waymo-NOTR and KITTI show that our method outperforms existing approaches. Our code will be released on https://lolrudy.github.io/No3DTrackSG/.
<div id='section'>PaperID: <span id='pid'>126, <a href='https://arxiv.org/pdf/2412.04945.pdf' target='_blank'>https://arxiv.org/pdf/2412.04945.pdf</a></span>   <span><a href='https://github.com/mschwimmbeck/HOLa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Michael Schwimmbeck, Serouj Khajarian, Konstantin Holzapfel, Johannes Schmidt, Stefanie Remmele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04945">HOLa: HoloLens Object Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of medical Augmented Reality (AR) applications, object tracking is a key challenge and requires a significant amount of annotation masks. As segmentation foundation models like the Segment Anything Model (SAM) begin to emerge, zero-shot segmentation requires only minimal human participation obtaining high-quality object masks. We introduce a HoloLens-Object-Labeling (HOLa) Unity and Python application based on the SAM-Track algorithm that offers fully automatic single object annotation for HoloLens 2 while requiring minimal human participation. HOLa does not have to be adjusted to a specific image appearance and could thus alleviate AR research in any application field. We evaluate HOLa for different degrees of image complexity in open liver surgery and in medical phantom experiments. Using HOLa for image annotation can increase the labeling speed by more than 500 times while providing Dice scores between 0.875 and 0.982, which are comparable to human annotators. Our code is publicly available at: https://github.com/mschwimmbeck/HOLa
<div id='section'>PaperID: <span id='pid'>127, <a href='https://arxiv.org/pdf/2412.03512.pdf' target='_blank'>https://arxiv.org/pdf/2412.03512.pdf</a></span>   <span><a href='https://compvis.github.io/distilldift' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Frank Fundel, Johannes Schusterbauer, Vincent Tao Hu, BjÃ¶rn Ommer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03512">Distillation of Diffusion Features for Semantic Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic correspondence, the task of determining relationships between different parts of images, underpins various applications including 3D reconstruction, image-to-image translation, object tracking, and visual place recognition. Recent studies have begun to explore representations learned in large generative image models for semantic correspondence, demonstrating promising results. Building on this progress, current state-of-the-art methods rely on combining multiple large models, resulting in high computational demands and reduced efficiency. In this work, we address this challenge by proposing a more computationally efficient approach. We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency. We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost. Furthermore, we demonstrate that by incorporating 3D data, we are able to further improve performance, without the need for human-annotated correspondences. Overall, our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications, such as semantic video correspondence. Our code and weights are publicly available on our project page.
<div id='section'>PaperID: <span id='pid'>128, <a href='https://arxiv.org/pdf/2412.02129.pdf' target='_blank'>https://arxiv.org/pdf/2412.02129.pdf</a></span>   <span><a href='https://github.com/ailovejinx/GSOT3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yifan Jiao, Yunhao Li, Junhua Ding, Qing Yang, Song Fu, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02129">GSOT3D: Towards Generic 3D Single Object Tracking in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel benchmark, GSOT3D, that aims at facilitating development of generic 3D single object tracking (SOT) in the wild. Specifically, GSOT3D offers 620 sequences with 123K frames, and covers a wide selection of 54 object categories. Each sequence is offered with multiple modalities, including the point cloud (PC), RGB image, and depth. This allows GSOT3D to support various 3D tracking tasks, such as single-modal 3D SOT on PC and multi-modal 3D SOT on RGB-PC or RGB-D, and thus greatly broadens research directions for 3D object tracking. To provide highquality per-frame 3D annotations, all sequences are labeled manually with multiple rounds of meticulous inspection and refinement. To our best knowledge, GSOT3D is the largest benchmark dedicated to various generic 3D object tracking tasks. To understand how existing 3D trackers perform and to provide comparisons for future research on GSOT3D, we assess eight representative point cloud-based tracking models. Our evaluation results exhibit that these models heavily degrade on GSOT3D, and more efforts are required for robust and generic 3D object tracking. Besides, to encourage future research, we present a simple yet effective generic 3D tracker, named PROT3D, that localizes the target object via a progressive spatial-temporal network and outperforms all current solutions by a large margin. By releasing GSOT3D, we expect to advance further 3D tracking in future research and applications. Our benchmark and model as well as the evaluation results will be publicly released at our webpage https://github.com/ailovejinx/GSOT3D.
<div id='section'>PaperID: <span id='pid'>129, <a href='https://arxiv.org/pdf/2412.01147.pdf' target='_blank'>https://arxiv.org/pdf/2412.01147.pdf</a></span>   <span><a href='https://uark-aicv.github.io/A2VIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Minh Tran, Thang Pham, Winston Bounsavy, Tri Nguyen, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01147">A2VIS: Amodal-Aware Approach to Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Handling occlusion remains a significant challenge for video instance-level tasks like Multiple Object Tracking (MOT) and Video Instance Segmentation (VIS). In this paper, we propose a novel framework, Amodal-Aware Video Instance Segmentation (A2VIS), which incorporates amodal representations to achieve a reliable and comprehensive understanding of both visible and occluded parts of objects in a video. The key intuition is that awareness of amodal segmentation through spatiotemporal dimension enables a stable stream of object information. In scenarios where objects are partially or completely hidden from view, amodal segmentation offers more consistency and less dramatic changes along the temporal axis compared to visible segmentation. Hence, both amodal and visible information from all clips can be integrated into one global instance prototype. To effectively address the challenge of video amodal segmentation, we introduce the spatiotemporal-prior Amodal Mask Head, which leverages visible information intra clips while extracting amodal characteristics inter clips. Through extensive experiments and ablation studies, we show that A2VIS excels in both MOT and VIS tasks in identifying and tracking object instances with a keen understanding of their full shape.
<div id='section'>PaperID: <span id='pid'>130, <a href='https://arxiv.org/pdf/2412.01132.pdf' target='_blank'>https://arxiv.org/pdf/2412.01132.pdf</a></span>   <span><a href='https://github.com/joe-rabbit/VideoQA_Pilot_Study' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Joseph Raj Vishal, Divesh Basina, Aarya Choudhary, Bharatesh Chakravarthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01132">Eyes on the Road: State-of-the-Art Video Question Answering Models Assessment for Traffic Monitoring Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video question answering (VideoQA) offer promising applications, especially in traffic monitoring, where efficient video interpretation is critical. Within ITS, answering complex, real-time queries like "How many red cars passed in the last 10 minutes?" or "Was there an incident between 3:00 PM and 3:05 PM?" enhances situational awareness and decision-making. Despite progress in vision-language models, VideoQA remains challenging, especially in dynamic environments involving multiple objects and intricate spatiotemporal relationships. This study evaluates state-of-the-art VideoQA models using non-benchmark synthetic and real-world traffic sequences. The framework leverages GPT-4o to assess accuracy, relevance, and consistency across basic detection, temporal reasoning, and decomposition queries. VideoLLaMA-2 excelled with 57% accuracy, particularly in compositional reasoning and consistent answers. However, all models, including VideoLLaMA-2, faced limitations in multi-object tracking, temporal coherence, and complex scene interpretation, highlighting gaps in current architectures. These findings underscore VideoQA's potential in traffic monitoring but also emphasize the need for improvements in multi-object tracking, temporal reasoning, and compositional capabilities. Enhancing these areas could make VideoQA indispensable for incident detection, traffic flow management, and responsive urban planning. The study's code and framework are open-sourced for further exploration: https://github.com/joe-rabbit/VideoQA_Pilot_Study
<div id='section'>PaperID: <span id='pid'>131, <a href='https://arxiv.org/pdf/2411.18855.pdf' target='_blank'>https://arxiv.org/pdf/2411.18855.pdf</a></span>   <span><a href='https://wvuvl.github.io/SiamABC/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Ram Zaveri, Shivang Patel, Yu Gu, Gianfranco Doretto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18855">Improving Accuracy and Generalization for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient visual trackers overfit to their training distributions and lack generalization abilities, resulting in them performing well on their respective in-distribution (ID) test sets and not as well on out-of-distribution (OOD) sequences, imposing limitations to their deployment in-the-wild under constrained resources. We introduce SiamABC, a highly efficient Siamese tracker that significantly improves tracking performance, even on OOD sequences. SiamABC takes advantage of new architectural designs in the way it bridges the dynamic variability of the target, and of new losses for training. Also, it directly addresses OOD tracking generalization by including a fast backward-free dynamic test-time adaptation method that continuously adapts the model according to the dynamic visual changes of the target. Our extensive experiments suggest that SiamABC shows remarkable performance gains in OOD sets while maintaining accurate performance on the ID benchmarks. SiamABC outperforms MixFormerV2-S by 7.6\% on the OOD AVisT benchmark while being 3x faster (100 FPS) on a CPU. Our code and models are available at https://wvuvl.github.io/SiamABC/.
<div id='section'>PaperID: <span id='pid'>132, <a href='https://arxiv.org/pdf/2411.17576.pdf' target='_blank'>https://arxiv.org/pdf/2411.17576.pdf</a></span>   <span><a href='https://github.com/jovanavidenovic/DAM4SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jovana Videnovic, Alan Lukezic, Matej Kristan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17576">A Distractor-Aware Memory for Visual Object Tracking with SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Memory-based trackers are video object segmentation methods that form the target model by concatenating recently tracked frames into a memory buffer and localize the target by attending the current image to the buffered frames. While already achieving top performance on many benchmarks, it was the recent release of SAM2 that placed memory-based trackers into focus of the visual object tracking community. Nevertheless, modern trackers still struggle in the presence of distractors. We argue that a more sophisticated memory model is required, and propose a new distractor-aware memory model for SAM2 and an introspection-based update strategy that jointly addresses the segmentation accuracy as well as tracking robustness. The resulting tracker is denoted as SAM2.1++. We also propose a new distractor-distilled DiDi dataset to study the distractor problem better. SAM2.1++ outperforms SAM2.1 and related SAM memory extensions on seven benchmarks and sets a solid new state-of-the-art on six of them.
<div id='section'>PaperID: <span id='pid'>133, <a href='https://arxiv.org/pdf/2411.15761.pdf' target='_blank'>https://arxiv.org/pdf/2411.15761.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15761">MambaTrack: Exploiting Dual-Enhancement for Night UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Night unmanned aerial vehicle (UAV) tracking is impeded by the challenges of poor illumination, with previous daylight-optimized methods demonstrating suboptimal performance in low-light conditions, limiting the utility of UAV applications. To this end, we propose an efficient mamba-based tracker, leveraging dual enhancement techniques to boost night UAV tracking. The mamba-based low-light enhancer, equipped with an illumination estimator and a damage restorer, achieves global image enhancement while preserving the details and structure of low-light images. Additionally, we advance a cross-modal mamba network to achieve efficient interactive learning between vision and language modalities. Extensive experiments showcase that our method achieves advanced performance and exhibits significantly improved computation and memory efficiency. For instance, our method is 2.8$\times$ faster than CiteTracker and reduces 50.2$\%$ GPU memory. Our codes are available at \url{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
<div id='section'>PaperID: <span id='pid'>134, <a href='https://arxiv.org/pdf/2411.12943.pdf' target='_blank'>https://arxiv.org/pdf/2411.12943.pdf</a></span>   <span><a href='https://github.com/wassimea/thermalMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Wassim El Ahmar, Dhanvin Kolhatkar, Farzan Nowruzi, Robert Laganiere
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12943">Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) in thermal imaging presents unique challenges due to the lack of visual features and the complexity of motion patterns. This paper introduces an innovative approach to improve MOT in the thermal domain by developing a novel box association method that utilizes both thermal object identity and motion similarity. Our method merges thermal feature sparsity and dynamic object tracking, enabling more accurate and robust MOT performance. Additionally, we present a new dataset comprised of a large-scale collection of thermal and RGB images captured in diverse urban environments, serving as both a benchmark for our method and a new resource for thermal imaging. We conduct extensive experiments to demonstrate the superiority of our approach over existing methods, showing significant improvements in tracking accuracy and robustness under various conditions. Our findings suggest that incorporating thermal identity with motion data enhances MOT performance. The newly collected dataset and source code is available at https://github.com/wassimea/thermalMOT
<div id='section'>PaperID: <span id='pid'>135, <a href='https://arxiv.org/pdf/2411.11922.pdf' target='_blank'>https://arxiv.org/pdf/2411.11922.pdf</a></span>   <span><a href='https://yangchris11.github.io/samurai/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11922">SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of memories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incorporating temporal motion cues with the proposed motion-aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, showcasing its ability to generalize without fine-tuning. In evaluations, SAMURAI achieves significant improvements in success rate and precision over existing trackers, with a 7.1% AUC gain on LaSOT$_{\text{ext}}$ and a 3.5% AO gain on GOT-10k. Moreover, it achieves competitive results compared to fully supervised methods on LaSOT, underscoring its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.
<div id='section'>PaperID: <span id='pid'>136, <a href='https://arxiv.org/pdf/2411.10564.pdf' target='_blank'>https://arxiv.org/pdf/2411.10564.pdf</a></span>   <span><a href='https://github.com/MahmudulHasan11085/Vision-Eagle-Attention.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Mahmudul Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10564">Vision Eagle Attention: a new lens for advancing image classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In computer vision tasks, the ability to focus on relevant regions within an image is crucial for improving model performance, particularly when key features are small, subtle, or spatially dispersed. Convolutional neural networks (CNNs) typically treat all regions of an image equally, which can lead to inefficient feature extraction. To address this challenge, I have introduced Vision Eagle Attention, a novel attention mechanism that enhances visual feature extraction using convolutional spatial attention. The model applies convolution to capture local spatial features and generates an attention map that selectively emphasizes the most informative regions of the image. This attention mechanism enables the model to focus on discriminative features while suppressing irrelevant background information. I have integrated Vision Eagle Attention into a lightweight ResNet-18 architecture, demonstrating that this combination results in an efficient and powerful model. I have evaluated the performance of the proposed model on three widely used benchmark datasets: FashionMNIST, Intel Image Classification, and OracleMNIST, with a primary focus on image classification. Experimental results show that the proposed approach improves classification accuracy. Additionally, this method has the potential to be extended to other vision tasks, such as object detection, segmentation, and visual tracking, offering a computationally efficient solution for a wide range of vision-based applications. Code is available at: https://github.com/MahmudulHasan11085/Vision-Eagle-Attention.git
<div id='section'>PaperID: <span id='pid'>137, <a href='https://arxiv.org/pdf/2411.09551.pdf' target='_blank'>https://arxiv.org/pdf/2411.09551.pdf</a></span>   <span><a href='https://github.com/serycjon/MFTIQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jonas Serych, Michal Neoral, Jiri Matas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09551">MFTIQ: Multi-Flow Tracker with Independent Matching Quality Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present MFTIQ, a novel dense long-term tracking model that advances the Multi-Flow Tracker (MFT) framework to address challenges in point-level visual tracking in video sequences. MFTIQ builds upon the flow-chaining concepts of MFT, integrating an Independent Quality (IQ) module that separates correspondence quality estimation from optical flow computations. This decoupling significantly enhances the accuracy and flexibility of the tracking process, allowing MFTIQ to maintain reliable trajectory predictions even in scenarios of prolonged occlusions and complex dynamics. Designed to be "plug-and-play", MFTIQ can be employed with any off-the-shelf optical flow method without the need for fine-tuning or architectural modifications. Experimental validations on the TAP-Vid Davis dataset show that MFTIQ with RoMa optical flow not only surpasses MFT but also performs comparably to state-of-the-art trackers while having substantially faster processing speed. Code and models available at https://github.com/serycjon/MFTIQ .
<div id='section'>PaperID: <span id='pid'>138, <a href='https://arxiv.org/pdf/2411.08216.pdf' target='_blank'>https://arxiv.org/pdf/2411.08216.pdf</a></span>   <span><a href='https://github.com/sjc042/gta-link.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiacheng Sun, Hsiang-Wei Huang, Cheng-Yen Yang, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08216">GTA: Global Tracklet Association for Multi-Object Tracking in Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking in sports scenarios has become one of the focal points in computer vision, experiencing significant advancements through the integration of deep learning techniques. Despite these breakthroughs, challenges remain, such as accurately re-identifying players upon re-entry into the scene and minimizing ID switches. In this paper, we propose an appearance-based global tracklet association algorithm designed to enhance tracking performance by splitting tracklets containing multiple identities and connecting tracklets seemingly from the same identity. This method can serve as a plug-and-play refinement tool for any multi-object tracker to further boost their performance. The proposed method achieved a new state-of-the-art performance on the SportsMOT dataset with HOTA score of 81.04%. Similarly, on the SoccerNet dataset, our method enhanced multiple trackers' performance, consistently increasing the HOTA score from 79.41% to 83.11%. These significant and consistent improvements across different trackers and datasets underscore our proposed method's potential impact on the application of sports player tracking. We open-source our project codebase at https://github.com/sjc042/gta-link.git.
<div id='section'>PaperID: <span id='pid'>139, <a href='https://arxiv.org/pdf/2411.06378.pdf' target='_blank'>https://arxiv.org/pdf/2411.06378.pdf</a></span>   <span><a href='https://github.com/hwcao17/pkf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanwen Cao, George J. Pappas, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06378">PKF: Probabilistic Data Association Kalman Filter for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we derive a new Kalman filter with probabilistic data association between measurements and states. We formulate a variational inference problem to approximate the posterior density of the state conditioned on the measurement data. We view the unknown data association as a latent variable and apply Expectation Maximization (EM) to obtain a filter with update step in the same form as the Kalman filter but with expanded measurement vector of all potential associations. We show that the association probabilities can be computed as permanents of matrices with measurement likelihood entries. We also propose an ambiguity check that associates only a subset of ambiguous measurements and states probabilistically, thus reducing the association time and preventing low-probability measurements from harming the estimation accuracy. Experiments in simulation show that our filter achieves lower tracking errors than the well-established joint probabilistic data association filter (JPDAF), while running at comparable rate. We also demonstrate the effectiveness of our filter in multi-object tracking (MOT) on multiple real-world datasets, including MOT17, MOT20, and DanceTrack. We achieve better higher order tracking accuracy (HOTA) than previous Kalman-filter methods and remain real-time. Associating only bounding boxes without deep features or velocities, our method ranks top-10 on both MOT17 and MOT20 in terms of HOTA. Given offline detections, our algorithm tracks at 250+ fps on a single laptop CPU. Code is available at https://github.com/hwcao17/pkf.
<div id='section'>PaperID: <span id='pid'>140, <a href='https://arxiv.org/pdf/2410.20421.pdf' target='_blank'>https://arxiv.org/pdf/2410.20421.pdf</a></span>   <span><a href='https://github.com/LiuYuML/NV-VOT211' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Liu, Arif Mahmood, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20421">NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many current visual object tracking benchmarks such as OTB100, NfS, UAV123, LaSOT, and GOT-10K, predominantly contain day-time scenarios while the challenges posed by the night-time has been less investigated. It is primarily because of the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. To this end, this paper presents NT-VOT211, a new benchmark tailored for evaluating visual object tracking algorithms in the challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. To the best of our knowledge, it is the largest night-time tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and distractors inherent to night-time tracking scenarios. Through a comprehensive analysis of results obtained from 42 diverse tracking algorithms on NT-VOT211, we uncover the strengths and limitations of these algorithms, highlighting opportunities for enhancements in visual object tracking, particularly in environments with suboptimal lighting. Besides, a leaderboard for revealing performance rankings, annotation tools, comprehensive meta-information and all the necessary code for reproducibility of results is made publicly available. We believe that our NT-VOT211 benchmark will not only be instrumental in facilitating field deployment of VOT algorithms, but will also help VOT enhancements and it will unlock new real-world tracking applications. Our dataset and other assets can be found at: {https://github.com/LiuYuML/NV-VOT211.
<div id='section'>PaperID: <span id='pid'>141, <a href='https://arxiv.org/pdf/2410.20395.pdf' target='_blank'>https://arxiv.org/pdf/2410.20395.pdf</a></span>   <span><a href='https://github.com/LiuYuML/Depth-Attention' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu Liu, Arif Mahmood, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20395">Depth Attention for Robust RGB Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB video object tracking is a fundamental task in computer vision. Its effectiveness can be improved using depth information, particularly for handling motion-blurred target. However, depth information is often missing in commonly used tracking benchmarks. In this work, we propose a new framework that leverages monocular depth estimation to counter the challenges of tracking targets that are out of view or affected by motion blur in RGB video sequences. Specifically, our work introduces following contributions. To the best of our knowledge, we are the first to propose a depth attention mechanism and to formulate a simple framework that allows seamlessly integration of depth information with state of the art tracking algorithms, without RGB-D cameras, elevating accuracy and robustness. We provide extensive experiments on six challenging tracking benchmarks. Our results demonstrate that our approach provides consistent gains over several strong baselines and achieves new SOTA performance. We believe that our method will open up new possibilities for more sophisticated VOT solutions in real-world scenarios. Our code and models are publicly released: https://github.com/LiuYuML/Depth-Attention.
<div id='section'>PaperID: <span id='pid'>142, <a href='https://arxiv.org/pdf/2410.17856.pdf' target='_blank'>https://arxiv.org/pdf/2410.17856.pdf</a></span>   <span><a href='https://craftjarvis.github.io/ROCKET-1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17856">ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. One critical issue is bridging the gap between discrete entities in low-level observations and the abstract concepts required for effective planning. A common solution is building hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language. However, language suffers from the inability to communicate detailed spatial information. We propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from past observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, supported by real-time object tracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to tackle complex tasks that demand spatial reasoning. Experiments in Minecraft show that our approach enables agents to achieve previously unattainable tasks, with a $\mathbf{76}\%$ absolute improvement in open-world interaction performance. Codes and demos are now available on the project page: https://craftjarvis.github.io/ROCKET-1.
<div id='section'>PaperID: <span id='pid'>143, <a href='https://arxiv.org/pdf/2410.17534.pdf' target='_blank'>https://arxiv.org/pdf/2410.17534.pdf</a></span>   <span><a href='https://github.com/Coo1Sea/OVT-B-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Haiji Liang, Ruize Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17534">OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary object perception has become an important topic in artificial intelligence, which aims to identify objects with novel classes that have not been seen during training. Under this setting, open-vocabulary object detection (OVD) in a single image has been studied in many literature. However, open-vocabulary object tracking (OVT) from a video has been studied less, and one reason is the shortage of benchmarks. In this work, we have built a new large-scale benchmark for open-vocabulary multi-object tracking namely OVT-B. OVT-B contains 1,048 categories of objects and 1,973 videos with 637,608 bounding box annotations, which is much larger than the sole open-vocabulary tracking dataset, i.e., OVTAO-val dataset (200+ categories, 900+ videos). The proposed OVT-B can be used as a new benchmark to pave the way for OVT research. We also develop a simple yet effective baseline method for OVT. It integrates the motion features for object tracking, which is an important feature for MOT but is ignored in previous OVT methods. Experimental results have verified the usefulness of the proposed benchmark and the effectiveness of our method. We have released the benchmark to the public at https://github.com/Coo1Sea/OVT-B-Dataset.
<div id='section'>PaperID: <span id='pid'>144, <a href='https://arxiv.org/pdf/2410.14977.pdf' target='_blank'>https://arxiv.org/pdf/2410.14977.pdf</a></span>   <span><a href='https://github.com/linh-gist/ms-glmb-nuScenes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Linh Van Ma, Muhammad Ishfaq Hussain, Kin-Choong Yow, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14977">3D Multi-Object Tracking Employing MS-GLMB Filter for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The MS-GLMB filter offers a robust framework for tracking multiple objects through the use of multi-sensor data. Building on this, the MV-GLMB and MV-GLMB-AB filters enhance the MS-GLMB capabilities by employing cameras for 3D multi-sensor multi-object tracking, effectively addressing occlusions. However, both filters depend on overlapping fields of view from the cameras to combine complementary information. In this paper, we introduce an improved approach that integrates an additional sensor, such as LiDAR, into the MS-GLMB framework for 3D multi-object tracking. Specifically, we present a new LiDAR measurement model, along with a multi-camera and LiDAR multi-object measurement model. Our experimental results demonstrate a significant improvement in tracking performance compared to existing MS-GLMB-based methods. Importantly, our method eliminates the need for overlapping fields of view, broadening the applicability of the MS-GLMB filter. Our source code for nuScenes dataset is available at https://github.com/linh-gist/ms-glmb-nuScenes.
<div id='section'>PaperID: <span id='pid'>145, <a href='https://arxiv.org/pdf/2410.11125.pdf' target='_blank'>https://arxiv.org/pdf/2410.11125.pdf</a></span>   <span><a href='https://huiyegit.github.io/UAV3D_Benchmark/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Hui Ye, Rajshekhar Sunderraman, Shihao Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11125">UAV3D: A Large-scale 3D Perception Benchmark for Unmanned Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs), equipped with cameras, are employed in numerous applications, including aerial photography, surveillance, and agriculture. In these applications, robust object detection and tracking are essential for the effective deployment of UAVs. However, existing benchmarks for UAV applications are mainly designed for traditional 2D perception tasks, restricting the development of real-world applications that require a 3D understanding of the environment. Furthermore, despite recent advancements in single-UAV perception, limited views of a single UAV platform significantly constrain its perception capabilities over long distances or in occluded areas. To address these challenges, we introduce UAV3D, a benchmark designed to advance research in both 3D and collaborative 3D perception tasks with UAVs. UAV3D comprises 1,000 scenes, each of which has 20 frames with fully annotated 3D bounding boxes on vehicles. We provide the benchmark for four 3D perception tasks: single-UAV 3D object detection, single-UAV object tracking, collaborative-UAV 3D object detection, and collaborative-UAV object tracking. Our dataset and code are available at https://huiyegit.github.io/UAV3D_Benchmark/.
<div id='section'>PaperID: <span id='pid'>146, <a href='https://arxiv.org/pdf/2410.10409.pdf' target='_blank'>https://arxiv.org/pdf/2410.10409.pdf</a></span>   <span><a href='https://github.com/mzahana/SMART-TRACK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Khaled Gabr, Mohamed Abdelkader, Imen Jarraya, Abdullah AlMusalami, Anis Koubaa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10409">SMART-TRACK: A Novel Kalman Filter-Guided Sensor Fusion For Robust UAV Object Tracking in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of sensor fusion and state estimation for object detection and localization, ensuring accurate tracking in dynamic environments poses significant challenges. Traditional methods like the Kalman Filter (KF) often fail when measurements are intermittent, leading to rapid divergence in state estimations. To address this, we introduce SMART (Sensor Measurement Augmentation and Reacquisition Tracker), a novel approach that leverages high-frequency state estimates from the KF to guide the search for new measurements, maintaining tracking continuity even when direct measurements falter. This is crucial for dynamic environments where traditional methods struggle. Our contributions include: 1) Versatile Measurement Augmentation Using KF Feedback: We implement a versatile measurement augmentation system that serves as a backup when primary object detectors fail intermittently. This system is adaptable to various sensors, demonstrated using depth cameras where KF's 3D predictions are projected into 2D depth image coordinates, integrating nonlinear covariance propagation techniques simplified to first-order approximations. 2) Open-source ROS2 Implementation: We provide an open-source ROS2 implementation of the SMART-TRACK framework, validated in a realistic simulation environment using Gazebo and ROS2, fostering broader adaptation and further research. Our results showcase significant enhancements in tracking stability, with estimation RMSE as low as 0.04 m during measurement disruptions, advancing the robustness of UAV tracking and expanding the potential for reliable autonomous UAV operations in complex scenarios. The implementation is available at https://github.com/mzahana/SMART-TRACK.
<div id='section'>PaperID: <span id='pid'>147, <a href='https://arxiv.org/pdf/2410.09243.pdf' target='_blank'>https://arxiv.org/pdf/2410.09243.pdf</a></span>   <span><a href='https://UARK-AICV.github.io/G2MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Duy Le Dinh Anh, Kim Hoang Tran, Quang-Thuc Nguyen, Ngan Hoang Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09243">Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent progress, Multi-Object Tracking (MOT) continues to face significant challenges, particularly its dependence on prior knowledge and predefined categories, complicating the tracking of unfamiliar objects. Generic Multiple Object Tracking (GMOT) emerges as a promising solution, requiring less prior information. Nevertheless, existing GMOT methods, primarily designed as OneShot-GMOT, rely heavily on initial bounding boxes and often struggle with variations in viewpoint, lighting, occlusion, and scale. To overcome the limitations inherent in both MOT and GMOT when it comes to tracking objects with specific generic attributes, we introduce Grounded-GMOT, an innovative tracking paradigm that enables users to track multiple generic objects in videos through natural language descriptors.
  Our contributions begin with the introduction of the G2MOT dataset, which includes a collection of videos featuring a wide variety of generic objects, each accompanied by detailed textual descriptions of their attributes. Following this, we propose a novel tracking method, KAM-SORT, which not only effectively integrates visual appearance with motion cues but also enhances the Kalman filter. KAM-SORT proves particularly advantageous when dealing with objects of high visual similarity from the same generic category in GMOT scenarios. Through comprehensive experiments, we demonstrate that Grounded-GMOT outperforms existing OneShot-GMOT approaches. Additionally, our extensive comparisons between various trackers highlight KAM-SORT's efficacy in GMOT, further establishing its significance in the field. Project page: https://UARK-AICV.github.io/G2MOT. The source code and dataset will be made publicly available.
<div id='section'>PaperID: <span id='pid'>148, <a href='https://arxiv.org/pdf/2410.04250.pdf' target='_blank'>https://arxiv.org/pdf/2410.04250.pdf</a></span>   <span><a href='https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/leggedrobotics/rsl_panoptic_mapping' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lorenzo Terenzi, Julian Nubert, Pol Eyschen, Pascal Roth, Simin Fei, Edo Jelavic, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04250">ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Construction sites are challenging environments for autonomous systems due to their unstructured nature and the presence of dynamic actors, such as workers and machinery. This work presents a comprehensive panoptic scene understanding solution designed to handle the complexities of such environments by integrating 2D panoptic segmentation with 3D LiDAR mapping. Our system generates detailed environmental representations in real-time by combining semantic and geometric data, supported by Kalman Filter-based tracking for dynamic object detection. We introduce a fine-tuning method that adapts large pre-trained panoptic segmentation models for construction site applications using a limited number of domain-specific samples. For this use case, we release a first-of-its-kind dataset of 502 hand-labeled sample images with panoptic annotations from construction sites. In addition, we propose a dynamic panoptic mapping technique that enhances scene understanding in unstructured environments. As a case study, we demonstrate the system's application for autonomous navigation, utilizing real-time RRT* for reactive path planning in dynamic scenarios. The dataset (https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/) and code (https://github.com/leggedrobotics/rsl_panoptic_mapping) for training and deployment are publicly available to support future research.
<div id='section'>PaperID: <span id='pid'>149, <a href='https://arxiv.org/pdf/2410.02249.pdf' target='_blank'>https://arxiv.org/pdf/2410.02249.pdf</a></span>   <span><a href='https://github.com/AndyCao1125/SpikeSlicer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahang Cao, Mingyuan Sun, Ziqing Wang, Hao Cheng, Qiang Zhang, Shibo Zhou, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02249">Spiking Neural Network as Adaptive Event Stream Slicer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event-based cameras are attracting significant interest as they provide rich edge information, high dynamic range, and high temporal resolution. Many state-of-the-art event-based algorithms rely on splitting the events into fixed groups, resulting in the omission of crucial temporal information, particularly when dealing with diverse motion scenarios (\eg, high/low speed).In this work, we propose SpikeSlicer, a novel-designed plug-and-play event processing method capable of splitting events stream adaptively.SpikeSlicer utilizes a low-energy spiking neural network (SNN) to trigger event slicing. To guide the SNN to fire spikes at optimal time steps, we propose the Spiking Position-aware Loss (SPA-Loss) to modulate the neuron's state. Additionally, we develop a Feedback-Update training strategy that refines the slicing decisions using feedback from the downstream artificial neural network (ANN). Extensive experiments demonstrate that our method yields significant performance improvements in event-based object tracking and recognition. Notably, SpikeSlicer provides a brand-new SNN-ANN cooperation paradigm, where the SNN acts as an efficient, low-energy data processor to assist the ANN in improving downstream performance, injecting new perspectives and potential avenues of exploration. Our code is available at https://github.com/AndyCao1125/SpikeSlicer.
<div id='section'>PaperID: <span id='pid'>150, <a href='https://arxiv.org/pdf/2409.19603.pdf' target='_blank'>https://arxiv.org/pdf/2409.19603.pdf</a></span>   <span><a href='https://github.com/showlab/VideoLISA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19603">One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.
<div id='section'>PaperID: <span id='pid'>151, <a href='https://arxiv.org/pdf/2409.17564.pdf' target='_blank'>https://arxiv.org/pdf/2409.17564.pdf</a></span>   <span><a href='https://github.com/LingyiHongfd/CompressTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Lingyi Hong, Jinglun Li, Xinyu Zhou, Shilin Yan, Pinxue Guo, Kaixun Jiang, Zhaoyu Chen, Shuyong Gao, Runze Li, Xingdong Sheng, Wei Zhang, Hong Lu, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17564">General Compression Framework for Efficient Transformer Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous works have attempted to improve tracking efficiency through lightweight architecture design or knowledge distillation from teacher models to compact student trackers. However, these solutions often sacrifice accuracy for speed to a great extent, and also have the problems of complex training process and structural limitations. Thus, we propose a general model compression framework for efficient transformer object tracking, named CompressTracker, to reduce model size while preserving tracking accuracy. Our approach features a novel stage division strategy that segments the transformer layers of the teacher model into distinct stages to break the limitation of model structure. Additionally, we also design a unique replacement training technique that randomly substitutes specific stages in the student model with those from the teacher model, as opposed to training the student model in isolation. Replacement training enhances the student model's ability to replicate the teacher model's behavior and simplifies the training process. To further forcing student model to emulate teacher model, we incorporate prediction guidance and stage-wise feature mimicking to provide additional supervision during the teacher model's compression process. CompressTracker is structurally agnostic, making it compatible with any transformer architecture. We conduct a series of experiment to verify the effectiveness and generalizability of our CompressTracker. Our CompressTracker-SUTrack, compressed from SUTrack, retains about 99 performance on LaSOT (72.2 AUC) while achieves 2.42x speed up. Code is available at https://github.com/LingyiHongfd/CompressTracker.
<div id='section'>PaperID: <span id='pid'>152, <a href='https://arxiv.org/pdf/2409.16902.pdf' target='_blank'>https://arxiv.org/pdf/2409.16902.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Chunhui Zhang, Li Liu, Guanjie Huang, Zhipeng Zhang, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16902">Underwater Camouflaged Object Tracking Meets Vision-Language SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale datasets. However, these datasets have primarily focused on open-air scenarios and have largely overlooked underwater animal tracking-especially the complex challenges posed by camouflaged marine animals. To bridge this gap, we take a step forward by proposing the first large-scale multi-modal underwater camouflaged object tracking dataset, namely UW-COT220. Based on the proposed dataset, this work first comprehensively evaluates current advanced visual object tracking methods, including SAM- and SAM2-based trackers, in challenging underwater environments, \eg, coral reefs. Our findings highlight the improvements of SAM2 over SAM, demonstrating its enhanced ability to handle the complexities of underwater camouflaged objects. Furthermore, we propose a novel vision-language tracking framework called VL-SAM2, based on the video foundation model SAM2. Extensive experimental results demonstrate that the proposed VL-SAM2 achieves state-of-the-art performance across underwater and open-air object tracking datasets. The dataset and codes are available at~{\color{magenta}{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}}.
<div id='section'>PaperID: <span id='pid'>153, <a href='https://arxiv.org/pdf/2409.16834.pdf' target='_blank'>https://arxiv.org/pdf/2409.16834.pdf</a></span>   <span><a href='https://github.com/vision4robotics/CGDenoiser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Yucheng Wang, Changhong Fu, Kunhan Lu, Liangliang Yao, Haobo Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16834">Conditional Generative Denoiser for Nighttime UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art (SOTA) visual object tracking methods have significantly enhanced the autonomy of unmanned aerial vehicles (UAVs). However, in low-light conditions, the presence of irregular real noise from the environments severely degrades the performance of these SOTA methods. Moreover, existing SOTA denoising techniques often fail to meet the real-time processing requirements when deployed as plug-and-play denoisers for UAV tracking. To address this challenge, this work proposes a novel conditional generative denoiser (CGDenoiser), which breaks free from the limitations of traditional deterministic paradigms and generates the noise conditioning on the input, subsequently removing it. To better align the input dimensions and accelerate inference, a novel nested residual Transformer conditionalizer is developed. Furthermore, an innovative multi-kernel conditional refiner is designed to pertinently refine the denoised output. Extensive experiments show that CGDenoiser promotes the tracking precision of the SOTA tracker by 18.18\% on DarkTrack2021 whereas working 5.8 times faster than the second well-performed denoiser. Real-world tests with complex challenges also prove the effectiveness and practicality of CGDenoiser. Code, video demo and supplementary proof for CGDenoier are now available at: \url{https://github.com/vision4robotics/CGDenoiser}.
<div id='section'>PaperID: <span id='pid'>154, <a href='https://arxiv.org/pdf/2409.16652.pdf' target='_blank'>https://arxiv.org/pdf/2409.16652.pdf</a></span>   <span><a href='https://github.com/vision4robotics/PRL-Track' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Changhong Fu, Xiang Lei, Haobo Zuo, Liangliang Yao, Guangze Zheng, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16652">Progressive Representation Learning for Real-Time UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has significantly promoted autonomous applications for unmanned aerial vehicles (UAVs). However, learning robust object representations for UAV tracking is especially challenging in complex dynamic environments, when confronted with aspect ratio change and occlusion. These challenges severely alter the original information of the object. To handle the above issues, this work proposes a novel progressive representation learning framework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided into coarse representation learning and fine representation learning. For coarse representation learning, two innovative regulators, which rely on appearance and semantic information, are designed to mitigate appearance interference and capture semantic information. Furthermore, for fine representation learning, a new hierarchical modeling generator is developed to intertwine coarse object representations. Exhaustive experiments demonstrate that the proposed PRL-Track delivers exceptional performance on three authoritative UAV tracking benchmarks. Real-world tests indicate that the proposed PRL-Track realizes superior tracking performance with 42.6 frames per second on the typical UAV platform equipped with an edge smart camera. The code, model, and demo videos are available at \url{https://github.com/vision4robotics/PRL-Track}.
<div id='section'>PaperID: <span id='pid'>155, <a href='https://arxiv.org/pdf/2409.16631.pdf' target='_blank'>https://arxiv.org/pdf/2409.16631.pdf</a></span>   <span><a href='https://github.com/vision4robotics/LDEnhancer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Liangliang Yao, Changhong Fu, Yiheng Wang, Haobo Zuo, Kunhan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16631">Enhancing Nighttime UAV Tracking with Light Distribution Suppression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has boosted extensive intelligent applications for unmanned aerial vehicles (UAVs). However, the state-of-the-art (SOTA) enhancers for nighttime UAV tracking always neglect the uneven light distribution in low-light images, inevitably leading to excessive enhancement in scenarios with complex illumination. To address these issues, this work proposes a novel enhancer, i.e., LDEnhancer, enhancing nighttime UAV tracking with light distribution suppression. Specifically, a novel image content refinement module is developed to decompose the light distribution information and image content information in the feature space, allowing for the targeted enhancement of the image content information. Then this work designs a new light distribution generation module to capture light distribution effectively. The features with light distribution information and image content information are fed into the different parameter estimation modules, respectively, for the parameter map prediction. Finally, leveraging two parameter maps, an innovative interweave iteration adjustment is proposed for the collaborative pixel-wise adjustment of low-light images. Additionally, a challenging nighttime UAV tracking dataset with uneven light distribution, namely NAT2024-2, is constructed to provide a comprehensive evaluation, which contains 40 challenging sequences with over 74K frames in total. Experimental results on the authoritative UAV benchmarks and the proposed NAT2024-2 demonstrate that LDEnhancer outperforms other SOTA low-light enhancers for nighttime UAV tracking. Furthermore, real-world tests on a typical UAV platform with an NVIDIA Orin NX confirm the practicality and efficiency of LDEnhancer. The code is available at https://github.com/vision4robotics/LDEnhancer.
<div id='section'>PaperID: <span id='pid'>156, <a href='https://arxiv.org/pdf/2409.16149.pdf' target='_blank'>https://arxiv.org/pdf/2409.16149.pdf</a></span>   <span><a href='https://github.com/megvii-research/MCTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiyang Wang, Shouzheng Qi, Jieyou Zhao, Hangning Zhou, Siyu Zhang, Guoan Wang, Kai Tu, Songlin Guo, Jianbo Zhao, Jian Li, Mu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16149">MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MCTrack, a new 3D multi-object tracking method that achieves state-of-the-art (SOTA) performance across KITTI, nuScenes, and Waymo datasets. Addressing the gap in existing tracking paradigms, which often perform well on specific datasets but lack generalizability, MCTrack offers a unified solution. Additionally, we have standardized the format of perceptual results across various datasets, termed BaseVersion, facilitating researchers in the field of multi-object tracking (MOT) to concentrate on the core algorithmic development without the undue burden of data preprocessing. Finally, recognizing the limitations of current evaluation metrics, we propose a novel set that assesses motion information output, such as velocity and acceleration, crucial for downstream tasks. The source codes of the proposed method are available at this link: https://github.com/megvii-research/MCTrack}{https://github.com/megvii-research/MCTrack
<div id='section'>PaperID: <span id='pid'>157, <a href='https://arxiv.org/pdf/2409.12159.pdf' target='_blank'>https://arxiv.org/pdf/2409.12159.pdf</a></span>   <span><a href='https://github.com/Walleclipse/WeHelp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Abulikemu Abuduweili, Alice Wu, Tianhao Wei, Weiye Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12159">WeHelp: A Shared Autonomy System for Wheelchair Users</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a large population of wheelchair users. Most of the wheelchair users need help with daily tasks. However, according to recent reports, their needs are not properly satisfied due to the lack of caregivers. Therefore, in this project, we develop WeHelp, a shared autonomy system aimed for wheelchair users. A robot with a WeHelp system has three modes, following mode, remote control mode and tele-operation mode. In the following mode, the robot follows the wheelchair user automatically via visual tracking. The wheelchair user can ask the robot to follow them from behind, by the left or by the right. When the wheelchair user asks for help, the robot will recognize the command via speech recognition, and then switch to the teleoperation mode or remote control mode. In the teleoperation mode, the wheelchair user takes over the robot with a joy stick and controls the robot to complete some complex tasks for their needs, such as opening doors, moving obstacles on the way, reaching objects on a high shelf or on the low ground, etc. In the remote control mode, a remote assistant takes over the robot and helps the wheelchair user complete some complex tasks for their needs. Our evaluation shows that the pipeline is useful and practical for wheelchair users. Source code and demo of the paper are available at \url{https://github.com/Walleclipse/WeHelp}.
<div id='section'>PaperID: <span id='pid'>158, <a href='https://arxiv.org/pdf/2409.11235.pdf' target='_blank'>https://arxiv.org/pdf/2409.11235.pdf</a></span>   <span><a href='https://github.com/siyuanliii/SLAck' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/siyuanliii/SLAck' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Siyuan Li, Lei Ke, Yung-Hsu Yang, Luigi Piccinelli, Mattia SegÃ¹, Martin Danelljan, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11235">SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers to novel categories not in the training set. Currently, the best-performing methods are mainly based on pure appearance matching. Due to the complexity of motion patterns in the large-vocabulary scenarios and unstable classification of the novel objects, the motion and semantics cues are either ignored or applied based on heuristics in the final matching steps by existing methods. In this paper, we present a unified framework SLAck that jointly considers semantics, location, and appearance priors in the early steps of association and learns how to integrate all valuable information through a lightweight spatial and temporal object graph. Our method eliminates complex post-processing heuristics for fusing different cues and boosts the association performance significantly for large-scale open-vocabulary tracking. Without bells and whistles, we outperform previous state-of-the-art methods for novel classes tracking on the open-vocabulary MOT and TAO TETA benchmarks. Our code is available at \href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}.
<div id='section'>PaperID: <span id='pid'>159, <a href='https://arxiv.org/pdf/2409.11234.pdf' target='_blank'>https://arxiv.org/pdf/2409.11234.pdf</a></span>   <span><a href='https://github.com/ydhcg-BoBo/STCMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianbo Ma, Chuanming Tang, Fei Wu, Can Zhao, Jianlin Zhang, Zhiyong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11234">STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos is important for diverse applications in computer vision. Current MOT trackers rely on accurate object detection results and precise matching of target reidentification (ReID). These methods focus on optimizing target spatial attributes while overlooking temporal cues in modelling object relationships, especially for challenging tracking conditions such as object deformation and blurring, etc. To address the above-mentioned issues, we propose a novel Spatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), which utilizes historical embedding features to model the representation of ReID and detection features in a sequential order. Concretely, a temporal embedding boosting module is introduced to enhance the discriminability of individual embedding based on adjacent frame cooperation. While the trajectory embedding is then propagated by a temporal detection refinement module to mine salient target locations in the temporal field. Extensive experiments on the VisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a new state-of-the-art performance in MOTA and IDF1 metrics. The source codes are released at https://github.com/ydhcg-BoBo/STCMOT.
<div id='section'>PaperID: <span id='pid'>160, <a href='https://arxiv.org/pdf/2409.09293.pdf' target='_blank'>https://arxiv.org/pdf/2409.09293.pdf</a></span>   <span><a href='https://github.com/balabooooo/AED' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Zimeng Fang, Chao Liang, Xue Zhou, Shuyuan Zhu, Xi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09293">Associate Everything Detected: Facilitating Tracking-by-Detection to the Unknown</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) emerges as a pivotal and highly promising branch in the field of computer vision. Classical closed-vocabulary MOT (CV-MOT) methods aim to track objects of predefined categories. Recently, some open-vocabulary MOT (OV-MOT) methods have successfully addressed the problem of tracking unknown categories. However, we found that the CV-MOT and OV-MOT methods each struggle to excel in the tasks of the other. In this paper, we present a unified framework, Associate Everything Detected (AED), that simultaneously tackles CV-MOT and OV-MOT by integrating with any off-the-shelf detector and supports unknown categories. Different from existing tracking-by-detection MOT methods, AED gets rid of prior knowledge (e.g. motion cues) and relies solely on highly robust feature learning to handle complex trajectories in OV-MOT tasks while keeping excellent performance in CV-MOT tasks. Specifically, we model the association task as a similarity decoding problem and propose a sim-decoder with an association-centric learning mechanism. The sim-decoder calculates similarities in three aspects: spatial, temporal, and cross-clip. Subsequently, association-centric learning leverages these threefold similarities to ensure that the extracted features are appropriate for continuous tracking and robust enough to generalize to unknown categories. Compared with existing powerful OV-MOT and CV-MOT methods, AED achieves superior performance on TAO, SportsMOT, and DanceTrack without any prior knowledge. Our code is available at https://github.com/balabooooo/AED.
<div id='section'>PaperID: <span id='pid'>161, <a href='https://arxiv.org/pdf/2409.06617.pdf' target='_blank'>https://arxiv.org/pdf/2409.06617.pdf</a></span>   <span><a href='https://github.com/emirhanbayar/Fast-Deep-OC-SORT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/emirhanbayar/Fast-StrongSORT,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Emirhan Bayar, Cemal Aker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06617">When to Extract ReID Features: A Selective Approach for Improved Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting and matching Re-Identification (ReID) features is used by many state-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularly effective against frequent and long-term occlusions. While end-to-end object detection and tracking have been the main focus of recent research, they have yet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus, from an application standpoint, methods with separate detection and embedding remain the best option for accuracy, modularity, and ease of implementation, though they are impractical for edge devices due to the overhead involved. In this paper, we investigate a selective approach to minimize the overhead of feature extraction while preserving accuracy, modularity, and ease of implementation. This approach can be integrated into various SOTA methods. We demonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT. Experiments on MOT17, MOT20, and DanceTrack datasets show that our mechanism retains the advantages of feature extraction during occlusions while significantly reducing runtime. Additionally, it improves accuracy by preventing confusion in the feature-matching stage, particularly in cases of deformation and appearance similarity, which are common in DanceTrack. https://github.com/emirhanbayar/Fast-StrongSORT, https://github.com/emirhanbayar/Fast-Deep-OC-SORT
<div id='section'>PaperID: <span id='pid'>162, <a href='https://arxiv.org/pdf/2409.04187.pdf' target='_blank'>https://arxiv.org/pdf/2409.04187.pdf</a></span>   <span><a href='https://github.com/Jumabek/LITE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Jumabek Alikhanov, Dilshod Obidov, Hakil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04187">LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID Feature Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is introduced as a novel multi-object tracking (MOT) approach. It enhances ReID-based trackers by eliminating inference, pre-processing, post-processing, and ReID model training costs. LITE uses real-time appearance features without compromising speed. By integrating appearance feature extraction directly into the tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE demonstrates significant performance improvements. The simplest implementation of LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS on the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four times faster on the more crowded MOT20 dataset, while maintaining similar accuracy. Additionally, a new evaluation framework for tracking-by-detection approaches reveals that conventional trackers like DeepSORT remain competitive with modern state-of-the-art trackers when evaluated under fair conditions. The code will be available post-publication at https://github.com/Jumabek/LITE.
<div id='section'>PaperID: <span id='pid'>163, <a href='https://arxiv.org/pdf/2409.02490.pdf' target='_blank'>https://arxiv.org/pdf/2409.02490.pdf</a></span>   <span><a href='https://fsoft-aic.github.io/TP-GMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Duy Le Dinh Anh, Kim Hoang Tran, Ngan Hoang Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02490">TP-GMOT: Tracking Generic Multiple Object by Textual Prompt with Motion-Appearance Cost (MAC) SORT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multi-Object Tracking (MOT) has made substantial advancements, it is limited by heavy reliance on prior knowledge and limited to predefined categories. In contrast, Generic Multiple Object Tracking (GMOT), tracking multiple objects with similar appearance, requires less prior information about the targets but faces challenges with variants like viewpoint, lighting, occlusion, and resolution. Our contributions commence with the introduction of the \textbf{\text{Refer-GMOT dataset}} a collection of videos, each accompanied by fine-grained textual descriptions of their attributes. Subsequently, we introduce a novel text prompt-based open-vocabulary GMOT framework, called \textbf{\text{TP-GMOT}}, which can track never-seen object categories with zero training examples. Within \text{TP-GMOT} framework, we introduce two novel components: (i) {\textbf{\text{TP-OD}}, an object detection by a textual prompt}, for accurately detecting unseen objects with specific characteristics. (ii) Motion-Appearance Cost SORT \textbf{\text{MAC-SORT}}, a novel object association approach that adeptly integrates motion and appearance-based matching strategies to tackle the complex task of tracking multiple generic objects with high similarity. Our contributions are benchmarked on the \text{Refer-GMOT} dataset for GMOT task. Additionally, to assess the generalizability of the proposed \text{TP-GMOT} framework and the effectiveness of \text{MAC-SORT} tracker, we conduct ablation studies on the DanceTrack and MOT20 datasets for the MOT task. Our dataset, code, and models will be publicly available at: https://fsoft-aic.github.io/TP-GMOT
<div id='section'>PaperID: <span id='pid'>164, <a href='https://arxiv.org/pdf/2409.00487.pdf' target='_blank'>https://arxiv.org/pdf/2409.00487.pdf</a></span>   <span><a href='https://github.com/Xavier-Lin/TrackSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors: <span id = 'author'>Bin Hu, Run Luo, Zelin Liu, Cheng Wang, Wenyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00487">TrackSSM: A General Motion Predictor by State-Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal motion modeling has always been a key component in multiple object tracking (MOT) which can ensure smooth trajectory movement and provide accurate positional information to enhance association precision. However, current motion models struggle to be both efficient and effective across different application scenarios. To this end, we propose TrackSSM inspired by the recently popular state space models (SSM), a unified encoder-decoder motion framework that uses data-dependent state space model to perform temporal motion of trajectories. Specifically, we propose Flow-SSM, a module that utilizes the position and motion information from historical trajectories to guide the temporal state transition of object bounding boxes. Based on Flow-SSM, we design a flow decoder. It is composed of a cascaded motion decoding module employing Flow-SSM, which can use the encoded flow information to complete the temporal position prediction of trajectories. Additionally, we propose a Step-by-Step Linear (S$^2$L) training strategy. By performing linear interpolation between the positions of the object in the previous frame and the current frame, we construct the pseudo labels of step-by-step linear training, ensuring that the trajectory flow information can better guide the object bounding box in completing temporal transitions. TrackSSM utilizes a simple Mamba-Block to build a motion encoder for historical trajectories, forming a temporal motion model with an encoder-decoder structure in conjunction with the flow decoder. TrackSSM is applicable to various tracking scenarios and achieves excellent tracking performance across multiple benchmarks, further extending the potential of SSM-like temporal motion models in multi-object tracking tasks. Code and models are publicly available at \url{https://github.com/Xavier-Lin/TrackSSM}.
<div id='section'>PaperID: <span id='pid'>165, <a href='https://arxiv.org/pdf/2410.01678.pdf' target='_blank'>https://arxiv.org/pdf/2410.01678.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ayesha Ishaq, Mohamed El Amine Boudjoghra, Jean Lahoud, Fahad Shahbaz Khan, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01678">Open3DTrack: Towards Open-Vocabulary 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking plays a critical role in autonomous driving by enabling the real-time monitoring and prediction of multiple objects' movements. Traditional 3D tracking systems are typically constrained by predefined object categories, limiting their adaptability to novel, unseen objects in dynamic environments. To address this limitation, we introduce open-vocabulary 3D tracking, which extends the scope of 3D tracking to include objects beyond predefined categories. We formulate the problem of open-vocabulary 3D tracking and introduce dataset splits designed to represent various open-vocabulary scenarios. We propose a novel approach that integrates open-vocabulary capabilities into a 3D tracking framework, allowing for generalization to unseen object classes. Our method effectively reduces the performance gap between tracking known and novel objects through strategic adaptation. Experimental results demonstrate the robustness and adaptability of our method in diverse outdoor driving scenarios. To the best of our knowledge, this work is the first to address open-vocabulary 3D tracking, presenting a significant advancement for autonomous systems in real-world settings. Code, trained models, and dataset splits are available publicly.
<div id='section'>PaperID: <span id='pid'>166, <a href='https://arxiv.org/pdf/2511.20886.pdf' target='_blank'>https://arxiv.org/pdf/2511.20886.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiancheng Pan, Runze Wang, Tianwen Qian, Mohammad Mahdi, Yanwei Fu, Xiangyang Xue, Xiaomeng Huang, Luc Van Gool, Danda Pani Paudel, Yuqian Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20886">V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).
<div id='section'>PaperID: <span id='pid'>167, <a href='https://arxiv.org/pdf/2505.12340.pdf' target='_blank'>https://arxiv.org/pdf/2505.12340.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jirong Zha, Yuxuan Fan, Kai Li, Han Li, Chen Gao, Xinlei Chen, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12340">DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State estimation is challenging for 3D object tracking with high maneuverability, as the target's state transition function changes rapidly, irregularly, and is unknown to the estimator. Existing work based on interacting multiple model (IMM) achieves more accurate estimation than single-filter approaches through model combination, aligning appropriate models for different motion modes of the target object over time. However, two limitations of conventional IMM remain unsolved. First, the solution space of the model combination is constrained as the target's diverse kinematic properties in different directions are ignored. Second, the model combination weights calculated by the observation likelihood are not accurate enough due to the measurement uncertainty. In this paper, we propose a novel framework, DIMM, to effectively combine estimates from different motion models in each direction, thus increasing the 3D object tracking accuracy. First, DIMM extends the model combination solution space of conventional IMM from a hyperplane to a hypercube by designing a 3D-decoupled multi-hierarchy filter bank, which describes the target's motion with various-order linear models. Second, DIMM generates more reliable combination weight matrices through a differentiable adaptive fusion network for importance allocation rather than solely relying on the observation likelihood; it contains an attention-based twin delayed deep deterministic policy gradient (TD3) method with a hierarchical reward. Experiments demonstrate that DIMM significantly improves the tracking accuracy of existing state estimation methods by 31.61%~99.23%.
<div id='section'>PaperID: <span id='pid'>168, <a href='https://arxiv.org/pdf/2504.06958.pdf' target='_blank'>https://arxiv.org/pdf/2504.06958.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06958">VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.
<div id='section'>PaperID: <span id='pid'>169, <a href='https://arxiv.org/pdf/2507.05718.pdf' target='_blank'>https://arxiv.org/pdf/2507.05718.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hang Que, Jie Yang, Tao Du, Shuqiang Xia, Chao-Kai Wen, Shi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05718">Cooperative Mapping, Localization, and Beam Management via Multi-Modal SLAM in ISAC Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous localization and mapping (SLAM) plays a critical role in integrated sensing and communication (ISAC) systems for sixth-generation (6G) millimeter-wave (mmWave) networks, enabling environmental awareness and precise user equipment (UE) positioning. While cooperative multi-user SLAM has demonstrated potential in leveraging distributed sensing, its application within multi-modal ISAC systems remains limited, particularly in terms of theoretical modeling and communication-layer integration. This paper proposes a novel multi-modal SLAM framework that addresses these limitations through three key contributions. First, a Bayesian estimation framework is developed for cooperative multi-user SLAM, along with a two-stage algorithm for robust radio map construction under dynamic and heterogeneous sensing conditions. Second, a multi-modal localization strategy is introduced, fusing SLAM results with camera-based multi-object tracking and inertial measurement unit (IMU) data via an error-aware model, significantly improving UE localization in multi-user scenarios. Third, a sensing-aided beam management scheme is proposed, utilizing global radio maps and localization data to generate UE-specific prior information for beam selection, thereby reducing inter-user interference and enhancing downlink spectral efficiency. Simulation results demonstrate that the proposed system improves radio map accuracy by up to 60%, enhances localization accuracy by 37.5%, and significantly outperforms traditional methods in both indoor and outdoor environments.
<div id='section'>PaperID: <span id='pid'>170, <a href='https://arxiv.org/pdf/2510.19560.pdf' target='_blank'>https://arxiv.org/pdf/2510.19560.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yao Deng, Xian Zhong, Wenxuan Liu, Zhaofei Yu, Jingling Yuan, Tiejun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19560">HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB cameras excel at capturing rich texture details with high spatial resolution, whereas event cameras offer exceptional temporal resolution and a high dynamic range (HDR). Leveraging their complementary strengths can substantially enhance object tracking under challenging conditions, such as high-speed motion, HDR environments, and dynamic background interference. However, a significant spatio-temporal asymmetry exists between these two modalities due to their fundamentally different imaging mechanisms, hindering effective multi-modal integration. To address this issue, we propose {Hierarchical Asymmetric Distillation} (HAD), a multi-modal knowledge distillation framework that explicitly models and mitigates spatio-temporal asymmetries. Specifically, HAD proposes a hierarchical alignment strategy that minimizes information loss while maintaining the student network's computational efficiency and parameter compactness. Extensive experiments demonstrate that HAD consistently outperforms state-of-the-art methods, and comprehensive ablation studies further validate the effectiveness and necessity of each designed component. The code will be released soon.
<div id='section'>PaperID: <span id='pid'>171, <a href='https://arxiv.org/pdf/2504.12709.pdf' target='_blank'>https://arxiv.org/pdf/2504.12709.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shumin Wang, Zhuoran Yang, Lidian Wang, Zhipeng Tang, Heng Li, Lehan Pan, Sha Zhang, Jie Peng, Jianmin Ji, Yanyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12709">Self-Supervised Pre-training with Combined Datasets for 3D Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The significant achievements of pre-trained models leveraging large volumes of data in the field of NLP and 2D vision inspire us to explore the potential of extensive data pre-training for 3D perception in autonomous driving. Toward this goal, this paper proposes to utilize massive unlabeled data from heterogeneous datasets to pre-train 3D perception models. We introduce a self-supervised pre-training framework that learns effective 3D representations from scratch on unlabeled data, combined with a prompt adapter based domain adaptation strategy to reduce dataset bias. The approach significantly improves model performance on downstream tasks such as 3D object detection, BEV segmentation, 3D object tracking, and occupancy prediction, and shows steady performance increase as the training data volume scales up, demonstrating the potential of continually benefit 3D perception models for autonomous driving. We will release the source code to inspire further investigations in the community.
<div id='section'>PaperID: <span id='pid'>172, <a href='https://arxiv.org/pdf/2411.00553.pdf' target='_blank'>https://arxiv.org/pdf/2411.00553.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gianluca Mancusi, Mattia Bernardi, Aniello Panariello, Angelo Porrello, Rita Cucchiara, Simone Calderara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00553">Is Multiple Object Tracking a Matter of Specialization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end transformer-based trackers have achieved remarkable performance on most human-related datasets. However, training these trackers in heterogeneous scenarios poses significant challenges, including negative interference - where the model learns conflicting scene-specific parameters - and limited domain generalization, which often necessitates expensive fine-tuning to adapt the models to new domains. In response to these challenges, we introduce Parameter-efficient Scenario-specific Tracking Architecture (PASTA), a novel framework that combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL). Specifically, we define key scenario attributes (e.g, camera-viewpoint, lighting condition) and train specialized PEFT modules for each attribute. These expert modules are combined in parameter space, enabling systematic generalization to new domains without increasing inference time. Extensive experiments on MOTSynth, along with zero-shot evaluations on MOT17 and PersonPath22 demonstrate that a neural tracker built from carefully selected modules surpasses its monolithic counterpart. We release models and code.
<div id='section'>PaperID: <span id='pid'>173, <a href='https://arxiv.org/pdf/2511.19475.pdf' target='_blank'>https://arxiv.org/pdf/2511.19475.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tianlu Zhang, Qiang Zhang, Guiguang Ding, Jungong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19475">Tracking and Segmenting Anything in Any Modality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking and segmentation play essential roles in video understanding, providing basic positional information and temporal association of objects within video sequences. Despite their shared objective, existing approaches often tackle these tasks using specialized architectures or modality-specific parameters, limiting their generalization and scalability. Recent efforts have attempted to unify multiple tracking and segmentation subtasks from the perspectives of any modality input or multi-task inference. However, these approaches tend to overlook two critical challenges: the distributional gap across different modalities and the feature representation gap across tasks. These issues hinder effective cross-task and cross-modal knowledge sharing, ultimately constraining the development of a true generalist model. To address these limitations, we propose a universal tracking and segmentation framework named SATA, which unifies a broad spectrum of tracking and segmentation subtasks with any modality input. Specifically, a Decoupled Mixture-of-Expert (DeMoE) mechanism is presented to decouple the unified representation learning task into the modeling process of cross-modal shared knowledge and specific information, thus enabling the model to maintain flexibility while enhancing generalization. Additionally, we introduce a Task-aware Multi-object Tracking (TaMOT) pipeline to unify all the task outputs as a unified set of instances with calibrated ID information, thereby alleviating the degradation of task-specific knowledge during multi-task training. SATA demonstrates superior performance on 18 challenging tracking and segmentation benchmarks, offering a novel perspective for more generalizable video understanding.
<div id='section'>PaperID: <span id='pid'>174, <a href='https://arxiv.org/pdf/2502.06583.pdf' target='_blank'>https://arxiv.org/pdf/2502.06583.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiantao Hu, Bineng Zhong, Qihua Liang, Zhiyi Mo, Liangtao Shi, Ying Tai, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06583">Adaptive Perception for Unified Visual Multi-modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, many multi-modal trackers prioritize RGB as the dominant modality, treating other modalities as auxiliary, and fine-tuning separately various multi-modal tasks. This imbalance in modality dependence limits the ability of methods to dynamically utilize complementary information from each modality in complex scenarios, making it challenging to fully perceive the advantages of multi-modal. As a result, a unified parameter model often underperforms in various multi-modal tracking tasks. To address this issue, we propose APTrack, a novel unified tracker designed for multi-modal adaptive perception. Unlike previous methods, APTrack explores a unified representation through an equal modeling strategy. This strategy allows the model to dynamically adapt to various modalities and tasks without requiring additional fine-tuning between different tasks. Moreover, our tracker integrates an adaptive modality interaction (AMI) module that efficiently bridges cross-modality interactions by generating learnable tokens. Experiments conducted on five diverse multi-modal datasets (RGBT234, LasHeR, VisEvent, DepthTrack, and VOT-RGBD2022) demonstrate that APTrack not only surpasses existing state-of-the-art unified multi-modal trackers but also outperforms trackers designed for specific multi-modal tasks.
<div id='section'>PaperID: <span id='pid'>175, <a href='https://arxiv.org/pdf/2503.11218.pdf' target='_blank'>https://arxiv.org/pdf/2503.11218.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andong Lu, Mai Wen, Jinhu Wang, Yuanzhi Guo, Chenglong Li, Jin Tang, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11218">Towards General Multimodal Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multimodal tracking studies focus on bi-modal scenarios such as RGB-Thermal, RGB-Event, and RGB-Language. Although promising tracking performance is achieved through leveraging complementary cues from different sources, it remains challenging in complex scenes due to the limitations of bi-modal scenarios. In this work, we introduce a general multimodal visual tracking task that fully exploits the advantages of four modalities, including RGB, thermal infrared, event, and language, for robust tracking under challenging conditions. To provide a comprehensive evaluation platform for general multimodal visual tracking, we construct QuadTrack600, a large-scale, high-quality benchmark comprising 600 video sequences (totaling 384.7K high-resolution (640x480) frame groups). In each frame group, all four modalities are spatially aligned and meticulously annotated with bounding boxes, while 21 sequence-level challenge attributes are provided for detailed performance analysis. Despite quad-modal data provides richer information, the differences in information quantity among modalities and the computational burden from four modalities are two challenging issues in fusing four modalities. To handle these issues, we propose a novel approach called QuadFusion, which incorporates an efficient Multiscale Fusion Mamba with four different scanning scales to achieve sufficient interactions of the four modalities while overcoming the exponential computational burden, for general multimodal visual tracking. Extensive experiments on the QuadTrack600 dataset and three bi-modal tracking datasets, including LasHeR, VisEvent, and TNL2K, validate the effectiveness of our QuadFusion.
<div id='section'>PaperID: <span id='pid'>176, <a href='https://arxiv.org/pdf/2503.07516.pdf' target='_blank'>https://arxiv.org/pdf/2503.07516.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weize Li, Yunhao Du, Qixiang Yin, Zhicheng Zhao, Fei Su, Daqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07516">Just Functioning as a Hook for Two-Stage Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) aims to localize target trajectories in videos specified by natural language expressions. Despite recent progress, the intrinsic relationship between the two subtasks of tracking and referring in RMOT has not been fully studied. In this paper, we present a systematic analysis of their interdependence, revealing that current two-stage Referring-by-Tracking (RBT) frameworks remain fundamentally limited by insufficient modeling of subtask interactions and inflexible reliance on semantic alignment modules like CLIP. To this end, we propose JustHook, a novel two-stage RBT framework where a Hook module is firstly designed to redefine the linkage between subtasks. The Hook is built centered on grid sampling at the feature-level and is used for context-aware target feature extraction. Moreover, we propose a Parallel Combined Decoder (PCD) that learns in a unified joint feature space rather than relying on pre-defined cross-modal embeddings. Our design not only enhances the interpretability and modularity but also significantly improves the generalization. Extensive experiments on Refer-KITTI, Refer-KITTI-V2, and Refer-Dance demonstrate that JustHook achieves state-of-the-art performance, improving the HOTA by +6.9\% on Refer-KITTI-V2 with superior efficiency. Code will be available soon.
<div id='section'>PaperID: <span id='pid'>177, <a href='https://arxiv.org/pdf/2505.04917.pdf' target='_blank'>https://arxiv.org/pdf/2505.04917.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenxu Peng, Chenxu Wang, Minrui Zou, Danyang Li, Zhengpeng Yang, Yimian Dai, Ming-Ming Cheng, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04917">A Simple Detector with Frame Dynamics is a Strong Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.
<div id='section'>PaperID: <span id='pid'>178, <a href='https://arxiv.org/pdf/2601.10611.pdf' target='_blank'>https://arxiv.org/pdf/2601.10611.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.10611">Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).
<div id='section'>PaperID: <span id='pid'>179, <a href='https://arxiv.org/pdf/2411.13317.pdf' target='_blank'>https://arxiv.org/pdf/2411.13317.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sivan Doveh, Nimrod Shabtay, Wei Lin, Eli Schwartz, Hilde Kuehne, Raja Giryes, Rogerio Feris, Leonid Karlinsky, James Glass, Assaf Arbelle, Shimon Ullman, M. Jehanzeb Mirza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13317">Teaching VLMs to Localize Specific Objects from In-context Examples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have shown remarkable capabilities across diverse visual tasks, including image recognition, video understanding, and Visual Question Answering (VQA) when explicitly trained for these tasks. Despite these advances, we find that present-day VLMs (including the proprietary GPT-4o) lack a fundamental cognitive ability: learning to localize specific objects in a scene by taking into account the context. In this work, we focus on the task of few-shot personalized localization, where a model is given a small set of annotated images (in-context examples) -- each with a category label and bounding box -- and is tasked with localizing the same object type in a query image. Personalized localization can be particularly important in cases of ambiguity of several related objects that can respond to a text or an object that is hard to describe with words. To provoke personalized localization abilities in models, we present a data-centric solution that fine-tunes them using carefully curated data from video object tracking datasets. By leveraging sequences of frames tracking the same object across multiple shots, we simulate instruction-tuning dialogues that promote context awareness. To reinforce this, we introduce a novel regularization technique that replaces object labels with pseudo-names, ensuring the model relies on visual context rather than prior knowledge. Our method significantly enhances the few-shot localization performance of recent VLMs ranging from 7B to 72B in size, without sacrificing generalization, as demonstrated on several benchmarks tailored towards evaluating personalized localization abilities. This work is the first to explore and benchmark personalized few-shot localization for VLMs -- exposing critical weaknesses in present-day VLMs, and laying a foundation for future research in context-driven vision-language applications.
<div id='section'>PaperID: <span id='pid'>180, <a href='https://arxiv.org/pdf/2505.03184.pdf' target='_blank'>https://arxiv.org/pdf/2505.03184.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiang Xu, Ruotong Li, Mengjun Yi, Baile XU, Furao Shen, Jian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03184">Interactive Instance Annotation with Siamese Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating instance masks is time-consuming and labor-intensive. A promising solution is to predict contours using a deep learning model and then allow users to refine them. However, most existing methods focus on in-domain scenarios, limiting their effectiveness for cross-domain annotation tasks. In this paper, we propose SiamAnno, a framework inspired by the use of Siamese networks in object tracking. SiamAnno leverages one-shot learning to annotate previously unseen objects by taking a bounding box as input and predicting object boundaries, which can then be adjusted by annotators. Trained on one dataset and tested on another without fine-tuning, SiamAnno achieves state-of-the-art (SOTA) performance across multiple datasets, demonstrating its ability to handle domain and environment shifts in cross-domain tasks. We also provide more comprehensive results compared to previous work, establishing a strong baseline for future research. To our knowledge, SiamAnno is the first model to explore Siamese architecture for instance annotation.
<div id='section'>PaperID: <span id='pid'>181, <a href='https://arxiv.org/pdf/2411.16934.pdf' target='_blank'>https://arxiv.org/pdf/2411.16934.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zaira Manigrasso, Matteo Dunnhofer, Antonino Furnari, Moritz Nottebaum, Antonio Finocchiaro, Davide Marana, Rosario Forte, Giovanni Maria Farinella, Christian Micheloni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16934">Online Episodic Memory Visual Query Localization with Egocentric Streaming Object Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Episodic memory retrieval enables wearable cameras to recall objects or events previously observed in video. However, existing formulations assume an "offline" setting with full video access at query time, limiting their applicability in real-world scenarios with power and storage-constrained wearable devices. Towards more application-ready episodic memory systems, we introduce Online Visual Query 2D (OVQ2D), a task where models process video streams online, observing each frame only once, and retrieve object localizations using a compact memory instead of full video history. We address OVQ2D with ESOM (Egocentric Streaming Object Memory), a novel framework integrating an object discovery module, an object tracking module, and a memory module that find, track, and store spatio-temporal object information for efficient querying. Experiments on Ego4D demonstrate ESOM's superiority over other online approaches, though OVQ2D remains challenging, with top performance at only ~4% success. ESOM's accuracy increases markedly with perfect object tracking (31.91%), discovery (40.55%), or both (81.92%), underscoring the need of applied research on these components.
<div id='section'>PaperID: <span id='pid'>182, <a href='https://arxiv.org/pdf/2512.21641.pdf' target='_blank'>https://arxiv.org/pdf/2512.21641.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21641">TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.
<div id='section'>PaperID: <span id='pid'>183, <a href='https://arxiv.org/pdf/2507.03441.pdf' target='_blank'>https://arxiv.org/pdf/2507.03441.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matthias Zeller, Daniel Casado Herraez, Jens Behley, Michael Heidingsfeld, Cyrill Stachniss
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03441">Radar Tracker: Moving Instance Tracking in Sparse and Noisy Radar Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots and autonomous vehicles should be aware of what happens in their surroundings. The segmentation and tracking of moving objects are essential for reliable path planning, including collision avoidance. We investigate this estimation task for vehicles using radar sensing. We address moving instance tracking in sparse radar point clouds to enhance scene interpretation. We propose a learning-based radar tracker incorporating temporal offset predictions to enable direct center-based association and enhance segmentation performance by including additional motion cues. We implement attention-based tracking for sparse radar scans to include appearance features and enhance performance. The final association combines geometric and appearance features to overcome the limitations of center-based tracking to associate instances reliably. Our approach shows an improved performance on the moving instance tracking benchmark of the RadarScenes dataset compared to the current state of the art.
<div id='section'>PaperID: <span id='pid'>184, <a href='https://arxiv.org/pdf/2505.19420.pdf' target='_blank'>https://arxiv.org/pdf/2505.19420.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenhua Wu, Chenpeng Su, Siting Zhu, Tianchen Deng, Jianhao Jiao, Guangming Wang, Dimitrios Kanoulas, Zhe Liu, Hesheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19420">CAD-SLAM: Consistency-Aware Dynamic SLAM with Dynamic-Static Decoupled Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in neural radiation fields (NeRF) and 3D Gaussian-based SLAM have achieved impressive localization accuracy and high-quality dense mapping in static scenes. However, these methods remain challenged in dynamic environments, where moving objects violate the static-world assumption and introduce inconsistent observations that degrade both camera tracking and map reconstruction. This motivates two fundamental problems: robustly identifying dynamic objects and modeling them online. To address these limitations, we propose CAD-SLAM, a Consistency-Aware Dynamic SLAM framework with dynamic-static decoupled mapping. Our key insight is that dynamic objects inherently violate cross-view and cross-time scene consistency. We detect object motion by analyzing geometric and texture discrepancies between historical map renderings and real-world observations. Once a moving object is identified, we perform bidirectional dynamic object tracking (both backward and forward in time) to achieve complete sequence-wise dynamic recognition. Our consistency-aware dynamic detection model achieves category-agnostic, instantaneous dynamic identification, which effectively mitigates motion-induced interference during localization and mapping. In addition, we introduce a dynamic-static decoupled mapping strategy that employs a temporal Gaussian model for online incremental dynamic modeling. Experiments conducted on multiple dynamic datasets demonstrate the flexible and accurate dynamic segmentation capabilities of our method, along with the state-of-the-art performance in both localization and mapping.
<div id='section'>PaperID: <span id='pid'>185, <a href='https://arxiv.org/pdf/2511.18264.pdf' target='_blank'>https://arxiv.org/pdf/2511.18264.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ruijie Fan, Junyan Ye, Huan Chen, Zilong Huang, Xiaolei Wang, Weijia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18264">SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.
<div id='section'>PaperID: <span id='pid'>186, <a href='https://arxiv.org/pdf/2502.18748.pdf' target='_blank'>https://arxiv.org/pdf/2502.18748.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaheer Mohamed, Tharindu Fernando, Sridha Sridharan, Peyman Moghadam, Clinton Fookes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18748">Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking using snapshot mosaic cameras is emerging as it provides enhanced spectral information alongside spatial data, contributing to a more comprehensive understanding of material properties. Using transformers, which have consistently outperformed convolutional neural networks (CNNs) in learning better feature representations, would be expected to be effective for Hyperspectral object tracking. However, training large transformers necessitates extensive datasets and prolonged training periods. This is particularly critical for complex tasks like object tracking, and the scarcity of large datasets in the hyperspectral domain acts as a bottleneck in achieving the full potential of powerful transformer models. This paper proposes an effective methodology that adapts large pretrained transformer-based foundation models for hyperspectral object tracking. We propose an adaptive, learnable spatial-spectral token fusion module that can be extended to any transformer-based backbone for learning inherent spatial-spectral features in hyperspectral data. Furthermore, our model incorporates a cross-modality training pipeline that facilitates effective learning across hyperspectral datasets collected with different sensor modalities. This enables the extraction of complementary knowledge from additional modalities, whether or not they are present during testing. Our proposed model also achieves superior performance with minimal training iterations.
<div id='section'>PaperID: <span id='pid'>187, <a href='https://arxiv.org/pdf/2511.12026.pdf' target='_blank'>https://arxiv.org/pdf/2511.12026.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rulin Zhou, Wenlong He, An Wang, Jianhang Zhang, Xuanhui Zeng, Xi Zhang, Chaowei Zhu, Haijun Hu, Hongliang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12026">Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.
<div id='section'>PaperID: <span id='pid'>188, <a href='https://arxiv.org/pdf/2502.17822.pdf' target='_blank'>https://arxiv.org/pdf/2502.17822.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Peng Zhang, Xin Li, Xin Lin, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17822">Easy-Poly: A Easy Polyhedral Framework For 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D multi-object tracking (3D MOT) have predominantly relied on tracking-by-detection pipelines. However, these approaches often neglect potential enhancements in 3D detection processes, leading to high false positives (FP), missed detections (FN), and identity switches (IDS), particularly in challenging scenarios such as crowded scenes, small-object configurations, and adverse weather conditions. Furthermore, limitations in data preprocessing, association mechanisms, motion modeling, and life-cycle management hinder overall tracking robustness. To address these issues, we present Easy-Poly, a real-time, filter-based 3D MOT framework for multiple object categories. Our contributions include: (1) An Augmented Proposal Generator utilizing multi-modal data augmentation and refined SpConv operations, significantly improving mAP and NDS on nuScenes; (2) A Dynamic Track-Oriented (DTO) data association algorithm that effectively manages uncertainties and occlusions through optimal assignment and multiple hypothesis handling; (3) A Dynamic Motion Modeling (DMM) incorporating a confidence-weighted Kalman filter and adaptive noise covariances, enhancing MOTA and AMOTA in challenging conditions; and (4) An extended life-cycle management system with adjustive thresholds to reduce ID switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 64.96% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 74.5%), while also running in real-time. These findings highlight Easy-Poly's adaptability and robustness in diverse scenarios, making it a compelling choice for autonomous driving and related 3D MOT applications. The source code of this paper will be published upon acceptance.
<div id='section'>PaperID: <span id='pid'>189, <a href='https://arxiv.org/pdf/2410.23907.pdf' target='_blank'>https://arxiv.org/pdf/2410.23907.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Run Luo, Zikai Song, Longze Chen, Yunshui Li, Min Yang, Wei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23907">IP-MOT: Instance Prompt Learning for Cross-Domain Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) aims to associate multiple objects across video frames and is a challenging vision task due to inherent complexities in the tracking environment. Most existing approaches train and track within a single domain, resulting in a lack of cross-domain generalizability to data from other domains. While several works have introduced natural language representation to bridge the domain gap in visual tracking, these textual descriptions often provide too high-level a view and fail to distinguish various instances within the same class. In this paper, we address this limitation by developing IP-MOT, an end-to-end transformer model for MOT that operates without concrete textual descriptions. Our approach is underpinned by two key innovations: Firstly, leveraging a pre-trained vision-language model, we obtain instance-level pseudo textual descriptions via prompt-tuning, which are invariant across different tracking scenes; Secondly, we introduce a query-balanced strategy, augmented by knowledge distillation, to further boost the generalization capabilities of our model. Extensive experiments conducted on three widely used MOT benchmarks, including MOT17, MOT20, and DanceTrack, demonstrate that our approach not only achieves competitive performance on same-domain data compared to state-of-the-art models but also significantly improves the performance of query-based trackers by large margins for cross-domain inputs.
<div id='section'>PaperID: <span id='pid'>190, <a href='https://arxiv.org/pdf/2509.24741.pdf' target='_blank'>https://arxiv.org/pdf/2509.24741.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xue-Feng Zhu, Tianyang Xu, Yifan Pan, Jinjie Gu, Xi Li, Jiwen Lu, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24741">Collaborating Vision, Depth, and Thermal Signals for Multi-Modal Tracking: Dataset and Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multi-modal object tracking approaches primarily focus on dual-modal paradigms, such as RGB-Depth or RGB-Thermal, yet remain challenged in complex scenarios due to limited input modalities. To address this gap, this work introduces a novel multi-modal tracking task that leverages three complementary modalities, including visible RGB, Depth (D), and Thermal Infrared (TIR), aiming to enhance robustness in complex scenarios. To support this task, we construct a new multi-modal tracking dataset, coined RGBDT500, which consists of 500 videos with synchronised frames across the three modalities. Each frame provides spatially aligned RGB, depth, and thermal infrared images with precise object bounding box annotations. Furthermore, we propose a novel multi-modal tracker, dubbed RDTTrack. RDTTrack integrates tri-modal information for robust tracking by leveraging a pretrained RGB-only tracking model and prompt learning techniques. In specific, RDTTrack fuses thermal infrared and depth modalities under a proposed orthogonal projection constraint, then integrates them with RGB signals as prompts for the pre-trained foundation tracking model, effectively harmonising tri-modal complementary cues. The experimental results demonstrate the effectiveness and advantages of the proposed method, showing significant improvements over existing dual-modal approaches in terms of tracking accuracy and robustness in complex scenarios.
<div id='section'>PaperID: <span id='pid'>191, <a href='https://arxiv.org/pdf/2508.13000.pdf' target='_blank'>https://arxiv.org/pdf/2508.13000.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Hui Li, Shaochuan Zhao, Tao Zhou, Chunyang Cheng, Xiaojun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13000">Omni Survey for Multimodality Analysis in Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of smart cities has led to the generation of massive amounts of multi-modal data in the context of a range of tasks that enable a comprehensive monitoring of the smart city infrastructure and services. This paper surveys one of the most critical tasks, multi-modal visual object tracking (MMVOT), from the perspective of multimodality analysis. Generally, MMVOT differs from single-modal tracking in four key aspects, data collection, modality alignment and annotation, model designing, and evaluation. Accordingly, we begin with an introduction to the relevant data modalities, laying the groundwork for their integration. This naturally leads to a discussion of challenges of multi-modal data collection, alignment, and annotation. Subsequently, existing MMVOT methods are categorised, based on different ways to deal with visible (RGB) and X modalities: programming the auxiliary X branch with replicated or non-replicated experimental configurations from the RGB branch. Here X can be thermal infrared (T), depth (D), event (E), near infrared (NIR), language (L), or sonar (S). The final part of the paper addresses evaluation and benchmarking. In summary, we undertake an omni survey of all aspects of multi-modal visual object tracking (VOT), covering six MMVOT tasks and featuring 338 references in total. In addition, we discuss the fundamental rhetorical question: Is multi-modal tracking always guaranteed to provide a superior solution to unimodal tracking with the help of information fusion, and if not, in what circumstances its application is beneficial. Furthermore, for the first time in this field, we analyse the distributions of the object categories in the existing MMVOT datasets, revealing their pronounced long-tail nature and a noticeable lack of animal categories when compared with RGB datasets.
<div id='section'>PaperID: <span id='pid'>192, <a href='https://arxiv.org/pdf/2412.12561.pdf' target='_blank'>https://arxiv.org/pdf/2412.12561.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenjun Huang, Yang Ni, Hanning Chen, Yirui He, Ian Bryant, Yezi Liu, Mohsen Imani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12561">Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to localize an arbitrary number of targets based on a language expression and continuously track them in a video. This intricate task involves reasoning on multi-modal data and precise target localization with temporal association. However, prior studies overlook the imbalanced data distribution between newborn targets and existing targets due to the nature of the task. In addition, they only indirectly fuse multi-modal features, struggling to deliver clear guidance on newborn target detection. To solve the above issues, we conduct a collaborative matching strategy to alleviate the impact of the imbalance, boosting the ability to detect newborn targets while maintaining tracking performance. In the encoder, we integrate and enhance the cross-modal and multi-scale fusion, overcoming the bottlenecks in previous work, where limited multi-modal information is shared and interacted between feature maps. In the decoder, we also develop a referring-infused adaptation that provides explicit referring guidance through the query tokens. The experiments showcase the superior performance of our model (+3.42%) compared to prior works, demonstrating the effectiveness of our designs.
<div id='section'>PaperID: <span id='pid'>193, <a href='https://arxiv.org/pdf/2510.19078.pdf' target='_blank'>https://arxiv.org/pdf/2510.19078.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhongyu Jiang, Wenhao Chai, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19078">UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a growing interest in developing effective alignment pipelines to generate unified representations from different modalities for multi-modal fusion and generation. As an important component of Human-Centric applications, Human Pose representations are critical in many downstream tasks, such as Human Pose Estimation, Action Recognition, Human-Computer Interaction, Object tracking, etc. Human Pose representations or embeddings can be extracted from images, 2D keypoints, 3D skeletons, mesh models, and lots of other modalities. Yet, there are limited instances where the correlation among all of those representations has been clearly researched using a contrastive paradigm. In this paper, we propose UniHPR, a unified Human Pose Representation learning pipeline, which aligns Human Pose embeddings from images, 2D and 3D human poses. To align more than two data representations at the same time, we propose a novel singular value-based contrastive learning loss, which better aligns different modalities and further boosts performance. To evaluate the effectiveness of the aligned representation, we choose 2D and 3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, with a simple 3D human pose decoder, UniHPR achieves remarkable performance metrics: MPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW dataset with cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D pose retrieval with our unified human pose representations in Human3.6M dataset, where the retrieval error is 9.24mm in MPJPE.
<div id='section'>PaperID: <span id='pid'>194, <a href='https://arxiv.org/pdf/2509.19851.pdf' target='_blank'>https://arxiv.org/pdf/2509.19851.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Benjamin Bogenberger, Oliver Harrison, Orrin Dahanaggamaarachchi, Lukas Brunke, Jingxing Qian, Siqi Zhou, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19851">Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots deployed in real-world environments, such as homes, must not only navigate safely but also understand their surroundings and adapt to environment changes. To perform tasks efficiently, they must build and maintain a semantic map that accurately reflects the current state of the environment. Existing research on semantic exploration largely focuses on static scenes without persistent object-level instance tracking. A consistent map is, however, crucial for real-world robotic applications where objects in the environment can be removed, reintroduced, or shifted over time. In this work, to close this gap, we propose an open-vocabulary, semantic exploration system for semi-static environments. Our system maintains a consistent map by building a probabilistic model of object instance stationarity, systematically tracking semi-static changes, and actively exploring areas that have not been visited for a prolonged period of time. In addition to active map maintenance, our approach leverages the map's semantic richness with LLM-based reasoning for open-vocabulary object-goal navigation. This enables the robot to search more efficiently by prioritizing contextually relevant areas. We evaluate our approach across multiple real-world semi-static environments. Our system detects 95% of map changes on average, improving efficiency by more than 29% as compared to random and patrol baselines. Overall, our approach achieves a mapping precision within 2% of a fully rebuilt map while requiring substantially less exploration and further completes object goal navigation tasks about 14% faster than the next-best tested strategy (coverage patrolling). A video of our work can be found at http://tiny.cc/sem-explor-semi-static .
<div id='section'>PaperID: <span id='pid'>195, <a href='https://arxiv.org/pdf/2506.20381.pdf' target='_blank'>https://arxiv.org/pdf/2506.20381.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ben Kang, Xin Chen, Jie Zhao, Chunjuan Bo, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20381">Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based visual trackers have demonstrated significant advancements due to their powerful modeling capabilities. However, their practicality is limited on resource-constrained devices because of their slow processing speeds. To address this challenge, we present HiT, a novel family of efficient tracking models that achieve high performance while maintaining fast operation across various devices. The core innovation of HiT lies in its Bridge Module, which connects lightweight transformers to the tracking framework, enhancing feature representation quality. Additionally, we introduce a dual-image position encoding approach to effectively encode spatial information. HiT achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark, outperforming all previous efficient trackers.Building on HiT, we propose DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by selecting routes with varying computational requirements. DyHiT uses search area features extracted by the backbone network and inputs them into an efficient dynamic router to classify tracking scenarios. Based on the classification, DyHiT applies a divide-and-conquer strategy, selecting appropriate routes to achieve a superior trade-off between accuracy and speed. The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free acceleration method based on the dynamic routing architecture of DyHiT. This method significantly improves the execution speed of various high-performance trackers without sacrificing accuracy. For instance, our acceleration method enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of 69.9% on the LaSOT.
<div id='section'>PaperID: <span id='pid'>196, <a href='https://arxiv.org/pdf/2503.08145.pdf' target='_blank'>https://arxiv.org/pdf/2503.08145.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunhao Li, Yifan Jiao, Dan Meng, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08145">Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to track objects without being limited to a predefined set of categories. Current OV-MOT methods typically rely primarily on instance-level detection and association, often overlooking trajectory information that is unique and essential for object tracking tasks. Utilizing trajectory information can enhance association stability and classification accuracy, especially in cases of occlusion and category ambiguity, thereby improving adaptability to novel classes. Thus motivated, in this paper we propose \textbf{TRACT}, an open-vocabulary tracker that leverages trajectory information to improve both object association and classification in OV-MOT. Specifically, we introduce a \textit{Trajectory Consistency Reinforcement} (\textbf{TCR}) strategy, that benefits tracking performance by improving target identity and category consistency. In addition, we present \textbf{TraCLIP}, a plug-and-play trajectory classification module. It integrates \textit{Trajectory Feature Aggregation} (\textbf{TFA}) and \textit{Trajectory Semantic Enrichment} (\textbf{TSE}) strategies to fully leverage trajectory information from visual and language perspectives for enhancing the classification results. Extensive experiments on OV-TAO show that our TRACT significantly improves tracking performance, highlighting trajectory information as a valuable asset for OV-MOT. Code will be released.
<div id='section'>PaperID: <span id='pid'>197, <a href='https://arxiv.org/pdf/2412.19138.pdf' target='_blank'>https://arxiv.org/pdf/2412.19138.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19138">SUTrack: Towards Simple and Unified Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a simple yet unified single object tracking (SOT) framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based, RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model trained in a single session. Due to the distinct nature of the data, current methods typically design individual architectures and train separate models for each task. This fragmentation results in redundant training processes, repetitive technological innovations, and limited cross-modal knowledge sharing. In contrast, SUTrack demonstrates that a single model with a unified input representation can effectively handle various common SOT tasks, eliminating the need for task-specific designs and separate training sessions. Additionally, we introduce a task-recognition auxiliary training strategy and a soft token type embedding to further enhance SUTrack's performance with minimal overhead. Experiments show that SUTrack outperforms previous task-specific counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a range of models catering edge devices as well as high-performance GPUs, striking a good trade-off between speed and accuracy. We hope SUTrack could serve as a strong foundation for further compelling research into unified tracking models. Code and models are available at github.com/chenxin-dlut/SUTrack.
<div id='section'>PaperID: <span id='pid'>198, <a href='https://arxiv.org/pdf/2510.00181.pdf' target='_blank'>https://arxiv.org/pdf/2510.00181.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Luis Burbano, Diego Ortiz, Qi Sun, Siwei Yang, Haoqin Tu, Cihang Xie, Yinzhi Cao, Alvaro A Cardenas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00181">CHAI: Command Hijacking against embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms state-of-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.
<div id='section'>PaperID: <span id='pid'>199, <a href='https://arxiv.org/pdf/2503.22199.pdf' target='_blank'>https://arxiv.org/pdf/2503.22199.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Long Gao, Yunhe Zhang, Langkun Chen, Yan Jiang, Weiying Xie, Yunsong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22199">Hyperspectral Adapter for Object Tracking based on Hyperspectral Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking based on hyperspectral video attracts increasing attention to the rich material and motion information in the hyperspectral videos. The prevailing hyperspectral methods adapt pretrained RGB-based object tracking networks for hyperspectral tasks by fine-tuning the entire network on hyperspectral datasets, which achieves impressive results in challenging scenarios. However, the performance of hyperspectral trackers is limited by the loss of spectral information during the transformation, and fine-tuning the entire pretrained network is inefficient for practical applications. To address the issues, a new hyperspectral object tracking method, hyperspectral adapter for tracking (HyA-T), is proposed in this work. The hyperspectral adapter for the self-attention (HAS) and the hyperspectral adapter for the multilayer perceptron (HAM) are proposed to generate the adaption information and to transfer the multi-head self-attention (MSA) module and the multilayer perceptron (MLP) in pretrained network for the hyperspectral object tracking task by augmenting the adaption information into the calculation of the MSA and MLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed to augment the original spectral information into the input of the tracking network. The proposed methods extract spectral information directly from the hyperspectral images, which prevent the loss of the spectral information. Moreover, only the parameters in the proposed methods are fine-tuned, which is more efficient than the existing methods. Extensive experiments were conducted on four datasets with various spectral bands, verifing the effectiveness of the proposed methods. The HyA-T achieves state-of-the-art performance on all the datasets.
<div id='section'>PaperID: <span id='pid'>200, <a href='https://arxiv.org/pdf/2411.06702.pdf' target='_blank'>https://arxiv.org/pdf/2411.06702.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jia Syuen Lim, Yadan Luo, Zhi Chen, Tianqi Wei, Scott Chapman, Zi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06702">Track Any Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Detection and Multi-Object Tracking of Sweet Peppers Challenge, we present Track Any Peppers (TAP) - a weakly supervised ensemble technique for sweet peppers tracking. TAP leverages the zero-shot detection capabilities of vision-language foundation models like Grounding DINO to automatically generate pseudo-labels for sweet peppers in video sequences with minimal human intervention. These pseudo-labels, refined when necessary, are used to train a YOLOv8 segmentation network. To enhance detection accuracy under challenging conditions, we incorporate pre-processing techniques such as relighting adjustments and apply depth-based filtering during post-inference. For object tracking, we integrate the Matching by Segment Anything (MASA) adapter with the BoT-SORT algorithm. Our approach achieves a HOTA score of 80.4%, MOTA of 66.1%, Recall of 74.0%, and Precision of 90.7%, demonstrating effective tracking of sweet peppers without extensive manual effort. This work highlights the potential of foundation models for efficient and accurate object detection and tracking in agricultural settings.
<div id='section'>PaperID: <span id='pid'>201, <a href='https://arxiv.org/pdf/2409.04979.pdf' target='_blank'>https://arxiv.org/pdf/2409.04979.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhiwei Lin, Zhe Liu, Yongtao Wang, Le Zhang, Ce Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04979">RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.
<div id='section'>PaperID: <span id='pid'>202, <a href='https://arxiv.org/pdf/2508.01599.pdf' target='_blank'>https://arxiv.org/pdf/2508.01599.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Minhaj Uddin Ahmad, Sagar Dasgupta, Mizanur Rahman, Sakib Khan, Md Wasiul Haque, Suhala Rabab Saba, David Bodoh, Nathan Huynh, Li Zhao, Eren Erman Ozguven
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01599">Lessons Learned from the Real-World Deployment of Multi-Sensor Fusion for Proactive Work Zone Safety Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proactive safety systems that anticipate and mitigate traffic risks before incidents occur are increasingly recognized as essential for improving work zone safety. Unlike traditional reactive methods, these systems rely on real-time sensing, trajectory prediction, and intelligent infrastructure to detect potential hazards. Existing simulation-based studies often overlook, and real-world deployment studies rarely discuss the practical challenges associated with deploying such systems in operational settings, particularly those involving roadside infrastructure and multi-sensor integration and fusion. This study addresses that gap by presenting deployment insights and technical lessons learned from a real-world implementation of a multi-sensor safety system at an active bridge repair work zone along the N-2/US-75 corridor in Lincoln, Nebraska. The deployed system combines LiDAR, radar, and camera sensors with an edge computing platform to support multi-modal object tracking, trajectory fusion, and real-time analytics. Specifically, this study presents key lessons learned across three critical stages of deployment: (1) sensor selection and placement, (2) sensor calibration, system integration, and validation, and (3) sensor fusion. Additionally, we propose a predictive digital twin framework that leverages fused trajectory data for early conflict detection and real-time warning generation, enabling proactive safety interventions.
<div id='section'>PaperID: <span id='pid'>203, <a href='https://arxiv.org/pdf/2503.11496.pdf' target='_blank'>https://arxiv.org/pdf/2503.11496.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaofeng Liang, Runwei Guan, Wangwang Lian, Daizong Liu, Xiaolou Sun, Dongming Wu, Yutao Yue, Weiping Ding, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11496">Cognitive Disentanglement for Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a significant application of multi-source information fusion in intelligent transportation perception systems, Referring Multi-Object Tracking (RMOT) involves localizing and tracking specific objects in video sequences based on language references. However, existing RMOT approaches often treat language descriptions as holistic embeddings and struggle to effectively integrate the rich semantic information contained in language expressions with visual features. This limitation is especially apparent in complex scenes requiring comprehensive understanding of both static object attributes and spatial motion information. In this paper, we propose a Cognitive Disentanglement for Referring Multi-Object Tracking (CDRMT) framework that addresses these challenges. It adapts the "what" and "where" pathways from the human visual processing system to RMOT tasks. Specifically, our framework first establishes cross-modal connections while preserving modality-specific characteristics. It then disentangles language descriptions and hierarchically injects them into object queries, refining object understanding from coarse to fine-grained semantic levels. Finally, we reconstruct language representations based on visual features, ensuring that tracked objects faithfully reflect the referring expression. Extensive experiments on different benchmark datasets demonstrate that CDRMT achieves substantial improvements over state-of-the-art methods, with average gains of 6.0% in HOTA score on Refer-KITTI and 3.2% on Refer-KITTI-V2. Our approach advances the state-of-the-art in RMOT while simultaneously providing new insights into multi-source information fusion.
<div id='section'>PaperID: <span id='pid'>204, <a href='https://arxiv.org/pdf/2502.01896.pdf' target='_blank'>https://arxiv.org/pdf/2502.01896.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nastaran Darabi, Divake Kumar, Sina Tayebati, Amit Ranjan Trivedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01896">INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present INTACT, a novel two-phase framework designed to enhance the robustness of deep neural networks (DNNs) against noisy LiDAR data in safety-critical perception tasks. INTACT combines meta-learning with adversarial curriculum training (ACT) to systematically address challenges posed by data corruption and sparsity in 3D point clouds. The meta-learning phase equips a teacher network with task-agnostic priors, enabling it to generate robust saliency maps that identify critical data regions. The ACT phase leverages these saliency maps to progressively expose a student network to increasingly complex noise patterns, ensuring targeted perturbation and improved noise resilience. INTACT's effectiveness is demonstrated through comprehensive evaluations on object detection, tracking, and classification benchmarks using diverse datasets, including KITTI, Argoverse, and ModelNet40. Results indicate that INTACT improves model robustness by up to 20% across all tasks, outperforming standard adversarial and curriculum training methods. This framework not only addresses the limitations of conventional training strategies but also offers a scalable and efficient solution for real-world deployment in resource-constrained safety-critical systems. INTACT's principled integration of meta-learning and adversarial training establishes a new paradigm for noise-tolerant 3D perception in safety-critical applications. INTACT improved KITTI Multiple Object Tracking Accuracy (MOTA) by 9.6% (64.1% -> 75.1%) and by 12.4% under Gaussian noise (52.5% -> 73.7%). Similarly, KITTI mean Average Precision (mAP) rose from 59.8% to 69.8% (50% point drop) and 49.3% to 70.9% (Gaussian noise), highlighting the framework's ability to enhance deep learning model resilience in safety-critical object tracking scenarios.
<div id='section'>PaperID: <span id='pid'>205, <a href='https://arxiv.org/pdf/2501.02467.pdf' target='_blank'>https://arxiv.org/pdf/2501.02467.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinyu Zhou, Jinglun Li, Lingyi Hong, Kaixun Jiang, Pinxue Guo, Weifeng Ge, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02467">DeTrack: In-model Latent Denoising Learning for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the model's robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we utilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets.
<div id='section'>PaperID: <span id='pid'>206, <a href='https://arxiv.org/pdf/2505.19990.pdf' target='_blank'>https://arxiv.org/pdf/2505.19990.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jack Hong, Shilin Yan, Zehao Xiao, Jiayin Cai, Xiaolong Jiang, Yao Hu, Henghui Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19990">Progressive Scaling Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a progressive scaling training strategy for visual object tracking, systematically analyzing the influence of training data volume, model size, and input resolution on tracking performance. Our empirical study reveals that while scaling each factor leads to significant improvements in tracking accuracy, naive training suffers from suboptimal optimization and limited iterative refinement. To address this issue, we introduce DT-Training, a progressive scaling framework that integrates small teacher transfer and dual-branch alignment to maximize model potential. The resulting scaled tracker consistently outperforms state-of-the-art methods across multiple benchmarks, demonstrating strong generalization and transferability of the proposed method. Furthermore, we validate the broader applicability of our approach to additional tasks, underscoring its versatility beyond tracking.
<div id='section'>PaperID: <span id='pid'>207, <a href='https://arxiv.org/pdf/2412.09991.pdf' target='_blank'>https://arxiv.org/pdf/2412.09991.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mengmeng Wang, Teli Ma, Shuo Xin, Xiaojun Hou, Jiazheng Xing, Guang Dai, Jingdong Wang, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09991">Visual Object Tracking across Diverse Data Modalities: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is an attractive and significant research area in computer vision, which aims to recognize and track specific targets in video sequences where the target objects are arbitrary and class-agnostic. The VOT technology could be applied in various scenarios, processing data of diverse modalities such as RGB, thermal infrared and point cloud. Besides, since no one sensor could handle all the dynamic and varying environments, multi-modal VOT is also investigated. This paper presents a comprehensive survey of the recent progress of both single-modal and multi-modal VOT, especially the deep learning methods. Specifically, we first review three types of mainstream single-modal VOT, including RGB, thermal infrared and point cloud tracking. In particular, we conclude four widely-used single-modal frameworks, abstracting their schemas and categorizing the existing inheritors. Then we summarize four kinds of multi-modal VOT, including RGB-Depth, RGB-Thermal, RGB-LiDAR and RGB-Language. Moreover, the comparison results in plenty of VOT benchmarks of the discussed modalities are presented. Finally, we provide recommendations and insightful observations, inspiring the future development of this fast-growing literature.
<div id='section'>PaperID: <span id='pid'>208, <a href='https://arxiv.org/pdf/2507.09095.pdf' target='_blank'>https://arxiv.org/pdf/2507.09095.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Md Hasan Shahriar, Md Mohaimin Al Barat, Harshavardhan Sundar, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09095">On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal fusion (MMF) plays a critical role in the perception of autonomous driving, which primarily fuses camera and LiDAR streams for a comprehensive and efficient scene understanding. However, its strict reliance on precise temporal synchronization exposes it to new vulnerabilities. In this paper, we introduce DejaVu, a novel attack that exploits network-induced delays to create subtle temporal misalignments across sensor streams, severely degrading downstream MMF-based perception tasks. Our comprehensive attack analysis across different models and datasets reveals these sensors' task-specific imbalanced sensitivities: object detection is overly dependent on LiDAR inputs while object tracking is highly reliant on the camera inputs. Consequently, with a single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to 88.5%, while with a three-frame camera delay, multiple object tracking accuracy (MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense patch that can work alongside the existing perception model to monitor temporal alignment through cross-modal temporal consistency. AION leverages multimodal shared representation learning and dynamic time warping to determine the path of temporal alignment and calculate anomaly scores based on the alignment. Our thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with low false positives across datasets and model architectures, demonstrating it as a robust and generalized defense against the temporal misalignment attacks.
<div id='section'>PaperID: <span id='pid'>209, <a href='https://arxiv.org/pdf/2505.18111.pdf' target='_blank'>https://arxiv.org/pdf/2505.18111.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cheng-Yen Yang, Hsiang-Wei Huang, Pyong-Kun Kim, Chien-Kai Kuo, Jui-Wei Chang, Kwang-Ju Kim, Chung-I Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18111">Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an effective approach for adapting the Segment Anything Model 2 (SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the powerful pre-trained capabilities of SAM2 and incorporates several key techniques to enhance its performance in VOT applications. By combining SAM2 with our proposed optimizations, we achieved a first place AUC score of 89.4 on the 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the effectiveness of our approach. This paper details our methodology, the specific enhancements made to SAM2, and a comprehensive analysis of our results in the context of VOT solutions along with the multi-modality aspect of the dataset.
<div id='section'>PaperID: <span id='pid'>210, <a href='https://arxiv.org/pdf/2503.22943.pdf' target='_blank'>https://arxiv.org/pdf/2503.22943.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haoyang Wang, Ruishan Guo, Pengtao Ma, Ciyu Ruan, Xinyu Luo, Wenhua Ding, Tianyang Zhong, Jingao Xu, Yunhao Liu, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22943">Event Camera Meets Resource-Aware Mobile Computing: Abstraction, Algorithm, Acceleration, Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in achieving high-accuracy and low-latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution and low latency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, lack of stable, persistent semantic information, and large data volume pose challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature from 2014 to 2025 and presents a comprehensive overview of event-based mobile sensing, encompassing its fundamental principles, event \textit{abstraction} methods, \textit{algorithm} advancements, and both hardware and software \textit{acceleration} strategies. We discuss key \textit{applications} of event cameras in mobile sensing, including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving the event camera with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms. To support ongoing research, we provide an open-source \textit{Online Sheet} with recent developments. We hope this survey serves as a reference, facilitating the adoption of event-based vision across diverse applications.
<div id='section'>PaperID: <span id='pid'>211, <a href='https://arxiv.org/pdf/2503.01907.pdf' target='_blank'>https://arxiv.org/pdf/2503.01907.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kunjun Li, Cheng-Yen Yang, Hsiang-Wei Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01907">Technical Report for ReID-SAM on SkiTB Visual Tracking Challenge 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report introduces ReID-SAM, a novel model developed for the SkiTB Challenge that addresses the complexities of tracking skier appearance. Our approach integrates the SAMURAI tracker with a person re-identification (Re-ID) module and advanced post-processing techniques to enhance accuracy in challenging skiing scenarios. We employ an OSNet-based Re-ID model to minimize identity switches and utilize YOLOv11 with Kalman filtering or STARK-based object detection for precise equipment tracking. When evaluated on the SkiTB dataset, ReID-SAM achieved a state-of-the-art F1-score of 0.870, surpassing existing methods across alpine, ski jumping, and freestyle skiing disciplines. These results demonstrate significant advancements in skier tracking accuracy and provide valuable insights for computer vision applications in winter sports.
<div id='section'>PaperID: <span id='pid'>212, <a href='https://arxiv.org/pdf/2511.11478.pdf' target='_blank'>https://arxiv.org/pdf/2511.11478.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nhat Chung, Taisei Hanyu, Toan Nguyen, Huy Le, Frederick Bumgarner, Duy Minh Ho Nguyen, Khoa Vo, Kashu Yamazaki, Chase Rainwater, Tung Kieu, Anh Nguyen, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11478">Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.
<div id='section'>PaperID: <span id='pid'>213, <a href='https://arxiv.org/pdf/2507.05663.pdf' target='_blank'>https://arxiv.org/pdf/2507.05663.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Neelay Joglekar, Fei Liu, Florian Richter, Michael C. Yip
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05663">Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote Center of Motion (RCM) robotic manipulators have revolutionized Minimally Invasive Surgery, enabling precise, dexterous surgical manipulation within the patient's body cavity without disturbing the insertion point on the patient. Accurate RCM tool control is vital for incorporating autonomous subtasks like suturing, blood suction, and tumor resection into robotic surgical procedures, reducing surgeon fatigue and improving patient outcomes. However, these cable-driven systems are subject to significant joint reading errors, corrupting the kinematics computation necessary to perform control. Although visual tracking with endoscopic cameras can correct errors on in-view joints, errors in the kinematic chain prior to the insertion point are irreparable because they remain out of view. No prior work has characterized the stability of control under these conditions. We fill this gap by designing a provably stable tracking-in-the-loop controller for the out-of-view portion of the RCM manipulator kinematic chain. We additionally incorporate this controller into a bilevel control scheme for the full kinematic chain. We rigorously benchmark our method in simulated and real world settings to verify our theoretical findings. Our work provides key insights into the next steps required for the transition from teleoperated to autonomous surgery.
<div id='section'>PaperID: <span id='pid'>214, <a href='https://arxiv.org/pdf/2601.01925.pdf' target='_blank'>https://arxiv.org/pdf/2601.01925.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lianjie Jia, Yuhan Wu, Binghao Ran, Yifan Wang, Lijun Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.01925">AR-MOT: Autoregressive Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.
<div id='section'>PaperID: <span id='pid'>215, <a href='https://arxiv.org/pdf/2512.06251.pdf' target='_blank'>https://arxiv.org/pdf/2512.06251.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fangzhou Lin, Yuping Wang, Yuliang Guo, Zixun Huang, Xinyu Huang, Haichong Zhang, Kazunori Yamada, Zhengzhong Tu, Liu Ren, Ziming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06251">NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.
<div id='section'>PaperID: <span id='pid'>216, <a href='https://arxiv.org/pdf/2512.10945.pdf' target='_blank'>https://arxiv.org/pdf/2512.10945.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Henghui Ding, Chang Liu, Shuting He, Kaining Ying, Xudong Jiang, Chen Change Loy, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10945">MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/
<div id='section'>PaperID: <span id='pid'>217, <a href='https://arxiv.org/pdf/2508.00358.pdf' target='_blank'>https://arxiv.org/pdf/2508.00358.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yan Gong, Mengjun Chen, Hao Liu, Gao Yongsheng, Lei Yang, Naibang Wang, Ziying Song, Haoqun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00358">Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) enables autonomous vehicles to continuously perceive dynamic objects, supplying essential temporal cues for prediction, behavior understanding, and safe planning. However, conventional tracking-by-detection methods typically rely on static coordinate transformations based on ego-vehicle poses, disregarding ego-vehicle speed-induced variations in observation noise and reference frame changes, which degrades tracking stability and accuracy in dynamic, high-speed scenarios. In this paper, we investigate the critical role of ego-vehicle speed in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that dynamically adapts uncertainty modeling to ego-vehicle speed, significantly improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that adaptively predicts key parameters of SG-LKF. To enhance inter-frame association and trajectory continuity, we introduce a self-supervised trajectory consistency loss jointly optimized with semantic and positional constraints. Extensive experiments show that SG-LKF ranks first among all vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.
<div id='section'>PaperID: <span id='pid'>218, <a href='https://arxiv.org/pdf/2504.09361.pdf' target='_blank'>https://arxiv.org/pdf/2504.09361.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiahuan Long, Tingsong Jiang, Wen Yao, Shuai Jia, Weijia Zhang, Weien Zhou, Chao Ma, Xiaoqian Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09361">PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple objects in a continuous video stream is crucial for many computer vision tasks. It involves detecting and associating objects with their respective identities across successive frames. Despite significant progress made in multiple object tracking (MOT), recent studies have revealed the vulnerability of existing MOT methods to adversarial attacks. Nevertheless, all of these attacks belong to digital attacks that inject pixel-level noise into input images, and are therefore ineffective in physical scenarios. To fill this gap, we propose PapMOT, which can generate physical adversarial patches against MOT for both digital and physical scenarios. Besides attacking the detection mechanism, PapMOT also optimizes a printable patch that can be detected as new targets to mislead the identity association process. Moreover, we introduce a patch enhancement strategy to further degrade the temporal consistency of tracking results across video frames, resulting in more aggressive attacks. We further develop new evaluation metrics to assess the robustness of MOT against such attacks. Extensive evaluations on multiple datasets demonstrate that our PapMOT can successfully attack various architectures of MOT trackers in digital scenarios. We also validate the effectiveness of PapMOT for physical attacks by deploying printed adversarial patches in the real world.
<div id='section'>PaperID: <span id='pid'>219, <a href='https://arxiv.org/pdf/2409.17025.pdf' target='_blank'>https://arxiv.org/pdf/2409.17025.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Adrito Das, Bilal Sidiqi, Laurent Mennillo, Zhehua Mao, Mikael Brudfors, Miguel Xochicale, Danyal Z. Khan, Nicola Newall, John G. Hanrahan, Matthew J. Clarkson, Danail Stoyanov, Hani J. Marcus, Sophia Bano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17025">Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improved surgical skill is generally associated with improved patient outcomes, although assessment is subjective; labour-intensive; and requires domain specific expertise. Automated data driven metrics can alleviate these difficulties, as demonstrated by existing machine learning instrument tracking models in minimally invasive surgery. However, these models have been tested on limited datasets of laparoscopic surgery, with a focus on isolated tasks and robotic surgery. In this paper, a new public dataset is introduced, focusing on simulated surgery, using the nasal phase of endoscopic pituitary surgery as an exemplar. Simulated surgery allows for a realistic yet repeatable environment, meaning the insights gained from automated assessment can be used by novice surgeons to hone their skills on the simulator before moving to real surgery. PRINTNet (Pituitary Real-time INstrument Tracking Network) has been created as a baseline model for this automated assessment. Consisting of DeepLabV3 for classification and segmentation; StrongSORT for tracking; and the NVIDIA Holoscan SDK for real-time performance, PRINTNet achieved 71.9% Multiple Object Tracking Precision running at 22 Frames Per Second. Using this tracking output, a Multilayer Perceptron achieved 87% accuracy in predicting surgical skill level (novice or expert), with the "ratio of total procedure time to instrument visible time" correlated with higher surgical skill. This therefore demonstrates the feasibility of automated surgical skill assessment in simulated endoscopic pituitary surgery. The new publicly available dataset can be found here: https://doi.org/10.5522/04/26511049.
<div id='section'>PaperID: <span id='pid'>220, <a href='https://arxiv.org/pdf/2511.01427.pdf' target='_blank'>https://arxiv.org/pdf/2511.01427.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Xu Zhou, Feng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01427">UniSOT: A Unified Framework for Multi-Modality Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking aims to localize target object with specific reference modalities (bounding box, natural language or both) in a sequence of specific video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different reference modalities enable various human-machine interactions, and different video modalities are demanded in complex scenarios to enhance tracking robustness. Existing trackers are designed for single or several video modalities with single or several reference modalities, which leads to separate model designs and limits practical applications. Practically, a unified tracker is needed to handle various requirements. To the best of our knowledge, there is still no tracker that can perform tracking with these above reference modalities across these video modalities simultaneously. Thus, in this paper, we present a unified tracker, UniSOT, for different combinations of three reference modalities and four video modalities with uniform parameters. Extensive experimental results on 18 visual tracking, vision-language tracking and RGB+X tracking benchmarks demonstrate that UniSOT shows superior performance against modality-specific counterparts. Notably, UniSOT outperforms previous counterparts by over 3.0\% AUC on TNL2K across all three reference modalities and outperforms Un-Track by over 2.0\% main metric across all three RGB+X video modalities.
<div id='section'>PaperID: <span id='pid'>221, <a href='https://arxiv.org/pdf/2412.09097.pdf' target='_blank'>https://arxiv.org/pdf/2412.09097.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shengcai Zhou, Halvin Yang, Luping Xiang, Kun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09097">Temporal-Assisted Beamforming and Trajectory Prediction in Sensing-Enabled UAV Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the evolving landscape of high-speed communication, the shift from traditional pilot-based methods to a Sensing-Oriented Approach (SOA) is anticipated to gain momentum. This paper delves into the development of an innovative Integrated Sensing and Communication (ISAC) framework, specifically tailored for beamforming and trajectory prediction processes. Central to this research is the exploration of an Unmanned Aerial Vehicle (UAV)-enabled communication system, which seamlessly integrates ISAC technology. This integration underscores the synergistic interplay between sensing and communication capabilities. The proposed system initially deploys omnidirectional beams for the sensing-focused phase, subsequently transitioning to directional beams for precise object tracking. This process incorporates an Extended Kalman Filtering (EKF) methodology for the accurate estimation and prediction of object states. A novel frame structure is introduced, employing historical sensing data to optimize beamforming in real-time for subsequent time slots, a strategy we refer to as 'temporal-assisted' beamforming. To refine the temporal-assisted beamforming technique, we employ Successive Convex Approximation (SCA) in tandem with Iterative Rank Minimization (IRM), yielding high-quality suboptimal solutions. Comparative analysis with conventional pilot-based systems reveals that our approach yields a substantial improvement of 156\% in multi-object scenarios and 136\% in single-object scenarios.
<div id='section'>PaperID: <span id='pid'>222, <a href='https://arxiv.org/pdf/2412.04915.pdf' target='_blank'>https://arxiv.org/pdf/2412.04915.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Khurram Azeem Hashmi, Talha Uddin Sheikh, Didier Stricker, Muhammad Zeshan Afzal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04915">Beyond Boxes: Mask-Guided Spatio-Temporal Feature Aggregation for Video Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The primary challenge in Video Object Detection (VOD) is effectively exploiting temporal information to enhance object representations. Traditional strategies, such as aggregating region proposals, often suffer from feature variance due to the inclusion of background information. We introduce a novel instance mask-based feature aggregation approach, significantly refining this process and deepening the understanding of object dynamics across video frames. We present FAIM, a new VOD method that enhances temporal Feature Aggregation by leveraging Instance Mask features. In particular, we propose the lightweight Instance Feature Extraction Module (IFEM) to learn instance mask features and the Temporal Instance Classification Aggregation Module (TICAM) to aggregate instance mask and classification features across video frames. Using YOLOX as a base detector, FAIM achieves 87.9% mAP on the ImageNet VID dataset at 33 FPS on a single 2080Ti GPU, setting a new benchmark for the speed-accuracy trade-off. Additional experiments on multiple datasets validate that our approach is robust, method-agnostic, and effective in multi-object tracking, demonstrating its broader applicability to video understanding tasks.
<div id='section'>PaperID: <span id='pid'>223, <a href='https://arxiv.org/pdf/2411.15600.pdf' target='_blank'>https://arxiv.org/pdf/2411.15600.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15600">How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language tracking (VLT) extends traditional single object tracking by incorporating textual information, providing semantic guidance to enhance tracking performance under challenging conditions like fast motion and deformations. However, current VLT trackers often underperform compared to single-modality methods on multiple benchmarks, with semantic information sometimes becoming a "distraction." To address this, we propose VLTVerse, the first fine-grained evaluation framework for VLT trackers that comprehensively considers multiple challenge factors and diverse semantic information, hoping to reveal the role of language in VLT. Our contributions include: (1) VLTVerse introduces 10 sequence-level challenge labels and 6 types of multi-granularity semantic information, creating a flexible and multi-dimensional evaluation space for VLT; (2) leveraging 60 subspaces formed by combinations of challenge factors and semantic types, we conduct systematic fine-grained evaluations of three mainstream SOTA VLT trackers, uncovering their performance bottlenecks across complex scenarios and offering a novel perspective on VLT evaluation; (3) through decoupled analysis of experimental results, we examine the impact of various semantic types on specific challenge factors in relation to different algorithms, providing essential guidance for enhancing VLT across data, evaluation, and algorithmic dimensions. The VLTVerse, toolkit, and results will be available at \url{http://metaverse.aitestunion.com}.
<div id='section'>PaperID: <span id='pid'>224, <a href='https://arxiv.org/pdf/2410.02492.pdf' target='_blank'>https://arxiv.org/pdf/2410.02492.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02492">DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual language tracking (VLT) has emerged as a cutting-edge research area, harnessing linguistic data to enhance algorithms with multi-modal inputs and broadening the scope of traditional single object tracking (SOT) to encompass video understanding applications. Despite this, most VLT benchmarks still depend on succinct, human-annotated text descriptions for each video. These descriptions often fall short in capturing the nuances of video content dynamics and lack stylistic variety in language, constrained by their uniform level of detail and a fixed annotation frequency. As a result, algorithms tend to default to a "memorize the answer" strategy, diverging from the core objective of achieving a deeper understanding of video content. Fortunately, the emergence of large language models (LLMs) has enabled the generation of diverse text. This work utilizes LLMs to generate varied semantic annotations (in terms of text lengths and granularities) for representative SOT benchmarks, thereby establishing a novel multi-modal benchmark. Specifically, we (1) propose a new visual language tracking benchmark with diverse texts, named DTVLT, based on five prominent VLT and SOT benchmarks, including three sub-tasks: short-term tracking, long-term tracking, and global instance tracking. (2) We offer four granularity texts in our benchmark, considering the extent and density of semantic information. We expect this multi-granular generation strategy to foster a favorable environment for VLT and video understanding research. (3) We conduct comprehensive experimental analyses on DTVLT, evaluating the impact of diverse text on tracking performance and hope the identified performance bottlenecks of existing algorithms can support further research in VLT and video understanding. The proposed benchmark, experimental results and toolkit will be released gradually on http://videocube.aitestunion.com/.
<div id='section'>PaperID: <span id='pid'>225, <a href='https://arxiv.org/pdf/2505.20718.pdf' target='_blank'>https://arxiv.org/pdf/2505.20718.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kui Wu, Shuhang Xu, Hao Chen, Churan Wang, Zhoujun Li, Yizhou Wang, Fangwei Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20718">VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel self-improving framework that enhances Embodied Visual Tracking (EVT) with Vision-Language Models (VLMs) to address the limitations of current active visual tracking systems in recovering from tracking failure. Our approach combines the off-the-shelf active tracking methods with VLMs' reasoning capabilities, deploying a fast visual policy for normal tracking and activating VLM reasoning only upon failure detection. The framework features a memory-augmented self-reflection mechanism that enables the VLM to progressively improve by learning from past experiences, effectively addressing VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate significant performance improvements, with our framework boosting success rates by $72\%$ with state-of-the-art RL-based approaches and $220\%$ with PID-based methods in challenging environments. This work represents the first integration of VLM-based reasoning to assist EVT agents in proactive failure recovery, offering substantial advances for real-world robotic applications that require continuous target monitoring in dynamic, unstructured environments. Project website: https://sites.google.com/view/evt-recovery-assistant.
<div id='section'>PaperID: <span id='pid'>226, <a href='https://arxiv.org/pdf/2505.20710.pdf' target='_blank'>https://arxiv.org/pdf/2505.20710.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kui Wu, Hao Chen, Churan Wang, Fakhri Karray, Zhoujun Li, Yizhou Wang, Fangwei Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20710">Hierarchical Instruction-aware Embodied Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for reinforcement learning-based models due to the substantial gap between high-level user instructions and low-level agent actions. While recent advancements in language models (e.g., LLMs, VLMs, VLAs) have improved instruction comprehension, these models face critical limitations in either inference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To address these challenges, we propose \textbf{Hierarchical Instruction-aware Embodied Visual Tracking (HIEVT)} agent, which bridges instruction comprehension and action generation using \textit{spatial goals} as intermediaries. HIEVT first introduces \textit{LLM-based Semantic-Spatial Goal Aligner} to translate diverse human instructions into spatial goals that directly annotate the desired spatial position. Then the \textit{RL-based Adaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to position the target as specified by the spatial goal. To benchmark UC-EVT tasks, we collect over ten million trajectories for training and evaluate across one seen environment and nine unseen challenging environments. Extensive experiments and real-world deployments demonstrate the robustness and generalizability of HIEVT across diverse environments, varying target dynamics, and complex instruction combinations. The complete project is available at https://sites.google.com/view/hievt.
<div id='section'>PaperID: <span id='pid'>227, <a href='https://arxiv.org/pdf/2412.10749.pdf' target='_blank'>https://arxiv.org/pdf/2412.10749.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhangbin Li, Jinxing Zhou, Jing Zhang, Shengeng Tang, Kun Li, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10749">Patch-level Sounding Object Tracking for Audio-Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Answering questions related to audio-visual scenes, i.e., the AVQA task, is becoming increasingly popular. A critical challenge is accurately identifying and tracking sounding objects related to the question along the timeline. In this paper, we present a new Patch-level Sounding Object Tracking (PSOT) method. It begins with a Motion-driven Key Patch Tracking (M-KPT) module, which relies on visual motion information to identify salient visual patches with significant movements that are more likely to relate to sounding objects and questions. We measure the patch-wise motion intensity map between neighboring video frames and utilize it to construct and guide a motion-driven graph network. Meanwhile, we design a Sound-driven KPT (S-KPT) module to explicitly track sounding patches. This module also involves a graph network, with the adjacency matrix regularized by the audio-visual correspondence map. The M-KPT and S-KPT modules are performed in parallel for each temporal segment, allowing balanced tracking of salient and sounding objects. Based on the tracked patches, we further propose a Question-driven KPT (Q-KPT) module to retain patches highly relevant to the question, ensuring the model focuses on the most informative clues. The audio-visual-question features are updated during the processing of these modules, which are then aggregated for final answer prediction. Extensive experiments on standard datasets demonstrate the effectiveness of our method, achieving competitive performance even compared to recent large-scale pretraining-based approaches.
<div id='section'>PaperID: <span id='pid'>228, <a href='https://arxiv.org/pdf/2410.08781.pdf' target='_blank'>https://arxiv.org/pdf/2410.08781.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pinxue Guo, Zixu Zhao, Jianxiong Gao, Chongruo Wu, Tong He, Zheng Zhang, Tianjun Xiao, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08781">VideoSAM: Open-World Video Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video segmentation is essential for advancing robotics and autonomous driving, particularly in open-world settings where continuous perception and object association across video frames are critical. While the Segment Anything Model (SAM) has excelled in static image segmentation, extending its capabilities to video segmentation poses significant challenges. We tackle two major hurdles: a) SAM's embedding limitations in associating objects across frames, and b) granularity inconsistencies in object segmentation. To this end, we introduce VideoSAM, an end-to-end framework designed to address these challenges by improving object tracking and segmentation consistency in dynamic environments. VideoSAM integrates an agglomerated backbone, RADIO, enabling object association through similarity metrics and introduces Cycle-ack-Pairs Propagation with a memory mechanism for stable object tracking. Additionally, we incorporate an autoregressive object-token mechanism within the SAM decoder to maintain consistent granularity across frames. Our method is extensively evaluated on the UVO and BURST benchmarks, and robotic videos from RoboTAP, demonstrating its effectiveness and robustness in real-world scenarios. All codes will be available.
<div id='section'>PaperID: <span id='pid'>229, <a href='https://arxiv.org/pdf/2602.16160.pdf' target='_blank'>https://arxiv.org/pdf/2602.16160.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Patrick Poggi, Divake Kumar, Theja Tulabandhula, Amit Ranjan Trivedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.16160">Uncertainty-Guided Inference-Time Depth Adaptation for Transformer-Based Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based single-object trackers achieve state-of-the-art accuracy but rely on fixed-depth inference, executing the full encoder--decoder stack for every frame regardless of visual complexity, thereby incurring unnecessary computational cost in long video sequences dominated by temporally coherent frames. We propose UncL-STARK, an architecture-preserving approach that enables dynamic, uncertainty-aware depth adaptation in transformer-based trackers without modifying the underlying network or adding auxiliary heads. The model is fine-tuned to retain predictive robustness at multiple intermediate depths using random-depth training with knowledge distillation, thus enabling safe inference-time truncation. At runtime, we derive a lightweight uncertainty estimate directly from the model's corner localization heatmaps and use it in a feedback-driven policy that selects the encoder and decoder depth for the next frame based on the prediction confidence by exploiting temporal coherence in video. Extensive experiments on GOT-10k and LaSOT demonstrate up to 12\% GFLOPs reduction, 8.9\% latency reduction, and 10.8\% energy savings while maintaining tracking accuracy within 0.2\% of the full-depth baseline across both short-term and long-term sequences.
<div id='section'>PaperID: <span id='pid'>230, <a href='https://arxiv.org/pdf/2510.05070.pdf' target='_blank'>https://arxiv.org/pdf/2510.05070.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05070">ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body loco-manipulation promises transformative capabilities for daily service and warehouse tasks. While recent advances in general motion tracking (GMT) have enabled humanoids to reproduce diverse human motions, these policies lack the precision and object awareness required for loco-manipulation. To this end, we introduce ResMimic, a two-stage residual learning framework for precise and expressive humanoid control from human motion data. First, a GMT policy, trained on large-scale human-only motion, serves as a task-agnostic base for generating human-like whole-body movements. An efficient but precise residual policy is then learned to refine the GMT outputs to improve locomotion and incorporate object interaction. To further facilitate efficient training, we design (i) a point-cloud-based object tracking reward for smoother optimization, (ii) a contact reward that encourages accurate humanoid body-object interactions, and (iii) a curriculum-based virtual object controller to stabilize early training. We evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results show substantial gains in task success, training efficiency, and robustness over strong baselines. Videos are available at https://resmimic.github.io/ .
<div id='section'>PaperID: <span id='pid'>231, <a href='https://arxiv.org/pdf/2508.08219.pdf' target='_blank'>https://arxiv.org/pdf/2508.08219.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wentao Sun, Quanyun Wu, Hanqing Xu, Kyle Gao, Zhengsen Xu, Yiping Chen, Dedong Zhang, Lingfei Ma, John S. Zelek, Jonathan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08219">SAGOnline: Segment Any Gaussians Online</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Current methods suffer from prohibitive computational costs, limited 3D spatial reasoning, and an inability to track multiple objects simultaneously. We present Segment Any Gaussians Online (SAGOnline), a lightweight and zero-shot framework for real-time 3D segmentation in Gaussian scenes that addresses these limitations through two key innovations: (1) a decoupled strategy that integrates video foundation models (e.g., SAM2) for view-consistent 2D mask propagation across synthesized views; and (2) a GPU-accelerated 3D mask generation and Gaussian-level instance labeling algorithm that assigns unique identifiers to 3D primitives, enabling lossless multi-object tracking and segmentation across views. SAGOnline achieves state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU) benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times in inference speed (27 ms/frame). Qualitative results demonstrate robust multi-object segmentation and tracking in complex scenes. Our contributions include: (i) a lightweight and zero-shot framework for 3D segmentation in Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling simultaneous segmentation and tracking, and (iii) the effective adaptation of 2D video foundation models to the 3D domain. This work allows real-time rendering and 3D scene understanding, paving the way for practical AR/VR and robotic applications.
<div id='section'>PaperID: <span id='pid'>232, <a href='https://arxiv.org/pdf/2411.18850.pdf' target='_blank'>https://arxiv.org/pdf/2411.18850.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lipeng Gu, Xuefeng Yan, Weiming Wang, Honghua Chen, Dingkun Zhu, Liangliang Nan, Mingqiang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18850">CrossTracker: Robust Multi-modal 3D Multi-Object Tracking via Cross Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fusion of camera- and LiDAR-based detections offers a promising solution to mitigate tracking failures in 3D multi-object tracking (MOT). However, existing methods predominantly exploit camera detections to correct tracking failures caused by potential LiDAR detection problems, neglecting the reciprocal benefit of refining camera detections using LiDAR data. This limitation is rooted in their single-stage architecture, akin to single-stage object detectors, lacking a dedicated trajectory refinement module to fully exploit the complementary multi-modal information. To this end, we introduce CrossTracker, a novel two-stage paradigm for online multi-modal 3D MOT. CrossTracker operates in a coarse-to-fine manner, initially generating coarse trajectories and subsequently refining them through an independent refinement process. Specifically, CrossTracker incorporates three essential modules: i) a multi-modal modeling (M^3) module that, by fusing multi-modal information (images, point clouds, and even plane geometry extracted from images), provides a robust metric for subsequent trajectory generation. ii) a coarse trajectory generation (C-TG) module that generates initial coarse dual-stream trajectories, and iii) a trajectory refinement (TR) module that refines coarse trajectories through cross correction between camera and LiDAR streams. Comprehensive experiments demonstrate the superior performance of our CrossTracker over its eighteen competitors, underscoring its effectiveness in harnessing the synergistic benefits of camera and LiDAR sensors for robust multi-modal 3D MOT.
<div id='section'>PaperID: <span id='pid'>233, <a href='https://arxiv.org/pdf/2412.07392.pdf' target='_blank'>https://arxiv.org/pdf/2412.07392.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhayy Ud Din, Ahsan B. Bakht, Waseem Akram, Yihao Dong, Lakmal Seneviratne, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07392">Benchmarking Vision-Based Object Tracking for USVs in Complex Maritime Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based target tracking is crucial for unmanned surface vehicles (USVs) to perform tasks such as inspection, monitoring, and surveillance. However, real-time tracking in complex maritime environments is challenging due to dynamic camera movement, low visibility, and scale variation. Typically, object detection methods combined with filtering techniques are commonly used for tracking, but they often lack robustness, particularly in the presence of camera motion and missed detections. Although advanced tracking methods have been proposed recently, their application in maritime scenarios is limited. To address this gap, this study proposes a vision-guided object-tracking framework for USVs, integrating state-of-the-art tracking algorithms with low-level control systems to enable precise tracking in dynamic maritime environments. We benchmarked the performance of seven distinct trackers, developed using advanced deep learning techniques such as Siamese Networks and Transformers, by evaluating them on both simulated and real-world maritime datasets. In addition, we evaluated the robustness of various control algorithms in conjunction with these tracking systems. The proposed framework was validated through simulations and real-world sea experiments, demonstrating its effectiveness in handling dynamic maritime conditions. The results show that SeqTrack, a Transformer-based tracker, performed best in adverse conditions, such as dust storms. Among the control algorithms evaluated, the linear quadratic regulator controller (LQR) demonstrated the most robust and smooth control, allowing for stable tracking of the USV.
<div id='section'>PaperID: <span id='pid'>234, <a href='https://arxiv.org/pdf/2411.01816.pdf' target='_blank'>https://arxiv.org/pdf/2411.01816.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Thanh Nguyen Canh, Huy-Hoang Ngo, Xiem HoangVan, Nak Young Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01816">Toward Integrating Semantic-aware Path Planning and Reliable Localization for UAV Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localization is one of the most crucial tasks for Unmanned Aerial Vehicle systems (UAVs) directly impacting overall performance, which can be achieved with various sensors and applied to numerous tasks related to search and rescue operations, object tracking, construction, etc. However, due to the negative effects of challenging environments, UAVs may lose signals for localization. In this paper, we present an effective path-planning system leveraging semantic segmentation information to navigate around texture-less and problematic areas like lakes, oceans, and high-rise buildings using a monocular camera. We introduce a real-time semantic segmentation architecture and a novel keyframe decision pipeline to optimize image inputs based on pixel distribution, reducing processing time. A hierarchical planner based on the Dynamic Window Approach (DWA) algorithm, integrated with a cost map, is designed to facilitate efficient path planning. The system is implemented in a photo-realistic simulation environment using Unity, aligning with segmentation model parameters. Comprehensive qualitative and quantitative evaluations validate the effectiveness of our approach, showing significant improvements in the reliability and efficiency of UAV localization in challenging environments.
<div id='section'>PaperID: <span id='pid'>235, <a href='https://arxiv.org/pdf/2411.01756.pdf' target='_blank'>https://arxiv.org/pdf/2411.01756.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yiming Sun, Fan Yu, Shaoxiang Chen, Yu Zhang, Junwei Huang, Chenhui Li, Yang Li, Changbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01756">ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking aims to locate a targeted object in a video sequence based on an initial bounding box. Recently, Vision-Language~(VL) trackers have proposed to utilize additional natural language descriptions to enhance versatility in various applications. However, VL trackers are still inferior to State-of-The-Art (SoTA) visual trackers in terms of tracking performance. We found that this inferiority primarily results from their heavy reliance on manual textual annotations, which include the frequent provision of ambiguous language descriptions. In this paper, we propose ChatTracker to leverage the wealth of world knowledge in the Multimodal Large Language Model (MLLM) to generate high-quality language descriptions and enhance tracking performance. To this end, we propose a novel reflection-based prompt optimization module to iteratively refine the ambiguous and inaccurate descriptions of the target with tracking feedback. To further utilize semantic information produced by MLLM, a simple yet effective VL tracking framework is proposed and can be easily integrated as a plug-and-play module to boost the performance of both VL and visual trackers. Experimental results show that our proposed ChatTracker achieves a performance comparable to existing methods.
<div id='section'>PaperID: <span id='pid'>236, <a href='https://arxiv.org/pdf/2410.16695.pdf' target='_blank'>https://arxiv.org/pdf/2410.16695.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yang Yu, Yuezun Li, Xin Sun, Junyu Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16695">MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Phytoplankton are a crucial component of aquatic ecosystems, and effective monitoring of them can provide valuable insights into ocean environments and ecosystem changes. Traditional phytoplankton monitoring methods are often complex and lack timely analysis. Therefore, deep learning algorithms offer a promising approach for automated phytoplankton monitoring. However, the lack of large-scale, high-quality training samples has become a major bottleneck in advancing phytoplankton tracking. In this paper, we propose a challenging benchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse background information and variations in motion during observation. The dataset includes 27 species of phytoplankton and zooplankton, 14 different backgrounds to simulate diverse and complex underwater environments, and a total of 140 videos. To enable accurate real-time observation of phytoplankton, we introduce a multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion Tracker(DSFT), which addresses issues such as focus shifts during tracking and the loss of small target information when computing frame-to-frame similarity. Specifically, we introduce an additional feature extractor to predict the residuals of the standard feature extractor's output, and compute multi-scale frame-to-frame similarity based on features from different layers of the extractor. Extensive experiments on the MPT have demonstrated the validity of the dataset and the superiority of DSFT in tracking phytoplankton, providing an effective solution for phytoplankton monitoring.
<div id='section'>PaperID: <span id='pid'>237, <a href='https://arxiv.org/pdf/2509.02028.pdf' target='_blank'>https://arxiv.org/pdf/2509.02028.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Halima Bouzidi, Haoyu Liu, Mohammad Abdullah Al Faruque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02028">See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-vision understanding has driven the development of advanced perception systems, most notably the emerging paradigm of Referring Multi-Object Tracking (RMOT). By leveraging natural-language queries, RMOT systems can selectively track objects that satisfy a given semantic description, guided through Transformer-based spatial-temporal reasoning modules. End-to-End (E2E) RMOT models further unify feature extraction, temporal memory, and spatial reasoning within a Transformer backbone, enabling long-range spatial-temporal modeling over fused textual-visual representations. Despite these advances, the reliability and robustness of RMOT remain underexplored. In this paper, we examine the security implications of RMOT systems from a design-logic perspective, identifying adversarial vulnerabilities that compromise both the linguistic-visual referring and track-object matching components. Additionally, we uncover a novel vulnerability in advanced RMOT models employing FIFO-based memory, whereby targeted and consistent attacks on their spatial-temporal reasoning introduce errors that persist within the history buffer over multiple subsequent frames. We present VEIL, a novel adversarial framework designed to disrupt the unified referring-matching mechanisms of RMOT models. We show that carefully crafted digital and physical perturbations can corrupt the tracking logic reliability, inducing track ID switches and terminations. We conduct comprehensive evaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL and demonstrate the urgent need for security-aware RMOT designs for critical large-scale applications.
<div id='section'>PaperID: <span id='pid'>238, <a href='https://arxiv.org/pdf/2506.18737.pdf' target='_blank'>https://arxiv.org/pdf/2506.18737.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shanliang Yao, Runwei Guan, Yi Ni, Sen Xu, Yong Yue, Xiaohui Zhu, Ryan Wen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18737">USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking in inland waterways plays a crucial role in safe and cost-effective applications, including waterborne transportation, sightseeing tours, environmental monitoring and surface rescue. Our Unmanned Surface Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU, delivers robust tracking capabilities in complex waterborne environments. By leveraging these sensors, our USV collected comprehensive object tracking data, which we present as USVTrack, the first 4D radar-camera tracking dataset tailored for autonomous driving in new generation waterborne transportation systems. Our USVTrack dataset presents rich scenarios, featuring diverse various waterways, varying times of day, and multiple weather and lighting conditions. Moreover, we present a simple but effective radar-camera matching method, termed RCM, which can be plugged into popular two-stage association trackers. Experimental results utilizing RCM demonstrate the effectiveness of the radar-camera matching in improving object tracking accuracy and reliability for autonomous driving in waterborne environments. The USVTrack dataset is public on https://usvtrack.github.io.
<div id='section'>PaperID: <span id='pid'>239, <a href='https://arxiv.org/pdf/2503.09191.pdf' target='_blank'>https://arxiv.org/pdf/2503.09191.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Juana Valeria Hurtado, Sajad Marvi, Rohit Mohan, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09191">Learning Appearance and Motion Cues for Panoptic Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoptic tracking enables pixel-level scene interpretation of videos by integrating instance tracking in panoptic segmentation. This provides robots with a spatio-temporal understanding of the environment, an essential attribute for their operation in dynamic environments. In this paper, we propose a novel approach for panoptic tracking that simultaneously captures general semantic information and instance-specific appearance and motion features. Unlike existing methods that overlook dynamic scene attributes, our approach leverages both appearance and motion cues through dedicated network heads. These interconnected heads employ multi-scale deformable convolutions that reason about scene motion offsets with semantic context and motion-enhanced appearance features to learn tracking embeddings. Furthermore, we introduce a novel two-step fusion module that integrates the outputs from both heads by first matching instances from the current time step with propagated instances from previous time steps and subsequently refines associations using motion-enhanced appearance embeddings, improving robustness in challenging scenarios. Extensive evaluations of our proposed \netname model on two benchmark datasets demonstrate that it achieves state-of-the-art performance in panoptic tracking accuracy, surpassing prior methods in maintaining object identities over time. To facilitate future research, we make the code available at http://panoptictracking.cs.uni-freiburg.de
<div id='section'>PaperID: <span id='pid'>240, <a href='https://arxiv.org/pdf/2412.06258.pdf' target='_blank'>https://arxiv.org/pdf/2412.06258.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Li Yin, Calvin Yeung, Qingrui Hu, Jun Ichikawa, Hirotsugu Azechi, Susumu Takahashi, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06258">Enhanced Multi-Object Tracking Using Pose-based Virtual Markers in 3x3 Basketball</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is crucial for various multi-agent analyses such as evaluating team sports tactics and player movements and performance. While pedestrian tracking has advanced with Tracking-by-Detection MOT, team sports like basketball pose unique challenges. These challenges include players' unpredictable movements, frequent close interactions, and visual similarities that complicate pose labeling and lead to significant occlusions, frequent ID switches, and high manual annotation costs. To address these challenges, we propose a novel pose-based virtual marker (VM) MOT method for team sports, named Sports-vmTracking. This method builds on the vmTracking approach developed for multi-animal tracking with active learning. First, we constructed a 3x3 basketball pose dataset for VMs and applied active learning to enhance model performance in generating VMs. Then, we overlaid the VMs on video to identify players, extract their poses with unique IDs, and convert these into bounding boxes for comparison with automated MOT methods. Using our 3x3 basketball dataset, we demonstrated that our VM configuration has been highly effective, and reduced the need for manual corrections and labeling during pose model training while maintaining high accuracy. Our approach achieved an average HOTA score of 72.3%, over 10 points higher than other state-of-the-art methods without VM, and resulted in 0 ID switches. Beyond improving performance in handling occlusions and minimizing ID switches, our framework could substantially increase the time and cost efficiency compared to traditional manual annotation.
<div id='section'>PaperID: <span id='pid'>241, <a href='https://arxiv.org/pdf/2412.02734.pdf' target='_blank'>https://arxiv.org/pdf/2412.02734.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhaofeng Hu, Sifan Zhou, Zhihang Yuan, Dawei Yang, Shibo Zhao, Ci-Jyun Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02734">MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking is essential in autonomous driving and robotics. Existing methods often struggle with sparse and incomplete point cloud scenarios. To address these limitations, we propose a Multimodal-guided Virtual Cues Projection (MVCP) scheme that generates virtual cues to enrich sparse point clouds. Additionally, we introduce an enhanced tracker MVCTrack based on the generated virtual cues. Specifically, the MVCP scheme seamlessly integrates RGB sensors into LiDAR-based systems, leveraging a set of 2D detections to create dense 3D virtual cues that significantly improve the sparsity of point clouds. These virtual cues can naturally integrate with existing LiDAR-based 3D trackers, yielding substantial performance gains. Extensive experiments demonstrate that our method achieves competitive performance on the NuScenes dataset.
<div id='section'>PaperID: <span id='pid'>242, <a href='https://arxiv.org/pdf/2510.25233.pdf' target='_blank'>https://arxiv.org/pdf/2510.25233.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jee Won Lee, Hansol Lim, Sooyeun Yang, Jongseong Brad Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25233">Hybrid Vision Servoing with Depp Alignment and GRU-Based Occlusion Recovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based control systems, such as image-based visual servoing (IBVS), have been extensively explored for precise robot manipulation. A persistent challenge, however, is maintaining robust target tracking under partial or full occlusions. Classical methods like Lucas-Kanade (LK) offer lightweight tracking but are fragile to occlusion and drift, while deep learning-based approaches often require continuous visibility and intensive computation. To address these gaps, we propose a hybrid visual tracking framework that bridges advanced perception with real-time servo control. First, a fast global template matcher constrains the pose search region; next, a deep-feature Lucas-Kanade module operating on early VGG layers refines alignment to sub-pixel accuracy (<2px); then, a lightweight residual regressor corrects local misalignments caused by texture degradation or partial occlusion. When visual confidence falls below a threshold, a GRU-based predictor seamlessly extrapolates pose updates from recent motion history. Crucially, the pipeline's final outputs-translation, rotation, and scale deltas-are packaged as direct control signals for 30Hz image-based servo loops. Evaluated on handheld video sequences with up to 90% occlusion, our system sustains under 2px tracking error, demonstrating the robustness and low-latency precision essential for reliable real-world robot vision applications.
<div id='section'>PaperID: <span id='pid'>243, <a href='https://arxiv.org/pdf/2509.02182.pdf' target='_blank'>https://arxiv.org/pdf/2509.02182.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shyma Alhuwaider, Motasem Alfarra, Juan C. Perez, Merey Ramazanova, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02182">ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel tracklet-based dataset for benchmarking test-time adaptation (TTA) methods. The aim of this dataset is to mimic the intricate challenges encountered in real-world environments such as images captured by hand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus on how models face distribution shifts, when deployed, and on violations to the customary independent-and-identically-distributed (i.i.d.) assumption in machine learning. Yet, these benchmarks fail to faithfully represent realistic scenarios that naturally display temporal dependencies, such as how consecutive frames from a video stream likely show the same object across time. We address this shortcoming of current datasets by proposing a novel TTA benchmark we call the "Inherent Temporal Dependencies" (ITD) dataset. We ensure the instances in ITD naturally embody temporal dependencies by collecting them from tracklets-sequences of object-centric images we compile from the bounding boxes of an object-tracking dataset. We use ITD to conduct a thorough experimental analysis of current TTA methods, and shed light on the limitations of these methods when faced with the challenges of temporal dependencies. Moreover, we build upon these insights and propose a novel adversarial memory initialization strategy to improve memory-based TTA methods. We find this strategy substantially boosts the performance of various methods on our challenging benchmark.
<div id='section'>PaperID: <span id='pid'>244, <a href='https://arxiv.org/pdf/2507.09880.pdf' target='_blank'>https://arxiv.org/pdf/2507.09880.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Keito Suzuki, Bang Du, Runfa Blark Li, Kunyao Chen, Lei Wang, Peng Liu, Ning Bi, Truong Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09880">OpenHuman4D: Open-Vocabulary 4D Human Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding dynamic 3D human representation has become increasingly critical in virtual and extended reality applications. However, existing human part segmentation methods are constrained by reliance on closed-set datasets and prolonged inference times, which significantly restrict their applicability. In this paper, we introduce the first 4D human parsing framework that simultaneously addresses these challenges by reducing the inference time and introducing open-vocabulary capabilities. Building upon state-of-the-art open-vocabulary 3D human parsing techniques, our approach extends the support to 4D human-centric video with three key innovations: 1) We adopt mask-based video object tracking to efficiently establish spatial and temporal correspondences, avoiding the necessity of segmenting all frames. 2) A novel Mask Validation module is designed to manage new target identification and mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating memory-conditioned attention and logits equalization for robust embedding fusion. Extensive experiments demonstrate the effectiveness and flexibility of the proposed method on 4D human-centric parsing tasks, achieving up to 93.3% acceleration compared to the previous state-of-the-art method, which was limited to parsing fixed classes.
<div id='section'>PaperID: <span id='pid'>245, <a href='https://arxiv.org/pdf/2601.14799.pdf' target='_blank'>https://arxiv.org/pdf/2601.14799.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qihua Liang, Liang Chen, Yaozong Zheng, Jian Nong, Zhiyi Mo, Bineng Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.14799">UBATrack: Spatio-Temporal State Space Model for General Multi-Modal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal object tracking has attracted considerable attention by integrating multiple complementary inputs (e.g., thermal, depth, and event data) to achieve outstanding performance. Although current general-purpose multi-modal trackers primarily unify various modal tracking tasks (i.e., RGB-Thermal infrared, RGB-Depth or RGB-Event tracking) through prompt learning, they still overlook the effective capture of spatio-temporal cues. In this work, we introduce a novel multi-modal tracking framework based on a mamba-style state space model, termed UBATrack. Our UBATrack comprises two simple yet effective modules: a Spatio-temporal Mamba Adapter (STMA) and a Dynamic Multi-modal Feature Mixer. The former leverages Mamba's long-sequence modeling capability to jointly model cross-modal dependencies and spatio-temporal visual cues in an adapter-tuning manner. The latter further enhances multi-modal representation capacity across multiple feature dimensions to improve tracking robustness. In this way, UBATrack eliminates the need for costly full-parameter fine-tuning, thereby improving the training efficiency of multi-modal tracking algorithms. Experiments show that UBATrack outperforms state-of-the-art methods on RGB-T, RGB-D, and RGB-E tracking benchmarks, achieving outstanding results on the LasHeR, RGBT234, RGBT210, DepthTrack, VOT-RGBD22, and VisEvent datasets.
<div id='section'>PaperID: <span id='pid'>246, <a href='https://arxiv.org/pdf/2512.01366.pdf' target='_blank'>https://arxiv.org/pdf/2512.01366.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yunzhe Li, Jiajun Yan, Yuzhou Wei, Kechen Liu, Yize Zhao, Chong Zhang, Hongzi Zhu, Li Lu, Shan Chang, Minyi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01366">BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists. In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user. The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud. To minimize the power consumption of the earbud and the phone while guaranteeing the best tracking accuracy, a novel 3D object tracking algorithm is devised, integrating both a Kalman filter based trajectory estimation scheme and an optimal image sampling strategy based on reinforcement learning. Moreover, the impact of constant user head movements on the tracking accuracy is significantly eliminated by leveraging the estimated pitch and yaw angles to correct the object depth estimation and align the camera coordinate system to the user's body coordinate system, respectively. We implement a prototype BlinkBud system and conduct extensive real-world experiments. Results show that BlinkBud is lightweight with ultra-low mean power consumptions of 29.8 mW and 702.6 mW on the earbud and smartphone, respectively, and can accurately detect hazards with a low average false positive ratio (FPR) and false negative ratio (FNR) of 4.90% and 1.47%, respectively.
<div id='section'>PaperID: <span id='pid'>247, <a href='https://arxiv.org/pdf/2511.21053.pdf' target='_blank'>https://arxiv.org/pdf/2511.21053.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenglizhao Chen, Shaofeng Liang, Runwei Guan, Xiaolou Sun, Haocheng Zhao, Haiyun Jiang, Tao Huang, Henghui Ding, Qing-Long Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21053">AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.
<div id='section'>PaperID: <span id='pid'>248, <a href='https://arxiv.org/pdf/2511.20716.pdf' target='_blank'>https://arxiv.org/pdf/2511.20716.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kun Guo, Yun Shen, Xijun Wang, Chaoqun You, Yun Rui, Tony Q. S. Quek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20716">Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.
<div id='section'>PaperID: <span id='pid'>249, <a href='https://arxiv.org/pdf/2510.14992.pdf' target='_blank'>https://arxiv.org/pdf/2510.14992.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Leela Krishna, Mengyang Zhao, Saicharithreddy Pasula, Harshit Rajgarhia, Abhishek Mukherji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14992">GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training robust world models requires large-scale, precisely labeled multimodal datasets, a process historically bottlenecked by slow and expensive manual annotation. We present a production-tested GAZE pipeline that automates the conversion of raw, long-form video into rich, task-ready supervision for world-model training. Our system (i) normalizes proprietary 360-degree formats into standard views and shards them for parallel processing; (ii) applies a suite of AI models (scene understanding, object tracking, audio transcription, PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii) consolidates signals into a structured output specification for rapid human validation. The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per review hour) and reduces human review volume by >80% through conservative auto-skipping of low-salience segments. By increasing label density and consistency while integrating privacy safeguards and chain-of-custody metadata, our method generates high-fidelity, privacy-aware datasets directly consumable for learning cross-modal dynamics and action-conditioned prediction. We detail our orchestration, model choices, and data dictionary to provide a scalable blueprint for generating high-quality world model training data without sacrificing throughput or governance.
<div id='section'>PaperID: <span id='pid'>250, <a href='https://arxiv.org/pdf/2505.20834.pdf' target='_blank'>https://arxiv.org/pdf/2505.20834.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20834">Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency. The code will be released.
<div id='section'>PaperID: <span id='pid'>251, <a href='https://arxiv.org/pdf/2503.12968.pdf' target='_blank'>https://arxiv.org/pdf/2503.12968.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12968">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.
<div id='section'>PaperID: <span id='pid'>252, <a href='https://arxiv.org/pdf/2502.05938.pdf' target='_blank'>https://arxiv.org/pdf/2502.05938.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sourav Sanyal, Amogh Joshi, Manish Nagaraj, Rohan Kumar Manna, Kaushik Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05938">Energy-Efficient Autonomous Aerial Navigation with Dynamic Vision Sensors: A Physics-Guided Neuromorphic Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based object tracking is a critical component for achieving autonomous aerial navigation, particularly for obstacle avoidance. Neuromorphic Dynamic Vision Sensors (DVS) or event cameras, inspired by biological vision, offer a promising alternative to conventional frame-based cameras. These cameras can detect changes in intensity asynchronously, even in challenging lighting conditions, with a high dynamic range and resistance to motion blur. Spiking neural networks (SNNs) are increasingly used to process these event-based signals efficiently and asynchronously. Meanwhile, physics-based artificial intelligence (AI) provides a means to incorporate system-level knowledge into neural networks via physical modeling. This enhances robustness, energy efficiency, and provides symbolic explainability. In this work, we present a neuromorphic navigation framework for autonomous drone navigation. The focus is on detecting and navigating through moving gates while avoiding collisions. We use event cameras for detecting moving objects through a shallow SNN architecture in an unsupervised manner. This is combined with a lightweight energy-aware physics-guided neural network (PgNN) trained with depth inputs to predict optimal flight times, generating near-minimum energy paths. The system is implemented in the Gazebo simulator and integrates a sensor-fused vision-to-planning neuro-symbolic framework built with the Robot Operating System (ROS) middleware. This work highlights the future potential of integrating event-based vision with physics-guided planning for energy-efficient autonomous navigation, particularly for low-latency decision-making.
<div id='section'>PaperID: <span id='pid'>253, <a href='https://arxiv.org/pdf/2501.00758.pdf' target='_blank'>https://arxiv.org/pdf/2501.00758.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenlong Xu, Bineng Zhong, Qihua Liang, Yaozong Zheng, Guorong Li, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00758">Less is More: Token Context-aware Learning for Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, several studies have shown that utilizing contextual information to perceive target states is crucial for object tracking. They typically capture context by incorporating multiple video frames. However, these naive frame-context methods fail to consider the importance of each patch within a reference frame, making them susceptible to noise and redundant tokens, which deteriorates tracking performance. To address this challenge, we propose a new token context-aware tracking pipeline named LMTrack, designed to automatically learn high-quality reference tokens for efficient visual tracking. Embracing the principle of Less is More, the core idea of LMTrack is to analyze the importance distribution of all reference tokens, where important tokens are collected, continually attended to, and updated. Specifically, a novel Token Context Memory module is designed to dynamically collect high-quality spatio-temporal information of a target in an autoregressive manner, eliminating redundant background tokens from the reference frames. Furthermore, an effective Unidirectional Token Attention mechanism is designed to establish dependencies between reference tokens and search frame, enabling robust cross-frame association and target localization. Extensive experiments demonstrate the superiority of our tracker, achieving state-of-the-art results on tracking benchmarks such as GOT-10K, TrackingNet, and LaSOT.
<div id='section'>PaperID: <span id='pid'>254, <a href='https://arxiv.org/pdf/2412.13615.pdf' target='_blank'>https://arxiv.org/pdf/2412.13615.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaohai Li, Bineng Zhong, Qihua Liang, Guorong Li, Zhiyi Mo, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13615">MambaLCT: Boosting Tracking via Long-term Context State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effectively constructing context information with long-term dependencies from video sequences is crucial for object tracking. However, the context length constructed by existing work is limited, only considering object information from adjacent frames or video clips, leading to insufficient utilization of contextual information. To address this issue, we propose MambaLCT, which constructs and utilizes target variation cues from the first frame to the current frame for robust tracking. First, a novel unidirectional Context Mamba module is designed to scan frame features along the temporal dimension, gathering target change cues throughout the entire sequence. Specifically, target-related information in frame features is compressed into a hidden state space through selective scanning mechanism. The target information across the entire video is continuously aggregated into target variation cues. Next, we inject the target change cues into the attention mechanism, providing temporal information for modeling the relationship between the template and search frames. The advantage of MambaLCT is its ability to continuously extend the length of the context, capturing complete target change cues, which enhances the stability and robustness of the tracker. Extensive experiments show that long-term context information enhances the model's ability to perceive targets in complex scenarios. MambaLCT achieves new SOTA performance on six benchmarks while maintaining real-time running speeds.
<div id='section'>PaperID: <span id='pid'>255, <a href='https://arxiv.org/pdf/2410.01806.pdf' target='_blank'>https://arxiv.org/pdf/2410.01806.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mattia Segu, Luigi Piccinelli, Siyuan Li, Yung-Hsu Yang, Bernt Schiele, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01806">Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their trajectories. However, it remains a key open research question on how to model long-range dependencies within tracklets, interdependencies among tracklets, and the associated temporal occlusions. To this end, we introduce Samba, a novel linear-time set-of-sequences model designed to jointly process multiple tracklets by synchronizing the multiple selective state-spaces used to model each tracklet. Samba autoregressively predicts the future track query for each sequence while maintaining synchronized long-term memory representations across tracklets. By integrating Samba into a tracking-by-propagation framework, we propose SambaMOTR, the first tracker effectively addressing the aforementioned issues, including long-range dependencies, tracklet interdependencies, and temporal occlusions. Additionally, we introduce an effective technique for dealing with uncertain observations (MaskObs) and an efficient training recipe to scale SambaMOTR to longer sequences. By modeling long-range dependencies and interactions among tracked objects, SambaMOTR implicitly learns to track objects accurately through occlusions without any hand-crafted heuristics. Our approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT, and SportsMOT datasets.
<div id='section'>PaperID: <span id='pid'>256, <a href='https://arxiv.org/pdf/2409.17221.pdf' target='_blank'>https://arxiv.org/pdf/2409.17221.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mattia Segu, Luigi Piccinelli, Siyuan Li, Luc Van Gool, Fisher Yu, Bernt Schiele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17221">Walker: Self-supervised Multiple Object Tracking by Walking on Temporal Appearance Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The supervision of state-of-the-art multiple object tracking (MOT) methods requires enormous annotation efforts to provide bounding boxes for all frames of all videos, and instance IDs to associate them through time. To this end, we introduce Walker, the first self-supervised tracker that learns from videos with sparse bounding box annotations, and no tracking labels. First, we design a quasi-dense temporal object appearance graph, and propose a novel multi-positive contrastive objective to optimize random walks on the graph and learn instance similarities. Then, we introduce an algorithm to enforce mutually-exclusive connective properties across instances in the graph, optimizing the learned topology for MOT. At inference time, we propose to associate detected instances to tracklets based on the max-likelihood transition state under motion-constrained bi-directional walks. Walker is the first self-supervised tracker to achieve competitive performance on MOT17, DanceTrack, and BDD100K. Remarkably, our proposal outperforms the previous self-supervised trackers even when drastically reducing the annotation requirements by up to 400x.
<div id='section'>PaperID: <span id='pid'>257, <a href='https://arxiv.org/pdf/2512.14595.pdf' target='_blank'>https://arxiv.org/pdf/2512.14595.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mengyu Li, Xingcheng Zhou, Guang Chen, Alois Knoll, Hu Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14595">TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.
<div id='section'>PaperID: <span id='pid'>258, <a href='https://arxiv.org/pdf/2511.19057.pdf' target='_blank'>https://arxiv.org/pdf/2511.19057.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hai Wu, Shuai Tang, Jiale Wang, Longkun Zou, Mingyue Guo, Rongqin Liang, Ke Chen, Yaowei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19057">LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.
<div id='section'>PaperID: <span id='pid'>259, <a href='https://arxiv.org/pdf/2509.14147.pdf' target='_blank'>https://arxiv.org/pdf/2509.14147.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fanxing Li, Shengyang Wang, Fangyu Sun, Shuyu Wu, Dexin Zuo, Wenxian Yu, Danping Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14147">StableTracker: Learning to Stably Track Target via Differentiable Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>FPV object tracking methods heavily rely on handcraft modular designs, resulting in hardware overload and cumulative error, which seriously degrades the tracking performance, especially for rapidly accelerating or decelerating targets. To address these challenges, we present \textbf{StableTracker}, a learning-based control policy that enables quadrotors to robustly follow the moving target from arbitrary perspectives. The policy is trained using backpropagation-through-time via differentiable simulation, allowing the quadrotor to maintain the target at the center of the visual field in both horizontal and vertical directions, while keeping a fixed relative distance, thereby functioning as an autonomous aerial camera. We compare StableTracker against both state-of-the-art traditional algorithms and learning baselines. Simulation experiments demonstrate that our policy achieves superior accuracy, stability and generalization across varying safe distances, trajectories, and target velocities. Furthermore, a real-world experiment on a quadrotor with an onboard computer validated practicality of the proposed approach.
<div id='section'>PaperID: <span id='pid'>260, <a href='https://arxiv.org/pdf/2508.01730.pdf' target='_blank'>https://arxiv.org/pdf/2508.01730.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jianbo Ma, Hui Luo, Qi Chen, Yuankai Qi, Yumei Sun, Amin Beheshti, Jianlin Zhang, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01730">Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.
<div id='section'>PaperID: <span id='pid'>261, <a href='https://arxiv.org/pdf/2507.19908.pdf' target='_blank'>https://arxiv.org/pdf/2507.19908.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mengmeng Wang, Haonan Wang, Yulong Li, Xiangjie Kong, Jiaxin Du, Guojiang Shen, Feng Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19908">TrackAny3D: Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D LiDAR-based single object tracking (SOT) relies on sparse and irregular point clouds, posing challenges from geometric variations in scale, motion patterns, and structural complexity across object categories. Current category-specific approaches achieve good accuracy but are impractical for real-world use, requiring separate models for each category and showing limited generalization. To tackle these issues, we propose TrackAny3D, the first framework to transfer large-scale pretrained 3D models for category-agnostic 3D SOT. We first integrate parameter-efficient adapters to bridge the gap between pretraining and tracking tasks while preserving geometric priors. Then, we introduce a Mixture-of-Geometry-Experts (MoGE) architecture that adaptively activates specialized subnetworks based on distinct geometric characteristics. Additionally, we design a temporal context optimization strategy that incorporates learnable temporal tokens and a dynamic mask weighting module to propagate historical information and mitigate temporal drift. Experiments on three commonly-used benchmarks show that TrackAny3D establishes new state-of-the-art performance on category-agnostic 3D SOT, demonstrating strong generalization and competitiveness. We hope this work will enlighten the community on the importance of unified models and further expand the use of large-scale pretrained models in this field.
<div id='section'>PaperID: <span id='pid'>262, <a href='https://arxiv.org/pdf/2507.07483.pdf' target='_blank'>https://arxiv.org/pdf/2507.07483.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiangqiang Wu, Yi Yu, Chenqi Kong, Ziquan Liu, Jia Wan, Haoliang Li, Alex C. Kot, Antoni B. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07483">Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of social media, vast amounts of user-uploaded videos (e.g., YouTube) are utilized as training data for Visual Object Tracking (VOT). However, the VOT community has largely overlooked video data-privacy issues, as many private videos have been collected and used for training commercial models without authorization. To alleviate these issues, this paper presents the first investigation on preventing personal video data from unauthorized exploitation by deep trackers. Existing methods for preventing unauthorized data use primarily focus on image-based tasks (e.g., image classification), directly applying them to videos reveals several limitations, including inefficiency, limited effectiveness, and poor generalizability. To address these issues, we propose a novel generative framework for generating Temporal Unlearnable Examples (TUEs), and whose efficient computation makes it scalable for usage on large-scale video datasets. The trackers trained w/ TUEs heavily rely on unlearnable noises for temporal matching, ignoring the original data structure and thus ensuring training video data-privacy. To enhance the effectiveness of TUEs, we introduce a temporal contrastive loss, which further corrupts the learning of existing trackers when using our TUEs for training. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in video data-privacy protection, with strong transferability across VOT models, datasets, and temporal matching tasks.
<div id='section'>PaperID: <span id='pid'>263, <a href='https://arxiv.org/pdf/2412.01543.pdf' target='_blank'>https://arxiv.org/pdf/2412.01543.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yufeng Jin, Vignesh Prasad, Snehal Jauhri, Mathias Franzius, Georgia Chalvatzaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01543">6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and accurate object pose estimation is an essential component for modern vision systems in many applications such as Augmented Reality, autonomous driving, and robotics. While research in model-based 6D object pose estimation has delivered promising results, model-free methods are hindered by the high computational load in rendering and inferring consistent poses of arbitrary objects in a live RGB-D video stream. To address this issue, we present 6DOPE-GS, a novel method for online 6D object pose estimation \& tracking with a single RGB-D camera by effectively leveraging advances in Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses and 3D object reconstruction. To achieve the necessary efficiency and accuracy for live tracking, our method uses incremental 2D Gaussian Splatting with an intelligent dynamic keyframe selection procedure to achieve high spatial object coverage and prevent erroneous pose updates. We also propose an opacity statistic-based pruning mechanism for adaptive Gaussian density control, to ensure training stability and efficiency. We evaluate our method on the HO3D and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of state-of-the-art baselines for model-free simultaneous 6D pose tracking and reconstruction while providing a 5$\times$ speedup. We also demonstrate the method's suitability for live, dynamic object tracking and reconstruction in a real-world setting.
<div id='section'>PaperID: <span id='pid'>264, <a href='https://arxiv.org/pdf/2409.14543.pdf' target='_blank'>https://arxiv.org/pdf/2409.14543.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Arjun Raj, Lei Wang, Tom Gedeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14543">TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately detecting and tracking high-speed, small objects, such as balls in sports videos, is challenging due to factors like motion blur and occlusion. Although recent deep learning frameworks like TrackNetV1, V2, and V3 have advanced tennis ball and shuttlecock tracking, they often struggle in scenarios with partial occlusion or low visibility. This is primarily because these models rely heavily on visual features without explicitly incorporating motion information, which is crucial for precise tracking and trajectory prediction. In this paper, we introduce an enhancement to the TrackNet family by fusing high-level visual features with learnable motion attention maps through a motion-aware fusion mechanism, effectively emphasizing the moving ball's location and improving tracking performance. Our approach leverages frame differencing maps, modulated by a motion prompt layer, to highlight key motion regions over time. Experimental results on the tennis ball and shuttlecock datasets show that our method enhances the tracking performance of both TrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built on top of the existing TrackNet, as TrackNetV4.
<div id='section'>PaperID: <span id='pid'>265, <a href='https://arxiv.org/pdf/2602.03214.pdf' target='_blank'>https://arxiv.org/pdf/2602.03214.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Guijie Wang, Tong Lin, Yifan Bai, Anjia Cao, Shiyi Liang, Wangbo Zhao, Xing Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.03214">FARTrack: Fast Autoregressive Visual Tracking with High Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model's inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU.
<div id='section'>PaperID: <span id='pid'>266, <a href='https://arxiv.org/pdf/2510.15347.pdf' target='_blank'>https://arxiv.org/pdf/2510.15347.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuxiao Sun, Yao Zhao, Meiqin Liu, Chao Yao, Jian Jin, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15347">Symmetric Entropy-Constrained Video Coding for Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As video transmission increasingly serves machine vision systems (MVS) instead of human vision systems (HVS), video coding for machines (VCM) has become a critical research topic. Existing VCM methods often bind codecs to specific downstream models, requiring retraining or supervised data and thus limiting generalization in multi-task scenarios. Recently, unified VCM frameworks have employed visual backbones (VB) and visual foundation models (VFM) to support multiple video understanding tasks with a single codec. They mainly utilize VB/VFM to maintain semantic consistency or suppress non-semantic information, but seldom explore how to directly link video coding with understanding under VB/VFM guidance. Hence, we propose a Symmetric Entropy-Constrained Video Coding framework for Machines (SEC-VCM). It establishes a symmetric alignment between the video codec and VB, allowing the codec to leverage VB's representation capabilities to preserve semantics and discard MVS-irrelevant information. Specifically, a bi-directional entropy-constraint (BiEC) mechanism ensures symmetry between the process of video decoding and VB encoding by suppressing conditional entropy. This helps the codec to explicitly handle semantic information beneficial for MVS while squeezing useless information. Furthermore, a semantic-pixel dual-path fusion (SPDF) module injects pixel-level priors into the final reconstruction. Through semantic-pixel fusion, it suppresses artifacts harmful to MVS and improves machine-oriented reconstruction quality. Experimental results show our framework achieves state-of-the-art (SOTA) in rate-task performance, with significant bitrate savings over VTM on video instance segmentation (37.41%), video object segmentation (29.83%), object detection (46.22%), and multiple object tracking (44.94%). We will release our code.
<div id='section'>PaperID: <span id='pid'>267, <a href='https://arxiv.org/pdf/2507.04762.pdf' target='_blank'>https://arxiv.org/pdf/2507.04762.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maria Damanaki, Ioulia Kapsali, Nikos Piperigkos, Alexandros Gkillas, Aris S. Lalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04762">Robustifying 3D Perception via Least-Squares Graphs for Multi-Agent Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The critical perception capabilities of EdgeAI systems, such as autonomous vehicles, are required to be resilient against adversarial threats, by enabling accurate identification and localization of multiple objects in the scene over time, mitigating their impact. Single-agent tracking offers resilience to adversarial attacks but lacks situational awareness, underscoring the need for multi-agent cooperation to enhance context understanding and robustness. This paper proposes a novel mitigation framework on 3D LiDAR scene against adversarial noise by tracking objects based on least-squares graph on multi-agent adversarial bounding boxes. Specifically, we employ the least-squares graph tool to reduce the induced positional error of each detection's centroid utilizing overlapped bounding boxes on a fully connected graph via differential coordinates and anchor points. Hence, the multi-vehicle detections are fused and refined mitigating the adversarial impact, and associated with existing tracks in two stages performing tracking to further suppress the adversarial threat. An extensive evaluation study on the real-world V2V4Real dataset demonstrates that the proposed method significantly outperforms both state-of-the-art single and multi-agent tracking frameworks by up to 23.3% under challenging adversarial conditions, operating as a resilient approach without relying on additional defense mechanisms.
<div id='section'>PaperID: <span id='pid'>268, <a href='https://arxiv.org/pdf/2506.09469.pdf' target='_blank'>https://arxiv.org/pdf/2506.09469.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maria Damanaki, Nikos Piperigkos, Alexandros Gkillas, Aris S. Lalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09469">Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) plays a crucial role in autonomous driving systems, as it lays the foundations for advanced perception and precise path planning modules. Nonetheless, single agent based MOT lacks in sensing surroundings due to occlusions, sensors failures, etc. Hence, the integration of multiagent information is essential for comprehensive understanding of the environment. This paper proposes a novel Cooperative MOT framework for tracking objects in 3D LiDAR scene by formulating and solving a graph topology-aware optimization problem so as to fuse information coming from multiple vehicles. By exploiting a fully connected graph topology defined by the detected bounding boxes, we employ the Graph Laplacian processing optimization technique to smooth the position error of bounding boxes and effectively combine them. In that manner, we reveal and leverage inherent coherences of diverse multi-agent detections, and associate the refined bounding boxes to tracked objects at two stages, optimizing localization and tracking accuracies. An extensive evaluation study has been conducted, using the real-world V2V4Real dataset, where the proposed method significantly outperforms the baseline frameworks, including the state-of-the-art deep-learning DMSTrack and V2V4Real, in various testing sequences.
<div id='section'>PaperID: <span id='pid'>269, <a href='https://arxiv.org/pdf/2505.11905.pdf' target='_blank'>https://arxiv.org/pdf/2505.11905.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Takuya Ikeda, Sergey Zakharov, Muhammad Zubair Irshad, Istvan Balazs Opra, Shun Iwase, Dian Chen, Mark Tjersland, Robert Lee, Alexandre Dilly, Rares Ambrus, Koichi Nishiwaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11905">GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for 6-DoF object tracking and high-quality 3D reconstruction from monocular RGBD video. Existing methods, while achieving impressive results, often struggle with complex objects, particularly those exhibiting symmetry, intricate geometry or complex appearance. To bridge these gaps, we introduce an adaptive method that combines 3D Gaussian Splatting, hybrid geometry/appearance tracking, and key frame selection to achieve robust tracking and accurate reconstructions across a diverse range of objects. Additionally, we present a benchmark covering these challenging object classes, providing high-quality annotations for evaluating both tracking and reconstruction performance. Our approach demonstrates strong capabilities in recovering high-fidelity object meshes, setting a new standard for single-sensor 3D reconstruction in open-world environments.
<div id='section'>PaperID: <span id='pid'>270, <a href='https://arxiv.org/pdf/2411.13183.pdf' target='_blank'>https://arxiv.org/pdf/2411.13183.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kuiran Wang, Xuehui Yu, Wenwen Yu, Guorong Li, Xiangyuan Lan, Qixiang Ye, Jianbin Jiao, Zhenjun Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13183">ClickTrack: Towards Real-time Interactive Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking(SOT) relies on precise object bounding box initialization. In this paper, we reconsidered the deficiencies in the current approaches to initializing single object trackers and propose a new paradigm for single object tracking algorithms, ClickTrack, a new paradigm using clicking interaction for real-time scenarios. Moreover, click as an input type inherently lack hierarchical information. To address ambiguity in certain special scenarios, we designed the Guided Click Refiner(GCR), which accepts point and optional textual information as inputs, transforming the point into the bounding box expected by the operator. The bounding box will be used as input of single object trackers. Experiments on LaSOT and GOT-10k benchmarks show that tracker combined with GCR achieves stable performance in real-time interactive scenarios. Furthermore, we explored the integration of GCR into the Segment Anything model(SAM), significantly reducing ambiguity issues when SAM receives point inputs.
<div id='section'>PaperID: <span id='pid'>271, <a href='https://arxiv.org/pdf/2409.00618.pdf' target='_blank'>https://arxiv.org/pdf/2409.00618.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lipeng Gu, Mingqiang Wei, Xuefeng Yan, Dingkun Zhu, Wei Zhao, Haoran Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00618">YOLOO: You Only Learn from Others Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal 3D multi-object tracking (MOT) typically necessitates extensive computational costs of deep neural networks (DNNs) to extract multi-modal representations. In this paper, we propose an intriguing question: May we learn from multiple modalities only during training to avoid multi-modal input in the inference phase? To answer it, we propose \textbf{YOLOO}, a novel multi-modal 3D MOT paradigm: You Only Learn from Others Once. YOLOO empowers the point cloud encoder to learn a unified tri-modal representation (UTR) from point clouds and other modalities, such as images and textual cues, all at once. Leveraging this UTR, YOLOO achieves efficient tracking solely using the point cloud encoder without compromising its performance, fundamentally obviating the need for computationally intensive DNNs. Specifically, YOLOO includes two core components: a unified tri-modal encoder (UTEnc) and a flexible geometric constraint (F-GC) module. UTEnc integrates a point cloud encoder with image and text encoders adapted from pre-trained CLIP. It seamlessly fuses point cloud information with rich visual-textual knowledge from CLIP into the point cloud encoder, yielding highly discriminative UTRs that facilitate the association between trajectories and detections. Additionally, F-GC filters out mismatched associations with similar representations but significant positional discrepancies. It further enhances the robustness of UTRs without requiring any scene-specific tuning, addressing a key limitation of customized geometric constraints (e.g., 3D IoU). Lastly, high-quality 3D trajectories are generated by a traditional data association component. By integrating these advancements into a multi-modal 3D MOT scheme, our YOLOO achieves substantial gains in both robustness and efficiency.
<div id='section'>PaperID: <span id='pid'>272, <a href='https://arxiv.org/pdf/2602.04692.pdf' target='_blank'>https://arxiv.org/pdf/2602.04692.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sijia Chen, Lijuan Ma, Yanqiu Yu, En Yu, Liman Liu, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.04692">DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.
<div id='section'>PaperID: <span id='pid'>273, <a href='https://arxiv.org/pdf/2511.17053.pdf' target='_blank'>https://arxiv.org/pdf/2511.17053.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Teng Fu, Mengyang Zhao, Ke Niu, Kaixin Peng, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17053">OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.
<div id='section'>PaperID: <span id='pid'>274, <a href='https://arxiv.org/pdf/2510.13016.pdf' target='_blank'>https://arxiv.org/pdf/2510.13016.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13016">SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding fine-grained actions and accurately localizing their corresponding actors in space and time are fundamental capabilities for advancing next-generation AI systems, including embodied agents, autonomous platforms, and human-AI interaction frameworks. Despite recent progress in video understanding, existing methods predominantly address either coarse-grained action recognition or generic object tracking, thereby overlooking the challenge of jointly detecting and tracking multiple objects according to their actions while grounding them temporally. To address this gap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task that requires models to simultaneously detect, track, and temporally localize all referent objects in videos based on natural language descriptions of their actions. To support this task, we construct SVAG-Bench, a large-scale benchmark comprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering a diverse range of objects, actions, and real-world scenes. We further propose SVAGFormer, a baseline framework that adapts state of the art vision language models for joint spatial and temporal grounding, and introduce SVAGEval, a standardized evaluation toolkit for fair and reproducible benchmarking. Empirical results show that existing models perform poorly on SVAG, particularly in dense or complex scenes, underscoring the need for more advanced reasoning over fine-grained object-action interactions in long videos.
<div id='section'>PaperID: <span id='pid'>275, <a href='https://arxiv.org/pdf/2509.14060.pdf' target='_blank'>https://arxiv.org/pdf/2509.14060.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jun Du, Weiwei Xing, Ming Li, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14060">VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multi-object tracking (MOT) algorithms typically overlook issues inherent in low-quality videos, leading to significant degradation in tracking performance when confronted with real-world image deterioration. Therefore, advancing the application of MOT algorithms in real-world low-quality video scenarios represents a critical and meaningful endeavor. To address the challenges posed by low-quality scenarios, inspired by vision-language models, this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking framework (VSE-MOT). Specifically, we first design a tri-branch architecture that leverages a vision-language model to extract global visual semantic information from images and fuse it with query vectors. Subsequently, to further enhance the utilization of visual semantic information, we introduce the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic information to suit multi-object tracking tasks, while the VSFM improves the efficacy of feature fusion. Through extensive experiments, we validate the effectiveness and superiority of the proposed method in real-world low-quality video scenarios. Its tracking performance metrics outperform those of existing methods by approximately 8% to 20%, while maintaining robust performance in conventional scenarios.
<div id='section'>PaperID: <span id='pid'>276, <a href='https://arxiv.org/pdf/2507.21460.pdf' target='_blank'>https://arxiv.org/pdf/2507.21460.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mianzhao Wang, Fan Shi, Xu Cheng, Feifei Zhang, Shengyong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21460">An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality 4D light field representation with efficient angular feature modeling is crucial for scene perception, as it can provide discriminative spatial-angular cues to identify moving targets. However, recent developments still struggle to deliver reliable angular modeling in the temporal domain, particularly in complex low-light scenes. In this paper, we propose a novel light field epipolar-plane structure image (ESI) representation that explicitly defines the geometric structure within the light field. By capitalizing on the abrupt changes in the angles of light rays within the epipolar plane, this representation can enhance visual expression in low-light scenes and reduce redundancy in high-dimensional light fields. We further propose an angular-temporal interaction network (ATINet) for light field object tracking that learns angular-aware representations from the geometric structural cues and angular-temporal interaction cues of light fields. Furthermore, ATINet can also be optimized in a self-supervised manner to enhance the geometric feature interaction across the temporal domain. Finally, we introduce a large-scale light field low-light dataset for object tracking. Extensive experimentation demonstrates that ATINet achieves state-of-the-art performance in single object tracking. Furthermore, we extend the proposed method to multiple object tracking, which also shows the effectiveness of high-quality light field angular-temporal modeling.
<div id='section'>PaperID: <span id='pid'>277, <a href='https://arxiv.org/pdf/2506.19621.pdf' target='_blank'>https://arxiv.org/pdf/2506.19621.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Noel JosÃ© Rodrigues Vicente, Enrique Lehner, Angel Villar-Corrales, Jan Nogga, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19621">VideoPCDNet: Video Parsing and Prediction with Phase Correlation Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting video content is essential for planning and reasoning in dynamic environments. Despite advancements, unsupervised learning of object representations and dynamics remains challenging. We present VideoPCDNet, an unsupervised framework for object-centric video decomposition and prediction. Our model uses frequency-domain phase correlation techniques to recursively parse videos into object components, which are represented as transformed versions of learned object prototypes, enabling accurate and interpretable tracking. By explicitly modeling object motion through a combination of frequency domain operations and lightweight learned modules, VideoPCDNet enables accurate unsupervised object tracking and prediction of future video frames. In our experiments, we demonstrate that VideoPCDNet outperforms multiple object-centric baseline models for unsupervised tracking and prediction on several synthetic datasets, while learning interpretable object and motion representations.
<div id='section'>PaperID: <span id='pid'>278, <a href='https://arxiv.org/pdf/2505.20381.pdf' target='_blank'>https://arxiv.org/pdf/2505.20381.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sijia Chen, Yanqiu Yu, En Yu, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20381">ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-object tracking (RMOT) is an important research field in computer vision. Its task form is to guide the models to track the objects that conform to the language instruction. However, the RMOT task commonly requires clear language instructions, such methods often fail to work when complex language instructions with reasoning characteristics appear. In this work, we propose a new task, called Reasoning-based Multi-Object Tracking (ReaMOT). ReaMOT is a more challenging task that requires accurate reasoning about objects that match the language instruction with reasoning characteristic and tracking the objects' trajectories. To advance the ReaMOT task and evaluate the reasoning capabilities of tracking models, we construct ReaMOT Challenge, a reasoning-based multi-object tracking benchmark built upon 12 datasets. Specifically, it comprises 1,156 language instructions with reasoning characteristic, 423,359 image-language pairs, and 869 diverse scenes, which is divided into three levels of reasoning difficulty. In addition, we propose a set of evaluation metrics tailored for the ReaMOT task. Furthermore, we propose ReaTrack, a training-free framework for reasoning-based multi-object tracking based on large vision-language models (LVLM) and SAM2, as a baseline for the ReaMOT task. Extensive experiments on the ReaMOT Challenge benchmark demonstrate the effectiveness of our ReaTrack framework.
<div id='section'>PaperID: <span id='pid'>279, <a href='https://arxiv.org/pdf/2505.12606.pdf' target='_blank'>https://arxiv.org/pdf/2505.12606.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiyu Xuan, Zechao Li, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12606">Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal object tracking integrates auxiliary modalities such as depth, thermal infrared, event flow, and language to provide additional information beyond RGB images, showing great potential in improving tracking stabilization in complex scenarios. Existing methods typically start from an RGB-based tracker and learn to understand auxiliary modalities only from training data. Constrained by the limited multi-modal training data, the performance of these methods is unsatisfactory. To alleviate this limitation, this work proposes a unified multi-modal tracker Diff-MM by exploiting the multi-modal understanding capability of the pre-trained text-to-image generation model. Diff-MM leverages the UNet of pre-trained Stable Diffusion as a tracking feature extractor through the proposed parallel feature extraction pipeline, which enables pairwise image inputs for object tracking. We further introduce a multi-modal sub-module tuning method that learns to gain complementary information between different modalities. By harnessing the extensive prior knowledge in the generation model, we achieve a unified tracker with uniform parameters for RGB-N/D/T/E tracking. Experimental results demonstrate the promising performance of our method compared with recently proposed trackers, e.g., its AUC outperforms OneTracker by 8.3% on TNL2K.
<div id='section'>PaperID: <span id='pid'>280, <a href='https://arxiv.org/pdf/2502.01357.pdf' target='_blank'>https://arxiv.org/pdf/2502.01357.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01357">Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D multi-object tracking (MOT) is vital for autonomous vehicles, yet LiDAR and camera-based methods degrade in adverse weather. Meanwhile, Radar-based solutions remain robust but often suffer from limited vertical resolution and simplistic motion models. Existing Kalman filter-based approaches also rely on fixed noise covariance, hampering adaptability when objects make sudden maneuvers. We propose Bayes-4DRTrack, a 4D Radar-based MOT framework that adopts a transformer-based motion prediction network to capture nonlinear motion dynamics and employs Bayesian approximation in both detection and prediction steps. Moreover, our two-stage data association leverages Doppler measurements to better distinguish closely spaced targets. Evaluated on the K-Radar dataset (including adverse weather scenarios), Bayes-4DRTrack demonstrates a 5.7% gain in Average Multi-Object Tracking Accuracy (AMOTA) over methods with traditional motion models and fixed noise covariance. These results showcase enhanced robustness and accuracy in demanding, real-world conditions.
<div id='section'>PaperID: <span id='pid'>281, <a href='https://arxiv.org/pdf/2410.10053.pdf' target='_blank'>https://arxiv.org/pdf/2410.10053.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pha Nguyen, Ngan Le, Jackson Cothren, Alper Yilmaz, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10053">DINTR: Tracking via Diffusion-based Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a fundamental task in computer vision, requiring the localization of objects of interest across video frames. Diffusion models have shown remarkable capabilities in visual generation, making them well-suited for addressing several requirements of the tracking problem. This work proposes a novel diffusion-based methodology to formulate the tracking task. Firstly, their conditional process allows for injecting indications of the target object into the generation process. Secondly, diffusion mechanics can be developed to inherently model temporal correspondences, enabling the reconstruction of actual frames in video. However, existing diffusion models rely on extensive and unnecessary mapping to a Gaussian noise domain, which can be replaced by a more efficient and stable interpolation process. Our proposed interpolation mechanism draws inspiration from classic image-processing techniques, offering a more interpretable, stable, and faster approach tailored specifically for the object tracking task. By leveraging the strengths of diffusion models while circumventing their limitations, our Diffusion-based INterpolation TrackeR (DINTR) presents a promising new paradigm and achieves a superior multiplicity on seven benchmarks across five indicator representations.
<div id='section'>PaperID: <span id='pid'>282, <a href='https://arxiv.org/pdf/2602.05037.pdf' target='_blank'>https://arxiv.org/pdf/2602.05037.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bishoy Galoaa, Xiangyu Bai, Utsav Nandi, Sai Siddhartha Vivek Dhir Rangoju, Somaieh Amraee, Sarah Ostadabbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.05037">UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.
<div id='section'>PaperID: <span id='pid'>283, <a href='https://arxiv.org/pdf/2601.06287.pdf' target='_blank'>https://arxiv.org/pdf/2601.06287.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Joseph Heyward, Nikhil Pathasarathy, Tyler Zhu, Aravindh Mahendran, João Carreira, Dima Damen, Andrew Zisserman, Viorica Pătrăucean
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06287">Perception Test 2025: Challenge Summary and a Unified VQA Extension</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Third Perception Test challenge was organised as a full-day workshop alongside the IEEE/CVF International Conference on Computer Vision (ICCV) 2025. Its primary goal is to benchmark state-of-the-art video models and measure the progress in multimodal perception. This year, the workshop featured 2 guest tracks as well: KiVA (an image understanding challenge) and Physic-IQ (a video generation challenge). In this report, we summarise the results from the main Perception Test challenge, detailing both the existing tasks as well as novel additions to the benchmark. In this iteration, we placed an emphasis on task unification, as this poses a more challenging test for current SOTA multimodal models. The challenge included five consolidated tracks: unified video QA, unified object and point tracking, unified action and sound localisation, grounded video QA, and hour-long video QA, alongside an analysis and interpretability track that is still open for submissions. Notably, the unified video QA track introduced a novel subset that reformulates traditional perception tasks (such as point tracking and temporal action localisation) as multiple-choice video QA questions that video-language models can natively tackle. The unified object and point tracking merged the original object tracking and point tracking tasks, whereas the unified action and sound localisation merged the original temporal action localisation and temporal sound localisation tracks. Accordingly, we required competitors to use unified approaches rather than engineered pipelines with task-specific models. By proposing such a unified challenge, Perception Test 2025 highlights the significant difficulties existing models face when tackling diverse perception tasks through unified interfaces.
<div id='section'>PaperID: <span id='pid'>284, <a href='https://arxiv.org/pdf/2511.03332.pdf' target='_blank'>https://arxiv.org/pdf/2511.03332.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yi Yang, Yiming Xu, Timo Kaiser, Hao Cheng, Bodo Rosenhahn, Michael Ying Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03332">Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.
<div id='section'>PaperID: <span id='pid'>285, <a href='https://arxiv.org/pdf/2503.01547.pdf' target='_blank'>https://arxiv.org/pdf/2503.01547.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Arash Nasr Esfahani, Hamed Hosseini, Mehdi Tale Masouleh, Ahmad Kalhor, Hedieh Sajedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01547">AI-Driven Relocation Tracking in Dynamic Kitchen Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As smart homes become more prevalent in daily life, the ability to understand dynamic environments is essential which is increasingly dependent on AI systems. This study focuses on developing an intelligent algorithm which can navigate a robot through a kitchen, recognizing objects, and tracking their relocation. The kitchen was chosen as the testing ground due to its dynamic nature as objects are frequently moved, rearranged and replaced. Various techniques, such as SLAM feature-based tracking and deep learning-based object detection (e.g., Faster R-CNN), are commonly used for object tracking. Additionally, methods such as optical flow analysis and 3D reconstruction have also been used to track the relocation of objects. These approaches often face challenges when it comes to problems such as lighting variations and partial occlusions, where parts of the object are hidden in some frames but visible in others. The proposed method in this study leverages the YOLOv5 architecture, initialized with pre-trained weights and subsequently fine-tuned on a custom dataset. A novel method was developed, introducing a frame-scoring algorithm which calculates a score for each object based on its location and features within all frames. This scoring approach helps to identify changes by determining the best-associated frame for each object and comparing the results in each scene, overcoming limitations seen in other methods while maintaining simplicity in design. The experimental results demonstrate an accuracy of 97.72%, a precision of 95.83% and a recall of 96.84% for this algorithm, which highlights the efficacy of the model in detecting spatial changes.
<div id='section'>PaperID: <span id='pid'>286, <a href='https://arxiv.org/pdf/2411.19941.pdf' target='_blank'>https://arxiv.org/pdf/2411.19941.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Joseph Heyward, JoÃ£o Carreira, Dima Damen, Andrew Zisserman, Viorica PÄtrÄucean
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19941">Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Following the successful 2023 edition, we organised the Second Perception Test challenge as a half-day workshop alongside the IEEE/CVF European Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking state-of-the-art video models and measuring the progress since last year using the Perception Test benchmark. This year, the challenge had seven tracks (up from six last year) and covered low-level and high-level tasks, with language and non-language interfaces, across video, audio, and text modalities; the additional track covered hour-long video understanding and introduced a novel video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks were: object tracking, point tracking, temporal action localisation, temporal sound localisation, multiple-choice video question-answering, grounded video question-answering, and hour-long video question-answering. We summarise in this report the challenge tasks and results, and introduce in detail the novel hour-long video QA benchmark 1h-walk VQA.
<div id='section'>PaperID: <span id='pid'>287, <a href='https://arxiv.org/pdf/2512.13684.pdf' target='_blank'>https://arxiv.org/pdf/2512.13684.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13684">Recurrent Video Masked Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.
<div id='section'>PaperID: <span id='pid'>288, <a href='https://arxiv.org/pdf/2509.12924.pdf' target='_blank'>https://arxiv.org/pdf/2509.12924.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik ForssÃ©n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12924">MATTER: Multiscale Attention for Registration Error Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking. This makes detecting and quantifying registration misalignment, i.e., PCR quality validation, an important task. All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes. In this work, we instead use regression for PCR validation, allowing for a more fine-grained quantification of the registration quality. We also extend previously used misalignment-related features by using multiscale extraction and attention-based aggregation. This leads to accurate and robust registration error estimation on diverse datasets, especially for point clouds with heterogeneous spatial densities. Furthermore, when used to guide a mapping downstream task, our method significantly improves the mapping quality for a given amount of re-registered frames, compared to the state-of-the-art classification-based method.
<div id='section'>PaperID: <span id='pid'>289, <a href='https://arxiv.org/pdf/2508.14776.pdf' target='_blank'>https://arxiv.org/pdf/2508.14776.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhichao Li, Arren Glover, Chiara Bartolozzi, Lorenzo Natale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14776">6-DoF Object Tracking with Event-based Optical Flow and Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking the position and orientation of objects in space (i.e., in 6-DoF) in real time is a fundamental problem in robotics for environment interaction. It becomes more challenging when objects move at high-speed due to frame rate limitations in conventional cameras and motion blur. Event cameras are characterized by high temporal resolution, low latency and high dynamic range, that can potentially overcome the impacts of motion blur. Traditional RGB cameras provide rich visual information that is more suitable for the challenging task of single-shot object pose estimation. In this work, we propose using event-based optical flow combined with an RGB based global object pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the core advantages of both types of vision sensors. Specifically, we propose an event-based optical flow algorithm for object motion measurement to implement an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF velocity with low frequency estimated pose from the global pose estimator, the method can track pose when objects move at high-speed. The proposed algorithm is tested and validated on both synthetic and real world data, demonstrating its effectiveness, especially in high-speed motion scenarios.
<div id='section'>PaperID: <span id='pid'>290, <a href='https://arxiv.org/pdf/2505.20455.pdf' target='_blank'>https://arxiv.org/pdf/2505.20455.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matthew Hong, Anthony Liang, Kevin Kim, Harshitha Rajaprakash, Jesse Thomason, Erdem BÄ±yÄ±k, Jesse Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20455">HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We hand the community HAND, a simple and time-efficient method for teaching robots new manipulation tasks through human hand demonstrations. Instead of relying on task-specific robot demonstrations collected via teleoperation, HAND uses easy-to-provide hand demonstrations to retrieve relevant behaviors from task-agnostic robot play data. Using a visual tracking pipeline, HAND extracts the motion of the human hand from the hand demonstration and retrieves robot sub-trajectories in two stages: first filtering by visual similarity, then retrieving trajectories with similar behaviors to the hand. Fine-tuning a policy on the retrieved data enables real-time learning of tasks in under four minutes, without requiring calibrated cameras or detailed hand pose estimation. Experiments also show that HAND outperforms retrieval baselines by over 2x in average task success rates on real robots. Videos can be found at our project website: https://liralab.usc.edu/handretrieval/.
<div id='section'>PaperID: <span id='pid'>291, <a href='https://arxiv.org/pdf/2505.07254.pdf' target='_blank'>https://arxiv.org/pdf/2505.07254.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mohamed Nagy, Naoufel Werghi, Bilal Hassan, Jorge Dias, Majid Khonji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07254">Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\% and 0.81\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\% in higher order tracking accuracy (HOTA) and 1.55\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.
<div id='section'>PaperID: <span id='pid'>292, <a href='https://arxiv.org/pdf/2501.16753.pdf' target='_blank'>https://arxiv.org/pdf/2501.16753.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hy Nguyen, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16753">Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into $N$ chunks, where $N$ is the number of heads. Each segment captures only a fraction of the original embeddings information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings -- this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors.
<div id='section'>PaperID: <span id='pid'>293, <a href='https://arxiv.org/pdf/2501.13994.pdf' target='_blank'>https://arxiv.org/pdf/2501.13994.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hy Nguyen, Bao Pham, Hung Du, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13994">CSAOT: Cooperative Multi-Agent System for Active Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object Tracking is essential for many computer vision applications, such as autonomous navigation, surveillance, and robotics. Unlike Passive Object Tracking (POT), which relies on static camera viewpoints to detect and track objects across consecutive frames, Active Object Tracking (AOT) requires a controller agent to actively adjust its viewpoint to maintain visual contact with a moving target in complex environments. Existing AOT solutions are predominantly single-agent-based, which struggle in dynamic and complex scenarios due to limited information gathering and processing capabilities, often resulting in suboptimal decision-making. Alleviating these limitations necessitates the development of a multi-agent system where different agents perform distinct roles and collaborate to enhance learning and robustness in dynamic and complex environments. Although some multi-agent approaches exist for AOT, they typically rely on external auxiliary agents, which require additional devices, making them costly. In contrast, we introduce the Collaborative System for Active Object Tracking (CSAOT), a method that leverages multi-agent deep reinforcement learning (MADRL) and a Mixture of Experts (MoE) framework to enable multiple agents to operate on a single device, thereby improving tracking performance and reducing costs. Our approach enhances robustness against occlusions and rapid motion while optimizing camera movements to extend tracking duration. We validated the effectiveness of CSAOT on various interactive maps with dynamic and stationary obstacles.
<div id='section'>PaperID: <span id='pid'>294, <a href='https://arxiv.org/pdf/2410.02638.pdf' target='_blank'>https://arxiv.org/pdf/2410.02638.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fabian Herzog, Johannes Gilg, Philipp Wolters, Torben Teepe, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02638">Spatial-Temporal Multi-Cuts for Online Multiple-Camera Vehicle Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate online multiple-camera vehicle tracking is essential for intelligent transportation systems, autonomous driving, and smart city applications. Like single-camera multiple-object tracking, it is commonly formulated as a graph problem of tracking-by-detection. Within this framework, existing online methods usually consist of two-stage procedures that cluster temporally first, then spatially, or vice versa. This is computationally expensive and prone to error accumulation. We introduce a graph representation that allows spatial-temporal clustering in a single, combined step: New detections are spatially and temporally connected with existing clusters. By keeping sparse appearance and positional cues of all detections in a cluster, our method can compare clusters based on the strongest available evidence. The final tracks are obtained online using a simple multicut assignment procedure. Our method does not require any training on the target scene, pre-extraction of single-camera tracks, or additional annotations. Notably, we outperform the online state-of-the-art on the CityFlow dataset in terms of IDF1 by more than 14%, and on the Synthehicle dataset by more than 25%, respectively. The code is publicly available.
<div id='section'>PaperID: <span id='pid'>295, <a href='https://arxiv.org/pdf/2511.20239.pdf' target='_blank'>https://arxiv.org/pdf/2511.20239.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jan Krejčí, Oliver Kost, Yuxuan Xia, Lennart Svensson, Ondřej Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20239">Occlusion-Aware Multi-Object Tracking via Expected Probability of Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses multi-object systems, where objects may occlude one another relative to the sensor. The standard point-object model for detection-based sensors is enhanced so that the probability of detection considers the presence of all objects. A principled tracking method is derived, assigning each object an expected probability of detection, where the expectation is taken over the reduced Palm density, which means conditionally on the object's existence. The assigned probability thus considers the object's visibility relative to the sensor, under the presence of other objects. Unlike existing methods, the proposed method systematically accounts for uncertainties related to all objects in a clear and manageable way. The method is demonstrated through a visual tracking application using the multi-Bernoulli mixture (MBM) filter with marks.
<div id='section'>PaperID: <span id='pid'>296, <a href='https://arxiv.org/pdf/2511.15580.pdf' target='_blank'>https://arxiv.org/pdf/2511.15580.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sifan Zhou, Yichao Cao, Jiahao Nie, Yuqian Fu, Ziyu Zhao, Xiaobo Lu, Shuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15580">CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.
<div id='section'>PaperID: <span id='pid'>297, <a href='https://arxiv.org/pdf/2509.13396.pdf' target='_blank'>https://arxiv.org/pdf/2509.13396.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinan Wang, Di Shi, Fengyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13396">Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel three-stage framework for real-time foreign object intrusion (FOI) detection and tracking in power transmission systems. The framework integrates: (1) a YOLOv7 segmentation model for fast and robust object localization, (2) a ConvNeXt-based feature extractor trained with triplet loss to generate discriminative embeddings, and (3) a feature-assisted IoU tracker that ensures resilient multi-object tracking under occlusion and motion. To enable scalable field deployment, the pipeline is optimized for deployment on low-cost edge hardware using mixed-precision inference. The system supports incremental updates by adding embeddings from previously unseen objects into a reference database without requiring model retraining. Extensive experiments on real-world surveillance and drone video datasets demonstrate the framework's high accuracy and robustness across diverse FOI scenarios. In addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's practicality and scalability for real-world edge applications.
<div id='section'>PaperID: <span id='pid'>298, <a href='https://arxiv.org/pdf/2509.02111.pdf' target='_blank'>https://arxiv.org/pdf/2509.02111.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Benjamin Missaoui, Orcun Cetintas, Guillem BrasÃ³, Tim Meinhardt, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02111">NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The long-standing division between \textit{online} and \textit{offline} Multi-Object Tracking (MOT) has led to fragmented solutions that fail to address the flexible temporal requirements of real-world deployment scenarios. Current \textit{online} trackers rely on frame-by-frame hand-crafted association strategies and struggle with long-term occlusions, whereas \textit{offline} approaches can cover larger time gaps, but still rely on heuristic stitching for arbitrarily long sequences. In this paper, we introduce NOOUGAT, the first tracker designed to operate with arbitrary temporal horizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework that processes non-overlapping subclips, and fuses them through a novel Autoregressive Long-term Tracking (ALT) layer. The subclip size controls the trade-off between latency and temporal context, enabling a wide range of deployment scenarios, from frame-by-frame to batch processing. NOOUGAT achieves state-of-the-art performance across both tracking regimes, improving \textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 on MOT20, with even greater gains in \textit{offline} mode.
<div id='section'>PaperID: <span id='pid'>299, <a href='https://arxiv.org/pdf/2508.14607.pdf' target='_blank'>https://arxiv.org/pdf/2508.14607.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pengzhi Zhong, Xinzhe Wang, Dan Zeng, Qihua Zhou, Feixiang He, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14607">SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential for low-power computation, yet their application in visual tasks remains largely confined to image classification, object detection, and event-based tracking. In contrast, real-world vision systems still widely use conventional RGB video streams, where the potential of directly-trained SNNs for complex temporal tasks such as multi-object tracking (MOT) remains underexplored. To address this challenge, we propose SMTrack-the first directly trained deep SNN framework for end-to-end multi-object tracking on standard RGB videos. SMTrack introduces an adaptive and scale-aware Normalized Wasserstein Distance loss (Asa-NWDLoss) to improve detection and localization performance under varying object scales and densities. Specifically, the method computes the average object size within each training batch and dynamically adjusts the normalization factor, thereby enhancing sensitivity to small objects. For the association stage, we incorporate the TrackTrack identity module to maintain robust and consistent object trajectories. Extensive evaluations on BEE24, MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking in complex scenarios.
<div id='section'>PaperID: <span id='pid'>300, <a href='https://arxiv.org/pdf/2508.13647.pdf' target='_blank'>https://arxiv.org/pdf/2508.13647.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jan KrejÄÃ­, Oliver Kost, Yuxuan Xia, Lennart Svensson, OndÅej Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13647">Model-based Multi-object Visual Tracking: Identification and Standard Model Limitations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper uses multi-object tracking methods known from the radar tracking community to address the problem of pedestrian tracking using 2D bounding box detections. The standard point-object (SPO) model is adopted, and the posterior density is computed using the Poisson multi-Bernoulli mixture (PMBM) filter. The selection of the model parameters rooted in continuous time is discussed, including the birth and survival probabilities. Some parameters are selected from the first principles, while others are identified from the data, which is, in this case, the publicly available MOT-17 dataset. Although the resulting PMBM algorithm yields promising results, a mismatch between the SPO model and the data is revealed. The model-based approach assumes that modifying the problematic components causing the SPO model-data mismatch will lead to better model-based algorithms in future developments.
<div id='section'>PaperID: <span id='pid'>301, <a href='https://arxiv.org/pdf/2508.09524.pdf' target='_blank'>https://arxiv.org/pdf/2508.09524.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yipei Wang, Shiyu Hu, Shukun Jia, Panxi Xu, Hongfei Ma, Yiping Ma, Jing Zhang, Xiaobo Lu, Xin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09524">SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the first systematic investigation and quantification of Similar Object Interference (SOI), a long-overlooked yet critical bottleneck in Single Object Tracking (SOT). Through controlled Online Interference Masking (OIM) experiments, we quantitatively demonstrate that eliminating interference sources leads to substantial performance improvements (AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a primary constraint for robust tracking and highlighting the feasibility of external cognitive guidance. Building upon these insights, we adopt natural language as a practical form of external guidance, and construct SOIBench-the first semantic cognitive guidance benchmark specifically targeting SOI challenges. It automatically mines SOI frames through multi-tracker collective judgment and introduces a multi-level annotation protocol to generate precise semantic guidance texts. Systematic evaluation on SOIBench reveals a striking finding: existing vision-language tracking (VLT) methods fail to effectively exploit semantic cognitive guidance, achieving only marginal improvements or even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we propose a novel paradigm employing large-scale vision-language models (VLM) as external cognitive engines that can be seamlessly integrated into arbitrary RGB trackers. This approach demonstrates substantial improvements under semantic cognitive guidance (AUC gains up to 0.93), representing a significant advancement over existing VLT methods. We hope SOIBench will serve as a standardized evaluation platform to advance semantic cognitive tracking research and contribute new insights to the tracking research community.
<div id='section'>PaperID: <span id='pid'>302, <a href='https://arxiv.org/pdf/2507.23251.pdf' target='_blank'>https://arxiv.org/pdf/2507.23251.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fereshteh Aghaee Meibodi, Shadi Alijani, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23251">A Deep Dive into Generic Object Tracking: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generic object tracking remains an important yet challenging task in computer vision due to complex spatio-temporal dynamics, especially in the presence of occlusions, similar distractors, and appearance variations. Over the past two decades, a wide range of tracking paradigms, including Siamese-based trackers, discriminative trackers, and, more recently, prominent transformer-based approaches, have been introduced to address these challenges. While a few existing survey papers in this field have either concentrated on a single category or widely covered multiple ones to capture progress, our paper presents a comprehensive review of all three categories, with particular emphasis on the rapidly evolving transformer-based methods. We analyze the core design principles, innovations, and limitations of each approach through both qualitative and quantitative comparisons. Our study introduces a novel categorization and offers a unified visual and tabular comparison of representative methods. Additionally, we organize existing trackers from multiple perspectives and summarize the major evaluation benchmarks, highlighting the fast-paced advancements in transformer-based tracking driven by their robust spatio-temporal modeling capabilities.
<div id='section'>PaperID: <span id='pid'>303, <a href='https://arxiv.org/pdf/2507.13706.pdf' target='_blank'>https://arxiv.org/pdf/2507.13706.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ãngel F. GarcÃ­a-FernÃ¡ndez, Jinhao Gu, Lennart Svensson, Yuxuan Xia, Jan KrejÄÃ­, Oliver Kost, OndÅej Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13706">GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces two quasi-metrics for performance assessment of multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an extension of the generalised optimal subpattern assignment (GOSPA) metric and measures the discrepancy between sets of objects. The other quasi-metric is an extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy between sets of trajectories. Similar to the GOSPA-based metrics, these quasi-metrics include costs for localisation error for properly detected objects, the number of false objects and the number of missed objects. The T-GOSPA quasi-metric also includes a track switching cost. Differently from the GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of penalising missed and false objects with different costs, and the localisation costs are not required to be symmetric. These properties can be useful in MOT evaluation in certain applications. The performance of several Bayesian MOT algorithms is assessed with the T-GOSPA quasi-metric via simulations.
<div id='section'>PaperID: <span id='pid'>304, <a href='https://arxiv.org/pdf/2506.15148.pdf' target='_blank'>https://arxiv.org/pdf/2506.15148.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuxuan Xia, Ãngel F. GarcÃ­a-FernÃ¡ndez, Johan Karlsson, Yu Ge, Lennart Svensson, Ting Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15148">Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a generalization of the trajectory general optimal sub-pattern assignment (GOSPA) metric for evaluating multi-object tracking algorithms that provide trajectory estimates with track-level uncertainties. This metric builds on the recently introduced probabilistic GOSPA metric to account for both the existence and state estimation uncertainties of individual object states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a multidimensional assignment problem, and its linear programming relaxation--also a valid metric--is computable in polynomial time. Additionally, this metric retains the interpretability of TGOSPA, and we show that its decomposition yields intuitive costs terms associated to expected localization error and existence probability mismatch error for properly detected objects, expected missed and false detection error, and track switch error. The effectiveness of the proposed metric is demonstrated through a simulation study.
<div id='section'>PaperID: <span id='pid'>305, <a href='https://arxiv.org/pdf/2506.03335.pdf' target='_blank'>https://arxiv.org/pdf/2506.03335.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dheeraj Khanna, Jerrin Bright, Yuhao Chen, John S. Zelek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03335">SportMamba: Adaptive Non-Linear Multi-Object Tracking with State Space Models for Team Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in team sports is particularly challenging due to the fast-paced motion and frequent occlusions resulting in motion blur and identity switches, respectively. Predicting player positions in such scenarios is particularly difficult due to the observed highly non-linear motion patterns. Current methods are heavily reliant on object detection and appearance-based tracking, which struggle to perform in complex team sports scenarios, where appearance cues are ambiguous and motion patterns do not necessarily follow a linear pattern. To address these challenges, we introduce SportMamba, an adaptive hybrid MOT technique specifically designed for tracking in dynamic team sports. The technical contribution of SportMamba is twofold. First, we introduce a mamba-attention mechanism that models non-linear motion by implicitly focusing on relevant embedding dependencies. Second, we propose a height-adaptive spatial association metric to reduce ID switches caused by partial occlusions by accounting for scale variations due to depth changes. Additionally, we extend the detection search space with adaptive buffers to improve associations in fast-motion scenarios. Our proposed technique, SportMamba, demonstrates state-of-the-art performance on various metrics in the SportsMOT dataset, which is characterized by complex motion and severe occlusion. Furthermore, we demonstrate its generalization capability through zero-shot transfer to VIP-HTD, an ice hockey dataset.
<div id='section'>PaperID: <span id='pid'>306, <a href='https://arxiv.org/pdf/2505.17201.pdf' target='_blank'>https://arxiv.org/pdf/2505.17201.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chaim Chai Elchik, Fatemeh Karimi Nejadasl, Seyed Sahand Mohammadi Ziabari, Ali Mohammed Mansoor Alsahag
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17201">A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in computer vision has made significant advancements, yet tracking small fish in underwater environments presents unique challenges due to complex 3D motions and data noise. Traditional single-view MOT models often fall short in these settings. This thesis addresses these challenges by adapting state-of-the-art single-view MOT models, FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological studies. The core contribution of this research is the development of a multi-view framework that utilizes stereo video inputs to enhance tracking accuracy and fish behavior pattern recognition. By integrating and evaluating these models on underwater fish video datasets, the study aims to demonstrate significant improvements in precision and reliability compared to single-view approaches. The proposed framework detects fish entities with a relative accuracy of 47% and employs stereo-matching techniques to produce a novel 3D output, providing a more comprehensive understanding of fish movements and interactions
<div id='section'>PaperID: <span id='pid'>307, <a href='https://arxiv.org/pdf/2412.08321.pdf' target='_blank'>https://arxiv.org/pdf/2412.08321.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jan KrejÄÃ­, Oliver Kost, OndÅej Straka, Yuxuan Xia, Lennart Svensson, Ãngel F. GarcÃ­a-FernÃ¡ndez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08321">TGOSPA Metric Parameters Selection and Evaluation for Visual Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking algorithms are deployed in various applications, each with different performance requirements. For example, track switches pose significant challenges for offline scene understanding, as they hinder the accuracy of data interpretation. Conversely, in online surveillance applications, their impact is often minimal. This disparity underscores the need for application-specific performance evaluations that are both simple and mathematically sound. The trajectory generalized optimal sub-pattern assignment (TGOSPA) metric offers a principled approach to evaluate multi-object tracking performance. It accounts for localization errors, the number of missed and false objects, and the number of track switches, providing a comprehensive assessment framework. This paper illustrates the effective use of the TGOSPA metric in computer vision tasks, addressing challenges posed by the need for application-specific scoring methodologies. By exploring the TGOSPA parameter selection, we enable users to compare, comprehend, and optimize the performance of algorithms tailored for specific tasks, such as target tracking and training of detector or re-ID modules.
<div id='section'>PaperID: <span id='pid'>308, <a href='https://arxiv.org/pdf/2412.00692.pdf' target='_blank'>https://arxiv.org/pdf/2412.00692.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yizhou Wang, Tim Meinhardt, Orcun Cetintas, Cheng-Yen Yang, Sameer Satish Pusegaonkar, Benjamin Missaoui, Sujit Biswas, Zheng Tang, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00692">MCBLT: Multi-Camera Multi-Object 3D Tracking in Long Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object perception from multi-view cameras is crucial for intelligent systems, particularly in indoor environments, e.g., warehouses, retail stores, and hospitals. Most traditional multi-target multi-camera (MTMC) detection and tracking methods rely on 2D object detection, single-view multi-object tracking (MOT), and cross-view re-identification (ReID) techniques, without properly handling important 3D information by multi-view image aggregation. In this paper, we propose a 3D object detection and tracking framework, named MCBLT, which first aggregates multi-view images with necessary camera calibration parameters to obtain 3D object detections in bird's-eye view (BEV). Then, we introduce hierarchical graph neural networks (GNNs) to track these 3D detections in BEV for MTMC tracking results. Unlike existing methods, MCBLT has impressive generalizability across different scenes and diverse camera settings, with exceptional capability for long-term association handling. As a result, our proposed MCBLT establishes a new state-of-the-art on the AICity'24 dataset with $81.22$ HOTA, and on the WildTrack dataset with $95.6$ IDF1.
<div id='section'>PaperID: <span id='pid'>309, <a href='https://arxiv.org/pdf/2411.15459.pdf' target='_blank'>https://arxiv.org/pdf/2411.15459.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinqi Liu, Li Zhou, Zikun Zhou, Jianqiu Chen, Zhenyu He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15459">MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision-language tracking task aims to perform object tracking based on various modality references. Existing Transformer-based vision-language tracking methods have made remarkable progress by leveraging the global modeling ability of self-attention. However, current approaches still face challenges in effectively exploiting the temporal information and dynamically updating reference features during tracking. Recently, the State Space Model (SSM), known as Mamba, has shown astonishing ability in efficient long-sequence modeling. Particularly, its state space evolving process demonstrates promising capabilities in memorizing multimodal temporal information with linear complexity. Witnessing its success, we propose a Mamba-based vision-language tracking model to exploit its state space evolving ability in temporal space for robust multimodal tracking, dubbed MambaVLT. In particular, our approach mainly integrates a time-evolving hybrid state space block and a selective locality enhancement block, to capture contextual information for multimodal modeling and adaptive reference feature update. Besides, we introduce a modality-selection module that dynamically adjusts the weighting between visual and language references, mitigating potential ambiguities from either reference type. Extensive experimental results show that our method performs favorably against state-of-the-art trackers across diverse benchmarks.
<div id='section'>PaperID: <span id='pid'>310, <a href='https://arxiv.org/pdf/2511.17609.pdf' target='_blank'>https://arxiv.org/pdf/2511.17609.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Linh Van Ma, Unse Fatima, Tepy Sokun Chriv, Haroon Imran, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17609">3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.
<div id='section'>PaperID: <span id='pid'>311, <a href='https://arxiv.org/pdf/2511.16494.pdf' target='_blank'>https://arxiv.org/pdf/2511.16494.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zongcai Tan, Lan Wei, Dandan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16494">Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.
<div id='section'>PaperID: <span id='pid'>312, <a href='https://arxiv.org/pdf/2510.20807.pdf' target='_blank'>https://arxiv.org/pdf/2510.20807.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20807">Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.
<div id='section'>PaperID: <span id='pid'>313, <a href='https://arxiv.org/pdf/2510.17860.pdf' target='_blank'>https://arxiv.org/pdf/2510.17860.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zenghuang Fu, Xiaofeng Han, Mingda Jia, Jin ming Yang, Qi Zeng, Muyang Zahng, Changwei Wang, Weiliang Meng, Xiaopeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17860">DMTrack: Deformable State-Space Modeling for UAV Multi-Object Tracking with Kalman Fusion and Uncertainty-Aware Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) from unmanned aerial vehicles (UAVs) presents unique challenges due to unpredictable object motion, frequent occlusions, and limited appearance cues inherent to aerial viewpoints. These issues are further exacerbated by abrupt UAV movements, leading to unreliable trajectory estimation and identity switches. Conventional motion models, such as Kalman filters or static sequence encoders, often fall short in capturing both linear and non-linear dynamics under such conditions. To tackle these limitations, we propose DMTrack, a deformable motion tracking framework tailored for UAV-based MOT. Our DMTrack introduces three key components: DeformMamba, a deformable state-space predictor that dynamically aggregates historical motion states for adaptive trajectory modeling; MotionGate, a lightweight gating module that fuses Kalman and Mamba predictions based on motion context and uncertainty; and an uncertainty-aware association strategy that enhances identity preservation by aligning motion trends with prediction confidence. Extensive experiments on the VisDrone-MOT and UAVDT benchmarks demonstrate that our DMTrack achieves state-of-the-art performance in identity consistency and tracking accuracy, particularly under high-speed and non-linear motion. Importantly, our method operates without appearance models and maintains competitive efficiency, highlighting its practicality for robust UAV-based tracking.
<div id='section'>PaperID: <span id='pid'>314, <a href='https://arxiv.org/pdf/2508.14370.pdf' target='_blank'>https://arxiv.org/pdf/2508.14370.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hamidreza Hashempoor, Yu Dong Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14370">FastTracker: Real-Time and Accurate Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional multi-object tracking (MOT) systems are predominantly designed for pedestrian tracking and often exhibit limited generalization to other object categories. This paper presents a generalized tracking framework capable of handling multiple object types, with a particular emphasis on vehicle tracking in complex traffic scenes. The proposed method incorporates two key components: (1) an occlusion-aware re-identification mechanism that enhances identity preservation for heavily occluded objects, and (2) a road-structure-aware tracklet refinement strategy that utilizes semantic scene priors such as lane directions, crosswalks, and road boundaries to improve trajectory continuity and accuracy. In addition, we introduce a new benchmark dataset comprising diverse vehicle classes with frame-level tracking annotations, specifically curated to support evaluation of vehicle-focused tracking methods. Extensive experimental results demonstrate that the proposed approach achieves robust performance on both the newly introduced dataset and several public benchmarks, highlighting its effectiveness in general-purpose object tracking. While our framework is designed for generalized multi-class tracking, it also achieves strong performance on conventional benchmarks, with HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark are available: github.com/Hamidreza-Hashempoor/FastTracker, huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.
<div id='section'>PaperID: <span id='pid'>315, <a href='https://arxiv.org/pdf/2508.11323.pdf' target='_blank'>https://arxiv.org/pdf/2508.11323.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Haonan Zhang, Xinyao Wang, Boxi Wu, Tu Zheng, Wang Yunhua, Zheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11323">Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking is a critical and challenging task in the field of autonomous driving. A common paradigm relies on modeling individual object motion, e.g., Kalman filters, to predict trajectories. While effective in simple scenarios, this approach often struggles in crowded environments or with inaccurate detections, as it overlooks the rich geometric relationships between objects. This highlights the need to leverage spatial cues. However, existing geometry-aware methods can be susceptible to interference from irrelevant objects, leading to ambiguous features and incorrect associations. To address this, we propose focusing on cue-consistency: identifying and matching stable spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency Tracker (DSC-Track) to implement this principle. Firstly, we design a unified spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative trajectory embeddings while suppressing interference. Secondly, our cue-consistency transformer module explicitly aligns consistent feature representations between historical tracks and current detections. Finally, a dynamic update mechanism preserves salient spatiotemporal information for stable online tracking. Extensive experiments on the nuScenes and Waymo Open Datasets validate the effectiveness and robustness of our approach. On the nuScenes benchmark, for instance, our method achieves state-of-the-art performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets, respectively.
<div id='section'>PaperID: <span id='pid'>316, <a href='https://arxiv.org/pdf/2507.23473.pdf' target='_blank'>https://arxiv.org/pdf/2507.23473.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bin Xie, Congxuan Zhang, Fagan Wang, Peng Liu, Feng Lu, Zhen Chen, Weiming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23473">CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread application of Unmanned Aerial Vehicles (UAVs) has raised serious public safety and privacy concerns, making UAV perception crucial for anti-UAV tasks. However, existing UAV tracking datasets predominantly feature conspicuous objects and lack diversity in scene complexity and attribute representation, limiting their applicability to real-world scenarios. To overcome these limitations, we present the CST Anti-UAV, a new thermal infrared dataset specifically designed for Single Object Tracking (SOT) in Complex Scenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k high-quality bounding box annotations, highlighting two key properties: a significant number of tiny-sized UAV targets and the diverse and complex scenes. To the best of our knowledge, CST Anti-UAV is the first dataset to incorporate complete manual frame-level attribute annotations, enabling precise evaluations under varied challenges. To conduct an in-depth performance analysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed dataset. Experimental results demonstrate that tracking tiny UAVs in complex environments remains a challenge, as the state-of-the-art method achieves only 35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410 dataset. These findings underscore the limitations of existing benchmarks and the need for further advancements in UAV tracking research. The CST Anti-UAV benchmark is about to be publicly released, which not only fosters the development of more robust SOT methods but also drives innovation in anti-UAV systems.
<div id='section'>PaperID: <span id='pid'>317, <a href='https://arxiv.org/pdf/2507.01535.pdf' target='_blank'>https://arxiv.org/pdf/2507.01535.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bingxi Liu, Calvin Chen, Junhao Li, Guyang Yu, Haoqian Song, Xuchen Liu, Jinqiang Cui, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01535">TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Vision Transformer (ViT) model has long struggled with the challenge of quadratic complexity, a limitation that becomes especially critical in unmanned aerial vehicle (UAV) tracking systems, where data must be processed in real time. In this study, we explore the recently proposed State-Space Model, Mamba, leveraging its computational efficiency and capability for long-sequence modeling to effectively process dense image sequences in tracking tasks. First, we highlight the issue of temporal inconsistency in existing Mamba-based methods, specifically the failure to account for temporal continuity in the Mamba scanning mechanism. Secondly, building upon this insight,we propose TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model for handling image sequence of tracking problem. In our framework, the mamba scan is performed in a nested way while independently process temporal and spatial coherent patch tokens. While the template frame is encoded as query token and utilized for tracking in every scan. Extensive experiments conducted on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves state-of-the-art precision while offering noticeable higher speed in UAV tracking.
<div id='section'>PaperID: <span id='pid'>318, <a href='https://arxiv.org/pdf/2502.17399.pdf' target='_blank'>https://arxiv.org/pdf/2502.17399.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Liuchuan Yu, Ching-I Huang, Hsueh-Cheng Wang, Lap-Fai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17399">Enriching Physical-Virtual Interaction in AR Gaming by Tracking Identical Real Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented reality (AR) games, particularly those designed for headsets, have become increasingly prevalent with advancements in both hardware and software. However, the majority of AR games still rely on pre-scanned or static scenes, and interaction mechanisms are often limited to controllers or hand-tracking. Additionally, the presence of identical objects in AR games poses challenges for conventional object tracking techniques, which often struggle to differentiate between identical objects or necessitate the installation of fixed cameras for global object movement tracking. In response to these limitations, we present a novel approach to address the tracking of identical objects in an AR scene to enrich physical-virtual interaction. Our method leverages partial scene observations captured by an AR headset, utilizing the perspective and spatial data provided by this technology. Object identities within the scene are determined through the solution of a label assignment problem using integer programming. To enhance computational efficiency, we incorporate a Voronoi diagram-based pruning method into our approach. Our implementation of this approach in a farm-to-table AR game demonstrates its satisfactory performance and robustness. Furthermore, we showcase the versatility and practicality of our method through applications in AR storytelling and a simulated gaming robot. Our video demo is available at: https://youtu.be/rPGkLYuKvCQ.
<div id='section'>PaperID: <span id='pid'>319, <a href='https://arxiv.org/pdf/2410.08529.pdf' target='_blank'>https://arxiv.org/pdf/2410.08529.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zekun Qian, Ruize Han, Junhui Hou, Linqi Song, Wei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08529">VOVTrack: Exploring the Potentiality in Videos for Open-Vocabulary Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary multi-object tracking (OVMOT) represents a critical new challenge involving the detection and tracking of diverse object categories in videos, encompassing both seen categories (base classes) and unseen categories (novel classes). This issue amalgamates the complexities of open-vocabulary object detection (OVD) and multi-object tracking (MOT). Existing approaches to OVMOT often merge OVD and MOT methodologies as separate modules, predominantly focusing on the problem through an image-centric lens. In this paper, we propose VOVTrack, a novel method that integrates object states relevant to MOT and video-centric training to address this challenge from a video object tracking standpoint. First, we consider the tracking-related state of the objects during tracking and propose a new prompt-guided attention mechanism for more accurate localization and classification (detection) of the time-varying objects. Subsequently, we leverage raw video data without annotations for training by formulating a self-supervised object similarity learning technique to facilitate temporal object association (tracking). Experimental results underscore that VOVTrack outperforms existing methods, establishing itself as a state-of-the-art solution for open-vocabulary tracking task.
<div id='section'>PaperID: <span id='pid'>320, <a href='https://arxiv.org/pdf/2409.16111.pdf' target='_blank'>https://arxiv.org/pdf/2409.16111.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yannik Blei, Michael Krawez, Nisarga Nilavadi, Tanja Katharina Kaiser, Wolfram Burgard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16111">CloudTrack: Scalable UAV Tracking with Cloud Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and rescue scenarios to gather information in the search area. The automatic identification of the person searched for in aerial footage could increase the autonomy of such systems, reduce the search time, and thus increase the missed person's chances of survival. In this paper, we present a novel approach to perform semantically conditioned open vocabulary object tracking that is specifically designed to cope with the limitations of UAV hardware. Our approach has several advantages. It can run with verbal descriptions of the missing person, e.g., the color of the shirt, it does not require dedicated training to execute the mission and can efficiently track a potentially moving person. Our experimental results demonstrate the versatility and efficacy of our approach.
<div id='section'>PaperID: <span id='pid'>321, <a href='https://arxiv.org/pdf/2512.22244.pdf' target='_blank'>https://arxiv.org/pdf/2512.22244.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Daniyal Ganiuly, Nurzhau Bolatbek, Assel Smaiyl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22244">Failure Analysis of Safety Controllers in Autonomous Vehicles Under Object-Based LiDAR Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles rely on LiDAR based perception to support safety critical control functions such as adaptive cruise control and automatic emergency braking. While previous research has shown that LiDAR perception can be manipulated through object based spoofing and injection attacks, the impact of such attacks on vehicle safety controllers is still not well understood. This paper presents a systematic failure analysis of longitudinal safety controllers under object based LiDAR attacks in highway driving scenarios. The study focuses on realistic cut in and car following situations in which adversarial objects introduce persistent perception errors without directly modifying vehicle control software. A high fidelity simulation framework integrating LiDAR perception, object tracking, and closed loop vehicle control is used to evaluate how false and displaced object detections propagate through the perception planning and control pipeline. The results demonstrate that even short duration LiDAR induced object hallucinations can trigger unsafe braking, delayed responses to real hazards, and unstable control behavior. In cut in scenarios, a clear increase in unsafe deceleration events and time to collision violations is observed when compared to benign conditions, despite identical controller parameters. The analysis further shows that controller failures are more strongly influenced by the temporal consistency of spoofed objects than by spatial inaccuracies alone. These findings reveal a critical gap between perception robustness and control level safety guarantees in autonomous driving systems. By explicitly characterizing safety controller failure modes under adversarial perception, this work provides practical insights for the design of attack aware safety mechanisms and more resilient control strategies for LiDAR dependent autonomous vehicles.
<div id='section'>PaperID: <span id='pid'>322, <a href='https://arxiv.org/pdf/2512.19583.pdf' target='_blank'>https://arxiv.org/pdf/2512.19583.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yinhuai Wang, Runyi Yu, Hok Wai Tsui, Xiaoyi Lin, Hui Zhang, Qihan Zhao, Ke Fan, Miao Li, Jie Song, Jingbo Wang, Qifeng Chen, Ping Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19583">Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.
<div id='section'>PaperID: <span id='pid'>323, <a href='https://arxiv.org/pdf/2512.10102.pdf' target='_blank'>https://arxiv.org/pdf/2512.10102.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Neelima Prasad, Jarek Reynolds, Neel Karsanbhai, Tanusree Sharma, Lotus Zhang, Abigale Stangl, Yang Wang, Leah Findlater, Danna Gurari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10102">Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/
<div id='section'>PaperID: <span id='pid'>324, <a href='https://arxiv.org/pdf/2509.21715.pdf' target='_blank'>https://arxiv.org/pdf/2509.21715.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xu Yang, Gady Agam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21715">Motion-Aware Transformer for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in videos remains challenging due to complex object motions and crowded scenes. Recent DETR-based frameworks offer end-to-end solutions but typically process detection and tracking queries jointly within a single Transformer Decoder layer, leading to conflicts and degraded association accuracy. We introduce the Motion-Aware Transformer (MATR), which explicitly predicts object movements across frames to update track queries in advance. By reducing query collisions, MATR enables more consistent training and improves both detection and association. Extensive experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers significant gains across standard metrics. On DanceTrack, MATR improves HOTA by more than 9 points over MOTR without additional data and reaches a new state-of-the-art score of 71.3 with supplementary data. MATR also achieves state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6 mHOTA) without relying on external datasets. These results demonstrate that explicitly modeling motion within end-to-end Transformers offers a simple yet highly effective approach to advancing multi-object tracking.
<div id='section'>PaperID: <span id='pid'>325, <a href='https://arxiv.org/pdf/2509.11873.pdf' target='_blank'>https://arxiv.org/pdf/2509.11873.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anne Marthe Sophie Ngo Bibinbe, Patrick Gagnon, Jamie Ahloy-Dallaire, Eric R. Paquet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11873">Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision livestock farming requires advanced monitoring tools to meet the increasing management needs of the industry. Computer vision systems capable of long-term multi-animal tracking (MAT) are essential for continuous behavioral monitoring in livestock production. MAT, a specialized subset of multi-object tracking (MOT), shares many challenges with MOT, but also faces domain-specific issues including frequent animal occlusion, highly similar appearances among animals, erratic motion patterns, and a wide range of behavior types. While some existing MAT tools are user-friendly and widely adopted, they often underperform compared to state-of-the-art MOT methods, which can result in inaccurate downstream tasks such as behavior analysis, health state estimation, and related applications. In this study, we benchmarked both MAT and MOT approaches for long-term tracking of pigs. We compared tools such as DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT, cross-input consistency, and newer approaches like Track-Anything and PromptTrack. All methods were evaluated on a 10-minute pig tracking dataset. Our results demonstrate that, overall, MOT approaches outperform traditional MAT tools, even for long-term tracking scenarios. These findings highlight the potential of recent MOT techniques to enhance the accuracy and reliability of automated livestock tracking.
<div id='section'>PaperID: <span id='pid'>326, <a href='https://arxiv.org/pdf/2507.21411.pdf' target='_blank'>https://arxiv.org/pdf/2507.21411.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kentaro Takahira, Yue Yu, Takanori Fujiwara, Ryo Suzuki, Huamin Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21411">InSituTale: Enhancing Augmented Data Storytelling with Physical Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented data storytelling enhances narrative delivery by integrating visualizations with physical environments and presenter actions. Existing systems predominantly rely on body gestures or speech to control visualizations, leaving interactions with physical objects largely underexplored. We introduce augmented physical data storytelling, an approach enabling presenters to manipulate visualizations through physical object interactions. To inform this approach, we first conducted a survey of data-driven presentations to identify common visualization commands. We then conducted workshops with nine HCI/VIS researchers to collect mappings between physical manipulations and these commands. Guided by these insights, we developed InSituTale, a prototype that combines object tracking via a depth camera with Vision-LLM for detecting real-world events. Through physical manipulations, presenters can dynamically execute various visualization commands, delivering cohesive data storytelling experiences that blend physical and digital elements. A user study with 12 participants demonstrated that InSituTale enables intuitive interactions, offers high utility, and facilitates an engaging presentation experience.
<div id='section'>PaperID: <span id='pid'>327, <a href='https://arxiv.org/pdf/2507.18594.pdf' target='_blank'>https://arxiv.org/pdf/2507.18594.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xuecheng Bai, Yuxiang Wang, Boyu Hu, Qinyuan Jie, Chuanzhi Xu, Hongru Xiao, Kechen Li, Vera Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18594">DRWKV: Focusing on Object Edges for Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.
<div id='section'>PaperID: <span id='pid'>328, <a href='https://arxiv.org/pdf/2506.05543.pdf' target='_blank'>https://arxiv.org/pdf/2506.05543.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sethuraman TV, Savya Khosla, Vignesh Srinivasakumar, Jiahui Huang, Seoung Wug Oh, Simon Jenni, Derek Hoiem, Joon-Young Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05543">FRAME: Pre-Training Video Feature Representations via Anticipation and Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense video prediction tasks, such as object tracking and semantic segmentation, require video encoders that generate temporally consistent, spatially dense features for every frame. However, existing approaches fall short: image encoders like DINO or CLIP lack temporal awareness, while video models such as VideoMAE underperform compared to image encoders on dense prediction tasks. We address this gap with FRAME, a self-supervised video frame encoder tailored for dense video understanding. FRAME learns to predict current and future DINO patch features from past and present RGB frames, leading to spatially precise and temporally coherent representations. To our knowledge, FRAME is the first video encoder to leverage image-based models for dense prediction while outperforming them on tasks requiring fine-grained visual correspondence. As an auxiliary capability, FRAME aligns its class token with CLIP's semantic space, supporting language-driven tasks such as video classification. We evaluate FRAME across six dense prediction tasks on seven datasets, where it consistently outperforms image encoders and existing self-supervised video models. Despite its versatility, FRAME maintains a compact architecture suitable for a range of downstream applications.
<div id='section'>PaperID: <span id='pid'>329, <a href='https://arxiv.org/pdf/2505.22882.pdf' target='_blank'>https://arxiv.org/pdf/2505.22882.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wen Yang, Zhixian Xie, Xuechao Zhang, Heni Ben Amor, Shan Lin, Wanxin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22882">TwinTrack: Bridging Vision and Contact Physics for Real-Time Tracking of Unknown Dynamic Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time tracking of previously unseen, highly dynamic objects in contact-rich environments -- such as during dexterous in-hand manipulation -- remains a significant challenge. Purely vision-based tracking often suffers from heavy occlusions due to the frequent contact interactions and motion blur caused by abrupt motion during contact impacts. We propose TwinTrack, a physics-aware visual tracking framework that enables robust and real-time 6-DoF pose tracking of unknown dynamic objects in a contact-rich scene by leveraging the contact physics of the observed scene. At the core of TwinTrack is an integration of Real2Sim and Sim2Real. In Real2Sim, we combine the complementary strengths of vision and contact physics to estimate object's collision geometry and physical properties: object's geometry is first reconstructed from vision, then updated along with other physical parameters from contact dynamics for physical accuracy. In Sim2Real, robust pose estimation of the object is achieved by adaptive fusion between visual tracking and prediction of the learned contact physics. TwinTrack is built on a GPU-accelerated, deeply customized physics engine to ensure real-time performance. We evaluate our method on two contact-rich scenarios: object falling with rich contact impacts against the environment, and contact-rich in-hand manipulation. Experimental results demonstrate that, compared to baseline methods, TwinTrack achieves significantly more robust, accurate, and real-time 6-DoF tracking in these challenging scenarios, with tracking speed exceeding 20 Hz. Project page: https://irislab.tech/TwinTrack-webpage/
<div id='section'>PaperID: <span id='pid'>330, <a href='https://arxiv.org/pdf/2505.00739.pdf' target='_blank'>https://arxiv.org/pdf/2505.00739.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qiushi Yang, Yuan Yao, Miaomiao Cui, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00739">MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional capabilities in interactive object segmentation for both images and videos. However, as a foundational model on interactive segmentation, SAM2 performs segmentation directly based on mask memory from the past six frames, leading to two significant challenges. Firstly, during inference in videos, objects may disappear since SAM2 relies solely on memory without accounting for object motion information, which limits its long-range object tracking capabilities. Secondly, its memory is constructed from fixed past frames, making it susceptible to challenges associated with object disappearance or occlusion, due to potentially inaccurate segmentation results in memory. To address these problems, we present MoSAM, incorporating two key strategies to integrate object motion cues into the model and establish more reliable feature memory. Firstly, we propose Motion-Guided Prompting (MGP), which represents the object motion in both sparse and dense manners, then injects them into SAM2 through a set of motion-guided prompts. MGP enables the model to adjust its focus towards the direction of motion, thereby enhancing the object tracking capabilities. Furthermore, acknowledging that past segmentation results may be inaccurate, we devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically identifies frames likely to contain accurate segmentation in both pixel- and frame-level. By eliminating potentially inaccurate mask predictions from memory, we can leverage more reliable memory features to exploit similar regions for improving segmentation results. Extensive experiments on various benchmarks of video object segmentation and video instance segmentation demonstrate that our MoSAM achieves state-of-the-art results compared to other competitors.
<div id='section'>PaperID: <span id='pid'>331, <a href='https://arxiv.org/pdf/2503.13023.pdf' target='_blank'>https://arxiv.org/pdf/2503.13023.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michal Danilowicz, Tomasz Kryjak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13023">Real-Time Multi-Object Tracking using YOLOv8 and SORT on a SoC FPGA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is one of the most important problems in computer vision and a key component of any vision-based perception system used in advanced autonomous mobile robotics. Therefore, its implementation on low-power and real-time embedded platforms is highly desirable. Modern MOT algorithms should be able to track objects of a given class (e.g. people or vehicles). In addition, the number of objects to be tracked is not known in advance, and they may appear and disappear at any time, as well as be obscured. For these reasons, the most popular and successful approaches have recently been based on the tracking paradigm. Therefore, the presence of a high quality object detector is essential, which in practice accounts for the vast majority of the computational and memory complexity of the whole MOT system. In this paper, we propose an FPGA (Field-Programmable Gate Array) implementation of an embedded MOT system based on a quantized YOLOv8 detector and the SORT (Simple Online Realtime Tracker) tracker. We use a modified version of the FINN framework to utilize external memory for model parameters and to support operations necessary required by YOLOv8. We discuss the evaluation of detection and tracking performance using the COCO and MOT15 datasets, where we achieve 0.21 mAP and 38.9 MOTA respectively. As the computational platform, we use an MPSoC system (Zynq UltraScale+ device from AMD/Xilinx) where the detector is deployed in reprogrammable logic and the tracking algorithm is implemented in the processor system.
<div id='section'>PaperID: <span id='pid'>332, <a href='https://arxiv.org/pdf/2502.17434.pdf' target='_blank'>https://arxiv.org/pdf/2502.17434.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hongyu Li, Mingxi Jia, Tuluhan Akbulut, Yu Xiang, George Konidaris, Srinath Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17434">V-HOP: Visuo-Haptic 6D Object Pose Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Project website: https://ivl.cs.brown.edu/research/v-hop
<div id='section'>PaperID: <span id='pid'>333, <a href='https://arxiv.org/pdf/2501.15953.pdf' target='_blank'>https://arxiv.org/pdf/2501.15953.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Meng Chu, Yicong Li, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15953">Understanding Long Videos via LLM-Powered Entity Relation Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The analysis of extended video content poses unique challenges in artificial intelligence, particularly when dealing with the complexity of tracking and understanding visual elements across time. Current methodologies that process video frames sequentially struggle to maintain coherent tracking of objects, especially when these objects temporarily vanish and later reappear in the footage. A critical limitation of these approaches is their inability to effectively identify crucial moments in the video, largely due to their limited grasp of temporal relationships. To overcome these obstacles, we present GraphVideoAgent, a cutting-edge system that leverages the power of graph-based object tracking in conjunction with large language model capabilities. At its core, our framework employs a dynamic graph structure that maps and monitors the evolving relationships between visual entities throughout the video sequence. This innovative approach enables more nuanced understanding of how objects interact and transform over time, facilitating improved frame selection through comprehensive contextual awareness. Our approach demonstrates remarkable effectiveness when tested against industry benchmarks. In evaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2 improvement over existing methods while requiring analysis of only 8.2 frames on average. Similarly, testing on the NExT-QA benchmark yielded a 2.0 performance increase with an average frame requirement of 8.1. These results underscore the efficiency of our graph-guided methodology in enhancing both accuracy and computational performance in long-form video understanding tasks.
<div id='section'>PaperID: <span id='pid'>334, <a href='https://arxiv.org/pdf/2501.10129.pdf' target='_blank'>https://arxiv.org/pdf/2501.10129.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Futian Wang, Fengxiang Liu, Xiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10129">Spatio-temporal Graph Learning on Adaptive Mined Key Frames for High-performance Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of multi-object tracking, the challenge of accurately capturing the spatial and temporal relationships between objects in video sequences remains a significant hurdle. This is further complicated by frequent occurrences of mutual occlusions among objects, which can lead to tracking errors and reduced performance in existing methods. Motivated by these challenges, we propose a novel adaptive key frame mining strategy that addresses the limitations of current tracking approaches. Specifically, we introduce a Key Frame Extraction (KFE) module that leverages reinforcement learning to adaptively segment videos, thereby guiding the tracker to exploit the intrinsic logic of the video content. This approach allows us to capture structured spatial relationships between different objects as well as the temporal relationships of objects across frames. To tackle the issue of object occlusions, we have developed an Intra-Frame Feature Fusion (IFF) module. Unlike traditional graph-based methods that primarily focus on inter-frame feature fusion, our IFF module uses a Graph Convolutional Network (GCN) to facilitate information exchange between the target and surrounding objects within a frame. This innovation significantly enhances target distinguishability and mitigates tracking loss and appearance similarity due to occlusions. By combining the strengths of both long and short trajectories and considering the spatial relationships between objects, our proposed tracker achieves impressive results on the MOT17 dataset, i.e., 68.6 HOTA, 81.0 IDF1, 66.6 AssA, and 893 IDS, proving its effectiveness and accuracy.
<div id='section'>PaperID: <span id='pid'>335, <a href='https://arxiv.org/pdf/2411.18476.pdf' target='_blank'>https://arxiv.org/pdf/2411.18476.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiangtao Shuai, Martin Baerveldt, Manh Nguyen-Duc, Anh Le-Tuan, Manfred Hauswirth, Danh Le-Phuoc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18476">A comparison of extended object tracking with multi-modal sensors in indoor environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a preliminary study of an efficient object tracking approach, comparing the performance of two different 3D point cloud sensory sources: LiDAR and stereo cameras, which have significant price differences. In this preliminary work, we focus on single object tracking. We first developed a fast heuristic object detector that utilizes prior information about the environment and target. The resulting target points are subsequently fed into an extended object tracking framework, where the target shape is parameterized using a star-convex hypersurface model. Experimental results show that our object tracking method using a stereo camera achieves performance similar to that of a LiDAR sensor, with a cost difference of more than tenfold.
<div id='section'>PaperID: <span id='pid'>336, <a href='https://arxiv.org/pdf/2411.08433.pdf' target='_blank'>https://arxiv.org/pdf/2411.08433.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoxiang Wang, Jiaxin Liu, Miaojie Feng, Zhaoxing Zhang, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08433">3D Multi-Object Tracking with Semi-Supervised GRU-Kalman Filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT), a fundamental component of environmental perception, is essential for intelligent systems like autonomous driving and robotic sensing. Although Tracking-by-Detection frameworks have demonstrated excellent performance in recent years, their application in real-world scenarios faces significant challenges. Object movement in complex environments is often highly nonlinear, while existing methods typically rely on linear approximations of motion. Furthermore, system noise is frequently modeled as a Gaussian distribution, which fails to capture the true complexity of the noise dynamics. These oversimplified modeling assumptions can lead to significant reductions in tracking precision. To address this, we propose a GRU-based MOT method, which introduces a learnable Kalman filter into the motion module. This approach is able to learn object motion characteristics through data-driven learning, thereby avoiding the need for manual model design and model error. At the same time, to avoid abnormal supervision caused by the wrong association between annotations and trajectories, we design a semi-supervised learning strategy to accelerate the convergence speed and improve the robustness of the model. Evaluation experiment on the nuScenes and Argoverse2 datasets demonstrates that our system exhibits superior performance and significant potential compared to traditional TBD methods.
<div id='section'>PaperID: <span id='pid'>337, <a href='https://arxiv.org/pdf/2411.08144.pdf' target='_blank'>https://arxiv.org/pdf/2411.08144.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yangge Li, Benjamin C Yang, Sayan Mitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08144">Visual Tracking with Intermittent Visibility: Switched Control Design and Implementation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of visual target tracking in scenarios where a pursuer may experience intermittent loss of visibility of the target. The design of a Switched Visual Tracker (SVT) is presented which aims to meet the competing requirements of maintaining both proximity and visibility. SVT alternates between a visual tracking mode for following the target, and a recovery mode for regaining visual contact when the target falls out of sight. We establish the stability of SVT by extending the average dwell time theorem from switched systems theory, which may be of independent interest. Our implementation of SVT on an Agilicious drone [1] illustrates its effectiveness on tracking various target trajectories: it reduces the average tracking error by up to 45% and significantly improves visibility duration compared to a baseline algorithm. The results show that our approach effectively handles intermittent vision loss, offering enhanced robustness and adaptability for real-world autonomous missions. Additionally, we demonstrate how the stability analysis provides valuable guidance for selecting parameters, such as tracking speed and recovery distance, to optimize the SVT's performance.
<div id='section'>PaperID: <span id='pid'>338, <a href='https://arxiv.org/pdf/2410.02094.pdf' target='_blank'>https://arxiv.org/pdf/2410.02094.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sabine Muzellec, Drew Linsley, Alekh K. Ashok, Ennio Mingolla, Girik Malik, Rufin VanRullen, Thomas Serre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02094">Tracking objects that change in appearance with phase synchrony</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objects we encounter often change appearance as we interact with them. Changes in illumination (shadows), object pose, or the movement of non-rigid objects can drastically alter available image features. How do biological visual systems track objects as they change? One plausible mechanism involves attentional mechanisms for reasoning about the locations of objects independently of their appearances -- a capability that prominent neuroscience theories have associated with computing through neural synchrony. Here, we describe a novel deep learning circuit that can learn to precisely control attention to features separately from their location in the world through neural synchrony: the complex-valued recurrent neural network (CV-RNN). Next, we compare object tracking in humans, the CV-RNN, and other deep neural networks (DNNs), using FeatureTracker: a large-scale challenge that asks observers to track objects as their locations and appearances change in precisely controlled ways. While humans effortlessly solved FeatureTracker, state-of-the-art DNNs did not. In contrast, our CV-RNN behaved similarly to humans on the challenge, providing a computational proof-of-concept for the role of phase synchronization as a neural substrate for tracking appearance-morphing objects as they move about.
<div id='section'>PaperID: <span id='pid'>339, <a href='https://arxiv.org/pdf/2601.02521.pdf' target='_blank'>https://arxiv.org/pdf/2601.02521.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Amirreza Parvahan, Mohammad Hoseyni, Javad Khoramdel, Amirhossein Nikoofard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.02521">CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated analysis of volumetric medical imaging on edge devices is severely constrained by the high memory and computational demands of 3D Convolutional Neural Networks (CNNs). This paper develops a lightweight computer vision framework that reconciles the efficiency of 2D detection with the necessity of 3D context by reformulating volumetric Computer Tomography (CT) data as sequential video streams. This video-viewpoint paradigm is applied to the time-sensitive task of Intracranial Hemorrhage (ICH) detection using the Hemorica dataset. To ensure operational efficiency, we benchmarked multiple generations of the YOLO architecture (v8, v10, v11 and v12) in their Nano configurations, selecting the version with the highest mAP@50 to serve as the slice-level backbone. A ByteTrack algorithm is then introduced to enforce anatomical consistency across the $z$-axis. To address the initialization lag inherent in video trackers, a hybrid inference strategy and a spatiotemporal consistency filter are proposed to distinguish true pathology from transient prediction noise. Experimental results on independent test data demonstrate that the proposed framework serves as a rigorous temporal validator, increasing detection Precision from 0.703 to 0.779 compared to the baseline 2D detector, while maintaining high sensitivity. By approximating 3D contextual reasoning at a fraction of the computational cost, this method provides a scalable solution for real-time patient prioritization in resource-constrained environments, such as mobile stroke units and IoT-enabled remote clinics.
<div id='section'>PaperID: <span id='pid'>340, <a href='https://arxiv.org/pdf/2512.02668.pdf' target='_blank'>https://arxiv.org/pdf/2512.02668.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qionglin Ren, Dawei Zhang, Chunxu Tian, Dan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02668">UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.
<div id='section'>PaperID: <span id='pid'>341, <a href='https://arxiv.org/pdf/2511.15077.pdf' target='_blank'>https://arxiv.org/pdf/2511.15077.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shengjing Tian, Yinan Han, Xiantong Zhao, Xuehu Liu, Qi Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15077">MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.
<div id='section'>PaperID: <span id='pid'>342, <a href='https://arxiv.org/pdf/2510.19981.pdf' target='_blank'>https://arxiv.org/pdf/2510.19981.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Martha Teiko Teye, Ori Maoz, Matthias Rottmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19981">FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework that builds on existing 3D detectors by introducing a transformer-based smoother and a fusion-driven tracker. Inspired by query-based tracking frameworks, FutrTrack employs a multimodal two-stage transformer refinement and tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without the need for an explicit motion model. The tracker assigns and propagates identities across frames, leveraging both geometric and semantic cues for robust re-identification under occlusion and viewpoint changes. Prior to tracking, we refine sequences of bounding boxes with a temporal smoother over a moving window to refine trajectories, reduce jitter, and improve spatial consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that query-based transformer tracking methods benefit significantly from multimodal sensor features compared with previous single-sensor approaches. With an aMOTA of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D MOT benchmarks, reducing identity switches while maintaining competitive accuracy. Our approach provides an efficient framework for improving transformer-based trackers to compete with other neural-network-based methods even with limited data and without pretraining.
<div id='section'>PaperID: <span id='pid'>343, <a href='https://arxiv.org/pdf/2508.08117.pdf' target='_blank'>https://arxiv.org/pdf/2508.08117.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xudong Han, Pengcheng Fang, Yueying Tian, Jianhui Yu, Xiaohao Cai, Daniel Roggen, Philip Birch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08117">GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in monocular videos is fundamentally challenged by occlusions and depth ambiguity, issues that conventional tracking-by-detection (TBD) methods struggle to resolve owing to a lack of geometric awareness. To address these limitations, we introduce GRASPTrack, a novel depth-aware MOT framework that integrates monocular depth estimation and instance segmentation into a standard TBD pipeline to generate high-fidelity 3D point clouds from 2D detections, thereby enabling explicit 3D geometric reasoning. These 3D point clouds are then voxelized to enable a precise and robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To further enhance tracking robustness, our approach incorporates Depth-aware Adaptive Noise Compensation, which dynamically adjusts the Kalman filter process noise based on occlusion severity for more reliable state estimation. Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which extends the motion direction consistency from the image plane into 3D space to improve motion-based association cues, particularly for objects with complex trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack benchmarks demonstrate that our method achieves competitive performance, significantly improving tracking robustness in complex scenes with frequent occlusions and intricate motion patterns.
<div id='section'>PaperID: <span id='pid'>344, <a href='https://arxiv.org/pdf/2507.16015.pdf' target='_blank'>https://arxiv.org/pdf/2507.16015.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matteo Dunnhofer, Zaira Manigrasso, Christian Micheloni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16015">Is Tracking really more challenging in First Person Egocentric Vision?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking and segmentation are becoming fundamental tasks for understanding human activities in egocentric vision. Recent research has benchmarked state-of-the-art methods and concluded that first person egocentric vision presents challenges compared to previously studied domains. However, these claims are based on evaluations conducted across significantly different scenarios. Many of the challenging characteristics attributed to egocentric vision are also present in third person videos of human-object activities. This raises a critical question: how much of the observed performance drop stems from the unique first person viewpoint inherent to egocentric vision versus the domain of human-object activities? To address this question, we introduce a new benchmark study designed to disentangle such factors. Our evaluation strategy enables a more precise separation of challenges related to the first person perspective from those linked to the broader domain of human-object activity understanding. By doing so, we provide deeper insights into the true sources of difficulty in egocentric tracking and segmentation, facilitating more targeted advancements on this task.
<div id='section'>PaperID: <span id='pid'>345, <a href='https://arxiv.org/pdf/2507.02393.pdf' target='_blank'>https://arxiv.org/pdf/2507.02393.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Seokyeong Lee, Sithu Aung, Junyong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02393">PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection (M3OD) has long faced challenges due to data scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity. Although various weakly supervised methods and pseudo-labeling methods have been proposed to address these issues, they are mostly limited by domain-specific learning or rely solely on shape information from a single observation. In this paper, we propose a novel pseudo-labeling framework that uses only video data and is more robust to occlusion, without requiring a multi-view setup, additional sensors, camera poses, or domain-specific training. Specifically, we explore a technique for aggregating the pseudo-LiDARs of both static and dynamic objects across temporally adjacent frames using object point tracking, enabling 3D attribute extraction in scenarios where 3D data acquisition is infeasible. Extensive experiments demonstrate that our method ensures reliable accuracy and strong scalability, making it a practical and effective solution for M3OD.
<div id='section'>PaperID: <span id='pid'>346, <a href='https://arxiv.org/pdf/2505.12753.pdf' target='_blank'>https://arxiv.org/pdf/2505.12753.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Martha Teiko Teye, Ori Maoz, Matthias Rottmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12753">LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking from LiDAR point clouds presents unique challenges due to the sparse and irregular nature of the data, compounded by the need for temporal coherence across frames. Traditional tracking systems often rely on hand-crafted features and motion models, which can struggle to maintain consistent object identities in crowded or fast-moving scenes. We present a lidar-based two-staged DETR inspired transformer; a smoother and tracker. The smoother stage refines lidar object detections, from any off-the-shelf detector, across a moving temporal window. The tracker stage uses a DETR-based attention block to maintain tracks across time by associating tracked objects with the refined detections using the point cloud as context. The model is trained on the datasets nuScenes and KITTI in both online and offline (forward peeking) modes demonstrating strong performance across metrics such as ID-switch and multiple object tracking accuracy (MOTA). The numerical results indicate that the online mode outperforms the lidar-only baseline and SOTA models on the nuScenes dataset, with an aMOTA of 0.724 and an aMOTP of 0.475, while the offline mode provides an additional 3 pp aMOTP.
<div id='section'>PaperID: <span id='pid'>347, <a href='https://arxiv.org/pdf/2505.04088.pdf' target='_blank'>https://arxiv.org/pdf/2505.04088.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shang Zhang, Huanbin Zhang, Dali Feng, Yujie Cui, Ruoyan Xiong, Cen He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04088">SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thermal infrared (TIR) object tracking often suffers from challenges such as target occlusion, motion blur, and background clutter, which significantly degrade the performance of trackers. To address these issues, this paper pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a bidirectional state-space model and a self-attention mechanism. Specifically, we introduce the Motion Mamba module into the Siamese architecture to ex-tract motion features and recover overlooked edge details using bidirectional modeling and self-attention. We propose a Siamese parameter-sharing strate-gy that allows certain convolutional layers to share weights. This approach reduces computational redundancy while preserving strong feature represen-tation. In addition, we design a motion edge-aware regression loss to improve tracking accuracy, especially for motion-blurred targets. Extensive experi-ments are conducted on four TIR tracking benchmarks, including LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT achieves superior performance in TIR target tracking.
<div id='section'>PaperID: <span id='pid'>348, <a href='https://arxiv.org/pdf/2504.20391.pdf' target='_blank'>https://arxiv.org/pdf/2504.20391.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tran Thien Dat Nguyen, Ba Tuong Vo, Ba-Ngu Vo, Hoa Van Nguyen, Changbeom Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20391">The Mean of Multi-Object Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the concept of a mean for trajectories and multi-object trajectories (defined as sets or multi-sets of trajectories) along with algorithms for computing them. Specifically, we use the FrÃ©chet mean, and metrics based on the optimal sub-pattern assignment (OSPA) construct, to extend the notion of average from vectors to trajectories and multi-object trajectories. Further, we develop efficient algorithms to compute these means using greedy search and Gibbs sampling. Using distributed multi-object tracking as an application, we demonstrate that the FrÃ©chet mean approach to multi-object trajectory consensus significantly outperforms state-of-the-art distributed multi-object tracking methods.
<div id='section'>PaperID: <span id='pid'>349, <a href='https://arxiv.org/pdf/2504.04097.pdf' target='_blank'>https://arxiv.org/pdf/2504.04097.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shaohang Han, Matti Vahs, Jana Tumova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04097">Risk-Aware Robot Control in Dynamic Environments Using Belief Control Barrier Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety for autonomous robots operating in dynamic environments can be challenging due to factors such as unmodeled dynamics, noisy sensor measurements, and partial observability. To account for these limitations, it is common to maintain a belief distribution over the true state. This belief could be a non-parametric, sample-based representation to capture uncertainty more flexibly. In this paper, we propose a novel form of Belief Control Barrier Functions (BCBFs) specifically designed to ensure safety in dynamic environments under stochastic dynamics and a sample-based belief about the environment state. Our approach incorporates provable concentration bounds on tail risk measures into BCBFs, effectively addressing possible multimodal and skewed belief distributions represented by samples. Moreover, the proposed method demonstrates robustness against distributional shifts up to a predefined bound. We validate the effectiveness and real-time performance (approximately 1kHz) of the proposed method through two simulated underwater robotic applications: object tracking and dynamic collision avoidance.
<div id='section'>PaperID: <span id='pid'>350, <a href='https://arxiv.org/pdf/2504.03047.pdf' target='_blank'>https://arxiv.org/pdf/2504.03047.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Reef Alturki, Adrian Hilton, Jean-Yves Guillemaut
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03047">Attention-Aware Multi-View Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In spite of the recent advancements in multi-object tracking, occlusion poses a significant challenge. Multi-camera setups have been used to address this challenge by providing a comprehensive coverage of the scene. Recent multi-view pedestrian detection models have highlighted the potential of an early-fusion strategy, projecting feature maps of all views to a common ground plane or the Bird's Eye View (BEV), and then performing detection. This strategy has been shown to improve both detection and tracking performance. However, the perspective transformation results in significant distortion on the ground plane, affecting the robustness of the appearance features of the pedestrians. To tackle this limitation, we propose a novel model that incorporates attention mechanisms in a multi-view pedestrian tracking scenario. Our model utilizes an early-fusion strategy for detection, and a cross-attention mechanism to establish robust associations between pedestrians in different frames, while efficiently propagating pedestrian features across frames, resulting in a more robust feature representation for each pedestrian. Extensive experiments demonstrate that our model outperforms state-of-the-art models, with an IDF1 score of $96.1\%$ on Wildtrack dataset, and $85.7\%$ on MultiviewX dataset.
<div id='section'>PaperID: <span id='pid'>351, <a href='https://arxiv.org/pdf/2502.16569.pdf' target='_blank'>https://arxiv.org/pdf/2502.16569.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ali Safa, Waqas Aman, Ali Al-Zawqari, Saif Al-Kuwari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16569">Benchmarking Online Object Trackers for Underwater Robot Position Locking Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomously controlling the position of Remotely Operated underwater Vehicles (ROVs) is of crucial importance for a wide range of underwater engineering applications, such as in the inspection and maintenance of underwater industrial structures. Consequently, studying vision-based underwater robot navigation and control has recently gained increasing attention to counter the numerous challenges faced in underwater conditions, such as lighting variability, turbidity, camera image distortions (due to bubbles), and ROV positional disturbances (due to underwater currents). In this paper, we propose (to the best of our knowledge) a first rigorous unified benchmarking of more than seven Machine Learning (ML)-based one-shot object tracking algorithms for vision-based position locking of ROV platforms. We propose a position-locking system that processes images of an object of interest in front of which the ROV must be kept stable. Then, our proposed system uses the output result of different object tracking algorithms to automatically correct the position of the ROV against external disturbances. We conducted numerous real-world experiments using a BlueROV2 platform within an indoor pool and provided clear demonstrations of the strengths and weaknesses of each tracking approach. Finally, to help alleviate the scarcity of underwater ROV data, we release our acquired data base as open-source with the hope of benefiting future research.
<div id='section'>PaperID: <span id='pid'>352, <a href='https://arxiv.org/pdf/2502.01207.pdf' target='_blank'>https://arxiv.org/pdf/2502.01207.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hannes Homburger, Stefan Wirtensohn, Patrick Hoher, Tim Baur, Dennis Griesser, Moritz Diehl, Johannes Reuter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01207">Solgenia -- A Test Vessel Toward Energy-Efficient Autonomous Water Taxi Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous surface vessels are a promising building block of the future's transport sector and are investigated by research groups worldwide. This paper presents a comprehensive and systematic overview of the autonomous research vessel Solgenia including the latest investigations and recently presented methods that contributed to the fields of autonomous systems, applied numerical optimization, nonlinear model predictive control, multi-extended-object-tracking, computer vision, and collision avoidance. These are considered to be the main components of autonomous water taxi applications. Autonomous water taxis have the potential to transform the traffic in cities close to the water into a more efficient, sustainable, and flexible future state. Regarding this transformation, the test platform Solgenia offers an opportunity to gain new insights by investigating novel methods in real-world experiments. An established test platform will strongly reduce the effort required for real-world experiments in the future.
<div id='section'>PaperID: <span id='pid'>353, <a href='https://arxiv.org/pdf/2501.07133.pdf' target='_blank'>https://arxiv.org/pdf/2501.07133.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiantong Zhao, Xiuping Liu, Shengjing Tian, Yinan Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07133">Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (3DSOT) in LiDAR point clouds is a critical task for outdoor perception, enabling real-time perception of object location, orientation, and motion. Despite the impressive performance of current 3DSOT methods, evaluating them on clean datasets inadequately reflects their comprehensive performance, as the adverse weather conditions in real-world surroundings has not been considered. One of the main obstacles is the lack of adverse weather benchmarks for the evaluation of 3DSOT. To this end, this work proposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather, which comprises two synthetic datasets (KITTI-A and nuScenes-A) and one real-world dataset (CADC-SOT) spanning three weather types: rain, fog, and snow. Based on this benchmark, five representative 3D trackers from different tracking frameworks conducted robustness evaluation, resulting in significant performance degradations. This prompts the question: What are the factors that cause current advanced methods to fail on such adverse weather samples? Consequently, we explore the impacts of adverse weather and answer the above question from three perspectives: 1) target distance; 2) template shape corruption; and 3) target shape corruption. Finally, based on domain randomization and contrastive learning, we designed a dual-branch tracking framework for adverse weather, named DRCT, achieving excellent performance in benchmarks.
<div id='section'>PaperID: <span id='pid'>354, <a href='https://arxiv.org/pdf/2501.04336.pdf' target='_blank'>https://arxiv.org/pdf/2501.04336.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zeyi Huang, Yuyang Ji, Xiaofang Wang, Nikhil Mehta, Tong Xiao, Donghyun Lee, Sigmund Vanvalkenburgh, Shengxin Zha, Bolin Lai, Licheng Yu, Ning Zhang, Yong Jae Lee, Miao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04336">Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-form video understanding with Large Vision Language Models is challenged by the need to analyze temporally dispersed yet spatially concentrated key moments within limited context windows. In this work, we introduce VideoMindPalace, a new framework inspired by the "Mind Palace", which organizes critical video moments into a topologically structured semantic graph. VideoMindPalace organizes key information through (i) hand-object tracking and interaction, (ii) clustered activity zones representing specific areas of recurring activities, and (iii) environment layout mapping, allowing natural language parsing by LLMs to provide grounded insights on spatio-temporal and 3D context. In addition, we propose the Video MindPalace Benchmark (VMB), to assess human-like reasoning, including spatial localization, temporal reasoning, and layout-aware sequential understanding. Evaluated on VMB and established video QA datasets, including EgoSchema, NExT-QA, IntentQA, and the Active Memories Benchmark, VideoMindPalace demonstrates notable gains in spatio-temporal coherence and human-aligned reasoning, advancing long-form video analysis capabilities in VLMs.
<div id='section'>PaperID: <span id='pid'>355, <a href='https://arxiv.org/pdf/2411.02220.pdf' target='_blank'>https://arxiv.org/pdf/2411.02220.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ryoma Yataka, Pu Perry Wang, Petros Boufounos, Ryuhei Takahashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02220">SIRA: Scalable Inter-frame Relation and Association for Radar Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional radar feature extraction faces limitations due to low spatial resolution, noise, multipath reflection, the presence of ghost targets, and motion blur. Such limitations can be exacerbated by nonlinear object motion, particularly from an ego-centric viewpoint. It becomes evident that to address these challenges, the key lies in exploiting temporal feature relation over an extended horizon and enforcing spatial motion consistency for effective association. To this end, this paper proposes SIRA (Scalable Inter-frame Relation and Association) with two designs. First, inspired by Swin Transformer, we introduce extended temporal relation, generalizing the existing temporal relation layer from two consecutive frames to multiple inter-frames with temporally regrouped window attention for scalability. Second, we propose motion consistency track with the concept of a pseudo-tracklet generated from observational data for better trajectory prediction and subsequent object association. Our approach achieves 58.11 mAP@0.5 for oriented object detection and 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing previous state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA, respectively.
<div id='section'>PaperID: <span id='pid'>356, <a href='https://arxiv.org/pdf/2410.20893.pdf' target='_blank'>https://arxiv.org/pdf/2410.20893.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shengjing Tian, Yinan Han, Xiantong Zhao, Bin Liu, Xiuping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20893">Evaluating the Robustness of LiDAR Point Cloud Tracking Against Adversarial Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we delve into the robustness of neural network-based LiDAR point cloud tracking models under adversarial attacks, a critical aspect often overlooked in favor of performance enhancement. These models, despite incorporating advanced architectures like Transformer or Bird's Eye View (BEV), tend to neglect robustness in the face of challenges such as adversarial attacks, domain shifts, or data corruption. We instead focus on the robustness of the tracking models under the threat of adversarial attacks. We begin by establishing a unified framework for conducting adversarial attacks within the context of 3D object tracking, which allows us to thoroughly investigate both white-box and black-box attack strategies. For white-box attacks, we tailor specific loss functions to accommodate various tracking paradigms and extend existing methods such as FGSM, C\&W, and PGD to the point cloud domain. In addressing black-box attack scenarios, we introduce a novel transfer-based approach, the Target-aware Perturbation Generation (TAPG) algorithm, with the dual objectives of achieving high attack performance and maintaining low perceptibility. This method employs a heuristic strategy to enforce sparse attack constraints and utilizes random sub-vector factorization to bolster transferability. Our experimental findings reveal a significant vulnerability in advanced tracking methods when subjected to both black-box and white-box attacks, underscoring the necessity for incorporating robustness against adversarial attacks into the design of LiDAR point cloud tracking models. Notably, compared to existing methods, the TAPG also strikes an optimal balance between the effectiveness of the attack and the concealment of the perturbations.
<div id='section'>PaperID: <span id='pid'>357, <a href='https://arxiv.org/pdf/2410.14093.pdf' target='_blank'>https://arxiv.org/pdf/2410.14093.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kosuke Tatsumura, Yohei Hamakawa, Masaya Yamasaki, Koji Oya, Hiroshi Fujimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14093">Enhancing In-vehicle Multiple Object Tracking Systems with Embeddable Ising Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A cognitive function of tracking multiple objects, needed in autonomous mobile vehicles, comprises object detection and their temporal association. While great progress owing to machine learning has been recently seen for elaborating the similarity matrix between the objects that have been recognized and the objects detected in a current video frame, less for the assignment problem that finally determines the temporal association, which is a combinatorial optimization problem. Here we show an in-vehicle multiple object tracking system with a flexible assignment function for tracking through multiple long-term occlusion events. To solve the flexible assignment problem formulated as a nondeterministic polynomial time-hard problem, the system relies on an embeddable Ising machine based on a quantum-inspired algorithm called simulated bifurcation. Using a vehicle-mountable computing platform, we demonstrate a realtime system-wide throughput (23 frames per second on average) with the enhanced functionality.
<div id='section'>PaperID: <span id='pid'>358, <a href='https://arxiv.org/pdf/2601.09240.pdf' target='_blank'>https://arxiv.org/pdf/2601.09240.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jiajun Chen, Jing Xiao, Shaohan Cao, Yuming Zhu, Liang Liao, Jun Pan, Mi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09240">DeTracker: Motion-decoupled Vehicle Detection and Tracking in Unstabilized Satellite Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Satellite videos provide continuous observations of surface dynamics but pose significant challenges for multi-object tracking (MOT), especially under unstabilized conditions where platform jitter and the weak appearance of tiny objects jointly degrade tracking performance. To address this problem, we propose DeTracker, a joint detection-and-tracking framework tailored for unstabilized satellite videos. DeTracker introduces a Global--Local Motion Decoupling (GLMD) module that explicitly separates satellite platform motion from true object motion through global alignment and local refinement, leading to improved trajectory stability and motion estimation accuracy. In addition, a Temporal Dependency Feature Pyramid (TDFP) module is developed to perform cross-frame temporal feature fusion, enhancing the continuity and discriminability of tiny-object representations. We further construct a new benchmark dataset, SDM-Car-SU, which simulates multi-directional and multi-speed platform motions to enable systematic evaluation of tracking robustness under varying motion perturbations. Extensive experiments on both simulated and real unstabilized satellite videos demonstrate that DeTracker significantly outperforms existing methods, achieving 61.1% MOTA on SDM-Car-SU and 47.3% MOTA on real satellite video data.
<div id='section'>PaperID: <span id='pid'>359, <a href='https://arxiv.org/pdf/2601.06550.pdf' target='_blank'>https://arxiv.org/pdf/2601.06550.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Yuhua Zhu, Wenhui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.06550">LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.
<div id='section'>PaperID: <span id='pid'>360, <a href='https://arxiv.org/pdf/2512.24838.pdf' target='_blank'>https://arxiv.org/pdf/2512.24838.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Md Ahmed Al Muzaddid, Jordan A. James, William J. Beksi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24838">CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple-object tracking (MOT) in agricultural environments presents major challenges due to repetitive patterns, similar object appearances, sudden illumination changes, and frequent occlusions. Contemporary trackers in this domain rely on the motion of objects rather than appearance for association. Nevertheless, they struggle to maintain object identities when targets undergo frequent and strong occlusions. The high similarity of object appearances makes integrating appearance-based association nontrivial for agricultural scenarios. To solve this problem we propose CropTrack, a novel MOT framework based on the combination of appearance and motion information. CropTrack integrates a reranking-enhanced appearance association, a one-to-many association with appearance-based conflict resolution strategy, and an exponential moving average prototype feature bank to improve appearance-based association. Evaluated on publicly available agricultural MOT datasets, CropTrack demonstrates consistent identity preservation, outperforming traditional motion-based tracking methods. Compared to the state of the art, CropTrack achieves significant gains in identification F1 and association accuracy scores with a lower number of identity switches.
<div id='section'>PaperID: <span id='pid'>361, <a href='https://arxiv.org/pdf/2512.09633.pdf' target='_blank'>https://arxiv.org/pdf/2512.09633.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09633">Benchmarking SAM2-based Trackers on FMOX</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.
<div id='section'>PaperID: <span id='pid'>362, <a href='https://arxiv.org/pdf/2512.09235.pdf' target='_blank'>https://arxiv.org/pdf/2512.09235.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Md Eimran Hossain Eimon, Hyomin Choi, Fabien Racapé, Mateen Ulhaq, Velibor Adzic, Hari Kalva, Borko Furht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09235">Efficient Feature Compression for Machines with Global Statistics Preservation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.
<div id='section'>PaperID: <span id='pid'>363, <a href='https://arxiv.org/pdf/2511.21139.pdf' target='_blank'>https://arxiv.org/pdf/2511.21139.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Baoli Sun, Xinzhu Ma, Ning Wang, Zhihui Wang, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21139">Referring Video Object Segmentation with Cross-Modality Proxy Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring video object segmentation (RVOS) is an emerging cross-modality task that aims to generate pixel-level maps of the target objects referred by given textual expressions. The main concept involves learning an accurate alignment of visual elements and language expressions within a semantic space. Recent approaches address cross-modality alignment through conditional queries, tracking the target object using a query-response based mechanism built upon transformer structure. However, they exhibit two limitations: (1) these conditional queries lack inter-frame dependency and variation modeling, making accurate target tracking challenging amid significant frame-to-frame variations; and (2) they integrate textual constraints belatedly, which may cause the video features potentially focus on the non-referred objects. Therefore, we propose a novel RVOS architecture called ProxyFormer, which introduces a set of proxy queries to integrate visual and text semantics and facilitate the flow of semantics between them. By progressively updating and propagating proxy queries across multiple stages of video feature encoder, ProxyFormer ensures that the video features are focused on the object of interest. This dynamic evolution also enables the establishment of inter-frame dependencies, enhancing the accuracy and coherence of object tracking. To mitigate high computational costs, we decouple cross-modality interactions into temporal and spatial dimensions. Additionally, we design a Joint Semantic Consistency (JSC) training strategy to align semantic consensus between the proxy queries and the combined video-text pairs. Comprehensive experiments on four widely used RVOS benchmarks demonstrate the superiority of our ProxyFormer to the state-of-the-art methods.
<div id='section'>PaperID: <span id='pid'>364, <a href='https://arxiv.org/pdf/2511.17681.pdf' target='_blank'>https://arxiv.org/pdf/2511.17681.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Weiyi Lv, Ning Zhang, Hanyang Sun, Haoran Jiang, Kai Zhao, Jing Xiao, Dan Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17681">Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.
<div id='section'>PaperID: <span id='pid'>365, <a href='https://arxiv.org/pdf/2510.09092.pdf' target='_blank'>https://arxiv.org/pdf/2510.09092.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Juanqin Liu, Leonardo Plotegher, Eloy Roura, Shaoming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09092">GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.
<div id='section'>PaperID: <span id='pid'>366, <a href='https://arxiv.org/pdf/2509.19096.pdf' target='_blank'>https://arxiv.org/pdf/2509.19096.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ilhan Skender, Kailin Tong, Selim Solmaz, Daniel Watzenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19096">Investigating Traffic Accident Detection Using Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic safety remains a critical global concern, with timely and accurate accident detection essential for hazard reduction and rapid emergency response. Infrastructure-based vision sensors offer scalable and efficient solutions for continuous real-time monitoring, facilitating automated detection of accidents directly from captured images. This research investigates the zero-shot capabilities of multimodal large language models (MLLMs) for detecting and describing traffic accidents using images from infrastructure cameras, thus minimizing reliance on extensive labeled datasets. Main contributions include: (1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA, explicitly addressing the scarcity of diverse, realistic, infrastructure-based accident data through controlled simulations; (2) Comparative performance analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in accident identification and descriptive capabilities without prior fine-tuning; and (3) Integration of advanced visual analytics, specifically YOLO for object detection, Deep SORT for multi-object tracking, and Segment Anything (SAM) for instance segmentation, into enhanced prompts to improve model accuracy and explainability. Key numerical results show Pixtral as the top performer with an F1-score of 0.71 and 83% recall, while Gemini models gained precision with enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and recall losses. Gemma 3 offered the most balanced performance with minimal metric fluctuation. These findings demonstrate the substantial potential of integrating MLLMs with advanced visual analytics techniques, enhancing their applicability in real-world automated traffic monitoring systems.
<div id='section'>PaperID: <span id='pid'>367, <a href='https://arxiv.org/pdf/2509.18451.pdf' target='_blank'>https://arxiv.org/pdf/2509.18451.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Prithvi Raj Singh, Raju Gottumukkala, Anthony Maida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18451">An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unpredictable movement patterns and small visual mark make precise tracking of fast-moving tiny objects like a racquetball one of the challenging problems in computer vision. This challenge is particularly relevant for sport robotics applications, where lightweight and accurate tracking systems can improve robot perception and planning capabilities. While Kalman filter-based tracking methods have shown success in general object tracking scenarios, their performance degrades substantially when dealing with rapidly moving objects that exhibit irregular bouncing behavior. In this study, we evaluate the performance of five state-of-the-art Kalman filter-based tracking methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom dataset containing 10,000 annotated racquetball frames captured at 720p-1280p resolution. We focus our analysis on two critical performance factors: inference speed and update frequency per image, examining how these parameters affect tracking accuracy and reliability for fast-moving tiny objects. Our experimental evaluation across four distinct scenarios reveals that DeepOCSORT achieves the lowest tracking error with an average ADE of 31.15 pixels compared to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms. However, our results show that all Kalman filter-based trackers exhibit significant tracking drift with spatial errors ranging from 3-11cm (ADE values: 31-114 pixels), indicating fundamental limitations in handling the unpredictable motion patterns of fast-moving tiny objects like racquetballs. Our analysis demonstrates that current tracking approaches require substantial improvements, with error rates 3-4x higher than standard object tracking benchmarks, highlighting the need for specialized methodologies for fast-moving tiny object tracking applications.
<div id='section'>PaperID: <span id='pid'>368, <a href='https://arxiv.org/pdf/2509.18272.pdf' target='_blank'>https://arxiv.org/pdf/2509.18272.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tornike Karchkhadze, Kuan-Lin Chen, Mojtaba Heydari, Robert Henzel, Alessandro Toso, Mehrez Souden, Joshua Atkins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18272">StereoFoley: Object-Aware Stereo Audio Generation from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present StereoFoley, a video-to-audio generation framework that produces semantically aligned, temporally synchronized, and spatially accurate stereo sound at 48 kHz. While recent generative video-to-audio models achieve strong semantic and temporal fidelity, they largely remain limited to mono or fail to deliver object-aware stereo imaging, constrained by the lack of professionally mixed, spatially accurate video-to-audio datasets. First, we develop and train a base model that generates stereo audio from video, achieving state-of-the-art in both semantic accuracy and synchronization. Next, to overcome dataset limitations, we introduce a synthetic data generation pipeline that combines video analysis, object tracking, and audio synthesis with dynamic panning and distance-based loudness controls, enabling spatially accurate object-aware sound. Finally, we fine-tune the base model on this synthetic dataset, yielding clear object-audio correspondence. Since no established metrics exist, we introduce stereo object-awareness measures and validate it through a human listening study, showing strong correlation with perception. This work establishes the first end-to-end framework for stereo object-aware video-to-audio generation, addressing a critical gap and setting a new benchmark in the field.
<div id='section'>PaperID: <span id='pid'>369, <a href='https://arxiv.org/pdf/2509.11453.pdf' target='_blank'>https://arxiv.org/pdf/2509.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11453">Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics and autonomous systems. Existing methods typically follow frame-wise motion estimation or a sequence-based paradigm. However, the two-frame methods are efficient but lack long-term temporal context, making them vulnerable in sparse or occluded scenes, while sequence-based methods that process multiple point clouds gain robustness at a significant computational cost. To resolve this dilemma, we propose a novel trajectory-based paradigm and its instantiation, TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame tracker by implicitly learning motion continuity from historical bounding box trajectories alone-without requiring additional, costly point cloud inputs. It first generates a fast, explicit motion proposal and then uses an implicit motion modeling module to predict the future trajectory, which in turn refines and corrects the initial proposal. Extensive experiments on the large-scale NuScenes benchmark show that TrajTrack achieves new state-of-the-art performance, dramatically improving tracking precision by 4.48% over a strong baseline while running at 56 FPS. Besides, we also demonstrate the strong generalizability of TrajTrack across different base trackers. Video is available at https://www.bilibili.com/video/BV1ahYgzmEWP.
<div id='section'>PaperID: <span id='pid'>370, <a href='https://arxiv.org/pdf/2509.09349.pdf' target='_blank'>https://arxiv.org/pdf/2509.09349.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ian Nell, Shane Gilroy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09349">Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behavior classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviors such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioral analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.
<div id='section'>PaperID: <span id='pid'>371, <a href='https://arxiv.org/pdf/2509.08421.pdf' target='_blank'>https://arxiv.org/pdf/2509.08421.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Keisuke Toida, Taigo Sakai, Naoki Kato, Kazutoyo Yokota, Takeshi Nakamura, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08421">Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-View Multi-Object Tracking (MVMOT) is essential for applications such as surveillance, autonomous driving, and sports analytics. However, maintaining consistent object identities across multiple cameras remains challenging due to viewpoint changes, lighting variations, and occlusions, which often lead to tracking errors.Recent methods project features from multiple cameras into a unified Bird's-Eye-View (BEV) space to improve robustness against occlusion. However, this projection introduces feature distortion and non-uniform density caused by variations in object scale with distance. These issues degrade the quality of the fused representation and reduce detection and tracking accuracy.To address these problems, we propose SCFusion, a framework that combines three techniques to improve multi-view feature integration. First, it applies a sparse transformation to avoid unnatural interpolation during projection. Next, it performs density-aware weighting to adaptively fuse features based on spatial confidence and camera distance. Finally, it introduces a multi-view consistency loss that encourages each camera to learn discriminative features independently before fusion.Experiments show that SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline method TrackTacular. These results demonstrate that SCFusion effectively mitigates the limitations of conventional BEV projection and provides a robust and accurate solution for multi-view object detection and tracking.
<div id='section'>PaperID: <span id='pid'>372, <a href='https://arxiv.org/pdf/2509.06536.pdf' target='_blank'>https://arxiv.org/pdf/2509.06536.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06536">Benchmarking EfficientTAM on FMO datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast and tiny object tracking remains a challenge in computer vision and in this paper we first introduce a JSON metadata file associated with four open source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we extend the description of the FMOs datasets with additional ground truth information in JSON format (called FMOX) with object size information. Finally we use our FMOX file to test a recently proposed foundational model for tracking (called EfficientTAM) showing that its performance compares well with the pipelines originally taylored for these FMO datasets. Our comparison of these state-of-the-art techniques on FMOX is provided with Trajectory Intersection of Union (TIoU) scores. The code and JSON is shared open source allowing FMOX to be accessible and usable for other machine learning pipelines aiming to process FMO datasets.
<div id='section'>PaperID: <span id='pid'>373, <a href='https://arxiv.org/pdf/2509.03499.pdf' target='_blank'>https://arxiv.org/pdf/2509.03499.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kevin Barnard, Elaine Liu, Kristine Walz, Brian Schlining, Nancy Jacobsen Stout, Lonny Lundsten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03499">DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarking multi-object tracking and object detection model performance is an essential step in machine learning model development, as it allows researchers to evaluate model detection and tracker performance on human-generated 'test' data, facilitating consistent comparisons between models and trackers and aiding performance optimization. In this study, a novel benchmark video dataset was developed and used to assess the performance of several Monterey Bay Aquarium Research Institute object detection models and a FathomNet single-class object detection model together with several trackers. The dataset consists of four video sequences representing midwater and benthic deep-sea habitats. Performance was evaluated using Higher Order Tracking Accuracy, a metric that balances detection, localization, and association accuracy. To the best of our knowledge, this is the first publicly available benchmark for multi-object tracking in deep-sea video footage. We provide the benchmark data, a clearly documented workflow for generating additional benchmark videos, as well as example Python notebooks for computing metrics.
<div id='section'>PaperID: <span id='pid'>374, <a href='https://arxiv.org/pdf/2507.12832.pdf' target='_blank'>https://arxiv.org/pdf/2507.12832.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuki Kondo, Norimichi Ukita, Riku Kanayama, Yuki Yoshida, Takayuki Yamaguchi, Xiang Yu, Guang Liang, Xinyao Liu, Guan-Zhang Wang, Wei-Ta Chu, Bing-Cheng Chuang, Jia-Hua Lee, Pin-Tseng Kuo, I-Hsuan Chu, Yi-Shein Hsiao, Cheng-Han Wu, Po-Yi Wu, Jui-Chien Tsou, Hsuan-Chi Liu, Chun-Yi Lee, Yuan-Fu Yang, Kosuke Shigematsu, Asuka Shin, Ba Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12832">MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.
<div id='section'>PaperID: <span id='pid'>375, <a href='https://arxiv.org/pdf/2501.13710.pdf' target='_blank'>https://arxiv.org/pdf/2501.13710.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>IÃ±aki Erregue, Kamal Nasrollahi, Sergio Escalera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13710">YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised Re-ID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT) solution that combines real-time object detection with self-supervised Re-Identification (Re-ID). By incorporating a dedicated Re-ID branch into YOLO11s, our model performs Joint Detection and Embedding (JDE), generating appearance features for each detection. The Re-ID branch is trained in a fully self-supervised setting while simultaneously training for detection, eliminating the need for costly identity-labeled datasets. The triplet loss, with hard positive and semi-hard negative mining strategies, is used for learning discriminative embeddings. Data association is enhanced with a custom tracking implementation that successfully integrates motion, appearance, and location cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20 benchmarks, surpassing existing JDE methods in terms of FPS and using up to ten times fewer parameters. Thus, making our method a highly attractive solution for real-world applications.
<div id='section'>PaperID: <span id='pid'>376, <a href='https://arxiv.org/pdf/2411.19167.pdf' target='_blank'>https://arxiv.org/pdf/2411.19167.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Shangchen Han, Fan Zhang, Linguang Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19167">HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.
<div id='section'>PaperID: <span id='pid'>377, <a href='https://arxiv.org/pdf/2411.15811.pdf' target='_blank'>https://arxiv.org/pdf/2411.15811.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Wenhui Zhao, Dingwen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15811">FastTrackTr:Towards Fast Multi-Object Tracking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based multi-object tracking (MOT) methods have captured the attention of many researchers in recent years. However, these models often suffer from slow inference speeds due to their structure or other issues. To address this problem, we revisited the Joint Detection and Tracking (JDT) method by looking back at past approaches. By integrating the original JDT approach with some advanced theories, this paper employs an efficient method of information transfer between frames on the DETR, constructing a fast and novel JDT-type MOT framework: FastTrackTr. Thanks to the superiority of this information transfer method, our approach not only reduces the number of queries required during tracking but also avoids the excessive introduction of network structures, ensuring model simplicity. Experimental results indicate that our method has the potential to achieve real-time tracking and exhibits competitive tracking accuracy across multiple datasets.
<div id='section'>PaperID: <span id='pid'>378, <a href='https://arxiv.org/pdf/2411.11514.pdf' target='_blank'>https://arxiv.org/pdf/2411.11514.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuai Li, Michael Burke, Subramanian Ramamoorthy, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11514">Learning a Neural Association Network for Self-supervised Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework to learn data association for multi-object tracking in a self-supervised manner. Fully-supervised learning methods are known to achieve excellent tracking performances, but acquiring identity-level annotations is tedious and time-consuming. Motivated by the fact that in real-world scenarios object motion can be usually represented by a Markov process, we present a novel expectation maximization (EM) algorithm that trains a neural network to associate detections for tracking, without requiring prior knowledge of their temporal correspondences. At the core of our method lies a neural Kalman filter, with an observation model conditioned on associations of detections parameterized by a neural network. Given a batch of frames as input, data associations between detections from adjacent frames are predicted by a neural network followed by a Sinkhorn normalization that determines the assignment probabilities of detections to states. Kalman smoothing is then used to obtain the marginal probability of observations given the inferred states, producing a training objective to maximize this marginal probability using gradient descent. The proposed framework is fully differentiable, allowing the underlying neural model to be trained end-to-end. We evaluate our approach on the challenging MOT17, MOT20, and BDD100K datasets and achieve state-of-the-art results in comparison to self-supervised trackers using public detections.
<div id='section'>PaperID: <span id='pid'>379, <a href='https://arxiv.org/pdf/2411.00608.pdf' target='_blank'>https://arxiv.org/pdf/2411.00608.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiang Li, Cheng Chen, Yuan-yao Lou, Mustafa Abdallah, Kwang Taik Kim, Saurabh Bagchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00608">HopTrack: A Real-time Multi-Object Tracking System for Embedded Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) poses significant challenges in computer vision. Despite its wide application in robotics, autonomous driving, and smart manufacturing, there is limited literature addressing the specific challenges of running MOT on embedded devices. State-of-the-art MOT trackers designed for high-end GPUs often experience low processing rates (<11fps) when deployed on embedded devices. Existing MOT frameworks for embedded devices proposed strategies such as fusing the detector model with the feature embedding model to reduce inference latency or combining different trackers to improve tracking accuracy, but tend to compromise one for the other. This paper introduces HopTrack, a real-time multi-object tracking system tailored for embedded devices. Our system employs a novel discretized static and dynamic matching approach along with an innovative content-aware dynamic sampling technique to enhance tracking accuracy while meeting the real-time requirement. Compared with the best high-end GPU modified baseline Byte (Embed) and the best existing baseline on embedded devices MobileNet-JDE, HopTrack achieves a processing speed of up to 39.29 fps on NVIDIA AGX Xavier with a multi-object tracking accuracy (MOTA) of up to 63.12% on the MOT16 benchmark, outperforming both counterparts by 2.15% and 4.82%, respectively. Additionally, the accuracy improvement is coupled with the reduction in energy consumption (20.8%), power (5%), and memory usage (8%), which are crucial resources on embedded devices. HopTrack is also detector agnostic allowing the flexibility of plug-and-play.
<div id='section'>PaperID: <span id='pid'>380, <a href='https://arxiv.org/pdf/2410.24183.pdf' target='_blank'>https://arxiv.org/pdf/2410.24183.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matteo Tesori, Giorgio Battistelli, Luigi Chisci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24183">Extended Object Tracking and Classification based on Linear Splines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a framework based on linear splines for 2-dimensional extended object tracking and classification. Unlike state of the art models, linear splines allow to represent extended objects whose contour is an arbitrarily complex curve. An exact likelihood is derived for the case in which noisy measurements can be scattered from any point on the contour of the extended object, while an approximate Monte Carlo likelihood is provided for the case wherein scattering points can be anywhere, i.e. inside or on the contour, on the object surface. Exploiting such likelihood to measure how well the observed data fit a given shape, a suitable estimator is developed. The proposed estimator models the extended object in terms of a kinematic state, providing object position and orientation, along with a shape vector, characterizing object contour and surface. The kinematic state is estimated via a nonlinear Kalman filter, while the shape vector is estimated via a Bayesian classifier so that classification is implicitly solved during shape estimation. Numerical experiments are provided to assess, compared to state of the art extended object estimators, the effectiveness of the proposed one.
<div id='section'>PaperID: <span id='pid'>381, <a href='https://arxiv.org/pdf/2410.16329.pdf' target='_blank'>https://arxiv.org/pdf/2410.16329.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhiqiang Zhong, Yang Yang, Fengqiang Wan, Henglu Wei, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16329">The Solution for Single Object Tracking Task of Perception Test Challenge 2024</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents our method for Single Object Tracking (SOT), which aims to track a specified object throughout a video sequence. We employ the LoRAT method. The essence of the work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. We train our model using the extensive LaSOT and GOT-10k datasets, which provide a solid foundation for robust performance. Additionally, we implement the alpha-refine technique for post-processing the bounding box outputs. Although the alpha-refine method does not yield the anticipated results, our overall approach achieves a score of 0.813, securing first place in the competition.
<div id='section'>PaperID: <span id='pid'>382, <a href='https://arxiv.org/pdf/2410.15518.pdf' target='_blank'>https://arxiv.org/pdf/2410.15518.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Thinh Phan, Isaac Phillips, Andrew Lockett, Michael T. Kidd, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15518">TrackMe:A Simple and Effective Multiple Object Tracking Annotation Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking, especially animal tracking, is one of the key topics that attract a lot of attention due to its benefits of animal behavior understanding and monitoring. Recent state-of-the-art tracking methods are founded on deep learning architectures for object detection, appearance feature extraction and track association. Despite the good tracking performance, these methods are trained and evaluated on common objects such as human and cars. To perform on the animal, there is a need to create large datasets of different types in multiple conditions. The dataset construction comprises of data collection and data annotation. In this work, we put more focus on the latter task. Particularly, we renovate the well-known tool, LabelMe, so as to assist common user with or without in-depth knowledge about computer science to annotate the data with less effort. The new tool named as TrackMe inherits the simplicity, high compatibility with varied systems, minimal hardware requirement and convenient feature utilization from the predecessor. TrackMe is an upgraded version with essential features for multiple object tracking annotation.
<div id='section'>PaperID: <span id='pid'>383, <a href='https://arxiv.org/pdf/2410.10527.pdf' target='_blank'>https://arxiv.org/pdf/2410.10527.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hanqing Guo, Canlun Zheng, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10527">Motion-guided small MAV detection in complex and non-planar scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a growing interest in the visual detection of micro aerial vehicles (MAVs) due to its importance in numerous applications. However, the existing methods based on either appearance or motion features encounter difficulties when the background is complex or the MAV is too small. In this paper, we propose a novel motion-guided MAV detector that can accurately identify small MAVs in complex and non-planar scenes. This detector first exploits a motion feature enhancement module to capture the motion features of small MAVs. Then it uses multi-object tracking and trajectory filtering to eliminate false positives caused by motion parallax. Finally, an appearance-based classifier and an appearance-based detector that operates on the cropped regions are used to achieve precise detection results. Our proposed method can effectively and efficiently detect extremely small MAVs from dynamic and complex backgrounds because it aggregates pixel-level motion features and eliminates false positives based on the motion and appearance features of MAVs. Experiments on the ARD-MAV dataset demonstrate that the proposed method could achieve high performance in small MAV detection under challenging conditions and outperform other state-of-the-art methods across various metrics
<div id='section'>PaperID: <span id='pid'>384, <a href='https://arxiv.org/pdf/2602.13772.pdf' target='_blank'>https://arxiv.org/pdf/2602.13772.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoyu Li, Yitao Wu, Xian Wu, Haolin Zhuo, Lijun Zhao, Lining Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.13772">Offline-Poly: A Polyhedral Framework For Offline 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Offline 3D multi-object tracking (MOT) is a critical component of the 4D auto-labeling (4DAL) process. It enhances pseudo-labels generated by high-performance detectors through the incorporation of temporal context. However, existing offline 3D MOT approaches are direct extensions of online frameworks and fail to fully exploit the advantages of offline setting. Moreover, these methods often depend on fixed upstream and customized architectures, limiting their adaptability. To address these limitations, we propose Offline-Poly, a general offline 3D MOT method based on a tracking-centric design. We introduce a standardized paradigm termed Tracking-by-Tracking (TBT), which operates exclusively on arbitrary off-the-shelf tracking outputs and produces offline-refined tracklets. This formulation decouples offline tracker from specific upstream detectors or trackers. Under the TBT paradigm, Offline-Poly accepts one or multiple coarse tracking results and processes them through a structured pipeline comprising pre-processing, hierarchical matching and fusion, and tracklet refinement. Each module is designed to capitalize on the two fundamental properties of offline tracking: resource unconstrainedness, which permits global optimization beyond real-time limits, and future observability, which enables tracklet reasoning over the full temporal horizon. Offline-Poly first eliminates short-term ghost tracklets and re-identifies fragmented segments using global scene context. It then constructs scene-level similarity to associate tracklets across multiple input sources. Finally, Offline-Poly refines tracklets by jointly leveraging local and global motion patterns. On nuScenes, we achieve SOTA performance with 77.6% AMOTA. On KITTI, it achieves leading results with 83.00% HOTA. Comprehensive experiments further validate the flexibility, generalizability, and modular effectiveness of Offline-Poly.
<div id='section'>PaperID: <span id='pid'>385, <a href='https://arxiv.org/pdf/2512.14426.pdf' target='_blank'>https://arxiv.org/pdf/2512.14426.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Simon Steuernagel, Marcus Baum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14426">Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended object tracking involves estimating both the physical extent and kinematic parameters of a target object, where typically multiple measurements are observed per time step. In this article, we propose a deterministic closed-form elliptical extended object tracker, based on decoupling of the kinematics, orientation, and axis lengths. By disregarding potential correlations between these state components, fewer approximations are required for the individual estimators than for an overall joint solution. The resulting algorithm outperforms existing algorithms, reaching the accuracy of sampling-based procedures. Additionally, a batch-based variant is introduced, yielding highly efficient computation while outperforming all comparable state-of-the-art algorithms. This is validated both by a simulation study using common models from literature, as well as an extensive quantitative evaluation on real automotive radar data.
<div id='section'>PaperID: <span id='pid'>386, <a href='https://arxiv.org/pdf/2511.18694.pdf' target='_blank'>https://arxiv.org/pdf/2511.18694.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuo Wen, Edwin Meriaux, Mariana Sosa Guzmán, Zhizun Wang, Junming Shi, Gregory Dudek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18694">Stable Multi-Drone GNSS Tracking System for Marine Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.
<div id='section'>PaperID: <span id='pid'>387, <a href='https://arxiv.org/pdf/2511.11824.pdf' target='_blank'>https://arxiv.org/pdf/2511.11824.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhongping Dong, Pengyang Yu, Shuangjian Li, Liming Chen, Mohand Tahar Kechadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11824">SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.
<div id='section'>PaperID: <span id='pid'>388, <a href='https://arxiv.org/pdf/2511.08865.pdf' target='_blank'>https://arxiv.org/pdf/2511.08865.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Cong Tai, Hansheng Wu, Haixu Long, Zhengbin Long, Zhaoyu Zheng, Haodong Xiang, Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08865">MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.
<div id='section'>PaperID: <span id='pid'>389, <a href='https://arxiv.org/pdf/2510.26369.pdf' target='_blank'>https://arxiv.org/pdf/2510.26369.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kazuma Kano, Yuki Mori, Shin Katayama, Kenta Urano, Takuro Yonezawa, Nobuo Kawaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26369">CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Worker location data is key to higher productivity in industrial sites. Cameras are a promising tool for localization in logistics warehouses since they also offer valuable environmental contexts such as package status. However, identifying individuals with only visual data is often impractical. Accordingly, several prior studies identified people in videos by comparing their trajectories and wearable sensor measurements. While this approach has advantages such as independence from appearance, the existing methods may break down under real-world conditions. To overcome this challenge, we propose CorVS, a novel data-driven person identification method based on correspondence between visual tracking trajectories and sensor measurements. Firstly, our deep learning model predicts correspondence probabilities and reliabilities for every pair of a trajectory and sensor measurements. Secondly, our algorithm matches the trajectories and sensor measurements over time using the predicted probabilities and reliabilities. We developed a dataset with actual warehouse operations and demonstrated the method's effectiveness for real-world applications.
<div id='section'>PaperID: <span id='pid'>390, <a href='https://arxiv.org/pdf/2510.17409.pdf' target='_blank'>https://arxiv.org/pdf/2510.17409.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dmitrii Galimzianov, Viacheslav Vyshegorodtsev, Ivan Nezhivykh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17409">Monitoring Horses in Stalls: From Object to Event Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring the behavior of stalled horses is essential for early detection of health and welfare issues but remains labor-intensive and time-consuming. In this study, we present a prototype vision-based monitoring system that automates the detection and tracking of horses and people inside stables using object detection and multi-object tracking techniques. The system leverages YOLOv11 and BoT-SORT for detection and tracking, while event states are inferred based on object trajectories and spatial relations within the stall. To support development, we constructed a custom dataset annotated with assistance from foundation models CLIP and GroundingDINO. The system distinguishes between five event types and accounts for the camera's blind spots. Qualitative evaluation demonstrated reliable performance for horse-related events, while highlighting limitations in detecting people due to data scarcity. This work provides a foundation for real-time behavioral monitoring in equine facilities, with implications for animal welfare and stable management.
<div id='section'>PaperID: <span id='pid'>391, <a href='https://arxiv.org/pdf/2510.09878.pdf' target='_blank'>https://arxiv.org/pdf/2510.09878.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Milad Khanchi, Maria Amer, Charalambos Poullis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09878">Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) methods often rely on Intersection-over-Union (IoU) for association. However, this becomes unreliable when objects are similar or occluded. Also, computing IoU for segmentation masks is computationally expensive. In this work, we use segmentation masks to capture object shapes, but we do not compute segmentation IoU. Instead, we fuse depth and mask features and pass them through a compact encoder trained self-supervised. This encoder produces stable object representations, which we use as an additional similarity cue alongside bounding box IoU and re-identification features for matching. We obtain depth maps from a zero-shot depth estimator and object masks from a promptable visual segmentation model to obtain fine-grained spatial cues. Our MOT method is the first to use the self-supervised encoder to refine segmentation masks without computing masks IoU. MOT can be divided into joint detection-ReID (JDR) and tracking-by-detection (TBD) models. The latter are computationally more efficient. Experiments of our TBD method on challenging benchmarks with non-linear motion, occlusion, and crowded scenes, such as SportsMOT and DanceTrack, show that our method outperforms the TBD state-of-the-art on most metrics, while achieving competitive performance on simpler benchmarks with linear motion, such as MOT17.
<div id='section'>PaperID: <span id='pid'>392, <a href='https://arxiv.org/pdf/2509.22910.pdf' target='_blank'>https://arxiv.org/pdf/2509.22910.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanwei Du, Jing-Chen Peng, Patricio A. Vela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22910">Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given that Visual SLAM relies on appearance cues for localization and scene understanding, texture-less or visually degraded environments (e.g., plain walls or low lighting) lead to poor pose estimation and track loss. However, robots are typically equipped with sensors that provide some form of dead reckoning odometry with reasonable short-time performance but unreliable long-time performance. The Good Weights (GW) algorithm described here provides a framework to adaptively integrate dead reckoning (DR) with passive visual SLAM for continuous and accurate frame-level pose estimation. Importantly, it describes how all modules in a comprehensive SLAM system must be modified to incorporate DR into its design. Adaptive weighting increases DR influence when visual tracking is unreliable and reduces when visual feature information is strong, maintaining pose track without overreliance on DR. Good Weights yields a practical solution for mobile navigation that improves visual SLAM performance and robustness. Experiments on collected datasets and in real-world deployment demonstrate the benefits of Good Weights.
<div id='section'>PaperID: <span id='pid'>393, <a href='https://arxiv.org/pdf/2508.12982.pdf' target='_blank'>https://arxiv.org/pdf/2508.12982.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jan KrejÄÃ­, OndÅej Straka, Petr Girg, JiÅÃ­ Benedikt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12982">Revisiting Functional Derivatives in Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probability generating functionals (PGFLs) are efficient and powerful tools for tracking independent objects in clutter. It was shown that PGFLs could be used for the elegant derivation of practical multi-object tracking algorithms, e.g., the probability hypothesis density (PHD) filter. However, derivations using PGFLs use the so-called functional derivatives whose definitions usually appear too complicated or heuristic, involving Dirac delta ``functions''. This paper begins by comparing different definitions of functional derivatives and exploring their relationships and implications for practical applications. It then proposes a rigorous definition of the functional derivative, utilizing straightforward yet precise mathematics for clarity. Key properties of the functional derivative are revealed and discussed.
<div id='section'>PaperID: <span id='pid'>394, <a href='https://arxiv.org/pdf/2508.02238.pdf' target='_blank'>https://arxiv.org/pdf/2508.02238.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xin Dong, Yiwei Zhang, Yangjie Cui, Jinwu Xiang, Daochun Li, Zhan Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02238">An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras offer significant advantages, including a wide dynamic range, high temporal resolution, and immunity to motion blur, making them highly promising for addressing challenging visual conditions. Extracting and utilizing effective information from asynchronous event streams is essential for the onboard implementation of event cameras. In this paper, we propose a streamlined event-based intensity reconstruction scheme, event-based single integration (ESI), to address such implementation challenges. This method guarantees the portability of conventional frame-based vision methods to event-based scenarios and maintains the intrinsic advantages of event cameras. The ESI approach reconstructs intensity images by performing a single integration of the event streams combined with an enhanced decay algorithm. Such a method enables real-time intensity reconstruction at a high frame rate, typically 100 FPS. Furthermore, the relatively low computation load of ESI fits onboard implementation suitably, such as in UAV-based visual tracking scenarios. Extensive experiments have been conducted to evaluate the performance comparison of ESI and state-of-the-art algorithms. Compared to state-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency improvements, superior reconstruction quality, and a high frame rate. As a result, ESI enhances UAV onboard perception significantly under visual adversary surroundings. In-flight tests, ESI demonstrates effective performance for UAV onboard visual tracking under extremely low illumination conditions(2-10lux), whereas other comparative algorithms fail due to insufficient frame rate, poor image quality, or limited real-time performance.
<div id='section'>PaperID: <span id='pid'>395, <a href='https://arxiv.org/pdf/2506.04122.pdf' target='_blank'>https://arxiv.org/pdf/2506.04122.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sharang Kaul, Mario Berk, Thiemo Gerbich, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04122">Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding reliable matches is essential in multi-object tracking to ensure the accuracy and reliability of perception systems in safety-critical applications such as autonomous vehicles. Effective matching mitigates perception errors, enhancing object identification and tracking for improved performance and safety. However, traditional metrics such as Intersection over Union (IoU) and Center Point Distances (CPDs), which are effective in 2D image planes, often fail to find critical matches in complex 3D scenes. To address this limitation, we introduce Contour Errors (CEs), an ego or object-centric metric for identifying matches of interest in tracking scenarios from a functional perspective. By comparing bounding boxes in the ego vehicle's frame, contour errors provide a more functionally relevant assessment of object matches. Extensive experiments on the nuScenes dataset demonstrate that contour errors improve the reliability of matches over the state-of-the-art 2D IoU and CPD metrics in tracking-by-detection methods. In 3D car tracking, our results show that Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges and 60% at far ranges compared to IoU in the evaluation stage.
<div id='section'>PaperID: <span id='pid'>396, <a href='https://arxiv.org/pdf/2505.00995.pdf' target='_blank'>https://arxiv.org/pdf/2505.00995.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Taewook Park, Jinwoo Lee, Hyondong Oh, Won-Jae Yun, Kyu-Wha Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00995">Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the agricultural workforce declines and labor costs rise, robotic yield estimation has become increasingly important. While unmanned ground vehicles (UGVs) are commonly used for indoor farm monitoring, their deployment in greenhouses is often constrained by infrastructure limitations, sensor placement challenges, and operational inefficiencies. To address these issues, we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial odometry algorithm for precise navigation in GNSS-denied environments and utilizes a 3D multi-object tracking algorithm to estimate the count and weight of cherry tomatoes. We evaluate the system using two dataset: one from a harvesting row and another from a growing row. In the harvesting-row dataset, the proposed system achieves 94.4\% counting accuracy and 87.5\% weight estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For the growing-row dataset, which consists of occluded unripened fruits, we qualitatively analyze tracking performance and highlight future research directions for improving perception in greenhouse with strong occlusions. Our findings demonstrate the potential of UAVs for efficient robotic yield estimation in commercial greenhouses.
<div id='section'>PaperID: <span id='pid'>397, <a href='https://arxiv.org/pdf/2505.00534.pdf' target='_blank'>https://arxiv.org/pdf/2505.00534.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, Rana Hammad Raza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00534">A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.
<div id='section'>PaperID: <span id='pid'>398, <a href='https://arxiv.org/pdf/2503.16318.pdf' target='_blank'>https://arxiv.org/pdf/2503.16318.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Edgar Sucar, Zihang Lai, Eldar Insafutdinov, Andrea Vedaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16318">Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DUSt3R has recently shown that one can reduce many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing the scene in 3D, and establishing image correspondences, to the prediction of a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. This formulation is elegant and powerful, but unable to tackle dynamic scenes. To address this challenge, we introduce the concept of Dynamic Point Maps (DPM), extending standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key intuition is that, when time is introduced, there are several possible spatial and time references that can be used to define the point maps. We identify a minimal subset of such combinations that can be regressed by a network to solve the sub tasks mentioned above. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks for video depth prediction, dynamic point cloud reconstruction, 3D scene flow and object pose tracking, achieving state-of-the-art performance. Code, models and additional results are available at https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/.
<div id='section'>PaperID: <span id='pid'>399, <a href='https://arxiv.org/pdf/2503.09807.pdf' target='_blank'>https://arxiv.org/pdf/2503.09807.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qingwu Liu, Nicolas Saunier, Guillaume-Alexandre Bilodeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09807">How good are deep learning methods for automated road safety analysis using video data? An experimental study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based multi-object detection (MOD) and multi-object tracking (MOT) are advancing at a fast pace. A variety of 2D and 3D MOD and MOT methods have been developed for monocular and stereo cameras. Road safety analysis can benefit from those advancements. As crashes are rare events, surrogate measures of safety (SMoS) have been developed for safety analyses. (Semi-)Automated safety analysis methods extract road user trajectories to compute safety indicators, for example, Time-to-Collision (TTC) and Post-encroachment Time (PET). Inspired by the success of deep learning in MOD and MOT, we investigate three MOT methods, including one based on a stereo-camera, using the annotated KITTI traffic video dataset. Two post-processing steps, IDsplit and SS, are developed to improve the tracking results and investigate the factors influencing the TTC. The experimental results show that, despite some advantages in terms of the numbers of interactions or similarity to the TTC distributions, all the tested methods systematically over-estimate the number of interactions and under-estimate the TTC: they report more interactions and more severe interactions, making the road user interactions appear less safe than they are. Further efforts will be directed towards testing more methods and more data, in particular from roadside sensors, to verify the results and improve the performance.
<div id='section'>PaperID: <span id='pid'>400, <a href='https://arxiv.org/pdf/2411.13346.pdf' target='_blank'>https://arxiv.org/pdf/2411.13346.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Karolina Trajkovska, MatjaÅ¾ Kljun, Klen ÄopiÄ Pucihar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13346">Gaze2AOI: Open Source Deep-learning Based System for Automatic Area of Interest Annotation with Eye Tracking Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Eye gaze is considered an important indicator for understanding and predicting user behaviour, as well as directing their attention across various domains including advertisement design, human-computer interaction and film viewing. In this paper, we present a novel method to enhance the analysis of user behaviour and attention by (i) augmenting video streams with automatically annotating and labelling areas of interest (AOIs), and (ii) integrating AOIs with collected eye gaze and fixation data. The tool provides key features such as time to first fixation, dwell time, and frequency of AOI revisits. By incorporating the YOLOv8 object tracking algorithm, the tool supports over 600 different object classes, providing a comprehensive set for a variety of video streams. This tool will be made available as open-source software, thereby contributing to broader research and development efforts in the field.
<div id='section'>PaperID: <span id='pid'>401, <a href='https://arxiv.org/pdf/2411.06197.pdf' target='_blank'>https://arxiv.org/pdf/2411.06197.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shukun Jia, Shiyu Hu, Yichao Cao, Feng Yang, Xin Lu, Xiaobo Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06197">Tracking by Detection and Query: An Efficient End-to-End Framework for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is dominated by two paradigms: tracking-by-detection (TBD) and tracking-by-query (TBQ). While TBD is decoupled and efficient, its fragmented association steps and heuristic matching pipelines often compromise robustness in complex scenarios. TBQ provides stronger semantic modeling through end-to-end learning, but suffers from high training cost and slow inference due to tight coupling between detection and association. To address these challenges, we propose TBDQ-Net, a unified tracking-by-detection-and-query (TBDQ) framework that effectively combines the strengths of both paradigms. Our method efficiently integrates pretrained, high-performance detectors with an MOT-tailored associator. The associator is lightweight and directly fetches information from the inference of detectors, enhancing the overall efficiency of the framework. The associator is also learnable, making it essential for fully end-to-end optimization, ensuring robust tracking capabilities. Specifically, the associator comprises two key modules: basic information interaction (BII) for comprehensive semantic interaction, and content-position alignment (CPA) for semantic and positional consistency. TBDQ-Net's effectiveness is extensively demonstrated on DanceTrack, SportsMOT and MOT20 benchmarks. As a structurally efficient and semantically robust tracking framework, it outperforms the leading TBD method by 6.0 IDF1 points on DanceTrack and achieves at least 37.5% faster inference than prominent TBQ methods.
<div id='section'>PaperID: <span id='pid'>402, <a href='https://arxiv.org/pdf/2410.13437.pdf' target='_blank'>https://arxiv.org/pdf/2410.13437.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Changcheng Xiao, Qiong Cao, Yujie Zhong, Xiang Zhang, Tao Wang, Canqun Yang, Long Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13437">Temporal-Enhanced Multimodal Transformer for Referring Multi-Object Tracking and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to locate an arbitrary number of target objects and maintain their identities referred by a language expression in a video. This intricate task involves the reasoning of linguistic and visual modalities, along with the temporal association of target objects. However, the seminal work employs only loose feature fusion and overlooks the utilization of long-term information on tracked objects. In this study, we introduce a compact Transformer-based method, termed TenRMOT. We conduct feature fusion at both encoding and decoding stages to fully exploit the advantages of Transformer architecture. Specifically, we incrementally perform cross-modal fusion layer-by-layer during the encoding phase. In the decoding phase, we utilize language-guided queries to probe memory features for accurate prediction of the desired objects. Moreover, we introduce a query update module that explicitly leverages temporal prior information of the tracked objects to enhance the consistency of their trajectories. In addition, we introduce a novel task called Referring Multi-Object Tracking and Segmentation (RMOTS) and construct a new dataset named Ref-KITTI Segmentation. Our dataset consists of 18 videos with 818 expressions, and each expression averages 10.7 masks, which poses a greater challenge compared to the typical single mask in most existing referring video segmentation datasets. TenRMOT demonstrates superior performance on both the referring multi-object tracking and the segmentation tasks.
<div id='section'>PaperID: <span id='pid'>403, <a href='https://arxiv.org/pdf/2409.11785.pdf' target='_blank'>https://arxiv.org/pdf/2409.11785.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shiming Ge, Zhao Luo, Chunhui Zhang, Yingying Hua, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11785">Distilling Channels for Efficient Deep Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep trackers have proven success in visual tracking. Typically, these trackers employ optimally pre-trained deep networks to represent all diverse objects with multi-channel features from some fixed layers. The deep networks employed are usually trained to extract rich knowledge from massive data used in object classification and so they are capable to represent generic objects very well. However, these networks are too complex to represent a specific moving object, leading to poor generalization as well as high computational and memory costs. This paper presents a novel and general framework termed channel distillation to facilitate deep trackers. To validate the effectiveness of channel distillation, we take discriminative correlation filter (DCF) and ECO for example. We demonstrate that an integrated formulation can turn feature compression, response map generation, and model update into a unified energy minimization problem to adaptively select informative feature channels that improve the efficacy of tracking moving objects on the fly. Channel distillation can accurately extract good channels, alleviating the influence of noisy channels and generally reducing the number of channels, as well as adaptively generalizing to different channels and networks. The resulting deep tracker is accurate, fast, and has low memory requirements. Extensive experimental evaluations on popular benchmarks clearly demonstrate the effectiveness and generalizability of our framework.
<div id='section'>PaperID: <span id='pid'>404, <a href='https://arxiv.org/pdf/2409.11749.pdf' target='_blank'>https://arxiv.org/pdf/2409.11749.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaoyu Li, Peidong Li, Lijun Zhao, Dedong Liu, Jinghan Gao, Xian Wu, Yitao Wu, Dixiao Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11749">RockTrack: A 3D Robust Multi-Camera-Ken Multi-Object Tracking Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT) obtains significant performance improvements with the rapid advancements in 3D object detection, particularly in cost-effective multi-camera setups. However, the prevalent end-to-end training approach for multi-camera trackers results in detector-specific models, limiting their versatility. Moreover, current generic trackers overlook the unique features of multi-camera detectors, i.e., the unreliability of motion observations and the feasibility of visual information. To address these challenges, we propose RockTrack, a 3D MOT method for multi-camera detectors. Following the Tracking-By-Detection framework, RockTrack is compatible with various off-the-shelf detectors. RockTrack incorporates a confidence-guided preprocessing module to extract reliable motion and image observations from distinct representation spaces from a single detector. These observations are then fused in an association module that leverages geometric and appearance cues to minimize mismatches. The resulting matches are propagated through a staged estimation process, forming the basis for heuristic noise modeling. Additionally, we introduce a novel appearance similarity metric for explicitly characterizing object affinities in multi-camera settings. RockTrack achieves state-of-the-art performance on the nuScenes vision-only tracking leaderboard with 59.1% AMOTA while demonstrating impressive computational efficiency.
<div id='section'>PaperID: <span id='pid'>405, <a href='https://arxiv.org/pdf/2409.03252.pdf' target='_blank'>https://arxiv.org/pdf/2409.03252.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Keisuke Toida, Naoki Kato, Osamu Segawa, Takeshi Nakamura, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03252">Gr-IoU: Ground-Intersection over Union for Robust Multi-Object Tracking with 3D Geometric Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a Ground IoU (Gr-IoU) to address the data association problem in multi-object tracking. When tracking objects detected by a camera, it often occurs that the same object is assigned different IDs in consecutive frames, especially when objects are close to each other or overlapping. To address this issue, we introduce Gr-IoU, which takes into account the 3D structure of the scene. Gr-IoU transforms traditional bounding boxes from the image space to the ground plane using the vanishing point geometry. The IoU calculated with these transformed bounding boxes is more sensitive to the front-to-back relationships of objects, thereby improving data association accuracy and reducing ID switches. We evaluated our Gr-IoU method on the MOT17 and MOT20 datasets, which contain diverse tracking scenarios including crowded scenes and sequences with frequent occlusions. Experimental results demonstrated that Gr-IoU outperforms conventional real-time methods without appearance features.
<div id='section'>PaperID: <span id='pid'>406, <a href='https://arxiv.org/pdf/2409.00339.pdf' target='_blank'>https://arxiv.org/pdf/2409.00339.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Makoto M. Itoh, Qingrui Hu, Takayuki Niizato, Hiroaki Kawashima, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00339">Fish Tracking Challenge 2024: A Multi-Object Tracking Competition with Sweetfish Schooling Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of collective animal behavior, especially in aquatic environments, presents unique challenges and opportunities for understanding movement and interaction patterns in the field of ethology, ecology, and bio-navigation. The Fish Tracking Challenge 2024 (https://ftc-2024.github.io/) introduces a multi-object tracking competition focused on the intricate behaviors of schooling sweetfish. Using the SweetFish dataset, participants are tasked with developing advanced tracking models to accurately monitor the locations of 10 sweetfishes simultaneously. This paper introduces the competition's background, objectives, the SweetFish dataset, and the appraoches of the 1st to 3rd winners and our baseline. By leveraging video data and bounding box annotations, the competition aims to foster innovation in automatic detection and tracking algorithms, addressing the complexities of aquatic animal movements. The challenge provides the importance of multi-object tracking for discovering the dynamics of collective animal behavior, with the potential to significantly advance scientific understanding in the above fields.
<div id='section'>PaperID: <span id='pid'>407, <a href='https://arxiv.org/pdf/2512.07776.pdf' target='_blank'>https://arxiv.org/pdf/2512.07776.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Maximilian Schall, Felix Leonard Knöfel, Noah Elias König, Jan Jonas Kubeler, Maximilian von Klinski, Joan Wilhelm Linnemann, Xiaoshi Liu, Iven Jelle Schlegelmilch, Ole Woyciniuk, Alexandra Schild, Dante Wasmuht, Magdalena Bermejo Espinet, German Illera Basas, Gerard de Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07776">GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species
<div id='section'>PaperID: <span id='pid'>408, <a href='https://arxiv.org/pdf/2511.09070.pdf' target='_blank'>https://arxiv.org/pdf/2511.09070.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wing Shing Wong, Chung Shue Chen, Yuan-Hsun Lo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09070">Color Multiset Codes based on Sunmao Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present results on coding using multisets instead of ordered sequences. The study is motivated by a moving object tracking problem in a sensor network and can find applications in settings where the order of the symbols in a codeword cannot be maintained or observed. In this paper a multiset coding scheme is proposed on source data that can be organized as a flat or cyclic multi-dimensional integer lattice (grid). A fundamental idea in the solution approach is to decompose the original source data grid into sub-grids. The original multiset coding problem can then be restricted to each of the sub-grid. Solutions for the sub-grids are subsequently piece together to form the desired solution. We name this circle of idea as sunmao construction in reference to woodwork construction method with ancient origin. Braid codes are specific solutions defined using the sunmao construction. They are easy to define for multi-dimensional grids. Moreover for a code of a given code set size and multiset cardinality, if we measure coding efficiency by the number of distinct symbols required, then braid codes have asymptotic order equal to those that are optimal. We also show that braid codes have interesting inherent error correction properties.
<div id='section'>PaperID: <span id='pid'>409, <a href='https://arxiv.org/pdf/2510.13235.pdf' target='_blank'>https://arxiv.org/pdf/2510.13235.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yukuan Zhang, Jiarui Zhao, Shangqing Nie, Jin Kuang, Shengsheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13235">EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal semantic cues, such as textual descriptions, have shown strong potential in enhancing target perception for tracking. However, existing methods rely on static textual descriptions from large language models, which lack adaptability to real-time target state changes and prone to hallucinations. To address these challenges, we propose a unified multimodal vision-language tracking framework, named EPIPTrack, which leverages explicit and implicit prompts for dynamic target modeling and semantic alignment. Specifically, explicit prompts transform spatial motion information into natural language descriptions to provide spatiotemporal guidance. Implicit prompts combine pseudo-words with learnable descriptors to construct individualized knowledge representations capturing appearance attributes. Both prompts undergo dynamic adjustment via the CLIP text encoder to respond to changes in target state. Furthermore, we design a Discriminative Feature Augmentor to enhance visual and cross-modal representations. Extensive experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack outperforms existing trackers in diverse scenarios, exhibiting robust adaptability and superior performance.
<div id='section'>PaperID: <span id='pid'>410, <a href='https://arxiv.org/pdf/2508.12777.pdf' target='_blank'>https://arxiv.org/pdf/2508.12777.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenguang Tao, Xiaotian Wang, Tian Yan, Jie Yan, Guodong Li, Kun Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12777">SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a key research direction in the field of multi-object tracking (MOT), UAV-based multi-object tracking has significant application value in the analysis and understanding of urban intelligent transportation systems. However, in complex UAV perspectives, challenges such as small target scale variations, occlusions, nonlinear crossing motions, and motion blur severely hinder the stability of multi-object tracking. To address these challenges, this paper proposes a novel multi-object tracking framework, SocialTrack, aimed at enhancing the tracking accuracy and robustness of small targets in complex urban traffic environments. The specialized small-target detector enhances the detection performance by employing a multi-scale feature enhancement mechanism. The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of trajectory prediction by incorporating a velocity dynamic modeling mechanism. The Group Motion Compensation Strategy (GMCS) models social group motion priors to provide stable state update references for low-quality tracks, significantly improving the target association accuracy in complex dynamic environments. Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical trajectory information to predict the future state of low-quality tracks, effectively mitigating identity switching issues. Extensive experiments on the UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing state-of-the-art (SOTA) methods across several key metrics. Significant improvements in MOTA and IDF1, among other core performance indicators, highlight its superior robustness and adaptability. Additionally, SocialTrack is highly modular and compatible, allowing for seamless integration with existing trackers to further enhance performance.
<div id='section'>PaperID: <span id='pid'>411, <a href='https://arxiv.org/pdf/2507.17793.pdf' target='_blank'>https://arxiv.org/pdf/2507.17793.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Joel Brogan, Matthew Yohe, David Cornett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17793">CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>What if you could piece together your own custom biometrics and AI analysis system, a bit like LEGO blocks? We aim to bring that technology to field operators in the field who require flexible, high-performance edge AI system that can be adapted on a moment's notice. This paper introduces CHAMP (Configurable Hot-swappable Architecture for Machine Perception), a modular edge computing platform that allows operators to dynamically swap in specialized AI "capability cartridges" for tasks like face recognition, object tracking, and document analysis. CHAMP leverages low-power FPGA-based accelerators on a high-throughput bus, orchestrated by a custom operating system (VDiSK) to enable plug-and-play AI pipelines and cryptographically secured biometric datasets. In this paper we describe the CHAMP design, including its modular scaling with multiple accelerators and the VDiSK operating system for runtime reconfiguration, along with its cryptographic capabilities to keep data stored on modules safe and private. Experiments demonstrate near-linear throughput scaling from 1 to 5 neural compute accelerators, highlighting both the performance gains and saturation limits of the USB3-based bus. Finally, we discuss applications of CHAMP in field biometrics, surveillance, and disaster response, and outline future improvements in bus protocols, cartridge capabilities, and system software.
<div id='section'>PaperID: <span id='pid'>412, <a href='https://arxiv.org/pdf/2507.02408.pdf' target='_blank'>https://arxiv.org/pdf/2507.02408.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Duong Nguyen-Ngoc Tran, Long Hoang Pham, Chi Dai Tran, Quoc Pham-Nam Ho, Huy-Hung Nguyen, Jae Wook Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02408">A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking in thermal images is essential for surveillance systems, particularly in challenging environments where RGB cameras struggle due to low visibility or poor lighting conditions. Thermal sensors enhance recognition tasks by capturing infrared signatures, but a major challenge is their low-level feature representation, which makes it difficult to accurately detect and track pedestrians. To address this, the paper introduces a novel tuning method for pedestrian tracking, specifically designed to handle the complex motion patterns in thermal imagery. The proposed framework optimizes two-stages, ensuring that each stage is tuned with the most suitable hyperparameters to maximize tracking performance. By fine-tuning hyperparameters for real-time tracking, the method achieves high accuracy without relying on complex reidentification or motion models. Extensive experiments on PBVS Thermal MOT dataset demonstrate that the approach is highly effective across various thermal camera conditions, making it a robust solution for real-world surveillance applications.
<div id='section'>PaperID: <span id='pid'>413, <a href='https://arxiv.org/pdf/2506.09159.pdf' target='_blank'>https://arxiv.org/pdf/2506.09159.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Antonio Calagna, Yenchia Yu, Paolo Giaccone, Carla Fabiana Chiasserini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09159">MOSE: A Novel Orchestration Framework for Stateful Microservice Migration at the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stateful migration has emerged as the dominant technology to support microservice mobility at the network edge while ensuring a satisfying experience to mobile end users. This work addresses two pivotal challenges, namely, the implementation and the orchestration of the migration process. We first introduce a novel framework that efficiently implements stateful migration and effectively orchestrates the migration process by fulfilling both network and application KPI targets. Through experimental validation using realistic microservices, we then show that our solution (i) greatly improves migration performance, yielding up to 77% decrease of the migration downtime with respect to the state of the art, and (ii) successfully addresses the strict user QoE requirements of critical scenarios featuring latency-sensitive microservices. Further, we consider two practical use cases, featuring, respectively, a UAV autopilot microservice and a multi-object tracking task, and demonstrate how our framework outperforms current state-of-the-art approaches in configuring the migration process and in meeting KPI targets.
<div id='section'>PaperID: <span id='pid'>414, <a href='https://arxiv.org/pdf/2505.18727.pdf' target='_blank'>https://arxiv.org/pdf/2505.18727.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xiaohe Li, Pengfei Li, Zide Fan, Ying Geng, Fangli Mou, Haohua Wu, Yunping Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18727">FusionTrack: End-to-End Multi-Object Tracking in Arbitrary Multi-View Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view multi-object tracking (MVMOT) has found widespread applications in intelligent transportation, surveillance systems, and urban management. However, existing studies rarely address genuinely free-viewpoint MVMOT systems, which could significantly enhance the flexibility and scalability of cooperative tracking systems. To bridge this gap, we first construct the Multi-Drone Multi-Object Tracking (MDMOT) dataset, captured by mobile drone swarms across diverse real-world scenarios, initially establishing the first benchmark for multi-object tracking in arbitrary multi-view environment. Building upon this foundation, we propose \textbf{FusionTrack}, an end-to-end framework that reasonably integrates tracking and re-identification to leverage multi-view information for robust trajectory association. Extensive experiments on our MDMOT and other benchmark datasets demonstrate that FusionTrack achieves state-of-the-art performance in both single-view and multi-view tracking.
<div id='section'>PaperID: <span id='pid'>415, <a href='https://arxiv.org/pdf/2505.08126.pdf' target='_blank'>https://arxiv.org/pdf/2505.08126.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Angus Apps, Ziwei Wang, Vladimir Perejogin, Timothy Molloy, Robert Mahony
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08126">Asynchronous Multi-Object Tracking with an Event Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Events cameras are ideal sensors for enabling robots to detect and track objects in highly dynamic environments due to their low latency output, high temporal resolution, and high dynamic range. In this paper, we present the Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and tracking multiple objects by processing individual raw events asynchronously. AEMOT detects salient event blob features by identifying regions of consistent optical flow using a novel Field of Active Flow Directions built from the Surface of Active Events. Detected features are tracked as candidate objects using the recently proposed Asynchronous Event Blob (AEB) tracker in order to construct small intensity patches of each candidate object. A novel learnt validation stage promotes or discards candidate objects based on classification of their intensity patches, with promoted objects having their position, velocity, size, and orientation estimated at their event rate. We evaluate AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with precision and recall performance exceeding that of alternative event-based detection and tracking algorithms by over 37%. Source code and the labelled event Bee Swarm Dataset will be open sourced
<div id='section'>PaperID: <span id='pid'>416, <a href='https://arxiv.org/pdf/2505.07110.pdf' target='_blank'>https://arxiv.org/pdf/2505.07110.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tong Zhang, Fenghua Shao, Runsheng Zhang, Yifan Zhuang, Liuqingqing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07110">DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Based on the DeepSORT algorithm, this study explores the application of visual tracking technology in intelligent human-computer interaction, especially in the field of gesture recognition and tracking. With the rapid development of artificial intelligence and deep learning technology, visual-based interaction has gradually replaced traditional input devices and become an important way for intelligent systems to interact with users. The DeepSORT algorithm can achieve accurate target tracking in dynamic environments by combining Kalman filters and deep learning feature extraction methods. It is especially suitable for complex scenes with multi-target tracking and fast movements. This study experimentally verifies the superior performance of DeepSORT in gesture recognition and tracking. It can accurately capture and track the user's gesture trajectory and is superior to traditional tracking methods in terms of real-time and accuracy. In addition, this study also combines gesture recognition experiments to evaluate the recognition ability and feedback response of the DeepSORT algorithm under different gestures (such as sliding, clicking, and zooming). The experimental results show that DeepSORT can not only effectively deal with target occlusion and motion blur but also can stably track in a multi-target environment, achieving a smooth user interaction experience. Finally, this paper looks forward to the future development direction of intelligent human-computer interaction systems based on visual tracking and proposes future research focuses such as algorithm optimization, data fusion, and multimodal interaction in order to promote a more intelligent and personalized interactive experience. Keywords-DeepSORT, visual tracking, gesture recognition, human-computer interaction
<div id='section'>PaperID: <span id='pid'>417, <a href='https://arxiv.org/pdf/2504.18708.pdf' target='_blank'>https://arxiv.org/pdf/2504.18708.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Longfei Han, Klaus KefferpÃ¼tz, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18708">Decentralized Fusion of 3D Extended Object Tracking based on a B-Spline Shape Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended Object Tracking (EOT) exploits the high resolution of modern sensors for detailed environmental perception. Combined with decentralized fusion, it contributes to a more scalable and robust perception system. This paper investigates the decentralized fusion of 3D EOT using a B-spline curve based model. The spline curve is used to represent the side-view profile, which is then extruded with a width to form a 3D shape. We use covariance intersection (CI) for the decentralized fusion and discuss the challenge of applying it to EOT. We further evaluate the tracking result of the decentralized fusion with simulated and real datasets of traffic scenarios. We show that the CI-based fusion can significantly improve the tracking performance for sensors with unfavorable perspective.
<div id='section'>PaperID: <span id='pid'>418, <a href='https://arxiv.org/pdf/2504.03258.pdf' target='_blank'>https://arxiv.org/pdf/2504.03258.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shuxiao Ding, Yutong Yang, Julian Wiederer, Markus Braun, Peizheng Li, Juergen Gall, Bin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03258">TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Query denoising has become a standard training strategy for DETR-based detectors by addressing the slow convergence issue. Besides that, query denoising can be used to increase the diversity of training samples for modeling complex scenarios which is critical for Multi-Object Tracking (MOT), showing its potential in MOT application. Existing approaches integrate query denoising within the tracking-by-attention paradigm. However, as the denoising process only happens within the single frame, it cannot benefit the tracker to learn temporal-related information. In addition, the attention mask in query denoising prevents information exchange between denoising and object queries, limiting its potential in improving association using self-attention. To address these issues, we propose TQD-Track, which introduces Temporal Query Denoising (TQD) tailored for MOT, enabling denoising queries to carry temporal information and instance-specific feature representation. We introduce diverse noise types onto denoising queries that simulate real-world challenges in MOT. We analyze our proposed TQD for different tracking paradigms, and find out the paradigm with explicit learned data association module, e.g. tracking-by-detection or alternating detection and association, benefit from TQD by a larger margin. For these paradigms, we further design an association mask in the association module to ensure the consistent interaction between track and detection queries as during inference. Extensive experiments on the nuScenes dataset demonstrate that our approach consistently enhances different tracking methods by only changing the training process, especially the paradigms with explicit association module.
<div id='section'>PaperID: <span id='pid'>419, <a href='https://arxiv.org/pdf/2503.10730.pdf' target='_blank'>https://arxiv.org/pdf/2503.10730.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Longfei Han, Klaus KefferpÃ¼tz, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10730">3D Extended Object Tracking based on Extruded B-Spline Side View Profiles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is an essential task for autonomous systems. With the advancement of 3D sensors, these systems can better perceive their surroundings using effective 3D Extended Object Tracking (EOT) methods. Based on the observation that common road users are symmetrical on the right and left sides in the traveling direction, we focus on the side view profile of the object. In order to leverage of the development in 2D EOT and balance the number of parameters of a shape model in the tracking algorithms, we propose a method for 3D extended object tracking (EOT) by describing the side view profile of the object with B-spline curves and forming an extrusion to obtain a 3D extent. The use of B-spline curves exploits their flexible representation power by allowing the control points to move freely. The algorithm is developed into an Extended Kalman Filter (EKF). For a through evaluation of this method, we use simulated traffic scenario of different vehicle models and realworld open dataset containing both radar and lidar data.
<div id='section'>PaperID: <span id='pid'>420, <a href='https://arxiv.org/pdf/2502.13875.pdf' target='_blank'>https://arxiv.org/pdf/2502.13875.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Huu-Thien Tran, Phuoc-Sang Pham, Thai-Son Tran, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13875">MEX: Memory-efficient Approach to Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) is a relatively new concept that has rapidly gained traction as a promising research direction at the intersection of computer vision and natural language processing. Unlike traditional multi-object tracking, RMOT identifies and tracks objects and incorporates textual descriptions for object class names, making the approach more intuitive. Various techniques have been proposed to address this challenging problem; however, most require the training of the entire network due to their end-to-end nature. Among these methods, iKUN has emerged as a particularly promising solution. Therefore, we further explore its pipeline and enhance its performance. In this paper, we introduce a practical module dubbed Memory-Efficient Cross-modality -- MEX. This memory-efficient technique can be directly applied to off-the-shelf trackers like iKUN, resulting in significant architectural improvements. Our method proves effective during inference on a single GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI dataset, which offers diverse autonomous driving scenes with relevant language expressions, is particularly useful for studying this problem. Empirically, our method demonstrates effectiveness and efficiency regarding HOTA tracking scores, substantially improving memory allocation and processing speed.
<div id='section'>PaperID: <span id='pid'>421, <a href='https://arxiv.org/pdf/2501.14587.pdf' target='_blank'>https://arxiv.org/pdf/2501.14587.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Viktor KozÃ¡k, Karel KoÅ¡nar, Jan Chudoba, Miroslav Kulich, Libor PÅeuÄil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14587">Visual Localization via Semantic Structures in Autonomous Photovoltaic Power Plant Inspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspection systems utilizing unmanned aerial vehicles (UAVs) equipped with thermal cameras are increasingly popular for the maintenance of photovoltaic (PV) power plants. However, automation of the inspection task is a challenging problem as it requires precise navigation to capture images from optimal distances and viewing angles.
  This paper presents a novel localization pipeline that directly integrates PV module detection with UAV navigation, allowing precise positioning during inspection. Detections are used to identify the power plant structures in the image and associate these with the power plant model. We define visually recognizable anchor points for the initial association and use object tracking to discern global associations. We present three distinct methods for visual segmentation of PV modules based on traditional computer vision, deep learning, and their fusion, and we evaluate their performance in relation to the proposed localization pipeline.
  The presented methods were verified and evaluated using custom aerial inspection data sets, demonstrating their robustness and applicability for real-time navigation. Additionally, we evaluate the influence of the power plant model's precision on the localization methods.
<div id='section'>PaperID: <span id='pid'>422, <a href='https://arxiv.org/pdf/2412.13273.pdf' target='_blank'>https://arxiv.org/pdf/2412.13273.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Andrei Znobishchev, Valerii Filev, Oleg Kudashev, Nikita Orlov, Humphrey Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13273">CompactFlowNet: Efficient Real-time Optical Flow Estimation on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CompactFlowNet, the first real-time mobile neural network for optical flow prediction, which involves determining the displacement of each pixel in an initial frame relative to the corresponding pixel in a subsequent frame. Optical flow serves as a fundamental building block for various video-related tasks, such as video restoration, motion estimation, video stabilization, object tracking, action recognition, and video generation. While current state-of-the-art methods prioritize accuracy, they often overlook constraints regarding speed and memory usage. Existing light models typically focus on reducing size but still exhibit high latency, compromise significantly on quality, or are optimized for high-performance GPUs, resulting in sub-optimal performance on mobile devices. This study aims to develop a mobile-optimized optical flow model by proposing a novel mobile device-compatible architecture, as well as enhancements to the training pipeline, which optimize the model for reduced weight, low memory utilization, and increased speed while maintaining minimal error. Our approach demonstrates superior or comparable performance to the state-of-the-art lightweight models on the challenging KITTI and Sintel benchmarks. Furthermore, it attains a significantly accelerated inference speed, thereby yielding real-time operational efficiency on the iPhone 8, while surpassing real-time performance levels on more advanced mobile devices.
<div id='section'>PaperID: <span id='pid'>423, <a href='https://arxiv.org/pdf/2411.09020.pdf' target='_blank'>https://arxiv.org/pdf/2411.09020.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Anirvan Dutta, Etienne Burdet, Mohsen Kaboli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09020">Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive exploration of the unknown physical properties of objects such as stiffness, mass, center of mass, friction coefficient, and shape is crucial for autonomous robotic systems operating continuously in unstructured environments. Precise identification of these properties is essential to manipulate objects in a stable and controlled way, and is also required to anticipate the outcomes of (prehensile or non-prehensile) manipulation actions such as pushing, pulling, lifting, etc. Our study focuses on autonomously inferring the physical properties of a diverse set of various homogeneous, heterogeneous, and articulated objects utilizing a robotic system equipped with vision and tactile sensors. We propose a novel predictive perception framework for identifying object properties of the diverse objects by leveraging versatile exploratory actions: non-prehensile pushing and prehensile pulling. As part of the framework, we propose a novel active shape perception to seamlessly initiate exploration. Our innovative dual differentiable filtering with Graph Neural Networks learns the object-robot interaction and performs consistent inference of indirectly observable time-invariant object properties. In addition, we formulate a $N$-step information gain approach to actively select the most informative actions for efficient learning and inference. Extensive real-robot experiments with planar objects show that our predictive perception framework results in better performance than the state-of-the-art baseline and demonstrate our framework in three major applications for i) object tracking, ii) goal-driven task, and iii) change in environment detection.
<div id='section'>PaperID: <span id='pid'>424, <a href='https://arxiv.org/pdf/2411.08335.pdf' target='_blank'>https://arxiv.org/pdf/2411.08335.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Muttahirul Islam, Nazmul Haque, Md. Hadiuzzaman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08335">DEEGITS: Deep Learning based Framework for Measuring Heterogenous Traffic State in Challenging Traffic Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents DEEGITS (Deep Learning Based Heterogeneous Traffic State Measurement), a comprehensive framework that leverages state-of-the-art convolutional neural network (CNN) techniques to accurately and rapidly detect vehicles and pedestrians, as well as to measure traffic states in challenging scenarios (i.e., congestion, occlusion). In this study, we enhance the training dataset through data fusion, enabling simultaneous detection of vehicles and pedestrians. Image preprocessing and augmentation are subsequently performed to improve the quality and quantity of the dataset. Transfer learning is applied on the YOLOv8 pretrained model to increase the model's capability to identify a diverse array of vehicles. Optimal hyperparameters are obtained using the Grid Search algorithm, with the Stochastic Gradient Descent (SGD) optimizer outperforming other optimizers under these settings. Extensive experimentation and evaluation demonstrate substantial accuracy within the detection framework, with the model achieving 0.794 mAP@0.5 on the validation set and 0.786 mAP@0.5 on the test set, surpassing previous benchmarks on similar datasets. The DeepSORT multi-object tracking algorithm is incorporated to track detected vehicles and pedestrians in this study. Finally, the framework is tested to measure heterogeneous traffic states in mixed traffic conditions. Two locations with differing traffic compositions and congestion levels are selected: one motorized-dominant location with moderate density and one non-motorized-dominant location with higher density. Errors are statistically insignificant for both cases, showing correlations from 0.99 to 0.88 and 0.91 to 0.97 for heterogeneous traffic flow and speed measurements, respectively.
<div id='section'>PaperID: <span id='pid'>425, <a href='https://arxiv.org/pdf/2411.03702.pdf' target='_blank'>https://arxiv.org/pdf/2411.03702.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Depanshu Sani, Saket Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03702">Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for robust scene understanding in mobile robotics and autonomous driving has highlighted the importance of integrating multiple sensing modalities. By combining data from diverse sensors like cameras and LIDARs, fusion techniques can overcome the limitations of individual sensors, enabling a more complete and accurate perception of the environment. We introduce a novel approach to multi-modal sensor fusion, focusing on developing a graph-based state representation that supports critical decision-making processes in autonomous driving. We present a Sensor-Agnostic Graph-Aware Kalman Filter [3], the first online state estimation technique designed to fuse multi-modal graphs derived from noisy multi-sensor data. The estimated graph-based state representations serve as a foundation for advanced applications like Multi-Object Tracking (MOT), offering a comprehensive framework for enhancing the situational awareness and safety of autonomous systems. We validate the effectiveness of our proposed framework through extensive experiments conducted on both synthetic and real-world driving datasets (nuScenes). Our results showcase an improvement in MOTA and a reduction in estimated position errors (MOTP) and identity switches (IDS) for tracked objects using the SAGA-KF. Furthermore, we highlight the capability of such a framework to develop methods that can leverage heterogeneous information (like semantic objects and geometric structures) from various sensing modalities, enabling a more holistic approach to scene understanding and enhancing the safety and effectiveness of autonomous systems.
<div id='section'>PaperID: <span id='pid'>426, <a href='https://arxiv.org/pdf/2410.15428.pdf' target='_blank'>https://arxiv.org/pdf/2410.15428.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chung Shue Chen, Wing Shing Wong, Yuan-Hsun Lo, Tsai-Lien Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15428">Multiset Combinatorial Gray Codes with Application to Proximity Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate coding schemes that map source symbols into multisets of an alphabet set. Such a formulation of source coding is an alternative approach to the traditional framework and is inspired by an object tracking problem over proximity sensor networks. We define a \textit{multiset combinatorial Gray code} as a mulitset code with fixed multiset cardinality that possesses combinatorial Gray code characteristic. For source codes that are organized as a grid, namely an integer lattice, we propose a solution by first constructing a mapping from the grid to the alphabet set, the codes are then defined as the images of rectangular blocks in the grid of fixed dimensions. We refer to the mapping as a \textit{color mapping} and the code as a \textit{color multiset code}. We propose the idea of product multiset code that enables us to construct codes for high dimensional grids based on 1-dimensional (1D) grids. We provide a detailed analysis of color multiset codes on 1D grids, focusing on codes that require the minimal number of colors. To illustrate the application of such a coding scheme, we consider an object tracking problem on 2D grids and show its efficiency, which comes from exploiting transmission parallelism. Some numerical results are presented to conclude the paper.
<div id='section'>PaperID: <span id='pid'>427, <a href='https://arxiv.org/pdf/2409.07904.pdf' target='_blank'>https://arxiv.org/pdf/2409.07904.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rongzihan Song, Zhenyu Weng, Huiping Zhuang, Jinchang Ren, Yongming Chen, Zhiping Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07904">FACT: Feature Adaptive Continual-learning Tracker for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) involves identifying multiple targets and assigning them corresponding IDs within a video sequence, where occlusions are often encountered. Recent methods address occlusions using appearance cues through online learning techniques to improve adaptivity or offline learning techniques to utilize temporal information from videos. However, most existing online learning-based MOT methods are unable to learn from all past tracking information to improve adaptivity on long-term occlusions while maintaining real-time tracking speed. On the other hand, temporal information-based offline learning methods maintain a long-term memory to store past tracking information, but this approach restricts them to use only local past information during tracking. To address these challenges, we propose a new MOT framework called the Feature Adaptive Continual-learning Tracker (FACT), which enables real-time tracking and feature learning for targets by utilizing all past tracking information. We demonstrate that the framework can be integrated with various state-of-the-art feature-based trackers, thereby improving their tracking ability. Specifically, we develop the feature adaptive continual-learning (FAC) module, a neural network that can be trained online to learn features adaptively using all past tracking information during tracking. Moreover, we also introduce a two-stage association module specifically designed for the proposed continual learning-based tracking. Extensive experiment results demonstrate that the proposed method achieves state-of-the-art online tracking performance on MOT17 and MOT20 benchmarks. The code will be released upon acceptance.
<div id='section'>PaperID: <span id='pid'>428, <a href='https://arxiv.org/pdf/2602.14771.pdf' target='_blank'>https://arxiv.org/pdf/2602.14771.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.14771">GOT-JEPA: Generic Object Tracking with Model Adaptation and Occlusion Handling using Joint-Embedding Predictive Architecture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The human visual system tracks objects by integrating current observations with previously observed information, adapting to target and scene changes, and reasoning about occlusion at fine granularity. In contrast, recent generic object trackers are often optimized for training targets, which limits robustness and generalization in unseen scenarios, and their occlusion reasoning remains coarse, lacking detailed modeling of occlusion patterns. To address these limitations in generalization and occlusion perception, we propose GOT-JEPA, a model-predictive pretraining framework that extends JEPA from predicting image features to predicting tracking models. Given identical historical information, a teacher predictor generates pseudo-tracking models from a clean current frame, and a student predictor learns to predict the same pseudo-tracking models from a corrupted version of the current frame. This design provides stable pseudo supervision and explicitly trains the predictor to produce reliable tracking models under occlusions, distractors, and other adverse observations, improving generalization to dynamic environments. Building on GOT-JEPA, we further propose OccuSolver to enhance occlusion perception for object tracking. OccuSolver adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. Conditioned on object priors iteratively generated by the tracker, OccuSolver incrementally refines visibility states, strengthens occlusion handling, and produces higher-quality reference labels that progressively improve subsequent model predictions. Extensive evaluations on seven benchmarks show that our method effectively enhances tracker generalization and robustness.
<div id='section'>PaperID: <span id='pid'>429, <a href='https://arxiv.org/pdf/2602.12983.pdf' target='_blank'>https://arxiv.org/pdf/2602.12983.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alejandro Monroy Muñoz, Rajeev Verma, Alexander Timans
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.12983">Detecting Object Tracking Failure via Sequential Hypothesis Testing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.
<div id='section'>PaperID: <span id='pid'>430, <a href='https://arxiv.org/pdf/2601.11508.pdf' target='_blank'>https://arxiv.org/pdf/2601.11508.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Emily Steiner, Jianhao Zheng, Henry Howard-Jenkins, Chris Xie, Iro Armeni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.11508">ReScene4D: Temporally Consistent Semantic Instance Segmentation of Evolving Indoor 3D Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Indoor environments evolve as objects move, appear, or disappear. Capturing these dynamics requires maintaining temporally consistent instance identities across intermittently captured 3D scans, even when changes are unobserved. We introduce and formalize the task of temporally sparse 4D indoor semantic instance segmentation (SIS), which jointly segments, identifies, and temporally associates object instances. This setting poses a challenge for existing 3DSIS methods, which require a discrete matching step due to their lack of temporal reasoning, and for 4D LiDAR approaches, which perform poorly due to their reliance on high-frequency temporal measurements that are uncommon in the longer-horizon evolution of indoor environments. We propose ReScene4D, a novel method that adapts 3DSIS architectures for 4DSIS without needing dense observations. It explores strategies to share information across observations, demonstrating that this shared context not only enables consistent instance tracking but also improves standard 3DSIS quality. To evaluate this task, we define a new metric, t-mAP, that extends mAP to reward temporal identity consistency. ReScene4D achieves state-of-the-art performance on the 3RScan dataset, establishing a new benchmark for understanding evolving indoor scenes.
<div id='section'>PaperID: <span id='pid'>431, <a href='https://arxiv.org/pdf/2512.17939.pdf' target='_blank'>https://arxiv.org/pdf/2512.17939.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuncheng Lu, Yucen Shi, Aobo Li, Zehao Li, Junying Li, Bo Wang, Tony Tae-Hyoung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17939">A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.
<div id='section'>PaperID: <span id='pid'>432, <a href='https://arxiv.org/pdf/2512.14998.pdf' target='_blank'>https://arxiv.org/pdf/2512.14998.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sibi Parivendan, Kashfia Sailunaz, Suresh Neethirajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14998">Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.
<div id='section'>PaperID: <span id='pid'>433, <a href='https://arxiv.org/pdf/2511.20418.pdf' target='_blank'>https://arxiv.org/pdf/2511.20418.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matvei Shelukhan, Timur Mamedov, Karina Kvanchiani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20418">StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.
<div id='section'>PaperID: <span id='pid'>434, <a href='https://arxiv.org/pdf/2511.20294.pdf' target='_blank'>https://arxiv.org/pdf/2511.20294.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dnyandeep Mandaokar, Bernhard Rinner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20294">SAFE-IMM: Robust and Lightweight Radar-Based Object Tracking on Mobile Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking maneuvering targets requires estimators that are both responsive and robust. Interacting Multiple Model (IMM) filters are a standard tracking approach, but fusing models via Gaussian mixtures can lag during maneuvers. Recent winnertakes-all (WTA) approaches react quickly but may produce discontinuities. We propose SAFE-IMM, a lightweight IMM variant for tracking on mobile and resource-limited platforms with a safe covariance-aware gate that permits WTA only when the implied jump from the mixture to the winner is provably bounded. In simulations and on nuScenes front-radar data, SAFE-IMM achieves high accuracy at real-time rates, reducing ID switches while maintaining competitive performance. The method is simple to integrate, numerically stable, and clutter-robust, offering a practical balance between responsiveness and smoothness.
<div id='section'>PaperID: <span id='pid'>435, <a href='https://arxiv.org/pdf/2511.19936.pdf' target='_blank'>https://arxiv.org/pdf/2511.19936.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Youngseo Kim, Dohyun Kim, Geonhee Han, Paul Hongsuck Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19936">Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image diffusion models, though originally developed for image generation, implicitly capture rich semantic structures that enable various recognition and localization tasks beyond synthesis. In this work, we investigate their self-attention maps can be reinterpreted as semantic label propagation kernels, providing robust pixel-level correspondences between relevant image regions. Extending this mechanism across frames yields a temporal propagation kernel that enables zero-shot object tracking via segmentation in videos. We further demonstrate the effectiveness of test-time optimization strategies-DDIM inversion, textual inversion, and adaptive head weighting-in adapting diffusion features for robust and consistent label propagation. Building on these findings, we introduce DRIFT, a framework for object tracking in videos leveraging a pretrained image diffusion model with SAM-guided mask refinement, achieving state-of-the-art zero-shot performance on standard video object segmentation benchmarks.
<div id='section'>PaperID: <span id='pid'>436, <a href='https://arxiv.org/pdf/2511.13904.pdf' target='_blank'>https://arxiv.org/pdf/2511.13904.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yuqiang Lin, Sam Lockyer, Florian Stanek, Markus Zarbock, Adrian Evans, Wenbin Li, Nic Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13904">SAE-MCVT: A Real-Time and Scalable Multi-Camera Vehicle Tracking Framework Powered by Edge Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In modern Intelligent Transportation Systems (ITS), cameras are a key component due to their ability to provide valuable information for multiple stakeholders. A central task is Multi-Camera Vehicle Tracking (MCVT), which generates vehicle trajectories and enables applications such as anomaly detection, traffic density estimation, and suspect vehicle tracking. However, most existing studies on MCVT emphasize accuracy while overlooking real-time performance and scalability. These two aspects are essential for real-world deployment and become increasingly challenging in city-scale applications as the number of cameras grows. To address this issue, we propose SAE-MCVT, the first scalable real-time MCVT framework. The system includes several edge devices that interact with one central workstation separately. On the edge side, live RTSP video streams are serialized and processed through modules including object detection, object tracking, geo-mapping, and feature extraction. Only lightweight metadata -- vehicle locations and deep appearance features -- are transmitted to the central workstation. On the central side, cross-camera association is calculated under the constraint of spatial-temporal relations between adjacent cameras, which are learned through a self-supervised camera link model. Experiments on the RoundaboutHD dataset show that SAE-MCVT maintains real-time operation on 2K 15 FPS video streams and achieves an IDF1 score of 61.2. To the best of our knowledge, this is the first scalable real-time MCVT framework suitable for city-scale deployment.
<div id='section'>PaperID: <span id='pid'>437, <a href='https://arxiv.org/pdf/2507.16639.pdf' target='_blank'>https://arxiv.org/pdf/2507.16639.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jonathan Henrich, Christian Post, Maximilian Zilke, Parth Shiroya, Emma Chanut, Amir Mollazadeh Yamchi, Ramin Yahyapour, Thomas Kneib, Imke Traulsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16639">Benchmarking pig detection and tracking under diverse and challenging conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To ensure animal welfare and effective management in pig farming, monitoring individual behavior is a crucial prerequisite. While monitoring tasks have traditionally been carried out manually, advances in machine learning have made it possible to collect individualized information in an increasingly automated way. Central to these methods is the localization of animals across space (object detection) and time (multi-object tracking). Despite extensive research of these two tasks in pig farming, a systematic benchmarking study has not yet been conducted. In this work, we address this gap by curating two datasets: PigDetect for object detection and PigTrack for multi-object tracking. The datasets are based on diverse image and video material from realistic barn conditions, and include challenging scenarios such as occlusions or bad visibility. For object detection, we show that challenging training images improve detection performance beyond what is achievable with randomly sampled images alone. Comparing different approaches, we found that state-of-the-art models offer substantial improvements in detection quality over real-time alternatives. For multi-object tracking, we observed that SORT-based methods achieve superior detection performance compared to end-to-end trainable models. However, end-to-end models show better association performance, suggesting they could become strong alternatives in the future. We also investigate characteristic failure cases of end-to-end models, providing guidance for future improvements. The detection and tracking models trained on our datasets perform well in unseen pens, suggesting good generalization capabilities. This highlights the importance of high-quality training data. The datasets and research code are made publicly available to facilitate reproducibility, re-use and further development.
<div id='section'>PaperID: <span id='pid'>438, <a href='https://arxiv.org/pdf/2506.19341.pdf' target='_blank'>https://arxiv.org/pdf/2506.19341.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhongping Dong, Liming Chen, Mohand Tahar Kechadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19341">Trajectory Prediction in Dynamic Object Tracking: A Critical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study provides a detailed analysis of current advancements in dynamic object tracking (DOT) and trajectory prediction (TP) methodologies, including their applications and challenges. It covers various approaches, such as feature-based, segmentation-based, estimation-based, and learning-based methods, evaluating their effectiveness, deployment, and limitations in real-world scenarios. The study highlights the significant impact of these technologies in automotive and autonomous vehicles, surveillance and security, healthcare, and industrial automation, contributing to safety and efficiency. Despite the progress, challenges such as improved generalization, computational efficiency, reduced data dependency, and ethical considerations still exist. The study suggests future research directions to address these challenges, emphasizing the importance of multimodal data integration, semantic information fusion, and developing context-aware systems, along with ethical and privacy-preserving frameworks.
<div id='section'>PaperID: <span id='pid'>439, <a href='https://arxiv.org/pdf/2506.19154.pdf' target='_blank'>https://arxiv.org/pdf/2506.19154.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mahdi Falaki, Maria A. Amer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19154">Lightweight RGB-T Tracking with Mobile Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-modality object tracking (e.g., RGB-only) encounters difficulties in challenging imaging conditions, such as low illumination and adverse weather conditions. To solve this, multimodal tracking (e.g., RGB-T models) aims to leverage complementary data such as thermal infrared features. While recent Vision Transformer-based multimodal trackers achieve strong performance, they are often computationally expensive due to large model sizes. In this work, we propose a novel lightweight RGB-T tracking algorithm based on Mobile Vision Transformers (MobileViT). Our tracker introduces a progressive fusion framework that jointly learns intra-modal and inter-modal interactions between the template and search regions using separable attention. This design produces effective feature representations that support more accurate target localization while achieving a small model size and fast inference speed. Compared to state-of-the-art efficient multimodal trackers, our model achieves comparable accuracy while offering significantly lower parameter counts (less than 4 million) and the fastest GPU inference speed of 122 frames per second. This paper is the first to propose a tracker using Mobile Vision Transformers for RGB-T tracking and multimodal tracking at large. Tracker code and model weights will be made publicly available upon acceptance.
<div id='section'>PaperID: <span id='pid'>440, <a href='https://arxiv.org/pdf/2505.18795.pdf' target='_blank'>https://arxiv.org/pdf/2505.18795.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Qing Li, Runze Gan, James R. Hopgood, Michael E. Davies, Simon J. Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18795">Distributed Expectation Propagation for Multi-Object Tracking over Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel distributed expectation propagation algorithm for multiple sensors, multiple objects tracking in cluttered environments. The proposed framework enables each sensor to operate locally while collaboratively exchanging moment estimates with other sensors, thus eliminating the need to transmit all data to a central processing node. Specifically, we introduce a fast and parallelisable Rao-Blackwellised Gibbs sampling scheme to approximate the tilted distributions, which enhances the accuracy and efficiency of expectation propagation updates. Results demonstrate that the proposed algorithm improves both communication and inference efficiency for multi-object tracking tasks with dynamic sensor connectivity and varying clutter levels.
<div id='section'>PaperID: <span id='pid'>441, <a href='https://arxiv.org/pdf/2505.16029.pdf' target='_blank'>https://arxiv.org/pdf/2505.16029.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shichao Li, Peiliang Li, Qing Lian, Peng Yun, Xiaozhi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16029">Learning better representations for crowded pedestrians in offboard LiDAR-camera 3D tracking-by-detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving pedestrians in highly crowded urban environments is a difficult long-tail problem for learning-based autonomous perception. Speeding up 3D ground truth generation for such challenging scenes is performance-critical yet very challenging. The difficulties include the sparsity of the captured pedestrian point cloud and a lack of suitable benchmarks for a specific system design study. To tackle the challenges, we first collect a new multi-view LiDAR-camera 3D multiple-object-tracking benchmark of highly crowded pedestrians for in-depth analysis. We then build an offboard auto-labeling system that reconstructs pedestrian trajectories from LiDAR point cloud and multi-view images. To improve the generalization power for crowded scenes and the performance for small objects, we propose to learn high-resolution representations that are density-aware and relationship-aware. Extensive experiments validate that our approach significantly improves the 3D pedestrian tracking performance towards higher auto-labeling efficiency. The code will be publicly available at this HTTP URL.
<div id='section'>PaperID: <span id='pid'>442, <a href='https://arxiv.org/pdf/2504.12506.pdf' target='_blank'>https://arxiv.org/pdf/2504.12506.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Victor Nan Fernandez-Ayala, Jorge Silva, Meng Guo, Dimos V. Dimarogonas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12506">Robust Visual Servoing under Human Supervision for Assembly Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a framework enabling mobile manipulators to reliably complete pick-and-place tasks for assembling structures from construction blocks. The picking uses an eye-in-hand visual servoing controller for object tracking with Control Barrier Functions (CBFs) to ensure fiducial markers in the blocks remain visible. An additional robot with an eye-to-hand setup ensures precise placement, critical for structural stability. We integrate human-in-the-loop capabilities for flexibility and fault correction and analyze robustness to camera pose errors, proposing adapted barrier functions to handle them. Lastly, experiments validate the framework on 6-DoF mobile arms.
<div id='section'>PaperID: <span id='pid'>443, <a href='https://arxiv.org/pdf/2504.09328.pdf' target='_blank'>https://arxiv.org/pdf/2504.09328.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sonia Laguna, Alberto Garcia-Garcia, Marie-Julie Rakotosaona, Stylianos Moschoglou, Leonhard Helminger, Sergio Orts-Escolano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09328">Text To 3D Object Generation For Scalable Room Assembly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications.
<div id='section'>PaperID: <span id='pid'>444, <a href='https://arxiv.org/pdf/2504.07163.pdf' target='_blank'>https://arxiv.org/pdf/2504.07163.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jordi Serra, Anton Aguilar, Ebrahim Abu-Helalah, RaÃºl Parada, Paolo Dini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07163">Multi-Object Tracking for Collision Avoidance Using Multiple Cameras in Open RAN Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper deals with the multi-object detection and tracking problem, within the scope of open Radio Access Network (RAN), for collision avoidance in vehicular scenarios. To this end, a set of distributed intelligent agents collocated with cameras are considered. The fusion of detected objects is done at an edge service, considering Open RAN connectivity. Then, the edge service predicts the objects trajectories for collision avoidance. Compared to the related work a more realistic Open RAN network is implemented and multiple cameras are used.
<div id='section'>PaperID: <span id='pid'>445, <a href='https://arxiv.org/pdf/2504.01457.pdf' target='_blank'>https://arxiv.org/pdf/2504.01457.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ting Meng, Chunyun Fu, Xiangyan Yan, Zheng Liang, Pan Ji, Jianwen Wang, Tao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01457">Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking plays a crucial role in various applications, such as autonomous driving and security surveillance. This study introduces Deep LG-Track, a novel multi-object tracker that incorporates three key enhancements to improve the tracking accuracy and robustness. First, an adaptive Kalman filter is developed to dynamically update the covariance of measurement noise based on detection confidence and trajectory disappearance. Second, a novel cost matrix is formulated to adaptively fuse motion and appearance information, leveraging localization confidence and detection confidence as weighting factors. Third, a dynamic appearance feature updating strategy is introduced, adjusting the relative weighting of historical and current appearance features based on appearance clarity and localization accuracy. Comprehensive evaluations on the MOT17 and MOT20 datasets demonstrate that the proposed Deep LG-Track consistently outperforms state-of-the-art trackers across multiple performance metrics, highlighting its effectiveness in multi-object tracking tasks.
<div id='section'>PaperID: <span id='pid'>446, <a href='https://arxiv.org/pdf/2412.08313.pdf' target='_blank'>https://arxiv.org/pdf/2412.08313.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Gergely SzabÃ³, ZsÃ³fia MolnÃ¡r, AndrÃ¡s HorvÃ¡th
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08313">Post-Hoc MOTS: Exploring the Capabilities of Time-Symmetric Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal forward-tracking has been the dominant approach for multi-object segmentation and tracking (MOTS). However, a novel time-symmetric tracking methodology has recently been introduced for the detection, segmentation, and tracking of budding yeast cells in pre-recorded samples. Although this architecture has demonstrated a unique perspective on stable and consistent tracking, as well as missed instance re-interpolation, its evaluation has so far been largely confined to settings related to videomicroscopic environments. In this work, we aim to reveal the broader capabilities, advantages, and potential challenges of this architecture across various specifically designed scenarios, including a pedestrian tracking dataset. We also conduct an ablation study comparing the model against its restricted variants and the widely used Kalman filter. Furthermore, we present an attention analysis of the tracking architecture for both pretrained and non-pretrained models
<div id='section'>PaperID: <span id='pid'>447, <a href='https://arxiv.org/pdf/2412.01041.pdf' target='_blank'>https://arxiv.org/pdf/2412.01041.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Susu Fang, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01041">LiDAR SLAMMOT based on Confidence-guided Data Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of autonomous driving or robotics, simultaneous localization and mapping (SLAM) and multi-object tracking (MOT) are two fundamental problems and are generally applied separately. Solutions to SLAM and MOT usually rely on certain assumptions, such as the static environment assumption for SLAM and the accurate ego-vehicle pose assumption for MOT. But in complex dynamic environments, it is difficult or even impossible to meet these assumptions. Therefore, the SLAMMOT, i.e., simultaneous localization, mapping, and moving object tracking, integrated system of SLAM and object tracking, has emerged for autonomous vehicles in dynamic environments. However, many conventional SLAMMOT solutions directly perform data association on the predictions and detections for object tracking, but ignore their quality. In practice, inaccurate predictions caused by continuous multi-frame missed detections in temporary occlusion scenarios, may degrade the performance of tracking, thereby affecting SLAMMOT. To address this challenge, this paper presents a LiDAR SLAMMOT based on confidence-guided data association (Conf SLAMMOT) method, which tightly couples the LiDAR SLAM and the confidence-guided data association based multi-object tracking into a graph optimization backend for estimating the state of the ego-vehicle and objects simultaneously. The confidence of prediction and detection are applied in the factor graph-based multi-object tracking for its data association, which not only avoids the performance degradation caused by incorrect initial assignments in some filter-based methods but also handles issues such as continuous missed detection in tracking while also improving the overall performance of SLAMMOT. Various comparative experiments demonstrate the superior advantages of Conf SLAMMOT, especially in scenes with some missed detections.
<div id='section'>PaperID: <span id='pid'>448, <a href='https://arxiv.org/pdf/2411.10072.pdf' target='_blank'>https://arxiv.org/pdf/2411.10072.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ishrath Ahamed, Chamith Dilshan Ranathunga, Dinuka Sandun Udayantha, Benny Kai Kiat Ng, Chau Yuen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10072">Real-Time AI-Driven People Tracking and Counting Using Overhead Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate people counting in smart buildings and intelligent transportation systems is crucial for energy management, safety protocols, and resource allocation. This is especially critical during emergencies, where precise occupant counts are vital for safe evacuation. Existing methods struggle with large crowds, often losing accuracy with even a few additional people. To address this limitation, this study proposes a novel approach combining a new object tracking algorithm, a novel counting algorithm, and a fine-tuned object detection model. This method achieves 97% accuracy in real-time people counting with a frame rate of 20-27 FPS on a low-power edge computer.
<div id='section'>PaperID: <span id='pid'>449, <a href='https://arxiv.org/pdf/2410.13240.pdf' target='_blank'>https://arxiv.org/pdf/2410.13240.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanpeng Jia, Ting Wang, Xieyuanli Chen, Shiliang Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13240">TRLO: An Efficient LiDAR Odometry with 3D Dynamic Object Tracking and Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous state estimation and mapping is an essential capability for mobile robots working in dynamic urban environment. The majority of existing SLAM solutions heavily rely on a primarily static assumption. However, due to the presence of moving vehicles and pedestrians, this assumption does not always hold, leading to localization accuracy decreased and maps distorted. To address this challenge, we propose TRLO, a dynamic LiDAR odometry that efficiently improves the accuracy of state estimation and generates a cleaner point cloud map. To efficiently detect dynamic objects in the surrounding environment, a deep learning-based method is applied, generating detection bounding boxes. We then design a 3D multi-object tracker based on Unscented Kalman Filter (UKF) and nearest neighbor (NN) strategy to reliably identify and remove dynamic objects. Subsequently, a fast two-stage iterative nearest point solver is employed to solve the state estimation using cleaned static point cloud. Note that a novel hash-based keyframe database management is proposed for fast access to search keyframes. Furthermore, all the detected object bounding boxes are leveraged to impose posture consistency constraint to further refine the final state estimation. Extensive evaluations and ablation studies conducted on the KITTI and UrbanLoco datasets demonstrate that our approach not only achieves more accurate state estimation but also generates cleaner maps, compared with baselines.
<div id='section'>PaperID: <span id='pid'>450, <a href='https://arxiv.org/pdf/2409.19891.pdf' target='_blank'>https://arxiv.org/pdf/2409.19891.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Matthew Ishige, Yasuhiro Yoshimura, Ryo Yonetani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19891">Opt-in Camera: Person Identification in Video via UWB Localization and Its Application to Opt-in Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents opt-in camera, a concept of privacy-preserving camera systems capable of recording only specific individuals in a crowd who explicitly consent to be recorded. Our system utilizes a mobile wireless communication tag attached to personal belongings as proof of opt-in and as a means of localizing tag carriers in video footage. Specifically, the on-ground positions of the wireless tag are first tracked over time using the unscented Kalman filter (UKF). The tag trajectory is then matched against visual tracking results for pedestrians found in videos to identify the tag carrier. Technically, we devise a dedicated trajectory matching technique based on constrained linear optimization, as well as a novel calibration technique that handles wireless tag-camera calibration and hyperparameter tuning for the UKF, which mitigates the non-line-of-sight (NLoS) issue in wireless localization. We implemented the proposed opt-in camera system using ultra-wideband (UWB) devices and an off-the-shelf webcam. Experimental results demonstrate that our system can perform opt-in recording of individuals in real-time at 10 fps, with reliable identification accuracy in crowds of 8-23 people in a confined space.
<div id='section'>PaperID: <span id='pid'>451, <a href='https://arxiv.org/pdf/2409.18901.pdf' target='_blank'>https://arxiv.org/pdf/2409.18901.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18901">Improving Visual Object Tracking through Visual Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a discriminative model to distinguish a target from its surrounding distractors is essential to generic visual object tracking. Dynamic target representation adaptation against distractors is challenging due to the limited discriminative capabilities of prevailing trackers. We present a new visual Prompting mechanism for generic Visual Object Tracking (PiVOT) to address this issue. PiVOT proposes a prompt generation network with the pre-trained foundation model CLIP to automatically generate and refine visual prompts, enabling the transfer of foundation model knowledge for tracking. While CLIP offers broad category-level knowledge, the tracker, trained on instance-specific data, excels at recognizing unique object instances. Thus, PiVOT first compiles a visual prompt highlighting potential target locations. To transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP to refine the visual prompt based on the similarities between candidate objects and the reference templates across potential targets. Once the visual prompt is refined, it can better highlight potential target locations, thereby reducing irrelevant prompt information. With the proposed prompting mechanism, the tracker can generate improved instance-aware feature maps through the guidance of the visual prompt, thus effectively reducing distractors. The proposed method does not involve CLIP during training, thereby keeping the same training complexity and preserving the generalization capability of the pretrained foundation model. Extensive experiments across multiple benchmarks indicate that PiVOT, using the proposed prompting method can suppress distracting objects and enhance the tracker.
<div id='section'>PaperID: <span id='pid'>452, <a href='https://arxiv.org/pdf/2602.16669.pdf' target='_blank'>https://arxiv.org/pdf/2602.16669.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bo Lang, Nirav Savaliya, Zhihao Zheng, Jinglun Feng, Zheng-Hang Yeh, Mooi Choo Chuah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2602.16669">PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.
<div id='section'>PaperID: <span id='pid'>453, <a href='https://arxiv.org/pdf/2512.20975.pdf' target='_blank'>https://arxiv.org/pdf/2512.20975.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yujin Noh, Inho Jake Park, Chigon Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20975">SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.
<div id='section'>PaperID: <span id='pid'>454, <a href='https://arxiv.org/pdf/2512.01934.pdf' target='_blank'>https://arxiv.org/pdf/2512.01934.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Chenyi Wang, Yanmao Man, Raymond Muller, Ming Li, Z. Berkay Celik, Ryan Gerdes, Jonathan Petit
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01934">Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.
<div id='section'>PaperID: <span id='pid'>455, <a href='https://arxiv.org/pdf/2511.23405.pdf' target='_blank'>https://arxiv.org/pdf/2511.23405.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Suhas Srinath, Hemang Jamadagni, Aditya Chadrasekar, Prathosh AP
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23405">MANTA: Physics-Informed Generalized Underwater Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater object tracking is challenging due to wavelength dependent attenuation and scattering, which severely distort appearance across depths and water conditions. Existing trackers trained on terrestrial data fail to generalize to these physics-driven degradations. We present MANTA, a physics-informed framework integrating representation learning with tracking design for underwater scenarios. We propose a dual-positive contrastive learning strategy coupling temporal consistency with Beer-Lambert augmentations to yield features robust to both temporal and underwater distortions. We further introduce a multi-stage pipeline augmenting motion-based tracking with a physics-informed secondary association algorithm that integrates geometric consistency and appearance similarity for re-identification under occlusion and drift. To complement standard IoU metrics, we propose Center-Scale Consistency (CSC) and Geometric Alignment Score (GAS) to assess geometric fidelity. Experiments on four underwater benchmarks (WebUOT-1M, UOT32, UTB180, UWCOT220) show that MANTA achieves state-of-the-art performance, improving Success AUC by up to 6 percent, while ensuring stable long-term generalized underwater tracking and efficient runtime.
<div id='section'>PaperID: <span id='pid'>456, <a href='https://arxiv.org/pdf/2511.19396.pdf' target='_blank'>https://arxiv.org/pdf/2511.19396.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jorge Ortigoso-Narro, Jose A. Belloch, Adrian Amor-Martin, Sandra Roger, Maximo Cobos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19396">Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.
<div id='section'>PaperID: <span id='pid'>457, <a href='https://arxiv.org/pdf/2511.17508.pdf' target='_blank'>https://arxiv.org/pdf/2511.17508.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alice Smith, Bob Johnson, Xiaoyu Zhu, Carol Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17508">Deep Learning-based Lightweight RGB Object Tracking for Augmented Reality Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented Reality (AR) applications often require robust real-time tracking of objects in the user's environment to correctly overlay virtual content. Recent advances in computer vision have produced highly accurate deep learning-based object trackers, but these models are typically too heavy in computation and memory for wearable AR devices. In this paper, we present a lightweight RGB object tracking algorithm designed specifically for resource-constrained AR platforms. The proposed tracker employs a compact Siamese neural network architecture and incorporates optimization techniques such as model pruning, quantization, and knowledge distillation to drastically reduce model size and inference cost while maintaining high tracking accuracy. We train the tracker offline on large video datasets using deep convolutional neural networks and then deploy it on-device for real-time tracking. Experimental results on standard tracking benchmarks show that our approach achieves comparable accuracy to state-of-the-art trackers, yet runs in real-time on a mobile AR headset at around 30 FPS -- more than an order of magnitude faster than prior high-performance trackers on the same hardware. This work enables practical, robust object tracking for AR use-cases, opening the door to more interactive and dynamic AR experiences on lightweight devices.
<div id='section'>PaperID: <span id='pid'>458, <a href='https://arxiv.org/pdf/2511.10442.pdf' target='_blank'>https://arxiv.org/pdf/2511.10442.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aarush Agarwal, Raymond He, Jan Kieseler, Matteo Cremonesi, Shah Rukh Qasim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10442">FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.
<div id='section'>PaperID: <span id='pid'>459, <a href='https://arxiv.org/pdf/2511.09455.pdf' target='_blank'>https://arxiv.org/pdf/2511.09455.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Rintaro Otsubo, Kanta Sawafuji, Hideo Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09455">Hand Held Multi-Object Tracking Dataset in American Football</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) plays a critical role in analyzing player behavior from videos, enabling performance evaluation. Current MOT methods are often evaluated using publicly available datasets. However, most of these focus on everyday scenarios such as pedestrian tracking or are tailored to specific sports, including soccer and basketball. Despite the inherent challenges of tracking players in American football, such as frequent occlusion and physical contact, no standardized dataset has been publicly available, making fair comparisons between methods difficult. To address this gap, we constructed the first dedicated detection and tracking dataset for the American football players and conducted a comparative evaluation of various detection and tracking methods. Our results demonstrate that accurate detection and tracking can be achieved even in crowded scenarios. Fine-tuning detection models improved performance over pre-trained models. Furthermore, when these fine-tuned detectors and re-identification models were integrated into tracking systems, we observed notable improvements in tracking accuracy compared to existing approaches. This work thus enables robust detection and tracking of American football players in challenging, high-density scenarios previously underserved by conventional methods.
<div id='section'>PaperID: <span id='pid'>460, <a href='https://arxiv.org/pdf/2510.21482.pdf' target='_blank'>https://arxiv.org/pdf/2510.21482.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Marek Socha, Michał Marczyk, Aleksander Kempski, Michał Cogiel, Paweł Foszner, Radosław Zawiski, Michał Staniszewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21482">GRAP-MOT: Unsupervised Graph-based Position Weighted Person Multi-camera Multi-object Tracking in a Highly Congested Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>GRAP-MOT is a new approach for solving the person MOT problem dedicated to videos of closed areas with overlapping multi-camera views, where person occlusion frequently occurs. Our novel graph-weighted solution updates a person's identification label online based on tracks and the person's characteristic features. To find the best solution, we deeply investigated all elements of the MOT process, including feature extraction, tracking, and community search. Furthermore, GRAP-MOT is equipped with a person's position estimation module, which gives additional key information to the MOT method, ensuring better results than methods without position data. We tested GRAP-MOT on recordings acquired in a closed-area model and on publicly available real datasets that fulfil the requirement of a highly congested space, showing the superiority of our proposition. Finally, we analyzed existing metrics used to compare MOT algorithms and concluded that IDF1 is more adequate than MOTA in such comparisons. We made our code, along with the acquired dataset, publicly available.
<div id='section'>PaperID: <span id='pid'>461, <a href='https://arxiv.org/pdf/2508.09585.pdf' target='_blank'>https://arxiv.org/pdf/2508.09585.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Stefan Haag, Bharanidhar Duraisamy, Felix Govaers, Wolfgang Koch, Martin Fritzsche, Juergen Dickmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09585">Offline Auto Labeling: BAAS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces BAAS, a new Extended Object Tracking (EOT) and fusion-based label annotation framework for radar detections in autonomous driving. Our framework utilizes Bayesian-based tracking, smoothing and eventually fusion methods to provide veritable and precise object trajectories along with shape estimation to provide annotation labels on the detection level under various supervision levels. Simultaneously, the framework provides evaluation of tracking performance and label annotation. If manually labeled data is available, each processing module can be analyzed independently or combined with other modules to enable closed-loop continuous improvements. The framework performance is evaluated in a challenging urban real-world scenario in terms of tracking performance and the label annotation errors. We demonstrate the functionality of the proposed approach for varying dynamic objects and class types
<div id='section'>PaperID: <span id='pid'>462, <a href='https://arxiv.org/pdf/2508.05514.pdf' target='_blank'>https://arxiv.org/pdf/2508.05514.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zewei Wu, CÃ©sar Teixeira, Wei Ke, Zhang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05514">Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual pedestrian tracking represents a promising research field, with extensive applications in intelligent surveillance, behavior analysis, and human-computer interaction. However, real-world applications face significant occlusion challenges. When multiple pedestrians interact or overlap, the loss of target features severely compromises the tracker's ability to maintain stable trajectories. Traditional tracking methods, which typically rely on full-body bounding box features extracted from {Re-ID} models and linear constant-velocity motion assumptions, often struggle in severe occlusion scenarios. To address these limitations, this work proposes an enhanced tracking framework that leverages richer feature representations and a more robust motion model. Specifically, the proposed method incorporates detection features from both the regression and classification branches of an object detector, embedding spatial and positional information directly into the feature representations. To further mitigate occlusion challenges, a head keypoint detection model is introduced, as the head is less prone to occlusion compared to the full body. In terms of motion modeling, we propose an iterative Kalman filtering approach designed to align with modern detector assumptions, integrating 3D priors to better complete motion trajectories in complex scenes. By combining these advancements in appearance and motion modeling, the proposed method offers a more robust solution for multi-object tracking in crowded environments where occlusions are prevalent.
<div id='section'>PaperID: <span id='pid'>463, <a href='https://arxiv.org/pdf/2508.05172.pdf' target='_blank'>https://arxiv.org/pdf/2508.05172.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zewei Wu, Longhao Wang, Cui Wang, CÃ©sar Teixeira, Wei Ke, Zhang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05172">Multi-tracklet Tracking for Generic Targets with Adaptive Detection Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking specific targets, such as pedestrians and vehicles, has been the focus of recent vision-based multitarget tracking studies. However, in some real-world scenarios, unseen categories often challenge existing methods due to low-confidence detections, weak motion and appearance constraints, and long-term occlusions. To address these issues, this article proposes a tracklet-enhanced tracker called Multi-Tracklet Tracking (MTT) that integrates flexible tracklet generation into a multi-tracklet association framework. This framework first adaptively clusters the detection results according to their short-term spatio-temporal correlation into robust tracklets and then estimates the best tracklet partitions using multiple clues, such as location and appearance over time to mitigate error propagation in long-term association. Finally, extensive experiments on the benchmark for generic multiple object tracking demonstrate the competitiveness of the proposed framework.
<div id='section'>PaperID: <span id='pid'>464, <a href='https://arxiv.org/pdf/2508.02067.pdf' target='_blank'>https://arxiv.org/pdf/2508.02067.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Manikanta Kotthapalli, Deepika Ravipati, Reshma Bhatia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02067">YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, object detection has advanced significantly, with the YOLO (You Only Look Once) family of models transforming the landscape of real-time vision applications through unified, end-to-end detection frameworks. From YOLOv1's pioneering regression-based detection to the latest YOLOv9, each version has systematically enhanced the balance between speed, accuracy, and deployment efficiency through continuous architectural and algorithmic advancements.. Beyond core object detection, modern YOLO architectures have expanded to support tasks such as instance segmentation, pose estimation, object tracking, and domain-specific applications including medical imaging and industrial automation. This paper offers a comprehensive review of the YOLO family, highlighting architectural innovations, performance benchmarks, extended capabilities, and real-world use cases. We critically analyze the evolution of YOLO models and discuss emerging research directions that extend their impact across diverse computer vision domains.
<div id='section'>PaperID: <span id='pid'>465, <a href='https://arxiv.org/pdf/2507.08548.pdf' target='_blank'>https://arxiv.org/pdf/2507.08548.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Alen Adamyan, TomÃ¡Å¡ ÄÃ­Å¾ek, Matej Straka, Klara Janouskova, Martin Schmid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08548">SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks and has become the state-of-the-art for visual object tracking. The model stores information from previous frames in a memory bank, enabling temporal consistency across video sequences. Recent methods augment SAM 2 with hand-crafted update rules to better handle distractors, occlusions, and object motion. We propose a fundamentally different approach using reinforcement learning for optimizing memory updates in SAM 2 by framing memory control as a sequential decision-making problem. In an overfitting setup with a separate agent per video, our method achieves a relative improvement over SAM 2 that exceeds by more than three times the gains of existing heuristics. These results reveal the untapped potential of the memory bank and highlight reinforcement learning as a powerful alternative to hand-crafted update rules for memory control in visual object tracking.
<div id='section'>PaperID: <span id='pid'>466, <a href='https://arxiv.org/pdf/2507.04116.pdf' target='_blank'>https://arxiv.org/pdf/2507.04116.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Fred Lydeard, Bashar I. Ahmad, Simon Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04116">Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a computationally efficient multi-object tracking approach that can minimise track breaks (e.g., in challenging environments and against agile targets), learn the measurement model parameters on-line (e.g., in dynamically changing scenes) and infer the class of the tracked objects, if joint tracking and kinematic behaviour classification is sought. It capitalises on the flexibilities offered by the integrated Gaussian process as a motion model and the convenient statistical properties of non-homogeneous Poisson processes as a suitable observation model. This can be combined with the proposed effective track revival / stitching mechanism. We accordingly introduce the two robust and adaptive trackers, Gaussian and Poisson Process with Classification (GaPP-Class) and GaPP with Revival and Classification (GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme that efficiently integrates track management and hyperparameter learning (including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class with the addition of a Markov Chain Monte Carlo kernel applied to each particle permitting track revival and stitching (e.g., within a few time steps after deleting a trajectory). Performance evaluation and benchmarking using synthetic and real data show that GaPP-Class and GaPP-ReaCtion outperform other state-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly reduces track breaks (e.g., by around 30% from real radar data and markedly more from simulated data).
<div id='section'>PaperID: <span id='pid'>467, <a href='https://arxiv.org/pdf/2506.22562.pdf' target='_blank'>https://arxiv.org/pdf/2506.22562.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Abhineet Singh, Nilanjan Ray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22562">Improving Token-based Object Detection with Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper improves upon the Pix2Seq object detector by extending it for videos. In the process, it introduces a new way to perform end-to-end video object detection that improves upon existing video detectors in two key ways. First, by representing objects as variable-length sequences of discrete tokens, we can succinctly represent widely varying numbers of video objects, with diverse shapes and locations, without having to inject any localization cues in the training process. This eliminates the need to sample the space of all possible boxes that constrains conventional detectors and thus solves the dual problems of loss sparsity during training and heuristics-based postprocessing during inference. Second, it conceptualizes and outputs the video objects as fully integrated and indivisible 3D boxes or tracklets instead of generating image-specific 2D boxes and linking these boxes together to construct the video object, as done in most conventional detectors. This allows it to scale effortlessly with available computational resources by simply increasing the length of the video subsequence that the network takes as input, even generalizing to multi-object tracking if the subsequence can span the entire video. We compare our video detector with the baseline Pix2Seq static detector on several datasets and demonstrate consistent improvement, although with strong signs of being bottlenecked by our limited computational resources. We also compare it with several video detectors on UA-DETRAC to show that it is competitive with the current state of the art even with the computational bottleneck. We make our code and models publicly available.
<div id='section'>PaperID: <span id='pid'>468, <a href='https://arxiv.org/pdf/2505.07336.pdf' target='_blank'>https://arxiv.org/pdf/2505.07336.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhixuan Zhang, Xiaopeng Li, Qi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07336">SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.
<div id='section'>PaperID: <span id='pid'>469, <a href='https://arxiv.org/pdf/2505.04392.pdf' target='_blank'>https://arxiv.org/pdf/2505.04392.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Petr Jahoda, Jan Cech
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04392">Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. The method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. The method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. Anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. A challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. Therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. Our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. The method is effective and performs in real time on standard consumer hardware.
<div id='section'>PaperID: <span id='pid'>470, <a href='https://arxiv.org/pdf/2504.21692.pdf' target='_blank'>https://arxiv.org/pdf/2504.21692.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zihan Zhou, Changrui Dai, Aibo Song, Xiaolin Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21692">Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful video analysis relies on accurate recognition of pixels across frames, and frame reconstruction methods based on video correspondence learning are popular due to their efficiency. Existing frame reconstruction methods, while efficient, neglect the value of direct involvement of multiple reference frames for reconstruction and decision-making aspects, especially in complex situations such as occlusion or fast movement. In this paper, we introduce a Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple reference frames to concisely and directly enhance frame reconstruction. Its core component is a Reference Frame Memory Engine that dynamically selects frames based on object pixel features to improve tracking accuracy. In addition, a Bidirectional Target Prediction Network is built to utilize multiple reference frames to improve the robustness of the model. Through experiments, our algorithm outperforms the state-of-the-art self-supervised techniques on two fine-grained video object tracking tasks: object segmentation and keypoint tracking.
<div id='section'>PaperID: <span id='pid'>471, <a href='https://arxiv.org/pdf/2504.20234.pdf' target='_blank'>https://arxiv.org/pdf/2504.20234.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Bartosz Ptak, Marek Kraft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20234">Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone-based crowd monitoring is the key technology for applications in surveillance, public safety, and event management. However, maintaining tracking continuity and consistency remains a significant challenge. Traditional detection-assignment tracking methods struggle with false positives, false negatives, and frequent identity switches, leading to degraded counting accuracy and making in-depth analysis impossible. This paper introduces a point-oriented online tracking algorithm that improves trajectory continuity and counting reliability in drone-based crowd monitoring. Our method builds on the Simple Online and Real-time Tracking (SORT) framework, replacing the original bounding-box assignment with a point-distance metric. The algorithm is enhanced with three cost-effective techniques: camera motion compensation, altitude-aware assignment, and classification-based trajectory validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use spatial feature maps from localisation algorithms for increased computational efficiency through neural network resource sharing are integrated to refine object tracking by reducing noise and handling missed detections. The proposed method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets, demonstrating substantial improvements in tracking metrics, reducing counting errors to 23% and 15%, respectively. The results also indicate a significant reduction of identity switches while maintaining high tracking accuracy, outperforming baseline online trackers and even an offline greedy optimisation method.
<div id='section'>PaperID: <span id='pid'>472, <a href='https://arxiv.org/pdf/2504.19719.pdf' target='_blank'>https://arxiv.org/pdf/2504.19719.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lukas Folkman, Quynh LK Vo, Colin Johnston, Bela Stantic, Kylie A Pitt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19719">A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing demand for aquaculture production necessitates the development of innovative, intelligent tools to effectively monitor and manage fish health and welfare. While non-invasive video monitoring has become a common practice in finfish aquaculture, existing intelligent monitoring methods predominantly focus on assessing body condition or fish swimming patterns and are often developed and evaluated in controlled tank environments, without demonstrating their applicability to real-world aquaculture settings in open sea farms. This underscores the necessity for methods that can monitor physiological traits directly within the production environment of sea fish farms. To this end, we have developed a computer vision method for monitoring ventilation rates of Atlantic salmon (Salmo salar), which was specifically designed for videos recorded in the production environment of commercial sea fish farms using the existing infrastructure. Our approach uses a fish head detection model, which classifies the mouth state as either open or closed using a convolutional neural network. This is followed with multiple object tracking to create temporal sequences of fish swimming across the field of view of the underwater video camera to estimate ventilation rates. The method demonstrated high efficiency, achieving a Pearson correlation coefficient of 0.82 between ground truth and predicted ventilation rates in a test set of 100 fish collected independently of the training data. By accurately identifying pens where fish exhibit signs of respiratory distress, our method offers broad applicability and the potential to transform fish health and welfare monitoring in finfish aquaculture.
<div id='section'>PaperID: <span id='pid'>473, <a href='https://arxiv.org/pdf/2503.16768.pdf' target='_blank'>https://arxiv.org/pdf/2503.16768.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Meng Zhou, Jiadong Xie, Mingsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16768">Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mainstream visual object tracking frameworks predominantly rely on template matching paradigms. Their performance heavily depends on the quality of template features, which becomes increasingly challenging to maintain in complex scenarios involving target deformation, occlusion, and background clutter. While existing spatiotemporal memory-based trackers emphasize memory capacity expansion, they lack effective mechanisms for dynamic feature selection and adaptive fusion. To address this gap, we propose a Dynamic Attention Mechanism in Spatiotemporal Memory Network (DASTM) with two key innovations: 1) A differentiable dynamic attention mechanism that adaptively adjusts channel-spatial attention weights by analyzing spatiotemporal correlations between the templates and memory features; 2) A lightweight gating network that autonomously allocates computational resources based on target motion states, prioritizing high-discriminability features in challenging scenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K benchmarks demonstrate our DASTM's superiority, achieving state-of-the-art performance in success rate, robustness, and real-time efficiency, thereby offering a novel solution for real-time tracking in complex environments.
<div id='section'>PaperID: <span id='pid'>474, <a href='https://arxiv.org/pdf/2503.09951.pdf' target='_blank'>https://arxiv.org/pdf/2503.09951.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Xinglong Sun, Haijiang Sun, Shan Jiang, Jiacheng Wang, Jiasong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09951">Target-aware Bidirectional Fusion Transformer for Aerial Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The trackers based on lightweight neural networks have achieved great success in the field of aerial remote sensing, most of which aggregate multi-stage deep features to lift the tracking quality. However, existing algorithms usually only generate single-stage fusion features for state decision, which ignore that diverse kinds of features are required for identifying and locating the object, limiting the robustness and precision of tracking. In this paper, we propose a novel target-aware Bidirectional Fusion transformer (BFTrans) for UAV tracking. Specifically, we first present a two-stream fusion network based on linear self and cross attentions, which can combine the shallow and the deep features from both forward and backward directions, providing the adjusted local details for location and global semantics for recognition. Besides, a target-aware positional encoding strategy is designed for the above fusion model, which is helpful to perceive the object-related attributes during the fusion phase. Finally, the proposed method is evaluated on several popular UAV benchmarks, including UAV-123, UAV20L and UAVTrack112. Massive experimental results demonstrate that our approach can exceed other state-of-the-art trackers and run with an average speed of 30.5 FPS on embedded platform, which is appropriate for practical drone deployments.
<div id='section'>PaperID: <span id='pid'>475, <a href='https://arxiv.org/pdf/2502.19705.pdf' target='_blank'>https://arxiv.org/pdf/2502.19705.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Juntao Liang, Jun Hou, Weijun Zhang, Yong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19705">CFTrack: Enhancing Lightweight Visual Tracking through Contrastive Learning and Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving both efficiency and strong discriminative ability in lightweight visual tracking is a challenge, especially on mobile and edge devices with limited computational resources. Conventional lightweight trackers often struggle with robustness under occlusion and interference, while deep trackers, when compressed to meet resource constraints, suffer from performance degradation. To address these issues, we introduce CFTrack, a lightweight tracker that integrates contrastive learning and feature matching to enhance discriminative feature representations. CFTrack dynamically assesses target similarity during prediction through a novel contrastive feature matching module optimized with an adaptive contrastive loss, thereby improving tracking accuracy. Extensive experiments on LaSOT, OTB100, and UAV123 show that CFTrack surpasses many state-of-the-art lightweight trackers, operating at 136 frames per second on the NVIDIA Jetson NX platform. Results on the HOOT dataset further demonstrate CFTrack's strong discriminative ability under heavy occlusion.
<div id='section'>PaperID: <span id='pid'>476, <a href='https://arxiv.org/pdf/2502.03760.pdf' target='_blank'>https://arxiv.org/pdf/2502.03760.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nhat-Tan Do, Nhi Ngoc-Yen Nguyen, Dieu-Phuong Nguyen, Trong-Hop Do
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03760">RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on Deep Learning and Big Data Technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in UAV-based video is challenging due to variations in viewpoint, low resolution, and the presence of small objects. While other research on MOT dedicated to aerial videos primarily focuses on the academic aspect by developing sophisticated algorithms, there is a lack of attention to the practical aspect of these systems. In this paper, we propose a novel real-time MOT framework that integrates Apache Kafka and Apache Spark for efficient and fault-tolerant video stream processing, along with state-of-the-art deep learning models YOLOv8/YOLOv10 and BYTETRACK/BoTSORT for accurate object detection and tracking. Our work highlights the importance of not only the advanced algorithms but also the integration of these methods with scalable and distributed systems. By leveraging these technologies, our system achieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set while maintaining a real-time processing speed of 28 FPS on a single GPU. Our work demonstrates the potential of big data technologies and deep learning for addressing the challenges of MOT in UAV applications.
<div id='section'>PaperID: <span id='pid'>477, <a href='https://arxiv.org/pdf/2501.03874.pdf' target='_blank'>https://arxiv.org/pdf/2501.03874.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Ning Zhang, Timothy Shea, Arto Nurmikko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03874">Neuromorphic Optical Tracking and Imaging of Randomly Moving Targets through Strongly Scattering Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking and acquiring simultaneous optical images of randomly moving targets obscured by scattering media remains a challenging problem of importance to many applications that require precise object localization and identification. In this work we develop an end-to-end neuromorphic optical engineering and computational approach to demonstrate how to track and image normally invisible objects by combining an event detecting camera with a multistage neuromorphic deep learning strategy. Photons emerging from dense scattering media are detected by the event camera and converted to pixel-wise asynchronized spike trains - a first step in isolating object-specific information from the dominant uninformative background. Spiking data is fed into a deep spiking neural network (SNN) engine where object tracking and image reconstruction are performed by two separate yet interconnected modules running in parallel in discrete time steps over the event duration. Through benchtop experiments we demonstrate tracking and imaging randomly moving objects in dense turbid media as well as image reconstruction of spatially stationary but optically dynamic objects. Standardized character sets serve as representative proxies for geometrically complex objects, underscoring the method's generality. The results highlight the advantages of a fully neuromorphic approach in meeting a major imaging technology with high computational efficiency and low power consumption.
<div id='section'>PaperID: <span id='pid'>478, <a href='https://arxiv.org/pdf/2412.14088.pdf' target='_blank'>https://arxiv.org/pdf/2412.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Lucas Dal'Col, Miguel Oliveira, VÃ­tor Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14088">Joint Perception and Prediction for Autonomous Driving: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception and prediction modules are critical components of autonomous driving systems, enabling vehicles to navigate safely through complex environments. The perception module is responsible for perceiving the environment, including static and dynamic objects, while the prediction module is responsible for predicting the future behavior of these objects. These modules are typically divided into three tasks: object detection, object tracking, and motion prediction. Traditionally, these tasks are developed and optimized independently, with outputs passed sequentially from one to the next. However, this approach has significant limitations: computational resources are not shared across tasks, the lack of joint optimization can amplify errors as they propagate throughout the pipeline, and uncertainty is rarely propagated between modules, resulting in significant information loss. To address these challenges, the joint perception and prediction paradigm has emerged, integrating perception and prediction into a unified model through multi-task learning. This strategy not only overcomes the limitations of previous methods, but also enables the three tasks to have direct access to raw sensor data, allowing richer and more nuanced environmental interpretations. This paper presents the first comprehensive survey of joint perception and prediction for autonomous driving. We propose a taxonomy that categorizes approaches based on input representation, scene context modeling, and output representation, highlighting their contributions and limitations. Additionally, we present a qualitative analysis and quantitative comparison of existing methods. Finally, we discuss future research directions based on identified gaps in the state-of-the-art.
<div id='section'>PaperID: <span id='pid'>479, <a href='https://arxiv.org/pdf/2412.08121.pdf' target='_blank'>https://arxiv.org/pdf/2412.08121.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Samuel NordstrÃ¶m, BjÃ¶rn Lindquist, George Nikolakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08121">DTAA: A Detect, Track and Avoid Architecture for navigation in spaces with Multiple Velocity Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proactive collision avoidance measures are imperative in environments where humans and robots coexist. Moreover, the introduction of high quality legged robots into workplaces highlighted the crucial role of a robust, fully autonomous safety solution for robots to be viable in shared spaces or in co-existence with humans. This article establishes for the first time ever an innovative Detect-Track-and-Avoid Architecture (DTAA) to enhance safety and overall mission performance. The proposed novel architectyre has the merit ot integrating object detection using YOLOv8, utilizing Ultralytics embedded object tracking, and state estimation of tracked objects through Kalman filters. Moreover, a novel heuristic clustering is employed to facilitate active avoidance of multiple closely positioned objects with similar velocities, creating sets of unsafe spaces for the Nonlinear Model Predictive Controller (NMPC) to navigate around. The NMPC identifies the most hazardous unsafe space, considering not only their current positions but also their predicted future locations. In the sequel, the NMPC calculates maneuvers to guide the robot along a path planned by D$^{*}_{+}$ towards its intended destination, while maintaining a safe distance to all identified obstacles. The efficacy of the novelly suggested DTAA framework is being validated by Real-life experiments featuring a Boston Dynamics Spot robot that demonstrates the robot's capability to consistently maintain a safe distance from humans in dynamic subterranean, urban indoor, and outdoor environments.
<div id='section'>PaperID: <span id='pid'>480, <a href='https://arxiv.org/pdf/2411.06896.pdf' target='_blank'>https://arxiv.org/pdf/2411.06896.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Hemal Naik, Junran Yang, Dipin Das, Margaret C Crofoot, Akanksha Rathore, Vivek Hari Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06896">BuckTales : A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding animal behaviour is central to predicting, understanding, and mitigating impacts of natural and anthropogenic changes on animal populations and ecosystems. However, the challenges of acquiring and processing long-term, ecologically relevant data in wild settings have constrained the scope of behavioural research. The increasing availability of Unmanned Aerial Vehicles (UAVs), coupled with advances in machine learning, has opened new opportunities for wildlife monitoring using aerial tracking. However, limited availability of datasets with wild animals in natural habitats has hindered progress in automated computer vision solutions for long-term animal tracking. Here we introduce BuckTales, the first large-scale UAV dataset designed to solve multi-object tracking (MOT) and re-identification (Re-ID) problem in wild animals, specifically the mating behaviour (or lekking) of blackbuck antelopes. Collected in collaboration with biologists, the MOT dataset includes over 1.2 million annotations including 680 tracks across 12 high-resolution (5.4K) videos, each averaging 66 seconds and featuring 30 to 130 individuals. The Re-ID dataset includes 730 individuals captured with two UAVs simultaneously. The dataset is designed to drive scalable, long-term animal behaviour tracking using multiple camera sensors. By providing baseline performance with two detectors, and benchmarking several state-of-the-art tracking methods, our dataset reflects the real-world challenges of tracking wild animals in socially and ecologically relevant contexts. In making these data widely available, we hope to catalyze progress in MOT and Re-ID for wild animals, fostering insights into animal behaviour, conservation efforts, and ecosystem dynamics through automated, long-term monitoring.
<div id='section'>PaperID: <span id='pid'>481, <a href='https://arxiv.org/pdf/2409.14220.pdf' target='_blank'>https://arxiv.org/pdf/2409.14220.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tomasz Stanczyk, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14220">Temporally Propagated Masks and Bounding Boxes: Combining the Best of Both Worlds for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) involves identifying and consistently tracking objects across video sequences. Traditional tracking-by-detection methods, while effective, often require extensive tuning and lack generalizability. On the other hand, segmentation mask-based methods are more generic but struggle with tracking management, making them unsuitable for MOT. We propose a novel approach, McByte, which incorporates a temporally propagated segmentation mask as a strong association cue within a tracking-by-detection framework. By combining bounding box and propagated mask information, McByte enhances robustness and generalizability without per-sequence tuning. Evaluated on four benchmark datasets - DanceTrack, MOT17, SoccerNet-tracking 2022, and KITTI-tracking - McByte demonstrates performance gain in all cases examined. At the same time, it outperforms existing mask-based methods. Implementation code will be provided upon acceptance.
<div id='section'>PaperID: <span id='pid'>482, <a href='https://arxiv.org/pdf/2601.09078.pdf' target='_blank'>https://arxiv.org/pdf/2601.09078.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Junze Shi, Yang Yu, Jian Shi, Haibo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2601.09078">Exploring Reliable Spatiotemporal Dependencies for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in transformer-based lightweight object tracking have established new standards across benchmarks, leveraging the global receptive field and powerful feature extraction capabilities of attention mechanisms. Despite these achievements, existing methods universally employ sparse sampling during training--utilizing only one template and one search image per sequence--which fails to comprehensively explore spatiotemporal information in videos. This limitation constrains performance and cause the gap between lightweight and high-performance trackers. To bridge this divide while maintaining real-time efficiency, we propose STDTrack, a framework that pioneers the integration of reliable spatiotemporal dependencies into lightweight trackers. Our approach implements dense video sampling to maximize spatiotemporal information utilization. We introduce a temporally propagating spatiotemporal token to guide per-frame feature extraction. To ensure comprehensive target state representation, we disign the Multi-frame Information Fusion Module (MFIFM), which augments current dependencies using historical context. The MFIFM operates on features stored in our constructed Spatiotemporal Token Maintainer (STM), where a quality-based update mechanism ensures information reliability. Considering the scale variation among tracking targets, we develop a multi-scale prediction head to dynamically adapt to objects of different sizes. Extensive experiments demonstrate state-of-the-art results across six benchmarks. Notably, on GOT-10k, STDTrack rivals certain high-performance non-real-time trackers (e.g., MixFormer) while operating at 192 FPS(GPU) and 41 FPS(CPU).
<div id='section'>PaperID: <span id='pid'>483, <a href='https://arxiv.org/pdf/2512.02789.pdf' target='_blank'>https://arxiv.org/pdf/2512.02789.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Tang Haonan, Chen Yanjun, Jiang Lezhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02789">TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.
<div id='section'>PaperID: <span id='pid'>484, <a href='https://arxiv.org/pdf/2508.13507.pdf' target='_blank'>https://arxiv.org/pdf/2508.13507.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Seungheon Baek, Jinhyuk Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13507">Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Badminton is known as one of the fastest racket sports in the world. Despite doubles matches being more prevalent in international tournaments than singles, previous research has mainly focused on singles due to the challenges in data availability and multi-person tracking. To address this gap, we designed an approach that transfers singles-trained models to doubles analysis. We extracted keypoints from the ShuttleSet single matches dataset using ViT-Pose and embedded them through a contrastive learning framework based on ST-GCN. To improve tracking stability, we incorporated a custom multi-object tracking algorithm that resolves ID switching issues from fast and overlapping player movements. A Transformer-based classifier then determines shot occurrences based on the learned embeddings. Our findings demonstrate the feasibility of extending pose-based shot recognition to doubles badminton, broadening analytics capabilities. This work establishes a foundation for doubles-specific datasets to enhance understanding of this predominant yet understudied format of the fast racket sport.
<div id='section'>PaperID: <span id='pid'>485, <a href='https://arxiv.org/pdf/2508.09796.pdf' target='_blank'>https://arxiv.org/pdf/2508.09796.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yingjie Wang, Zhixing Wang, Le Zheng, Tianxiao Liu, Roujing Li, Xueyao Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09796">MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in human-dominant scenarios, which involves continuously tracking multiple people within video sequences, remains a significant challenge in computer vision due to targets' complex motion and severe occlusions. Conventional tracking-by-detection methods are fundamentally limited by their reliance on Kalman filter (KF) and rigid Intersection over Union (IoU)-based association. The motion model in KF often mismatches real-world object dynamics, causing filtering errors, while rigid association struggles under occlusions, leading to identity switches or target loss. To address these issues, we propose MeMoSORT, a simple, online, and real-time MOT tracker with two key innovations. First, the Memory-assisted Kalman filter (MeKF) uses memory-augmented neural networks to compensate for mismatches between assumed and actual object motion. Second, the Motion-adaptive IoU (Mo-IoU) adaptively expands the matching space and incorporates height similarity to reduce the influence of detection errors and association failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of 67.9\% and 82.1\%, respectively.
<div id='section'>PaperID: <span id='pid'>486, <a href='https://arxiv.org/pdf/2508.01752.pdf' target='_blank'>https://arxiv.org/pdf/2508.01752.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kumail Abbas, Zeeshan Afzal, Aqeel Raza, Taha Mansouri, Andrew W. Dowsey, Chaidate Inchaisri, Ali Alameer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01752">Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Activity and behaviour correlate with dairy cow health and welfare, making continual and accurate monitoring crucial for disease identification and farm productivity. Manual observation and frequent assessments are laborious and inconsistent for activity monitoring. In this study, we developed a unique multi-camera, real-time tracking system for indoor-housed Holstein Friesian dairy cows. This technology uses cutting-edge computer vision techniques, including instance segmentation and tracking algorithms to monitor cow activity seamlessly and accurately. An integrated top-down barn panorama was created by geometrically aligning six camera feeds using homographic transformations. The detection phase used a refined YOLO11-m model trained on an overhead cow dataset, obtaining high accuracy (mAP\@0.50 = 0.97, F1 = 0.95). SAMURAI, an upgraded Segment Anything Model 2.1, generated pixel-precise cow masks for instance segmentation utilizing zero-shot learning and motion-aware memory. Even with occlusion and fluctuating posture, a motion-aware Linear Kalman filter and IoU-based data association reliably identified cows over time for object tracking. The proposed system significantly outperformed Deep SORT Realtime. Multi-Object Tracking Accuracy (MOTA) was 98.7% and 99.3% in two benchmark video sequences, with IDF1 scores above 99% and near-zero identity switches. This unified multi-camera system can track dairy cows in complex interior surroundings in real time, according to our data. The system reduces redundant detections across overlapping cameras, maintains continuity as cows move between viewpoints, with the aim of improving early sickness prediction through activity quantification and behavioural classification.
<div id='section'>PaperID: <span id='pid'>487, <a href='https://arxiv.org/pdf/2507.05229.pdf' target='_blank'>https://arxiv.org/pdf/2507.05229.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Markiyan Kostiv, Anatolii Adamovskyi, Yevhen Cherniavskyi, Mykyta Varenyk, Ostap Viniavskyi, Igor Krashenyi, Oles Dobosevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05229">Self-Supervised Real-Time Tracking of Military Vehicles in Low-FPS UAV Footage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to maintain consistent identities of objects across video frames. Associating objects in low-frame-rate videos captured by moving unmanned aerial vehicles (UAVs) in actual combat scenarios is complex due to rapid changes in object appearance and position within the frame. The task becomes even more challenging due to image degradation caused by cloud video streaming and compression algorithms. We present how instance association learning from single-frame annotations can overcome these challenges. We show that global features of the scene provide crucial context for low-FPS instance association, allowing our solution to be robust to distractors and gaps in detections. We also demonstrate that such a tracking approach maintains high association quality even when reducing the input image resolution and latent representation size for faster inference. Finally, we present a benchmark dataset of annotated military vehicles collected from publicly available data sources. This paper was initially presented at the NATO Science and Technology Organization Symposium (ICMCIS) organized by the Information Systems Technology (IST)Scientific and Technical Committee, IST-209-RSY - the ICMCIS, held in Oeiras, Portugal, 13-14 May 2025.
<div id='section'>PaperID: <span id='pid'>488, <a href='https://arxiv.org/pdf/2506.21980.pdf' target='_blank'>https://arxiv.org/pdf/2506.21980.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Biao Wang, Wenwen Li, Jiawei Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21980">R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual single object tracking aims to continuously localize and estimate the scale of a target in subsequent video frames, given only its initial state in the first frame. This task has traditionally been framed as a template matching problem, evolving through major phases including correlation filters, two-stream networks, and one-stream networks with significant progress achieved. However, these methods typically require explicit classification and regression modeling, depend on supervised training with large-scale datasets, and are limited to the single task of tracking, lacking flexibility. In recent years, multi-modal large language models (MLLMs) have advanced rapidly. Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational capabilities, demonstrate excellent performance in grounding tasks. This has spurred interest in applying such models directly to visual tracking. However, experiments reveal that Qwen2.5-VL struggles with template matching between image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement learning method on a small-scale dataset with a rule-based reward function. The resulting model, R1-Track, achieved notable performance on the GOT-10k benchmark. R1-Track supports flexible initialization via bounding boxes or text descriptions while retaining most of the original model's general capabilities. And we further discuss potential improvements for R1-Track. This rough technical report summarizes our findings as of May 2025.
<div id='section'>PaperID: <span id='pid'>489, <a href='https://arxiv.org/pdf/2506.14256.pdf' target='_blank'>https://arxiv.org/pdf/2506.14256.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Deepak Ghimire, Joonwhoan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14256">Comparison of Two Methods for Stationary Incident Detection Based on Background Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.
<div id='section'>PaperID: <span id='pid'>490, <a href='https://arxiv.org/pdf/2506.07850.pdf' target='_blank'>https://arxiv.org/pdf/2506.07850.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Arash Rocky, Q. M. Jonathan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07850">SAM2Auto: Auto Annotation Using FLASH</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) lag behind Large Language Models due to the scarcity of annotated datasets, as creating paired visual-textual annotations is labor-intensive and expensive. To address this bottleneck, we introduce SAM2Auto, the first fully automated annotation pipeline for video datasets requiring no human intervention or dataset-specific training. Our approach consists of two key components: SMART-OD, a robust object detection system that combines automatic mask generation with open-world object detection capabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a multi-object real-time video instance segmentation (VIS) that maintains consistent object identification across video frames even with intermittent detection gaps. Unlike existing open-world detection methods that require frame-specific hyperparameter tuning and suffer from numerous false positives, our system employs statistical approaches to minimize detection errors while ensuring consistent object tracking throughout entire video sequences. Extensive experimental validation demonstrates that SAM2Auto achieves comparable accuracy to manual annotation while dramatically reducing annotation time and eliminating labor costs. The system successfully handles diverse datasets without requiring retraining or extensive parameter adjustments, making it a practical solution for large-scale dataset creation. Our work establishes a new baseline for automated video annotation and provides a pathway for accelerating VLM development by addressing the fundamental dataset bottleneck that has constrained progress in vision-language understanding.
<div id='section'>PaperID: <span id='pid'>491, <a href='https://arxiv.org/pdf/2505.22677.pdf' target='_blank'>https://arxiv.org/pdf/2505.22677.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jisu Kim, Alex Mattingly, Eung-Joo Lee, Benjamin S. Riggan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22677">Using Cross-Domain Detection Loss to Infer Multi-Scale Information for Improved Tiny Head Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head detection and tracking are essential for downstream tasks, but current methods often require large computational budgets, which increase latencies and ties up resources (e.g., processors, memory, and bandwidth). To address this, we propose a framework to enhance tiny head detection and tracking by optimizing the balance between performance and efficiency. Our framework integrates (1) a cross-domain detection loss, (2) a multi-scale module, and (3) a small receptive field detection mechanism. These innovations enhance detection by bridging the gap between large and small detectors, capturing high-frequency details at multiple scales during training, and using filters with small receptive fields to detect tiny heads. Evaluations on the CroHD and CrowdHuman datasets show improved Multiple Object Tracking Accuracy (MOTA) and mean Average Precision (mAP), demonstrating the effectiveness of our approach in crowded scenes.
<div id='section'>PaperID: <span id='pid'>492, <a href='https://arxiv.org/pdf/2504.18165.pdf' target='_blank'>https://arxiv.org/pdf/2504.18165.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Michel Gokan Khan, Renan Guarese, Fabian Johnson, Xi Vincent Wang, Anders Bergman, Benjamin Edvinsson, Mario Romero, JÃ©rÃ©my Vachier, Jan Kronqvist
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18165">PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.
<div id='section'>PaperID: <span id='pid'>493, <a href='https://arxiv.org/pdf/2504.11310.pdf' target='_blank'>https://arxiv.org/pdf/2504.11310.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Dayong Liu, Qingrui Zhang, Zeyang Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11310">Intelligent driving vehicle front multi-target tracking and detection based on YOLOv5 and point cloud 3D projection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-target tracking and detection tasks, it is necessary to continuously track multiple targets, such as vehicles, pedestrians, etc. To achieve this goal, the system must be able to continuously acquire and process image frames containing these targets. These consecutive frame images enable the algorithm to update the position and state of the target in real-time in each frame of the image. How to accurately associate the detected target with the target in the previous or next frame to form a stable trajectory is a complex problem. Therefore, a multi object tracking and detection method for intelligent driving vehicles based on YOLOv5 and point cloud 3D projection is proposed. Using Retinex algorithm to enhance the image of the environment in front of the vehicle, remove light interference in the image, and build an intelligent detection model based on YOLOv5 network structure. The enhanced image is input into the model, and multiple targets in front of the vehicle are identified through feature extraction and target localization. By combining point cloud 3D projection technology, the correlation between the position changes of adjacent frame images in the projection coordinate system can be inferred. By sequentially projecting the multi-target recognition results of multiple consecutive frame images into the 3D laser point cloud environment, effective tracking of the motion trajectories of all targets in front of the vehicle can be achieved. The experimental results show that the application of this method for intelligent driving vehicle front multi-target tracking and detection yields a MOTA (Tracking Accuracy) value greater than 30, demonstrating its superior tracking and detection performance.
<div id='section'>PaperID: <span id='pid'>494, <a href='https://arxiv.org/pdf/2504.02519.pdf' target='_blank'>https://arxiv.org/pdf/2504.02519.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Christian Alexander Holz, Christian Bader, Markus Enzweiler, Matthias DrÃ¼ppel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02519">Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents novel Machine Learning (ML) methodologies for Multi-Object Tracking (MOT), specifically designed to meet the increasing complexity and precision demands of Advanced Driver Assistance Systems (ADAS). We introduce three Neural Network (NN) models that address key challenges in MOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii) the Single-Association Network (SANT) for mapping individual Sensor Object (SO) to existing tracks, and (iii) the Multi-Association Network (MANTa) for associating multiple SOs to multiple tracks. These models are seamlessly integrated into a traditional Kalman Filter (KF) framework, maintaining the system's modularity by replacing relevant components without disrupting the overall architecture. Importantly, all three networks are designed to be run in a realtime, embedded environment. Each network contains less than 50k trainable parameters. Our evaluation, conducted on the public KITTI tracking dataset, demonstrates significant improvements in tracking performance. SPENT reduces the Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT and MANTa achieve up to 95% accuracy in sensor object-to-track assignments. These results underscore the effectiveness of incorporating task-specific NNs into traditional tracking systems, boosting performance and robustness while preserving modularity, maintainability, and interpretability.
<div id='section'>PaperID: <span id='pid'>495, <a href='https://arxiv.org/pdf/2503.04500.pdf' target='_blank'>https://arxiv.org/pdf/2503.04500.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yu-Hsi Chen, Chin-Tien Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04500">ReynoldsFlow: Exquisite Flow Estimation via Reynolds Transport Theorem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical flow is a fundamental technique for motion estimation, widely applied in video stabilization, interpolation, and object tracking. Traditional optical flow estimation methods rely on restrictive assumptions like brightness constancy and slow motion constraints. Recent deep learning-based flow estimations require extensive training on large domain-specific datasets, making them computationally demanding. Also, artificial intelligence (AI) advances have enabled deep learning models to take advantage of optical flow as an important feature for object tracking and motion analysis. Since optical flow is commonly encoded in HSV for visualization, its conversion to RGB for neural network processing is nonlinear and may introduce perceptual distortions. These transformations amplify the sensitivity to estimation errors, potentially affecting the predictive accuracy of the networks. To address these challenges that are influential to the performance of downstream network models, we propose Reynolds flow, a novel training-free flow estimation inspired by the Reynolds transport theorem, offering a principled approach to modeling complex motion dynamics. In addition to conventional HSV-based visualization of Reynolds flow, we also introduce an RGB-encoded representation of Reynolds flow designed to improve flow visualization and feature enhancement for neural networks. We evaluated the effectiveness of Reynolds flow in video-based tasks. Experimental results on three benchmarks, tiny object detection on UAVDB, infrared object detection on Anti-UAV, and pose estimation on GolfDB, demonstrate that networks trained with RGB-encoded Reynolds flow achieve SOTA performance, exhibiting improved robustness and efficiency across all tasks.
<div id='section'>PaperID: <span id='pid'>496, <a href='https://arxiv.org/pdf/2502.18867.pdf' target='_blank'>https://arxiv.org/pdf/2502.18867.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Akhil Penta, Vaibhav Adwani, Ankush Chopra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18867">Enhanced Transformer-Based Tracking for Skiing Events: Overcoming Multi-Camera Challenges, Scale Variations and Rapid Motion -- SkiTB Visual Tracking Challenge 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate skier tracking is essential for performance analysis, injury prevention, and optimizing training strategies in alpine sports. Traditional tracking methods often struggle with occlusions, dynamic movements, and varying environmental conditions, limiting their effectiveness. In this work, we used STARK (Spatio-Temporal Transformer Network for Visual Tracking), a transformer-based model, to track skiers. We adapted STARK to address domain-specific challenges such as camera movements, camera changes, occlusions, etc. by optimizing the model's architecture and hyperparameters to better suit the dataset.
<div id='section'>PaperID: <span id='pid'>497, <a href='https://arxiv.org/pdf/2502.04478.pdf' target='_blank'>https://arxiv.org/pdf/2502.04478.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Luiz C. S. de Araujo, Carlos M. S. Figueiredo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04478">OneTrack-M: A multitask approach to transformer-based MOT models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) is a critical problem in computer vision, essential for understanding how objects move and interact in videos. This field faces significant challenges such as occlusions and complex environmental dynamics, impacting model accuracy and efficiency. While traditional approaches have relied on Convolutional Neural Networks (CNNs), introducing transformers has brought substantial advancements. This work introduces OneTrack-M, a transformer-based MOT model designed to enhance tracking computational efficiency and accuracy. Our approach simplifies the typical transformer-based architecture by eliminating the need for a decoder model for object detection and tracking. Instead, the encoder alone serves as the backbone for temporal data interpretation, significantly reducing processing time and increasing inference speed. Additionally, we employ innovative data pre-processing and multitask training techniques to address occlusion and diverse objective challenges within a single set of weights. Experimental results demonstrate that OneTrack-M achieves at least 25% faster inference times compared to state-of-the-art models in the literature while maintaining or improving tracking accuracy metrics. These improvements highlight the potential of the proposed solution for real-time applications such as autonomous vehicles, surveillance systems, and robotics, where rapid responses are crucial for system effectiveness.
<div id='section'>PaperID: <span id='pid'>498, <a href='https://arxiv.org/pdf/2412.10453.pdf' target='_blank'>https://arxiv.org/pdf/2412.10453.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kailas PS, Selvakumaran R, Palani Murugan, Ramesh Kumar, Malaya Kumar Biswal M
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10453">Analysis of Object Detection Models for Tiny Object in Satellite Imagery: A Dataset-Centric Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, significant advancements have been made in deep learning-based object detection algorithms, revolutionizing basic computer vision tasks, notably in object detection, tracking, and segmentation. This paper delves into the intricate domain of Small-Object-Detection (SOD) within satellite imagery, highlighting the unique challenges stemming from wide imaging ranges, object distribution, and their varying appearances in bird's-eye-view satellite images. Traditional object detection models face difficulties in detecting small objects due to limited contextual information and class imbalances. To address this, our research presents a meticulously curated dataset comprising 3000 images showcasing cars, ships, and airplanes in satellite imagery. Our study aims to provide valuable insights into small object detection in satellite imagery by empirically evaluating state-of-the-art models. Furthermore, we tackle the challenges of satellite video-based object tracking, employing the Byte Track algorithm on the SAT-MTB dataset. Through rigorous experimentation, we aim to offer a comprehensive understanding of the efficacy of state-of-the-art models in Small-Object-Detection for satellite applications. Our findings shed light on the effectiveness of these models and pave the way for future advancements in satellite imagery analysis.
<div id='section'>PaperID: <span id='pid'>499, <a href='https://arxiv.org/pdf/2412.01119.pdf' target='_blank'>https://arxiv.org/pdf/2412.01119.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Mojtaba S. Fazli, Shannon Quinn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01119">Object Tracking in a $360^o$ View: A Novel Perspective on Bridging the Gap to Biomedical Advancements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a fundamental tool in modern innovation, with applications in defense systems, autonomous vehicles, and biomedical research. It enables precise identification, monitoring, and spatiotemporal analysis of objects across sequential frames, providing insights into dynamic behaviors. In cell biology, object tracking is vital for uncovering cellular mechanisms, such as migration, interactions, and responses to drugs or pathogens. These insights drive breakthroughs in understanding disease progression and therapeutic interventions.
  Over time, object tracking methods have evolved from traditional feature-based approaches to advanced machine learning and deep learning frameworks. While classical methods are reliable in controlled settings, they struggle in complex environments with occlusions, variable lighting, and high object density. Deep learning models address these challenges by delivering greater accuracy, adaptability, and robustness.
  This review categorizes object tracking techniques into traditional, statistical, feature-based, and machine learning paradigms, with a focus on biomedical applications. These methods are essential for tracking cells and subcellular structures, advancing our understanding of health and disease. Key performance metrics, including accuracy, efficiency, and adaptability, are discussed. The paper explores limitations of current methods and highlights emerging trends to guide the development of next-generation tracking systems for biomedical research and broader scientific domains.
<div id='section'>PaperID: <span id='pid'>500, <a href='https://arxiv.org/pdf/2411.19134.pdf' target='_blank'>https://arxiv.org/pdf/2411.19134.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Peilin Tian, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19134">Visual SLAMMOT Considering Multiple Motion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous Localization and Mapping (SLAM) and Multi-Object Tracking (MOT) are pivotal tasks in the realm of autonomous driving, attracting considerable research attention. While SLAM endeavors to generate real-time maps and determine the vehicle's pose in unfamiliar settings, MOT focuses on the real-time identification and tracking of multiple dynamic objects. Despite their importance, the prevalent approach treats SLAM and MOT as independent modules within an autonomous vehicle system, leading to inherent limitations. Classical SLAM methodologies often rely on a static environment assumption, suitable for indoor rather than dynamic outdoor scenarios. Conversely, conventional MOT techniques typically rely on the vehicle's known state, constraining the accuracy of object state estimations based on this prior. To address these challenges, previous efforts introduced the unified SLAMMOT paradigm, yet primarily focused on simplistic motion patterns. In our team's previous work IMM-SLAMMOT\cite{IMM-SLAMMOT}, we present a novel methodology incorporating consideration of multiple motion models into SLAMMOT i.e. tightly coupled SLAM and MOT, demonstrating its efficacy in LiDAR-based systems. This paper studies feasibility and advantages of instantiating this methodology as visual SLAMMOT, bridging the gap between LiDAR and vision-based sensing mechanisms. Specifically, we propose a solution of visual SLAMMOT considering multiple motion models and validate the inherent advantages of IMM-SLAMMOT in the visual domain.
<div id='section'>PaperID: <span id='pid'>501, <a href='https://arxiv.org/pdf/2411.18443.pdf' target='_blank'>https://arxiv.org/pdf/2411.18443.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jonathan Lichtenfeld, Kevin Daun, Oskar von Stryk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18443">Efficient Dynamic LiDAR Odometry for Mobile Robots with Structured Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a real-time dynamic LiDAR odometry pipeline for mobile robots in Urban Search and Rescue (USAR) scenarios. Existing approaches to dynamic object detection often rely on pretrained learned networks or computationally expensive volumetric maps. To enhance efficiency on computationally limited robots, we reuse data between the odometry and detection module. Utilizing a range image segmentation technique and a novel residual-based heuristic, our method distinguishes dynamic from static objects before integrating them into the point cloud map. The approach demonstrates robust object tracking and improved map accuracy in environments with numerous dynamic objects. Even highly non-rigid objects, such as running humans, are accurately detected at point level without prior downsampling of the point cloud and hence, without loss of information. Evaluation on simulated and real-world data validates its computational efficiency. Compared to a state-of-the-art volumetric method, our approach shows comparable detection performance at a fraction of the processing time, adding only 14 ms to the odometry module for dynamic object detection and tracking. The implementation and a new real-world dataset are available as open-source for further research.
<div id='section'>PaperID: <span id='pid'>502, <a href='https://arxiv.org/pdf/2410.20079.pdf' target='_blank'>https://arxiv.org/pdf/2410.20079.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>InPyo Song, Jangwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20079">SFTrack: A Robust Scale and Motion Adaptive Algorithm for Tracking Small and Fast Moving Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of multi-object tracking in Unmanned Aerial Vehicle (UAV) footage. It plays a critical role in various UAV applications, including traffic monitoring systems and real-time suspect tracking by the police. However, this task is highly challenging due to the fast motion of UAVs, as well as the small size of target objects in the videos caused by the high-altitude and wide angle views of drones. In this study, we thus introduce a simple yet more effective method compared to previous work to overcome these challenges. Our approach involves a new tracking strategy, which initiates the tracking of target objects from low-confidence detections commonly encountered in UAV application scenarios. Additionally, we propose revisiting traditional appearance-based matching algorithms to improve the association of low-confidence detections. To evaluate the effectiveness of our method, we conducted benchmark evaluations on two UAV-specific datasets (VisDrone2019, UAVDT) and one general object tracking dataset (MOT17). The results demonstrate that our approach surpasses current state-of-the art methodologies, highlighting its robustness and adaptability in diverse tracking environments. Furthermore, we have improved the annotation of the UAVDT dataset by rectifying several errors and addressing omissions found in the original annotations. We will provide this refined version of the dataset to facilitate better benchmarking in the field.
<div id='section'>PaperID: <span id='pid'>503, <a href='https://arxiv.org/pdf/2410.08769.pdf' target='_blank'>https://arxiv.org/pdf/2410.08769.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Jan MÃ¼ller, Adrian Pigors
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08769">Efficient Multi-Object Tracking on Edge Devices via Reconstruction-Based Channel Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of multi-object tracking (MOT) technologies presents the dual challenge of maintaining high performance while addressing critical security and privacy concerns. In applications such as pedestrian tracking, where sensitive personal data is involved, the potential for privacy violations and data misuse becomes a significant issue if data is transmitted to external servers. To mitigate these risks, processing data directly on an edge device, such as a smart camera, has emerged as a viable solution. Edge computing ensures that sensitive information remains local, thereby aligning with stringent privacy principles and significantly reducing network latency. However, the implementation of MOT on edge devices is not without its challenges. Edge devices typically possess limited computational resources, necessitating the development of highly optimized algorithms capable of delivering real-time performance under these constraints. The disparity between the computational requirements of state-of-the-art MOT algorithms and the capabilities of edge devices emphasizes a significant obstacle. To address these challenges, we propose a neural network pruning method specifically tailored to compress complex networks, such as those used in modern MOT systems. This approach optimizes MOT performance by ensuring high accuracy and efficiency within the constraints of limited edge devices, such as NVIDIA's Jetson Orin Nano. By applying our pruning method, we achieve model size reductions of up to 70% while maintaining a high level of accuracy and further improving performance on the Jetson Orin Nano, demonstrating the effectiveness of our approach for edge computing applications.
<div id='section'>PaperID: <span id='pid'>504, <a href='https://arxiv.org/pdf/2409.17533.pdf' target='_blank'>https://arxiv.org/pdf/2409.17533.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Felix Limanta, Kuniaki Uto, Koichi Shinoda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17533">CAMOT: Camera Angle-aware Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes CAMOT, a simple camera angle estimator for multi-object tracking to tackle two problems: 1) occlusion and 2) inaccurate distance estimation in the depth direction. Under the assumption that multiple objects are located on a flat plane in each video frame, CAMOT estimates the camera angle using object detection. In addition, it gives the depth of each object, enabling pseudo-3D MOT. We evaluated its performance by adding it to various 2D MOT methods on the MOT17 and MOT20 datasets and confirmed its effectiveness. Applying CAMOT to ByteTrack, we obtained 63.8% HOTA, 80.6% MOTA, and 78.5% IDF1 in MOT17, which are state-of-the-art results. Its computational cost is significantly lower than the existing deep-learning-based depth estimators for tracking.
<div id='section'>PaperID: <span id='pid'>505, <a href='https://arxiv.org/pdf/2409.09841.pdf' target='_blank'>https://arxiv.org/pdf/2409.09841.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Oriel Perl, Ido Leshem, Uria Franko, Yuval Goldman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09841">Tracking Virtual Meetings in the Wild: Re-identification in Multi-Participant Virtual Meetings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, workplaces and educational institutes have widely adopted virtual meeting platforms. This has led to a growing interest in analyzing and extracting insights from these meetings, which requires effective detection and tracking of unique individuals. In practice, there is no standardization in video meetings recording layout, and how they are captured across the different platforms and services. This, in turn, creates a challenge in acquiring this data stream and analyzing it in a uniform fashion. Our approach provides a solution to the most general form of video recording, usually consisting of a grid of participants (\cref{fig:videomeeting}) from a single video source with no metadata on participant locations, while using the least amount of constraints and assumptions as to how the data was acquired. Conventional approaches often use YOLO models coupled with tracking algorithms, assuming linear motion trajectories akin to that observed in CCTV footage. However, such assumptions fall short in virtual meetings, where participant video feed window can abruptly change location across the grid. In an organic video meeting setting, participants frequently join and leave, leading to sudden, non-linear movements on the video grid. This disrupts optical flow-based tracking methods that depend on linear motion. Consequently, standard object detection and tracking methods might mistakenly assign multiple participants to the same tracker. In this paper, we introduce a novel approach to track and re-identify participants in remote video meetings, by utilizing the spatio-temporal priors arising from the data in our domain. This, in turn, increases tracking capabilities compared to the use of general object tracking. Our approach reduces the error rate by 95% on average compared to YOLO-based tracking methods as a baseline.
<div id='section'>PaperID: <span id='pid'>506, <a href='https://arxiv.org/pdf/2512.05171.pdf' target='_blank'>https://arxiv.org/pdf/2512.05171.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Aleksandr Abramov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05171">Two-Stage Camera Calibration Method for Multi-Camera Systems Using Scene Geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Calibration of multi-camera systems is a key task for accurate object tracking. However, it remains a challenging problem in real-world conditions, where traditional methods are not applicable due to the lack of accurate floor plans, physical access to place calibration patterns, or synchronized video streams. This paper presents a novel two-stage calibration method that overcomes these limitations. In the first stage, partial calibration of individual cameras is performed based on an operator's annotation of natural geometric primitives (parallel, perpendicular, and vertical lines, or line segments of equal length). This allows estimating key parameters (roll, pitch, focal length) and projecting the camera's Effective Field of View (EFOV) onto the horizontal plane in a base 3D coordinate system. In the second stage, precise system calibration is achieved through interactive manipulation of the projected EFOV polygons. The operator adjusts their position, scale, and rotation to align them with the floor plan or, in its absence, using virtual calibration elements projected onto all cameras in the system. This determines the remaining extrinsic parameters (camera position and yaw). Calibration requires only a static image from each camera, eliminating the need for physical access or synchronized video. The method is implemented as a practical web service. Comparative analysis and demonstration videos confirm the method's applicability, accuracy, and flexibility, enabling the deployment of precise multi-camera tracking systems in scenarios previously considered infeasible.
<div id='section'>PaperID: <span id='pid'>507, <a href='https://arxiv.org/pdf/2509.01095.pdf' target='_blank'>https://arxiv.org/pdf/2509.01095.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Zhihong Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01095">An End-to-End Framework for Video Multi-Person Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based human pose estimation models aim to address scenarios that cannot be effectively solved by static image models such as motion blur, out-of-focus and occlusion. Most existing approaches consist of two stages: detecting human instances in each image frame and then using a temporal model for single-person pose estimation. This approach separates the spatial and temporal dimensions and cannot capture the global spatio-temporal context between spatial instances for end-to-end optimization. In addition, it relies on separate detectors and complex post-processing such as RoI cropping and NMS, which reduces the inference efficiency of the video scene. To address the above problems, we propose VEPE (Video End-to-End Pose Estimation), a simple and flexible framework for end-to-end pose estimation in video. The framework utilizes three crucial spatio-temporal Transformer components: the Spatio-Temporal Pose Encoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and the Spatio-Temporal Pose Decoder (STPD). These components are designed to effectively utilize temporal context for optimizing human body pose estimation. Furthermore, to reduce the mismatch problem during the cross-frame pose query matching process, we propose an instance consistency mechanism, which aims to enhance the consistency and discrepancy of the cross-frame instance query and realize the instance tracking function, which in turn accurately guides the pose query to perform cross-frame matching. Extensive experiments on the Posetrack dataset show that our approach outperforms most two-stage models and improves inference efficiency by 300%.
<div id='section'>PaperID: <span id='pid'>508, <a href='https://arxiv.org/pdf/2508.08269.pdf' target='_blank'>https://arxiv.org/pdf/2508.08269.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Sagar Verma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08269">emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tendon-driven robotic hands offer unparalleled dexterity for manipulation tasks, but learning control policies for such systems presents unique challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a direct one-to-one mapping between motion capture (mocap) data and tendon controls, making the learning process complex and expensive. Additionally, visual tracking methods for real-world applications are prone to occlusions and inaccuracies, further complicating joint tracking. Wrist-wearable surface electromyography (sEMG) sensors present an inexpensive, robust alternative to capture hand motion. However, mapping sEMG signals to tendon control remains a significant challenge despite the availability of EMG-to-pose data sets and regression-based models in the existing literature.
  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic hands, extending the emg2pose dataset, which includes recordings from 193 subjects, spanning 370 hours and 29 stages with diverse gestures. This dataset incorporates tendon control signals derived using the MyoSuite MyoHand model, addressing limitations such as invalid poses in prior methods. We provide three baseline regression models to demonstrate emg2tendon utility and propose a novel diffusion-based regression model for predicting tendon control from sEMG recordings. This dataset and modeling framework marks a significant step forward for tendon-driven dexterous robotic manipulation, laying the groundwork for scalable and accurate tendon control in robotic hands. https://emg2tendon.github.io/
<div id='section'>PaperID: <span id='pid'>509, <a href='https://arxiv.org/pdf/2508.00272.pdf' target='_blank'>https://arxiv.org/pdf/2508.00272.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Wenyue Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00272">Towards Robust Semantic Correspondence: A Benchmark and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic correspondence aims to identify semantically meaningful relationships between different images and is a fundamental challenge in computer vision. It forms the foundation for numerous tasks such as 3D reconstruction, object tracking, and image editing. With the progress of large-scale vision models, semantic correspondence has achieved remarkable performance in controlled and high-quality conditions. However, the robustness of semantic correspondence in challenging scenarios is much less investigated. In this work, we establish a novel benchmark for evaluating semantic correspondence in adverse conditions. The benchmark dataset comprises 14 distinct challenging scenarios that reflect commonly encountered imaging issues, including geometric distortion, image blurring, digital artifacts, and environmental occlusion. Through extensive evaluations, we provide several key insights into the robustness of semantic correspondence approaches: (1) All existing methods suffer from noticeable performance drops under adverse conditions; (2) Using large-scale vision models can enhance overall robustness, but fine-tuning on these models leads to a decline in relative robustness; (3) The DINO model outperforms the Stable Diffusion in relative robustness, and their fusion achieves better absolute robustness; Moreover, We evaluate common robustness enhancement strategies for semantic correspondence and find that general data augmentations are ineffective, highlighting the need for task-specific designs. These results are consistent across both our dataset and real-world benchmarks.
<div id='section'>PaperID: <span id='pid'>510, <a href='https://arxiv.org/pdf/2507.22421.pdf' target='_blank'>https://arxiv.org/pdf/2507.22421.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Shahla John
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22421">Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time video analysis remains a challenging problem in computer vision, requiring efficient processing of both spatial and temporal information while maintaining computational efficiency. Existing approaches often struggle to balance accuracy and speed, particularly in resource-constrained environments. In this work, we present a unified framework that leverages advanced spatial-temporal modeling techniques for simultaneous action recognition and object tracking. Our approach builds upon recent advances in parallel sequence modeling and introduces a novel hierarchical attention mechanism that adaptively focuses on relevant spatial regions across temporal sequences. We demonstrate that our method achieves state-of-the-art performance on standard benchmarks while maintaining real-time inference speeds. Extensive experiments on UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action recognition accuracy and 2.8% in tracking precision compared to existing methods, with 40% faster inference time.
<div id='section'>PaperID: <span id='pid'>511, <a href='https://arxiv.org/pdf/2506.13769.pdf' target='_blank'>https://arxiv.org/pdf/2506.13769.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Filippo Leveni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13769">Non-planar Object Detection and Identification by Features Matching and Triangulation Growth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.
<div id='section'>PaperID: <span id='pid'>512, <a href='https://arxiv.org/pdf/2506.13457.pdf' target='_blank'>https://arxiv.org/pdf/2506.13457.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Momir AdÅ¾emoviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13457">Deep Learning-Based Multi-Object Tracking: A Comprehensive Survey from Foundations to State-of-the-Art</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a core task in computer vision that involves detecting objects in video frames and associating them across time. The rise of deep learning has significantly advanced MOT, particularly within the tracking-by-detection paradigm, which remains the dominant approach. Advancements in modern deep learning-based methods accelerated in 2022 with the introduction of ByteTrack for tracking-by-detection and MOTR for end-to-end tracking. Our survey provides an in-depth analysis of deep learning-based MOT methods, systematically categorizing tracking-by-detection approaches into five groups: joint detection and embedding, heuristic-based, motion-based, affinity learning, and offline methods. In addition, we examine end-to-end tracking methods and compare them with existing alternative approaches. We evaluate the performance of recent trackers across multiple benchmarks and specifically assess their generality by comparing results across different domains. Our findings indicate that heuristic-based methods achieve state-of-the-art results on densely populated datasets with linear object motion, while deep learning-based association methods, in both tracking-by-detection and end-to-end approaches, excel in scenarios with complex motion patterns.
<div id='section'>PaperID: <span id='pid'>513, <a href='https://arxiv.org/pdf/2501.01206.pdf' target='_blank'>https://arxiv.org/pdf/2501.01206.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Karolina Prawda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01206">Sensitivity of Room Impulse Responses in Changing Acoustic Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Changes in room acoustics, such as modifications to surface absorption or the insertion of a scattering object, significantly impact measured room impulse responses (RIRs). These changes can affect the performance of systems used in echo cancellation and active acoustics and support tasks such as navigation and object tracking. Recognizing and quantifying such changes is, therefore, critical for advancing technologies based on room acoustics. This study introduces a method for analyzing acoustic environment changes by evaluating the similarity of consecutively recorded RIRs. Short-time coherence is employed to characterize modifications, including changes in wall absorption or the presence of a moving person in the room. A sensitivity rating is further used to quantify the magnitude of these changes. The results clearly differentiate between types of modifications -- atmospheric variation, changes in absorption, and human presence. The methods described provide a novel approach to analyzing and interpreting room acoustics, emphasizing RIR similarity and extracting information from temporal and spectral signal properties.
<div id='section'>PaperID: <span id='pid'>514, <a href='https://arxiv.org/pdf/2501.00843.pdf' target='_blank'>https://arxiv.org/pdf/2501.00843.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Nathanael L. Baisa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00843">FusionSORT: Fusion Methods for Online Multi-object Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.
<div id='section'>PaperID: <span id='pid'>515, <a href='https://arxiv.org/pdf/2412.07966.pdf' target='_blank'>https://arxiv.org/pdf/2412.07966.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Kurt H. W. Stolle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07966">Balancing Shared and Task-Specific Representations: A Hybrid Approach to Depth-Aware Video Panoptic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Multiformer, a novel approach to depth-aware video panoptic segmentation (DVPS) based on the mask transformer paradigm. Our method learns object representations that are shared across segmentation, monocular depth estimation, and object tracking subtasks. In contrast to recent unified approaches that progressively refine a common object representation, we propose a hybrid method using task-specific branches within each decoder block, ultimately fusing them into a shared representation at the block interfaces. Extensive experiments on the Cityscapes-DVPS and SemKITTI-DVPS datasets demonstrate that Multiformer achieves state-of-the-art performance across all DVPS metrics, outperforming previous methods by substantial margins. With a ResNet-50 backbone, Multiformer surpasses the previous best result by 3.0 DVPQ points while also improving depth estimation accuracy. Using a Swin-B backbone, Multiformer further improves performance by 4.0 DVPQ points. Multiformer also provides valuable insights into the design of multi-task decoder architectures.
<div id='section'>PaperID: <span id='pid'>516, <a href='https://arxiv.org/pdf/2411.10028.pdf' target='_blank'>https://arxiv.org/pdf/2411.10028.pdf</a></span>  </div></span></div><div id = 'author'>Authors: <span id = 'author'>Yanzhao Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10028">MOT FCG++: Enhanced Representation of Spatio-temporal Motion and Appearance Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of multi-object tracking (MOT) is to detect and track all objects in a scene across frames, while maintaining a unique identity for each object. Most existing methods rely on the spatial-temporal motion features and appearance embedding features of the detected objects in consecutive frames. Effectively and robustly representing the spatial and appearance features of long trajectories has become a critical factor affecting the performance of MOT. We propose a novel approach for appearance and spatial-temporal motion feature representation, improving upon the hierarchical clustering association method MOT FCG. For spatialtemporal motion features, we first propose Diagonal Modulated GIoU, which more accurately represents the relationship between the position and shape of the objects. Second, Mean Constant Velocity Modeling is proposed to reduce the effect of observation noise on target motion state estimation. For appearance features, we utilize a dynamic appearance representation that incorporates confidence information, enabling the trajectory appearance features to be more robust and global. Based on the baseline model MOT FCG, we have realized further improvements in the performance of all. we achieved 63.1 HOTA, 76.9 MOTA and 78.2 IDF1 on the MOT17 test set, and also achieved competitive performance on the MOT20 and DanceTrack sets.
