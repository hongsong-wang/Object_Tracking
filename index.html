<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2512.22799.pdf' target='_blank'>https://arxiv.org/pdf/2512.22799.pdf</a></span>   <span><a href='https://github.com/jcwang0602/VPTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingchao Wang, Kaiwen Zhou, Zhijian Wu, Kunhua Ji, Dingjiang Huang, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22799">VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2512.22624.pdf' target='_blank'>https://arxiv.org/pdf/2512.22624.pdf</a></span>   <span><a href='https://github.com/HamadYA/SAM3_Tracking_Zoo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad Alansari, Muzammal Naseer, Hasan Al Marzouqi, Naoufel Werghi, Sajid Javed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22624">Rethinking Memory Design in SAM-Based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>\noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.22581.pdf' target='_blank'>https://arxiv.org/pdf/2512.22581.pdf</a></span>   <span><a href='https://marwan99.github.io/kv_tracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marwan Taher, Ignacio Alzugaray, Kirill Mazur, Xin Kong, Andrew J. Davison
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22581">KV-Tracker: Real-Time Pose Tracking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $π^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\sim}27$ FPS.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.22105.pdf' target='_blank'>https://arxiv.org/pdf/2512.22105.pdf</a></span>   <span><a href='https://github.com/Robotmurlock/TDLP' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Robotmurlock/TDLP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir Adžemović
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22105">Learning Association via Track-Detection Matching for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking aims to maintain object identities over time by associating detections across video frames. Two dominant paradigms exist in literature: tracking-by-detection methods, which are computationally efficient but rely on handcrafted association heuristics, and end-to-end approaches, which learn association from data at the cost of higher computational complexity. We propose Track-Detection Link Prediction (TDLP), a tracking-by-detection method that performs per-frame association via link prediction between tracks and detections, i.e., by predicting the correct continuation of each track at every frame. TDLP is architecturally designed primarily for geometric features such as bounding boxes, while optionally incorporating additional cues, including pose and appearance. Unlike heuristic-based methods, TDLP learns association directly from data without handcrafted rules, while remaining modular and computationally efficient compared to end-to-end trackers. Extensive experiments on multiple benchmarks demonstrate that TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods. Finally, we provide a detailed analysis comparing link prediction with metric learning-based association and show that link prediction is more effective, particularly when handling heterogeneous features such as detection bounding boxes. Our code is available at \href{https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP}.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2512.15066.pdf' target='_blank'>https://arxiv.org/pdf/2512.15066.pdf</a></span>   <span><a href='https://github.com/XiAooZ/MWNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxiao Zhang, Runshi Zhang, Junchen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15066">Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2512.13130.pdf' target='_blank'>https://arxiv.org/pdf/2512.13130.pdf</a></span>   <span><a href='https://github.com/shl-shawn/LeafTrackNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanghua Liu, Majharulislam Babor, Christoph Verduyn, Breght Vandenberghe, Bruno Betoni Parodi, Cornelia Weltzien, Marina M. -C. Höhne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13130">LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2512.13007.pdf' target='_blank'>https://arxiv.org/pdf/2512.13007.pdf</a></span>   <span><a href='https://github.com/nagonch/LiFT-6DoF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolai Goncharov, James L. Gray, Donald G. Dansereau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13007">Light Field Based 6DoF Tracking of Previously Unobserved Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2512.07599.pdf' target='_blank'>https://arxiv.org/pdf/2512.07599.pdf</a></span>   <span><a href='https://github.com/AutoLab-SAI-SJTU/AutoSeg3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanshi Wang, Zijian Cai, Jin Gao, Yiwei Zhang, Weiming Hu, Ke Wang, Zhipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07599">Online Segment Any 3D Thing as Instance Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2512.07385.pdf' target='_blank'>https://arxiv.org/pdf/2512.07385.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Zhipeng Zhang, Yong Wang, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07385">How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2512.02392.pdf' target='_blank'>https://arxiv.org/pdf/2512.02392.pdf</a></span>   <span><a href='https://github.com/Spongebobbbbbbbb/FDTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Shao, Yuchen Yang, Rui Yu, Weilong Li, Xu Guo, Huaicheng Yan, Wei Wang, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02392">From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2512.01885.pdf' target='_blank'>https://arxiv.org/pdf/2512.01885.pdf</a></span>   <span><a href='https://github.com/bozeklab/TransientTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Florian Bürger, Martim Dias Gomes, Nica Gutu, Adrián E. Granada, Noémie Moreau, Katarzyna Bozek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01885">TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2511.22896.pdf' target='_blank'>https://arxiv.org/pdf/2511.22896.pdf</a></span>   <span><a href='https://vranlee.github.io/DM-3-T/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiran Li, Yeqiang Liu, Yijie Wei, Mina Han, Qiannan Guo, Zhenbo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22896">DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a fundamental task in computer vision with critical applications in autonomous driving and robotics. Multimodal MOT that integrates visible light and thermal infrared information is particularly essential for robust autonomous driving systems. However, effectively fusing these heterogeneous modalities is challenging. Simple strategies like concatenation or addition often fail to bridge the significant non-linear distribution gap between their feature representations, which can lead to modality conflicts and degrade tracking accuracy. Drawing inspiration from the connection between multimodal MOT and the iterative refinement in diffusion models, this paper proposes DM$^3$T, a novel framework that reformulates multimodal fusion as an iterative feature alignment process to generate accurate and temporally coherent object trajectories. Our approach performs iterative cross-modal harmonization through a proposed Cross-Modal Diffusion Fusion (C-MDF) module. In this process, features from both modalities provide mutual guidance, iteratively projecting them onto a shared, consistent feature manifold. This enables the learning of complementary information and achieves deeper fusion compared to conventional methods. Additionally, we introduce a plug-and-play Diffusion Refiner (DR) to enhance and refine the unified feature representation. To further improve tracking robustness, we design a Hierarchical Tracker that adaptively handles confidence estimation. DM$^3$T unifies object detection, state estimation, and data association into a comprehensive online tracking framework without complex post-processing. Extensive experiments on the VT-MOT benchmark demonstrate that our method achieves 41.7 HOTA, representing a 1.54% relative improvement over existing state-of-the-art methods. The code and models are available at https://vranlee.github.io/DM-3-T/.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2511.18344.pdf' target='_blank'>https://arxiv.org/pdf/2511.18344.pdf</a></span>   <span><a href='https://xuefeng-zhu5.github.io/MM-UAV/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyang Xu, Jinjie Gu, Xuefeng Zhu, XiaoJun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18344">A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the proliferation of low altitude unmanned aerial vehicles (UAVs), visual multi-object tracking is becoming a critical security technology, demanding significant robustness even in complex environmental conditions. However, tracking UAVs using a single visual modality often fails in challenging scenarios, such as low illumination, cluttered backgrounds, and rapid motion. Although multi-modal multi-object UAV tracking is more resilient, the development of effective solutions has been hindered by the absence of dedicated public datasets. To bridge this gap, we release MM-UAV, the first large-scale benchmark for Multi-Modal UAV Tracking, integrating three key sensing modalities, e.g. RGB, infrared (IR), and event signals. The dataset spans over 30 challenging scenarios, with 1,321 synchronised multi-modal sequences, and more than 2.8 million annotated frames. Accompanying the dataset, we provide a novel multi-modal multi-UAV tracking framework, designed specifically for UAV tracking applications and serving as a baseline for future research. Our framework incorporates two key technical innovations, e.g. an offset-guided adaptive alignment module to resolve spatio mismatches across sensors, and an adaptive dynamic fusion module to balance complementary information conveyed by different modalities. Furthermore, to overcome the limitations of conventional appearance modelling in multi-object tracking, we introduce an event-enhanced association mechanism that leverages motion cues from the event modality for more reliable identity maintenance. Comprehensive experiments demonstrate that the proposed framework consistently outperforms state-of-the-art methods. To foster further research in multi-modal UAV tracking, both the dataset and source code will be made publicly available at https://xuefeng-zhu5.github.io/MM-UAV/.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2511.17967.pdf' target='_blank'>https://arxiv.org/pdf/2511.17967.pdf</a></span>   <span><a href='https://github.com/IdolLab/CADTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Yuhao Wang, Xiantao Hu, Wenning Hao, Pingping Zhang, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17967">CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2511.17045.pdf' target='_blank'>https://arxiv.org/pdf/2511.17045.pdf</a></span>   <span><a href='https://github.com/OrcustD/RacketVision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linfeng Dong, Yuchen Yang, Hao Wu, Wei Wang, Yuenan Hou, Zhihang Zhong, Xiao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17045">RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2511.16227.pdf' target='_blank'>https://arxiv.org/pdf/2511.16227.pdf</a></span>   <span><a href='https://github.com/xuboyue1999/SwiTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyue Xu, Ruichao Hou, Tongwei Ren, Dongming Zhou, Gangshan Wu, Jinde Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16227">SwiTrack: Tri-State Switch for Cross-Modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\% and 4.3\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2511.13105.pdf' target='_blank'>https://arxiv.org/pdf/2511.13105.pdf</a></span>   <span><a href='https://github.com/VisualScienceLab-KHU/PlugTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungjae Kim, SeungJoon Lee, MyeongAh Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13105">PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2511.04128.pdf' target='_blank'>https://arxiv.org/pdf/2511.04128.pdf</a></span>   <span><a href='https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengyu Tang, Zeyuan Lu, Jiazhi Dong, Changdong Yu, Xiaoyu Wang, Yaohui Lyu, Weihao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04128">DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate perception of the marine environment through robust multi-object tracking (MOT) is essential for ensuring safe vessel navigation and effective maritime surveillance. However, the complicated maritime environment often causes camera motion and subsequent visual degradation, posing significant challenges to MOT. To address this challenge, we propose an efficient Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the framework is a parallel tracker with affine compensation, which incorporates an object detection and re-identification (ReID) branch, along with a dedicated branch for dynamic camera motion estimation. Specifically, a Reversible Columnar Detection Network (RCDN) is integrated into the detection module to leverage multi-level visual features for robust object detection. Furthermore, a lightweight Transformer-based appearance extractor (Li-TAE) is designed to capture global contextual information and generate robust appearance features. Another branch decouples platform-induced and target-intrinsic motion by constructing a projective transformation, applying platform-motion compensation within the Kalman filter, and thereby stabilizing true object trajectories. Finally, a clustering-optimized feature fusion module effectively combines motion and appearance cues to ensure identity consistency under noise, occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT attains the fastest runtime among existing ReID-based MOT frameworks while maintaining high identity consistency and robustness to jitter and occlusion. Code is available at: https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2511.01768.pdf' target='_blank'>https://arxiv.org/pdf/2511.01768.pdf</a></span>   <span><a href='https://github.com/happinesslz/UniLION' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Liu, Jinghua Hou, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01768">UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at https://github.com/happinesslz/UniLION
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2511.00510.pdf' target='_blank'>https://arxiv.org/pdf/2511.00510.pdf</a></span>   <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Luo, Hao Shi, Kunyu Peng, Fei Teng, Sheng Wu, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.00510">OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360° Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360° FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at https://github.com/xifen523/OmniTrack.
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2510.24410.pdf' target='_blank'>https://arxiv.org/pdf/2510.24410.pdf</a></span>   <span><a href='https://github.com/SDU-VelKoTek/GenTrack2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24410">A Hybrid Approach for Visual Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a visual multi-object tracking method that jointly employs stochastic and deterministic mechanisms to ensure identifier consistency for unknown and time-varying target numbers under nonlinear dynamics. A stochastic particle filter addresses nonlinear dynamics and non-Gaussian noise, with support from particle swarm optimization (PSO) to guide particles toward state distribution modes and mitigate divergence through proposed fitness measures incorporating motion consistency, appearance similarity, and social-interaction cues with neighboring targets. Deterministic association further enforces identifier consistency via a proposed cost matrix incorporating spatial consistency between particles and current detections, detection confidences, and track penalties. Subsequently, a novel scheme is proposed for the smooth updating of target states while preserving their identities, particularly for weak tracks during interactions with other targets and prolonged occlusions. Moreover, velocity regression over past states provides trend-seed velocities, enhancing particle sampling and state updates. The proposed tracker is designed to operate flexibly for both pre-recorded videos and camera live streams, where future frames are unavailable. Experimental results confirm superior performance compared to state-of-the-art trackers. The source-code reference implementations of both the proposed method and compared-trackers are provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack2
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2510.24399.pdf' target='_blank'>https://arxiv.org/pdf/2510.24399.pdf</a></span>   <span><a href='https://github.com/SDU-VelKoTek/GenTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24399">GenTrack: A New Generation of Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel multi-object tracking (MOT) method, dubbed GenTrack, whose main contributions include: a hybrid tracking approach employing both stochastic and deterministic manners to robustly handle unknown and time-varying numbers of targets, particularly in maintaining target identity (ID) consistency and managing nonlinear dynamics, leveraging particle swarm optimization (PSO) with some proposed fitness measures to guide stochastic particles toward their target distribution modes, enabling effective tracking even with weak and noisy object detectors, integration of social interactions among targets to enhance PSO-guided particles as well as improve continuous updates of both strong (matched) and weak (unmatched) tracks, thereby reducing ID switches and track loss, especially during occlusions, a GenTrack-based redefined visual MOT baseline incorporating a comprehensive state and observation model based on space consistency, appearance, detection confidence, track penalties, and social scores for systematic and efficient target updates, and the first-ever publicly available source-code reference implementation with minimal dependencies, featuring three variants, including GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation. Experimental results have shown that GenTrack provides superior performance on standard benchmarks and real-world scenarios compared to state-of-the-art trackers, with integrated implementations of baselines for fair comparison. Potential directions for future work are also discussed. The source-code reference implementations of both the proposed method and compared-trackers are provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2510.23368.pdf' target='_blank'>https://arxiv.org/pdf/2510.23368.pdf</a></span>   <span><a href='https://github.com/HengLan/PlanarTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Jiao, Xinran Liu, Xiaoqiong Liu, Xiaohui Yuan, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23368">PlanarTrack: A high-quality and challenging benchmark for large-scale planar object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planar tracking has drawn increasing interest owing to its key roles in robotics and augmented reality. Despite recent great advancement, further development of planar tracking, particularly in the deep learning era, is largely limited compared to generic tracking due to the lack of large-scale platforms. To mitigate this, we propose PlanarTrack, a large-scale high-quality and challenging benchmark for planar tracking. Specifically, PlanarTrack consists of 1,150 sequences with over 733K frames, including 1,000 short-term and 150 new long-term videos, which enables comprehensive evaluation of short- and long-term tracking performance. All videos in PlanarTrack are recorded in unconstrained conditions from the wild, which makes PlanarTrack challenging but more realistic for real-world applications. To ensure high-quality annotations, each video frame is manually annotated by four corner points with multi-round meticulous inspection and refinement. To enhance target diversity of PlanarTrack, we only capture a unique target in one sequence, which is different from existing benchmarks. To our best knowledge, PlanarTrack is by far the largest and most diverse and challenging dataset dedicated to planar tracking. To understand performance of existing methods on PlanarTrack and to provide a comparison for future research, we evaluate 10 representative planar trackers with extensive comparison and in-depth analysis. Our evaluation reveals that, unsurprisingly, the top planar trackers heavily degrade on the challenging PlanarTrack, which indicates more efforts are required for improving planar tracking. Our data and results will be released at https://github.com/HengLan/PlanarTrack
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2510.20794.pdf' target='_blank'>https://arxiv.org/pdf/2510.20794.pdf</a></span>   <span><a href='https://github.com/radar-lab/Radar_Camera_MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Cheng, Siyang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20794">Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a Multi-Object Tracking (MOT) framework that fuses radar and camera data to enhance tracking efficiency while minimizing manual interventions. Contrary to many studies that underutilize radar and assign it a supplementary role--despite its capability to provide accurate range/depth information of targets in a world 3D coordinate system--our approach positions radar in a crucial role. Meanwhile, this paper utilizes common features to enable online calibration to autonomously associate detections from radar and camera. The main contributions of this work include: (1) the development of a radar-camera fusion MOT framework that exploits online radar-camera calibration to simplify the integration of detection results from these two sensors, (2) the utilization of common features between radar and camera data to accurately derive real-world positions of detected objects, and (3) the adoption of feature matching and category-consistency checking to surpass the limitations of mere position matching in enhancing sensor association accuracy. To the best of our knowledge, we are the first to investigate the integration of radar-camera common features and their use in online calibration for achieving MOT. The efficacy of our framework is demonstrated by its ability to streamline the radar-camera mapping process and improve tracking precision, as evidenced by real-world experiments conducted in both controlled environments and actual traffic scenarios. Code is available at https://github.com/radar-lab/Radar_Camera_MOT
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2510.12565.pdf' target='_blank'>https://arxiv.org/pdf/2510.12565.pdf</a></span>   <span><a href='https://github.com/Annzstbl/MMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianhao Li, Tingfa Xu, Ying Wang, Haolin Qin, Xu Lin, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12565">MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone-based multi-object tracking is essential yet highly challenging due to small targets, severe occlusions, and cluttered backgrounds. Existing RGB-based tracking algorithms heavily depend on spatial appearance cues such as color and texture, which often degrade in aerial views, compromising reliability. Multispectral imagery, capturing pixel-level spectral reflectance, provides crucial cues that enhance object discriminability under degraded spatial conditions. However, the lack of dedicated multispectral UAV datasets has hindered progress in this domain. To bridge this gap, we introduce MMOT, the first challenging benchmark for drone-based multispectral multi-object tracking. It features three key characteristics: (i) Large Scale - 125 video sequences with over 488.8K annotations across eight categories; (ii) Comprehensive Challenges - covering diverse conditions such as extreme small targets, high-density scenarios, severe occlusions, and complex motion; and (iii) Precise Oriented Annotations - enabling accurate localization and reduced ambiguity under aerial perspectives. To better extract spectral features and leverage oriented annotations, we further present a multispectral and orientation-aware MOT scheme adapting existing methods, featuring: (i) a lightweight Spectral 3D-Stem integrating spectral features while preserving compatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for precise state estimation; and (iii) an end-to-end orientation-adaptive transformer. Extensive experiments across representative trackers consistently show that multispectral input markedly improves tracking performance over RGB baselines, particularly for small and densely packed objects. We believe our work will advance drone-based multispectral multi-object tracking research. Our MMOT, code, and benchmarks are publicly available at https://github.com/Annzstbl/MMOT.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2510.07134.pdf' target='_blank'>https://arxiv.org/pdf/2510.07134.pdf</a></span>   <span><a href='https://pku-epic.github.io/TrackVLA-plus-plus-Web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07134">TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2510.06619.pdf' target='_blank'>https://arxiv.org/pdf/2510.06619.pdf</a></span>   <span><a href='https://github.com/Fengtao191/MSITrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Feng, Tingfa Xu, Haolin Qin, Tianhao Li, Shuaihao Han, Xuyang Zou, Zhan Lv, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06619">MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking in real-world scenarios presents numerous challenges including occlusion, interference from similar objects and complex backgrounds-all of which limit the effectiveness of RGB-based trackers. Multispectral imagery, which captures pixel-level spectral reflectance, enhances target discriminability. However, the availability of multispectral tracking datasets remains limited. To bridge this gap, we introduce MSITrack, the largest and most diverse multispectral single object tracking dataset to date. MSITrack offers the following key features: (i) More Challenging Attributes-including interference from similar objects and similarity in color and texture between targets and backgrounds in natural scenarios, along with a wide range of real-world tracking challenges; (ii) Richer and More Natural Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack far exceeds the scope of existing benchmarks. Many of these scenes and categories are introduced to the multispectral tracking domain for the first time; (iii) Larger Scale-300 videos comprising over 129k frames of multispectral imagery. To ensure annotation precision, each frame has undergone meticulous processing, manual labeling and multi-stage verification. Extensive evaluations using representative trackers demonstrate that the multispectral data in MSITrack significantly improves performance over RGB-only baselines, highlighting its potential to drive future advancements in the field. The MSITrack dataset is publicly available at: https://github.com/Fengtao191/MSITrack.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2509.17323.pdf' target='_blank'>https://arxiv.org/pdf/2509.17323.pdf</a></span>   <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17323">DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Multi-Object Tracking (MOT) is a crucial component of robotic perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D cues, such as bounding boxes and motion modeling, which struggle under occlusions and close-proximity interactions. Trackers relying on these 2D cues are particularly unreliable in robotic environments, where dense targets and frequent occlusions are common. While depth information has the potential to alleviate these issues, most existing MOT datasets lack depth annotations, leading to its underexploited role in the domain. To unveil the potential of depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based detector enhanced with instance-level depth information. Specifically, we propose two key innovations: (i) foundation model-based instance-level soft depth label supervision, which refines depth prediction, and (ii) the distillation of dense depth maps to maintain global depth consistency. These strategies enable DepTR-MOT to output instance-level depth during inference, without requiring foundation models and without additional computational cost. By incorporating depth cues, our method enhances the robustness of the TBD paradigm, effectively resolving occlusion and close-proximity challenges. Experiments on both the QuadTrack and DanceTrack datasets demonstrate the effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47, respectively. In particular, results on QuadTrack, a robotic platform MOT dataset, highlight the advantages of our method in handling occlusion and close-proximity challenges in robotic tracking. The source code will be made publicly available at https://github.com/warriordby/DepTR-MOT.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2509.16527.pdf' target='_blank'>https://arxiv.org/pdf/2509.16527.pdf</a></span>   <span><a href='https://george-zhuang.github.io/lbm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangze Zheng, Shijie Lin, Haobo Zuo, Si Si, Ming-Shan Wang, Changhong Fu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16527">Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes the Lattice Boltzmann Model (LBM) to learn real-world pixel dynamicity for visual tracking. LBM decomposes visual representations into dynamic pixel lattices and solves pixel motion states through collision-streaming processes. Specifically, the high-dimensional distribution of the target pixels is acquired through a multilayer predict-update network to estimate the pixel positions and visibility. The predict stage formulates lattice collisions among the spatial neighborhood of target pixels and develops lattice streaming within the temporal visual context. The update stage rectifies the pixel distributions with online visual representations. Compared with existing methods, LBM demonstrates practical applicability in an online and real-time manner, which can efficiently adapt to real-world visual tracking tasks. Comprehensive evaluations of real-world point tracking benchmarks such as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B further demonstrates LBM's real-world practicality.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2509.13864.pdf' target='_blank'>https://arxiv.org/pdf/2509.13864.pdf</a></span>   <span><a href='https://github.com/jovanavidenovic/DAM4SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovana Videnovic, Matej Kristan, Alan Lukezic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13864">Distractor-Aware Memory-Based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent emergence of memory-based video segmentation methods such as SAM2 has led to models with excellent performance in segmentation tasks, achieving leading results on numerous benchmarks. However, these modes are not fully adjusted for visual object tracking, where distractors (i.e., objects visually similar to the target) pose a key challenge. In this paper we propose a distractor-aware drop-in memory module and introspection-based management method for SAM2, leading to DAM4SAM. Our design effectively reduces the tracking drift toward distractors and improves redetection capability after object occlusion. To facilitate the analysis of tracking in the presence of distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results on ten. Furthermore, integrating the proposed distractor-aware memory into a real-time tracker EfficientTAM leads to 11% improvement and matches tracking quality of the non-real-time SAM2.1-L on multiple tracking and segmentation benchmarks, while integration with edge-based tracker EdgeTAM delivers 4% performance boost, demonstrating a very good generalization across architectures.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2509.12913.pdf' target='_blank'>https://arxiv.org/pdf/2509.12913.pdf</a></span>   <span><a href='https://github.com/to/be/released' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hojat Ardi, Amir Jahanshahi, Ali Diba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12913">T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial object tracking remains a challenging task due to scale variations, dynamic backgrounds, clutter, and frequent occlusions. While most existing trackers emphasize spatial cues, they often overlook temporal dependencies, resulting in limited robustness in long-term tracking and under occlusion. Furthermore, correlation-based Siamese trackers are inherently constrained by the linear nature of correlation operations, making them ineffective against complex, non-linear appearance changes. To address these limitations, we introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends the SiamTPN architecture with explicit temporal modeling. Our approach incorporates temporal feature fusion and attention-based interactions, strengthening temporal consistency and enabling richer feature representations. These enhancements yield significant improvements over the baseline and achieve performance competitive with state-of-the-art trackers. Crucially, despite the added temporal modules, T-SiamTPN preserves computational efficiency. Deployed on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1 FPS, demonstrating its suitability for real-world embedded applications without notable runtime overhead. Experimental results highlight substantial gains: compared to the baseline, T-SiamTPN improves success rate by 13.7% and precision by 14.7%. These findings underscore the importance of temporal modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and efficient solution for aerial object tracking. Code is available at: https://github.com/to/be/released
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2509.11772.pdf' target='_blank'>https://arxiv.org/pdf/2509.11772.pdf</a></span>   <span><a href='https://github.com/hcmr-lab/Seg2Track-SAM2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Diogo MendonÃ§a, Tiago Barros, Cristiano Premebida, Urbano J. Nunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11772">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to operate reliably in dynamic environments. MOT ensures consistent object identity assignment and precise spatial delineation. Recent advances in foundation models, such as SAM2, have demonstrated strong zero-shot generalization for video segmentation, but their direct application to MOTS (MOT+Segmentation) remains limited by insufficient identity management and memory efficiency. This work introduces Seg2Track-SAM2, a framework that integrates pre-trained object detectors with SAM2 and a novel Seg2Track module to address track initialization, track management, and reinforcement. The proposed approach requires no fine-tuning and remains detector-agnostic. Experimental results on KITTI MOT and KITTI MOTS benchmarks show that Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth overall in both car and pedestrian classes on KITTI MOTS, while establishing a new benchmark in association accuracy (AssA). Furthermore, a sliding-window memory strategy reduces memory usage by up to 75% with negligible performance degradation, supporting deployment under resource constraints. These results confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot tracking, enhanced identity preservation, and efficient memory utilization. The code is available at https://github.com/hcmr-lab/Seg2Track-SAM2
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2509.11323.pdf' target='_blank'>https://arxiv.org/pdf/2509.11323.pdf</a></span>   <span><a href='https://github.com/SongJgit/TBDTracker' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SongJgit/filternet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Song, Wei Mei, Yunfeng Xu, Qiang Fu, Renke Kou, Lina Bu, Yucheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11323">Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion estimation is a crucial component in multi-object tracking (MOT). It predicts the trajectory of objects by analyzing the changes in their positions in consecutive frames of images, reducing tracking failures and identity switches. The Kalman filter (KF) based on the linear constant-velocity model is one of the most commonly used methods in MOT. However, it may yield unsatisfactory results when KF's parameters are mismatched and objects move in non-stationary. In this work, we utilize the learning-aided filter to handle the motion estimation of MOT. In particular, we propose a novel method named Semantic-Independent KalmanNet (SIKNet), which encodes the state vector (the input feature) using a Semantic-Independent Encoder (SIE) by two steps. First, the SIE uses a 1D convolution with a kernel size of 1, which convolves along the dimension of homogeneous-semantic elements across different state vectors to encode independent semantic information. Then it employs a fully-connected layer and a nonlinear activation layer to encode nonlinear and cross-dependency information between heterogeneous-semantic elements. To independently evaluate the performance of the motion estimation module in MOT, we constructed a large-scale semi-simulated dataset from several open-source MOT datasets. Experimental results demonstrate that the proposed SIKNet outperforms the traditional KF and achieves superior robustness and accuracy than existing learning-aided filters. The code is available at (https://github.com/SongJgit/filternet and https://github.com/SongJgit/TBDTracker).
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2509.09977.pdf' target='_blank'>https://arxiv.org/pdf/2509.09977.pdf</a></span>   <span><a href='https://github.com/lsying009/ISTASTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siying Liu, Zikai Wang, Hanle Zheng, Yifan Hu, Xilin Wang, Qingkai Yang, Jibin Wu, Hao Guo, Lei Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09977">ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-Event tracking has become a promising trend in visual object tracking to leverage the complementary strengths of both RGB images and dynamic spike events for improved performance. However, existing artificial neural networks (ANNs) struggle to fully exploit the sparse and asynchronous nature of event streams. Recent efforts toward hybrid architectures combining ANNs and spiking neural networks (SNNs) have emerged as a promising solution in RGB-Event perception, yet effectively fusing features across heterogeneous paradigms remains a challenge. In this work, we propose ISTASTrack, the first transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model employs a vision transformer to extract spatial context from RGB inputs and a spiking transformer to capture spatio-temporal dynamics from event streams. To bridge the modality and paradigm gap between ANN and SNN features, we systematically design a model-based ISTA adapter for bidirectional feature interaction between the two branches, derived from sparse representation theory by unfolding the iterative shrinkage thresholding algorithm. Additionally, we incorporate a temporal downsampling attention module within the adapter to align multi-step SNN features with single-step ANN features in the latent space, improving temporal fusion. Experimental results on RGB-Event tracking benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that ISTASTrack achieves state-of-the-art performance while maintaining high energy efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking. The code is publicly available at https://github.com/lsying009/ISTASTrack.git.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2509.09962.pdf' target='_blank'>https://arxiv.org/pdf/2509.09962.pdf</a></span>   <span><a href='https://github.com/ngobibibnbe/uncertain-identity-aware-tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anne Marthe Sophie Ngo Bibinbe, Chiron Bang, Patrick Gagnon, Jamie Ahloy-Dallaire, Eric R. Paquet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09962">An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The need for long-term multi-object tracking (MOT) is growing due to the demand for analyzing individual behaviors in videos that span several minutes. Unfortunately, due to identity switches between objects, the tracking performance of existing MOT approaches decreases over time, making them difficult to apply for long-term tracking. However, in many real-world applications, such as in the livestock sector, it is possible to obtain sporadic identifications for some of the animals from sources like feeders. To address the challenges of long-term MOT, we propose a new framework that combines both uncertain identities and tracking using a Hidden Markov Model (HMM) formulation. In addition to providing real-world identities to animals, our HMM framework improves the F1 score of ByteTrack, a leading MOT approach even with re-identification, on a 10 minute pig tracking dataset with 21 identifications at the pen's feeding station. We also show that our approach is robust to the uncertainty of identifications, with performance increasing as identities are provided more frequently. The improved performance of our HMM framework was also validated on the MOT17 and MOT20 benchmark datasets using both ByteTrack and FairMOT. The code for this new HMM framework and the new 10-minute pig tracking video dataset are available at: https://github.com/ngobibibnbe/uncertain-identity-aware-tracking
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2509.08265.pdf' target='_blank'>https://arxiv.org/pdf/2509.08265.pdf</a></span>   <span><a href='https://github.com/lgao001/HyMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Gao, Yunhe Zhang, Yan Jiang, Weiying Xie, Yunsong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08265">Hyperspectral Mamba for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking holds great promise due to the rich spectral information and fine-grained material distinctions in hyperspectral images, which are beneficial in challenging scenarios. While existing hyperspectral trackers have made progress by either transforming hyperspectral data into false-color images or incorporating modality fusion strategies, they often fail to capture the intrinsic spectral information, temporal dependencies, and cross-depth interactions. To address these limitations, a new hyperspectral object tracking network equipped with Mamba (HyMamba), is proposed. It unifies spectral, cross-depth, and temporal modeling through state space modules (SSMs). The core of HyMamba lies in the Spectral State Integration (SSI) module, which enables progressive refinement and propagation of spectral features with cross-depth and temporal spectral information. Embedded within each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial and spectral information synchronously via three directional scanning SSMs. Based on SSI and HSM, HyMamba constructs joint features from false-color and hyperspectral inputs, and enhances them through interaction with original spectral features extracted from raw hyperspectral images. Extensive experiments conducted on seven benchmark datasets demonstrate that HyMamba achieves state-of-the-art performance. For instance, it achieves 73.0\% of the AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will be released at https://github.com/lgao001/HyMamba.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2508.11531.pdf' target='_blank'>https://arxiv.org/pdf/2508.11531.pdf</a></span>   <span><a href='https://github.com/wsumel/MST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilei Wang, Gong Cheng, Pujian Lai, Dong Gao, Junwei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11531">Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient trackers achieve faster runtime by reducing computational complexity and model parameters. However, this efficiency often compromises the expense of weakened feature representation capacity, thus limiting their ability to accurately capture target states using single-layer features. To overcome this limitation, we propose Multi-State Tracker (MST), which utilizes highly lightweight state-specific enhancement (SSE) to perform specialized enhancement on multi-state features produced by multi-state generation (MSG) and aggregates them in an interactive and adaptive manner using cross-state interaction (CSI). This design greatly enhances feature representation while incurring minimal computational overhead, leading to improved tracking robustness in complex environments. Specifically, the MSG generates multiple state representations at multiple stages during feature extraction, while SSE refines them to highlight target-specific features. The CSI module facilitates information exchange between these states and ensures the integration of complementary features. Notably, the introduced SSE and CSI modules adopt a highly lightweight hidden state adaptation-based state space duality (HSA-SSD) design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters. Experimental results demonstrate that MST outperforms all previous efficient trackers across multiple datasets, significantly improving tracking accuracy and robustness. In particular, it shows excellent runtime performance, with an AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2508.10655.pdf' target='_blank'>https://arxiv.org/pdf/2508.10655.pdf</a></span>   <span><a href='https://github.com/Zhangyong-Tang/UniBench300' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Chunyang Cheng, Tao Zhou, Xiaojun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10655">Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \textit{inconsistency} between training and testing, thus leading to performance \textit{degradation}. To address these issues, this work advances in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\%. \ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2508.10567.pdf' target='_blank'>https://arxiv.org/pdf/2508.10567.pdf</a></span>   <span><a href='https://phi-wol.github.io/sparcad/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Wolters, Johannes Gilg, Torben Teepe, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10567">SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2508.10432.pdf' target='_blank'>https://arxiv.org/pdf/2508.10432.pdf</a></span>   <span><a href='https://github.com/01upup10/CRISP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baichen Liu, Qi Lyu, Xudong Wang, Jiahua Dong, Lianqing Liu, Zhi Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10432">CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual video instance segmentation demands both the plasticity to absorb new object categories and the stability to retain previously learned ones, all while preserving temporal consistency across frames. In this work, we introduce Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier attempt tailored to address the instance-wise, category-wise, and task-wise confusion in continual video instance segmentation. For instance-wise learning, we model instance tracking and construct instance correlation loss, which emphasizes the correlation with the prior query space while strengthening the specificity of the current task query. For category-wise learning, we build an adaptive residual semantic prompt (ARSP) learning framework, which constructs a learnable semantic residual prompt pool generated by category text and uses an adjustive query-prompt matching mechanism to build a mapping relationship between the query of the current task and the semantic residual prompt. Meanwhile, a semantic consistency loss based on the contrastive learning is introduced to maintain semantic coherence between object queries and residual prompts during incremental training. For task-wise learning, to ensure the correlation at the inter-task level within the query space, we introduce a concise yet powerful initialization strategy for incremental prompts. Extensive experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that CRISP significantly outperforms existing continual segmentation methods in the long-term continual video instance segmentation task, avoiding catastrophic forgetting and effectively improving segmentation and classification performance. The code is available at https://github.com/01upup10/CRISP.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2508.07250.pdf' target='_blank'>https://arxiv.org/pdf/2508.07250.pdf</a></span>   <span><a href='https://github.com/bearshng/suit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengchao Xiong, Zhenxing Wu, Sen Jia, Yuntao Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07250">SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal structure, offer distinct advantages in challenging tracking scenarios such as cluttered backgrounds and small objects. However, existing methods primarily focus on spatial interactions between the template and search regions, often overlooking spectral interactions, leading to suboptimal performance. To address this issue, this paper investigates spectral interactions from both the architectural and training perspectives. At the architectural level, we first establish band-wise long-range spatial relationships between the template and search regions using Transformers. We then model spectral interactions using the inclusion-exclusion principle from set theory, treating them as the union of spatial interactions across all bands. This enables the effective integration of both shared and band-specific spatial cues. At the training level, we introduce a spectral loss to enforce material distribution alignment between the template and predicted regions, enhancing robustness to shape deformation and appearance variations. Extensive experiments demonstrate that our tracker achieves state-of-the-art tracking performance. The source code, trained models and results will be publicly available via https://github.com/bearshng/suit to support reproducibility.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2508.05630.pdf' target='_blank'>https://arxiv.org/pdf/2508.05630.pdf</a></span>   <span><a href='https://github.com/henghuiding/MOSE-api' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip H. S. Torr, Song Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05630">MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To bridge this gap, the coMplex video Object SEgmentation (MOSEv1) dataset was introduced to facilitate VOS research in complex scenes. Building on the foundations and insights of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces much greater scene complexity, including {more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), and scenarios requiring external knowledge.} We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops on MOSEv2. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and observe similar declines, demonstrating that MOSEv2 poses challenges across tasks. These results highlight that despite strong performance on existing datasets, current VOS methods still fall short under real-world complexities. Based on our analysis of the observed challenges, we further propose several practical tricks that enhance model performance. MOSEv2 is publicly available at https://MOSE.video.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2508.05221.pdf' target='_blank'>https://arxiv.org/pdf/2508.05221.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Open_VLTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Liye Jin, Xufeng Lou, Shiao Wang, Lan Chen, Bo Jiang, Zhipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05221">ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the model's reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on https://github.com/Event-AHU/Open_VLTrack
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2508.02512.pdf' target='_blank'>https://arxiv.org/pdf/2508.02512.pdf</a></span>   <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Wu, Fei Teng, Hao Shi, Qi Jiang, Kai Luo, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02512">QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data-caused by inherent kinematic constraints and complex sensor calibration challenges-fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer-the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available at https://github.com/losehu/QuaDreamer.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2508.01802.pdf' target='_blank'>https://arxiv.org/pdf/2508.01802.pdf</a></span>   <span><a href='https://github.com/AtomScott/SoccerTrack-v2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Atom Scott, Ikuma Uchida, Kento Kuroda, Yufi Kim, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01802">SoccerTrack v2: A Full-Pitch Multi-View Soccer Dataset for Game State Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SoccerTrack v2 is a new public dataset for advancing multi-object tracking (MOT), game state reconstruction (GSR), and ball action spotting (BAS) in soccer analytics. Unlike prior datasets that use broadcast views or limited scenarios, SoccerTrack v2 provides 10 full-length, panoramic 4K recordings of university-level matches, captured with BePro cameras for complete player visibility. Each video is annotated with GSR labels (2D pitch coordinates, jersey-based player IDs, roles, teams) and BAS labels for 12 action classes (e.g., Pass, Drive, Shot). This technical report outlines the datasets structure, collection pipeline, and annotation process. SoccerTrack v2 is designed to advance research in computer vision and soccer analytics, enabling new benchmarks and practical applications in tactical analysis and automated tools.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2507.21732.pdf' target='_blank'>https://arxiv.org/pdf/2507.21732.pdf</a></span>   <span><a href='https://github.com/Sam1224/SAMITE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianxiong Xu, Lanyun Zhu, Chenxi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21732">SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is widely used in applications like autonomous driving to continuously track targets in videos. Existing methods can be roughly categorized into template matching and autoregressive methods, where the former usually neglects the temporal dependencies across frames and the latter tends to get biased towards the object categories during training, showing weak generalizability to unseen classes. To address these issues, some methods propose to adapt the video foundation model SAM2 for VOT, where the tracking results of each frame would be encoded as memory for conditioning the rest of frames in an autoregressive manner. Nevertheless, existing methods fail to overcome the challenges of object occlusions and distractions, and do not have any measures to intercept the propagation of tracking errors. To tackle them, we present a SAMITE model, built upon SAM2 with additional modules, including: (1) Prototypical Memory Bank: We propose to quantify the feature-wise and position-wise correctness of each frame's tracking results, and select the best frames to condition subsequent frames. As the features of occluded and distracting objects are feature-wise and position-wise inaccurate, their scores would naturally be lower and thus can be filtered to intercept error propagation; (2) Positional Prompt Generator: To further reduce the impacts of distractors, we propose to generate positional mask prompts to provide explicit positional clues for the target, leading to more accurate tracking. Extensive experiments have been conducted on six benchmarks, showing the superiority of SAMITE. The code is available at https://github.com/Sam1224/SAMITE.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2507.21606.pdf' target='_blank'>https://arxiv.org/pdf/2507.21606.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/SSTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaozong Zheng, Bineng Zhong, Qihua Liang, Ning Li, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21606">Decoupled Spatio-Temporal Consistency Learning for Self-Supervised Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of visual tracking has been largely driven by datasets with manual box annotations. However, these box annotations require tremendous human effort, limiting the scale and diversity of existing tracking datasets. In this work, we present a novel Self-Supervised Tracking framework named \textbf{\tracker}, designed to eliminate the need of box annotations. Specifically, a decoupled spatio-temporal consistency training framework is proposed to learn rich target information across timestamps through global spatial localization and local temporal association. This allows for the simulation of appearance and motion variations of instances in real-world scenarios. Furthermore, an instance contrastive loss is designed to learn instance-level correspondences from a multi-view perspective, offering robust instance supervision without additional labels. This new design paradigm enables {\tracker} to effectively learn generic tracking representations in a self-supervised manner, while reducing reliance on extensive box annotations. Extensive experiments on nine benchmark datasets demonstrate that {\tracker} surpasses \textit{SOTA} self-supervised tracking methods, achieving an improvement of more than 25.3\%, 20.4\%, and 14.8\% in AUC (AO) score on the GOT10K, LaSOT, TrackingNet datasets, respectively. Code: https://github.com/GXNU-ZhongLab/SSTrack.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2507.19754.pdf' target='_blank'>https://arxiv.org/pdf/2507.19754.pdf</a></span>   <span><a href='https://github.com/Seung-Hun-Lee/LOMM' target='_blank'>  GitHub</a></span> <span><a href='https://seung-hun-lee.github.io/projects/LOMM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seunghun Lee, Jiwan Seo, Minwoo Choi, Kiljoon Han, Jaehoon Jeong, Zane Durante, Ehsan Adeli, Sang Hyun Park, Sunghoon Im
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19754">Latest Object Memory Management for Temporally Consistent Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present Latest Object Memory Management (LOMM) for temporally consistent video instance segmentation that significantly improves long-term instance tracking. At the core of our method is Latest Object Memory (LOM), which robustly tracks and continuously updates the latest states of objects by explicitly modeling their presence in each frame. This enables consistent tracking and accurate identity management across frames, enhancing both performance and reliability through the VIS process. Moreover, we introduce Decoupled Object Association (DOA), a strategy that separately handles newly appearing and already existing objects. By leveraging our memory system, DOA accurately assigns object indices, improving matching accuracy and ensuring stable identity consistency, even in dynamic scenes where objects frequently appear and disappear. Extensive experiments and ablation studies demonstrate the superiority of our method over traditional approaches, setting a new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of 54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos. Project page: https://seung-hun-lee.github.io/projects/LOMM/
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2507.19239.pdf' target='_blank'>https://arxiv.org/pdf/2507.19239.pdf</a></span>   <span><a href='https://github.com/zhongjiaru/CoopTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaru Zhong, Jiahao Wang, Jiahui Xu, Xiaofan Li, Zaiqing Nie, Haibao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19239">CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\% mAP and 32.8\% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2507.16191.pdf' target='_blank'>https://arxiv.org/pdf/2507.16191.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/RSTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fansheng Zeng, Bineng Zhong, Haiying Xia, Yufei Tan, Xiantao Hu, Liangtao Shi, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16191">Explicit Context Reasoning with Supervision for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contextual reasoning with constraints is crucial for enhancing temporal consistency in cross-frame modeling for visual tracking. However, mainstream tracking algorithms typically associate context by merely stacking historical information without explicitly supervising the association process, making it difficult to effectively model the target's evolving dynamics. To alleviate this problem, we propose RSTrack, which explicitly models and supervises context reasoning via three core mechanisms. \textit{1) Context Reasoning Mechanism}: Constructs a target state reasoning pipeline, converting unconstrained contextual associations into a temporal reasoning process that predicts the current representation based on historical target states, thereby enhancing temporal consistency. \textit{2) Forward Supervision Strategy}: Utilizes true target features as anchors to constrain the reasoning pipeline, guiding the predicted output toward the true target distribution and suppressing drift in the context reasoning process. \textit{3) Efficient State Modeling}: Employs a compression-reconstruction mechanism to extract the core features of the target, removing redundant information across frames and preventing ineffective contextual associations. These three mechanisms collaborate to effectively alleviate the issue of contextual association divergence in traditional temporal modeling. Experimental results show that RSTrack achieves state-of-the-art performance on multiple benchmark datasets while maintaining real-time running speeds. Our code is available at https://github.com/GXNU-ZhongLab/RSTrack.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2507.15292.pdf' target='_blank'>https://arxiv.org/pdf/2507.15292.pdf</a></span>   <span><a href='https://szupc.github.io/EndoControlMag/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>An Wang, Rulin Zhou, Mengya Xu, Yiru Ye, Longfei Gou, Yiting Chang, Hao Chen, Chwee Ming Lim, Jiankun Wang, Hongliang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15292">EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visualizing subtle vascular motions in endoscopic surgery is crucial for surgical precision and decision-making, yet remains challenging due to the complex and dynamic nature of surgical scenes. To address this, we introduce EndoControlMag, a training-free, Lagrangian-based framework with mask-conditioned vascular motion magnification tailored to endoscopic environments. Our approach features two key modules: a Periodic Reference Resetting (PRR) scheme that divides videos into short overlapping clips with dynamically updated reference frames to prevent error accumulation while maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification (HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores using a pretrained visual tracking model to maintain accurate localization despite occlusions and view changes. It then applies one of two adaptive softening strategies to surrounding tissues: motion-based softening that modulates magnification strength proportional to observed tissue displacement, or distance-based exponential decay that simulates biomechanical force attenuation. This dual-mode approach accommodates diverse surgical scenarios-motion-based softening excels with complex tissue deformations while distance-based softening provides stability during unreliable optical flow conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four different surgery types and various challenging scenarios, including occlusions, instrument disturbance, view changes, and vessel deformations. Quantitative metrics, visual assessments, and expert surgeon evaluations demonstrate that EndoControlMag significantly outperforms existing methods in both magnification accuracy and visual quality while maintaining robustness across challenging surgical conditions. The code, dataset, and video results are available at https://szupc.github.io/EndoControlMag/.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2507.14613.pdf' target='_blank'>https://arxiv.org/pdf/2507.14613.pdf</a></span>   <span><a href='https://github.com/apple1986/DD-SAM2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoping Xu, Christopher Kabat, You Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14613">Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in medical image segmentation have been driven by deep learning; however, most existing methods remain limited by modality-specific designs and exhibit poor adaptability to dynamic medical imaging scenarios. The Segment Anything Model 2 (SAM2) and its related variants, which introduce a streaming memory mechanism for real-time video segmentation, present new opportunities for prompt-based, generalizable solutions. Nevertheless, adapting these models to medical video scenarios typically requires large-scale datasets for retraining or transfer learning, leading to high computational costs and the risk of catastrophic forgetting. To address these challenges, we propose DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction with minimal parameter overhead. This design enables effective fine-tuning of SAM2 on medical videos with limited training data. Unlike existing adapter-based methods focused solely on static images, DD-SAM2 fully exploits SAM2's streaming memory for medical video object tracking and segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation) and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best of our knowledge, this work provides an initial attempt at systematically exploring adapter-based SAM2 fine-tuning for medical video segmentation and tracking. Code, datasets, and models will be publicly available at https://github.com/apple1986/DD-SAM2.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2507.12087.pdf' target='_blank'>https://arxiv.org/pdf/2507.12087.pdf</a></span>   <span><a href='https://github.com/Salvatore-Love/YOLOv8-SMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Yu, Xinyao Liu, Guang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12087">YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned Aerial Vehicle (UAV) perspective is a highly challenging computer vision task. The difficulty stems from three main sources: the extreme scarcity of target appearance features, the complex motion entanglement caused by the combined dynamics of the camera and the targets themselves, and the frequent occlusions and identity ambiguity arising from dense flocking behavior. This paper details our championship-winning solution in the MVA 2025 "Finding Birds" Small Multi-Object Tracking Challenge (SMOT4SB), which adopts the tracking-by-detection paradigm with targeted innovations at both the detection and association levels. On the detection side, we propose a systematic training enhancement framework named \textbf{SliceTrain}. This framework, through the synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic augmentation, effectively addresses the problem of insufficient learning for small objects in high-resolution image training. On the tracking side, we designed a robust tracker that is completely independent of appearance information. By integrating a \textbf{motion direction maintenance (EMA)} mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding box expansion and distance penalty} into the OC-SORT framework, our tracker can stably handle irregular motion and maintain target identities. Our method achieves state-of-the-art performance on the SMOT4SB public test set, reaching an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness and advancement of our framework in solving complex real-world SMOT problems. The source code will be made available at https://github.com/Salvatore-Love/YOLOv8-SMOT.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2507.07603.pdf' target='_blank'>https://arxiv.org/pdf/2507.07603.pdf</a></span>   <span><a href='https://github.com/LouisFinner/HiM2SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixiang Chen, Guolei Sun, Yawei Li, Jie Qin, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07603">HiM2SAM: Enhancing SAM2 with Hierarchical Motion Estimation and Memory Optimization towards Long-term Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents enhancements to the SAM2 framework for video object tracking task, addressing challenges such as occlusions, background clutter, and target reappearance. We introduce a hierarchical motion estimation strategy, combining lightweight linear prediction with selective non-linear refinement to improve tracking accuracy without requiring additional training. In addition, we optimize the memory bank by distinguishing long-term and short-term memory frames, enabling more reliable tracking under long-term occlusions and appearance changes. Experimental results show consistent improvements across different model scales. Our method achieves state-of-the-art performance on LaSOT and LaSOText with the large model, achieving 9.6% and 7.2% relative improvements in AUC over the original SAM2, and demonstrates even larger relative gains on smaller models, highlighting the effectiveness of our trainless, low-overhead improvements for boosting long-term tracking performance. The code is available at https://github.com/LouisFinner/HiM2SAM.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2507.06543.pdf' target='_blank'>https://arxiv.org/pdf/2507.06543.pdf</a></span>   <span><a href='https://github.com/naver-ai/tobo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taekyung Kim, Dongyoon Han, Byeongho Heo, Jeongeun Park, Sangdoo Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06543">Token Bottleneck: One Token to Remember Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2507.06400.pdf' target='_blank'>https://arxiv.org/pdf/2507.06400.pdf</a></span>   <span><a href='https://vranlee.github.io/SU-T/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiran Li, Yeqiang Liu, Qiannan Guo, Yijie Wei, Hwa Liang Leo, Zhenbo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06400">When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) technology has made significant progress in terrestrial applications, but underwater tracking scenarios remain underexplored despite their importance to marine ecology and aquaculture. In this paper, we present Multiple Fish Tracking Dataset 2025 (MFT25), a comprehensive dataset specifically designed for underwater multiple fish tracking, featuring 15 diverse video sequences with 408,578 meticulously annotated bounding boxes across 48,066 frames. Our dataset captures various underwater environments, fish species, and challenging conditions including occlusions, similar appearances, and erratic motion patterns. Additionally, we introduce Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework featuring an Unscented Kalman Filter (UKF) optimized for non-linear swimming patterns of fish and a novel Fish-Intersection-over-Union (FishIoU) matching that accounts for the unique morphological characteristics of aquatic species. Extensive experiments demonstrate that our SU-T baseline achieves state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while revealing fundamental differences between fish tracking and terrestrial object tracking scenarios. The dataset and codes are released at https://vranlee.github.io/SU-T/.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2507.05899.pdf' target='_blank'>https://arxiv.org/pdf/2507.05899.pdf</a></span>   <span><a href='https://github.com/supertyd/FlexTrack/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuedong Tan, Jiawei Shao, Eduard Zamfir, Ruanjun Li, Zhaochong An, Chao Ma, Danda Paudel, Luc Van Gool, Radu Timofte, Zongwei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05899">What You Have is What You Track: Adaptive and Robust Multimodal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal data is known to be helpful for visual tracking by improving robustness to appearance variations. However, sensor synchronization challenges often compromise data availability, particularly in video settings where shortages can be temporal. Despite its importance, this area remains underexplored. In this paper, we present the first comprehensive study on tracker performance with temporally incomplete multimodal data. Unsurprisingly, under such a circumstance, existing trackers exhibit significant performance degradation, as their rigid architectures lack the adaptability needed to effectively handle missing modalities. To address these limitations, we propose a flexible framework for robust multimodal tracking. We venture that a tracker should dynamically activate computational units based on missing data rates. This is achieved through a novel Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity, coupled with a video-level masking strategy that ensures both temporal consistency and spatial completeness which is critical for effective video tracking. Surprisingly, our model not only adapts to varying missing rates but also adjusts to scene complexity. Extensive experiments show that our model achieves SOTA performance across 9 benchmarks, excelling in both conventional complete and missing modality settings. The code and benchmark will be publicly available at https://github.com/supertyd/FlexTrack/tree/main.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2507.02479.pdf' target='_blank'>https://arxiv.org/pdf/2507.02479.pdf</a></span>   <span><a href='https://github.com/loseevaya/CrowdTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Fu, Yuwen Chen, Zhuofan Chen, Mengyang Zhao, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02479">CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2507.00648.pdf' target='_blank'>https://arxiv.org/pdf/2507.00648.pdf</a></span>   <span><a href='https://github.com/Z-Z188/UMDATrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Yao, Rui Zhu, Ziqi Wang, Wenqi Ren, Yanyang Yan, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00648">UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has gained promising progress in past decades. Most of the existing approaches focus on learning target representation in well-conditioned daytime data, while for the unconstrained real-world scenarios with adverse weather conditions, e.g. nighttime or foggy environment, the tremendous domain shift leads to significant performance degradation. In this paper, we propose UMDATrack, which is capable of maintaining high-quality target state prediction under various adverse weather conditions within a unified domain adaptation framework. Specifically, we first use a controllable scenario generator to synthesize a small amount of unlabeled videos (less than 2% frames in source daytime datasets) in multiple weather conditions under the guidance of different text prompts. Afterwards, we design a simple yet effective domain-customized adapter (DCA), allowing the target objects' representation to rapidly adapt to various weather conditions without redundant model updating. Furthermore, to enhance the localization consistency between source and target domains, we propose a target-aware confidence alignment module (TCA) following optimal transport theorem. Extensive experiments demonstrate that UMDATrack can surpass existing advanced visual trackers and lead new state-of-the-art performance by a significant margin. Our code is available at https://github.com/Z-Z188/UMDATrack.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2506.23972.pdf' target='_blank'>https://arxiv.org/pdf/2506.23972.pdf</a></span>   <span><a href='https://github.com/xuboyue1999/mmtrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyue Xu, Ruichao Hou, Tongwei Ren, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23972">Visual and Memory Dual Adapter for Multi-Modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt-learning-based multi-modal trackers have achieved promising progress by employing lightweight visual adapters to incorporate auxiliary modality features into frozen foundation models. However, existing approaches often struggle to learn reliable prompts due to limited exploitation of critical cues across frequency and temporal domains. In this paper, we propose a novel visual and memory dual adapter (VMDA) to construct more robust and discriminative representations for multi-modal tracking. Specifically, we develop a simple but effective visual adapter that adaptively transfers discriminative cues from auxiliary modality to dominant modality by jointly modeling the frequency, spatial, and channel-wise features. Additionally, we design the memory adapter inspired by the human memory mechanism, which stores global temporal cues and performs dynamic update and retrieval operations to ensure the consistent propagation of reliable temporal information across video sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth, and RGB-Event tracking. Code and models are available at https://github.com/xuboyue1999/mmtrack.git.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2506.23783.pdf' target='_blank'>https://arxiv.org/pdf/2506.23783.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Mamba_FETrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiao Wang, Ju Huang, Qingchuan Ma, Jinfeng Gao, Chunyi Xu, Xiao Wang, Lan Chen, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23783">Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining traditional RGB cameras with bio-inspired event cameras for robust object tracking has garnered increasing attention in recent years. However, most existing multimodal tracking algorithms depend heavily on high-complexity Vision Transformer architectures for feature extraction and fusion across modalities. This not only leads to substantial computational overhead but also limits the effectiveness of cross-modal interactions. In this paper, we propose an efficient RGB-Event object tracking framework based on the linear-complexity Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a lightweight Prompt Generator that utilizes embedded features from each modality, together with a shared prompt pool, to dynamically generate modality-specific learnable prompt vectors. These prompts, along with the modality-specific embedded features, are then fed into a Vision Mamba-based FEMamba backbone, which facilitates prompt-guided feature extraction, cross-modal interaction, and fusion in a unified manner. Finally, the fused representations are passed to the tracking head for accurate target localization. Extensive experimental evaluations on multiple RGB-Event tracking benchmarks, including short-term COESOT dataset and long-term datasets, i.e., FE108 and FELT V2, demonstrate the superior performance and efficiency of the proposed tracking framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/Mamba_FETrack
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2506.17119.pdf' target='_blank'>https://arxiv.org/pdf/2506.17119.pdf</a></span>   <span><a href='https://github.com/GreatenAnoymous/RGBTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Guo, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17119">RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a robust framework, RGBTrack, for real-time 6D pose estimation and tracking that operates solely on RGB data, thereby eliminating the need for depth input for such dynamic and precise object pose tracking tasks. Building on the FoundationPose architecture, we devise a novel binary search strategy combined with a render-and-compare mechanism to efficiently infer depth and generate robust pose hypotheses from true-scale CAD models. To maintain stable tracking in dynamic scenarios, including rapid movements and occlusions, RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman filter and a state machine for proactive object pose recovery. In addition, RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale using an initial depth estimate, enabling seamless integration with modern generative reconstruction techniques. Extensive evaluations on benchmark datasets demonstrate that RGBTrack's novel depth-free approach achieves competitive accuracy and real-time performance, making it a promising practical solution candidate for application areas including robotics, augmented reality, and computer vision.
  The source code for our implementation will be made publicly available at https://github.com/GreatenAnoymous/RGBTrack.git.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2506.15945.pdf' target='_blank'>https://arxiv.org/pdf/2506.15945.pdf</a></span>   <span><a href='https://github.com/arc-l/karl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kowndinya Boyalakuntla, Abdeslam Boularias, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15945">KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic object tracking and grasping over eye-on-hand (EoH) systems, significantly expanding such systems capabilities in challenging, realistic environments. In comparison to the previous state-of-the-art, KARL (1) incorporates a novel six-stage RL curriculum that doubles the system's motion range, thereby greatly enhancing the system's grasping performance, (2) integrates a robust Kalman filter layer between the perception and reinforcement learning (RL) control modules, enabling the system to maintain an uncertain but continuous 6D pose estimate even when the target object temporarily exits the camera's field-of-view or undergoes rapid, unpredictable motion, and (3) introduces mechanisms to allow retries to gracefully recover from unavoidable policy execution failures. Extensive evaluations conducted in both simulation and real-world experiments qualitatively and quantitatively corroborate KARL's advantage over earlier systems, achieving higher grasp success rates and faster robot execution speed. Source code and supplementary materials for KARL will be made available at: https://github.com/arc-l/karl.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2506.12105.pdf' target='_blank'>https://arxiv.org/pdf/2506.12105.pdf</a></span>   <span><a href='https://github.com/softwarePupil/VSMB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxiang Chen, Wei Zhao, Rufei Zhang, Nannan Li, Dongjin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12105">Multiple Object Tracking in Video SAR: A Benchmark and Tracking Baseline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of multi-object tracking using video synthetic aperture radar (Video SAR), Doppler shifts induced by target motion result in artifacts that are easily mistaken for shadows caused by static occlusions. Moreover, appearance changes of the target caused by Doppler mismatch may lead to association failures and disrupt trajectory continuity. A major limitation in this field is the lack of public benchmark datasets for standardized algorithm evaluation. To address the above challenges, we collected and annotated 45 video SAR sequences containing moving targets, and named the Video SAR MOT Benchmark (VSMB). Specifically, to mitigate the effects of trailing and defocusing in moving targets, we introduce a line feature enhancement mechanism that emphasizes the positive role of motion shadows and reduces false alarms induced by static occlusions. In addition, to mitigate the adverse effects of target appearance variations, we propose a motion-aware clue discarding mechanism that substantially improves tracking robustness in Video SAR. The proposed model achieves state-of-the-art performance on the VSMB, and the dataset and model are released at https://github.com/softwarePupil/VSMB.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2506.02866.pdf' target='_blank'>https://arxiv.org/pdf/2506.02866.pdf</a></span>   <span><a href='https://github.com/AhsanBaidar/MVTD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahsan Baidar Bakht, Muhayy Ud Din, Sajid Javed, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02866">MVTD: A Benchmark Dataset for Maritime Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is a fundamental task with widespread applications in autonomous navigation, surveillance, and maritime robotics. Despite significant advances in generic object tracking, maritime environments continue to present unique challenges, including specular water reflections, low-contrast targets, dynamically changing backgrounds, and frequent occlusions. These complexities significantly degrade the performance of state-of-the-art tracking algorithms, highlighting the need for domain-specific datasets. To address this gap, we introduce the Maritime Visual Tracking Dataset (MVTD), a comprehensive and publicly available benchmark specifically designed for maritime VOT. MVTD comprises 182 high-resolution video sequences, totaling approximately 150,000 frames, and includes four representative object classes: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset captures a diverse range of operational conditions and maritime scenarios, reflecting the real-world complexities of maritime environments. We evaluated 14 recent SOTA tracking algorithms on the MVTD benchmark and observed substantial performance degradation compared to their performance on general-purpose datasets. However, when fine-tuned on MVTD, these models demonstrate significant performance gains, underscoring the effectiveness of domain adaptation and the importance of transfer learning in specialized tracking contexts. The MVTD dataset fills a critical gap in the visual tracking community by providing a realistic and challenging benchmark for maritime scenarios. Dataset and Source Code can be accessed here "https://github.com/AhsanBaidar/MVTD".
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2506.01373.pdf' target='_blank'>https://arxiv.org/pdf/2506.01373.pdf</a></span>   <span><a href='https://github.com/tstanczyk95/McByte' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomasz Stanczyk, Seongro Yoon, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01373">No Train Yet Gain: Towards Generic Multi-Object Tracking in Sports and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is essential for sports analytics, enabling performance evaluation and tactical insights. However, tracking in sports is challenging due to fast movements, occlusions, and camera shifts. Traditional tracking-by-detection methods require extensive tuning, while segmentation-based approaches struggle with track processing. We propose McByte, a tracking-by-detection framework that integrates temporally propagated segmentation mask as an association cue to improve robustness without per-video tuning. Unlike many existing methods, McByte does not require training, relying solely on pre-trained models and object detectors commonly used in the community. Evaluated on SportsMOT, DanceTrack, SoccerNet-tracking 2022 and MOT17, McByte demonstrates strong performance across sports and general pedestrian tracking. Our results highlight the benefits of mask propagation for a more adaptable and generalizable MOT approach. Code will be made available at https://github.com/tstanczyk95/McByte.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2506.00774.pdf' target='_blank'>https://arxiv.org/pdf/2506.00774.pdf</a></span>   <span><a href='https://github.com/Milad-Khanchi/DepthMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Khanchi, Maria Amer, Charalambos Poullis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00774">Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current motion-based multiple object tracking (MOT) approaches rely heavily on Intersection-over-Union (IoU) for object association. Without using 3D features, they are ineffective in scenarios with occlusions or visually similar objects. To address this, our paper presents a novel depth-aware framework for MOT. We estimate depth using a zero-shot approach and incorporate it as an independent feature in the association process. Additionally, we introduce a Hierarchical Alignment Score that refines IoU by integrating both coarse bounding box overlap and fine-grained (pixel-level) alignment to improve association accuracy without requiring additional learnable parameters. To our knowledge, this is the first MOT framework to incorporate 3D features (monocular depth) as an independent decision matrix in the association step. Our framework achieves state-of-the-art results on challenging benchmarks without any training nor fine-tuning. The code is available at https://github.com/Milad-Khanchi/DepthMOT
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2506.00325.pdf' target='_blank'>https://arxiv.org/pdf/2506.00325.pdf</a></span>   <span><a href='https://github.com/pgao-lab/DiffDf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Xu, Peng Gao, Wen-Jia Tang, Fei Wang, Ru-Yue Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00325">Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although deep learning-based visual tracking methods have made significant progress, they exhibit vulnerabilities when facing carefully designed adversarial attacks, which can lead to a sharp decline in tracking performance. To address this issue, this paper proposes for the first time a novel adversarial defense method based on denoise diffusion probabilistic models, termed DiffDf, aimed at effectively improving the robustness of existing visual tracking methods against adversarial attacks. DiffDf establishes a multi-scale defense mechanism by combining pixel-level reconstruction loss, semantic consistency loss, and structural similarity loss, effectively suppressing adversarial perturbations through a gradual denoising process. Extensive experimental results on several mainstream datasets show that the DiffDf method demonstrates excellent generalization performance for trackers with different architectures, significantly improving various evaluation metrics while achieving real-time inference speeds of over 30 FPS, showcasing outstanding defense performance and efficiency. Codes are available at https://github.com/pgao-lab/DiffDf.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2505.23704.pdf' target='_blank'>https://arxiv.org/pdf/2505.23704.pdf</a></span>   <span><a href='https://github.com/HamadYA/CLDTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad Alansari, Sajid Javed, Iyyakutti Iyappan Ganapathi, Sara Alansari, Muzammal Naseer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23704">CLDTracker: A Comprehensive Language Description for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VOT remains a fundamental yet challenging task in computer vision due to dynamic appearance changes, occlusions, and background clutter. Traditional trackers, relying primarily on visual cues, often struggle in such complex scenarios. Recent advancements in VLMs have shown promise in semantic understanding for tasks like open-vocabulary detection and image captioning, suggesting their potential for VOT. However, the direct application of VLMs to VOT is hindered by critical limitations: the absence of a rich and comprehensive textual representation that semantically captures the target object's nuances, limiting the effective use of language information; inefficient fusion mechanisms that fail to optimally integrate visual and textual features, preventing a holistic understanding of the target; and a lack of temporal modeling of the target's evolving appearance in the language domain, leading to a disconnect between the initial description and the object's subsequent visual changes. To bridge these gaps and unlock the full potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive Language Description framework for robust visual Tracking. Our tracker introduces a dual-branch architecture consisting of a textual and a visual branch. In the textual branch, we construct a rich bag of textual descriptions derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with semantic and contextual cues to address the lack of rich textual representation. Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves SOTA performance, validating the effectiveness of leveraging robust and temporally-adaptive vision-language representations for tracking. Code and models are publicly available at: https://github.com/HamadYA/CLDTracker
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2505.23189.pdf' target='_blank'>https://arxiv.org/pdf/2505.23189.pdf</a></span>   <span><a href='https://pku-epic.github.io/TrackVLA-web' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoan Wang, Jiazhao Zhang, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23189">TrackVLA: Embodied Visual Tracking in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed. Our project page is: https://pku-epic.github.io/TrackVLA-web.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2505.21795.pdf' target='_blank'>https://arxiv.org/pdf/2505.21795.pdf</a></span>   <span><a href='https://github.com/ClaudiaCuttano/SANSA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ClaudiaCuttano/SANSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21795">SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at https://github.com/ClaudiaCuttano/SANSA.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2505.16321.pdf' target='_blank'>https://arxiv.org/pdf/2505.16321.pdf</a></span>   <span><a href='https://github.com/zj5559/Motion-Prompt-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16321">Efficient Motion Prompt Learning for Robust Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the challenges of processing temporal information, most trackers depend solely on visual discriminability and overlook the unique temporal coherence of video data. In this paper, we propose a lightweight and plug-and-play motion prompt tracking method. It can be easily integrated into existing vision-based trackers to build a joint tracking framework leveraging both motion and vision cues, thereby achieving robust tracking through efficient prompt learning. A motion encoder with three different positional encodings is proposed to encode the long-term motion trajectory into the visual embedding space, while a fusion decoder and an adaptive weight mechanism are designed to dynamically fuse visual and motion features. We integrate our motion module into three different trackers with five models in total. Experiments on seven challenging tracking benchmarks demonstrate that the proposed motion module significantly improves the robustness of vision-based trackers, with minimal training costs and negligible speed sacrifice. Code is available at https://github.com/zj5559/Motion-Prompt-Tracking.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2505.15928.pdf' target='_blank'>https://arxiv.org/pdf/2505.15928.pdf</a></span>   <span><a href='https://github.com/t-montes/viqagent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tony Montes, Fernando Lozano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15928">ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Video Question Answering (VideoQA) have introduced LLM-based agents, modular frameworks, and procedural solutions, yielding promising results. These systems use dynamic agents and memory-based mechanisms to break down complex tasks and refine answers. However, significant improvements remain in tracking objects for grounding over time and decision-making based on reasoning to better align object references with language model outputs, as newer models get better at both tasks. This work presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA) that combines a Chain-of-Thought framework with grounding reasoning alongside YOLO-World to enhance object tracking and alignment. This approach establishes a new state-of-the-art in VideoQA and Video Understanding, showing enhanced performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also enables cross-checking of grounding timeframes, improving accuracy and providing valuable support for verification and increased output reliability across multiple video domains. The code is available at https://github.com/t-montes/viqagent.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2505.12903.pdf' target='_blank'>https://arxiv.org/pdf/2505.12903.pdf</a></span>   <span><a href='https://github.com/Event-AHU/SlowFast_Event_Track' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiao Wang, Xiao Wang, Liye Jin, Bo Jiang, Lin Zhu, Lan Chen, Yonghong Tian, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12903">Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing tracking algorithms typically rely on low-frame-rate RGB cameras coupled with computationally intensive deep neural network architectures to achieve effective tracking. However, such frame-based methods inherently face challenges in achieving low-latency performance and often fail in resource-constrained environments. Visual object tracking using bio-inspired event cameras has emerged as a promising research direction in recent years, offering distinct advantages for low-latency applications. In this paper, we propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different operational requirements, termed SFTrack. The proposed framework supports two complementary modes, i.e., a high-precision slow tracker for scenarios with sufficient computational resources, and an efficient fast tracker tailored for latency-aware, resource-constrained environments. Specifically, our framework first performs graph-based representation learning from high-temporal-resolution event streams, and then integrates the learned graph-structured information into two FlashAttention-based vision backbones, yielding the slow and fast trackers, respectively. The fast tracker achieves low latency through a lightweight network design and by producing multiple bounding box outputs in a single forward pass. Finally, we seamlessly combine both trackers via supervised fine-tuning and further enhance the fast tracker's performance through a knowledge distillation strategy. Extensive experiments on public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the effectiveness and efficiency of our proposed method across different real-world scenarios. The source code has been released on https://github.com/Event-AHU/SlowFast_Event_Track.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2505.08999.pdf' target='_blank'>https://arxiv.org/pdf/2505.08999.pdf</a></span>   <span><a href='https://github.com/pgao-lab/AMGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Long Tian, Peng Gao, Xiao Liu, Long Xu, Hamido Fujita, Hanan Aljuai, Mao-Li Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08999">Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, visual tracking methods based on convolutional neural networks and Transformers have achieved remarkable performance and have been successfully applied in fields such as autonomous driving. However, the numerous security issues exposed by deep learning models have gradually affected the reliable application of visual tracking methods in real-world scenarios. Therefore, how to reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks has become a critical problem that needs to be addressed. To this end, we propose an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking. This method integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing, which can significantly enhance the transferability and attack effectiveness of adversarial examples. AMGA randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model. This paradigm minimizes the gap between white- and black-box adversarial attacks, thus achieving excellent attack performance in black-box scenarios. Extensive experimental results on large-scale datasets such as OTB2015, LaSOT, and GOT-10k demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples. Codes and data are available at https://github.com/pgao-lab/AMGA.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2505.05936.pdf' target='_blank'>https://arxiv.org/pdf/2505.05936.pdf</a></span>   <span><a href='https://github.com/Nightwatch-Fox11/CGTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihong Li, Xiaoqiong Liu, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05936">CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in visual object tracking have markedly improved the capabilities of unmanned aerial vehicle (UAV) tracking, which is a critical component in real-world robotics applications. While the integration of hierarchical lightweight networks has become a prevalent strategy for enhancing efficiency in UAV tracking, it often results in a significant drop in network capacity, which further exacerbates challenges in UAV scenarios, such as frequent occlusions and extreme changes in viewing angles. To address these issues, we introduce a novel family of UAV trackers, termed CGTrack, which combines explicit and implicit techniques to expand network capacity within a coarse-to-fine framework. Specifically, we first introduce a Hierarchical Feature Cascade (HFC) module that leverages the spirit of feature reuse to increase network capacity by integrating the deep semantic cues with the rich spatial information, incurring minimal computational costs while enhancing feature representation. Based on this, we design a novel Lightweight Gated Center Head (LGCH) that utilizes gating mechanisms to decouple target-oriented coordinates from previously expanded features, which contain dense local discriminative information. Extensive experiments on three challenging UAV tracking benchmarks demonstrate that CGTrack achieves state-of-the-art performance while running fast. Code will be available at https://github.com/Nightwatch-Fox11/CGTrack.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2505.01257.pdf' target='_blank'>https://arxiv.org/pdf/2505.01257.pdf</a></span>   <span><a href='https://github.com/TrackingLaboratory/CAMELTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vladimir Somers, Baptiste Standaert, Victor Joos, Alexandre Alahi, Christophe De Vleeschouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01257">CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online multi-object tracking has been recently dominated by tracking-by-detection (TbD) methods, where recent advances rely on increasingly sophisticated heuristics for tracklet representation, feature fusion, and multi-stage matching. The key strength of TbD lies in its modular design, enabling the integration of specialized off-the-shelf models like motion predictors and re-identification. However, the extensive usage of human-crafted rules for temporal associations makes these methods inherently limited in their ability to capture the complex interplay between various tracking cues. In this work, we introduce CAMEL, a novel association module for Context-Aware Multi-Cue ExpLoitation, that learns resilient association strategies directly from data, breaking free from hand-crafted heuristics while maintaining TbD's valuable modularity. At its core, CAMEL employs two transformer-based modules and relies on a novel association-centric training scheme to effectively model the complex interactions between tracked targets and their various association cues. Unlike end-to-end detection-by-tracking approaches, our method remains lightweight and fast to train while being able to leverage external off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack, achieves state-of-the-art performance on multiple tracking benchmarks. Our code is available at https://github.com/TrackingLaboratory/CAMELTrack.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2504.21716.pdf' target='_blank'>https://arxiv.org/pdf/2504.21716.pdf</a></span>   <span><a href='https://github.com/marc1198/chat-hsr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Glocker, Peter HÃ¶nig, Matthias Hirschmanner, Markus Vincze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21716">LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2504.18068.pdf' target='_blank'>https://arxiv.org/pdf/2504.18068.pdf</a></span>   <span><a href='https://github.com/bytepioneerX/s3mot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohao Yan, Shaoquan Feng, Xingxing Li, Yuxuan Zhou, Chunxi Xia, Shengyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18068">S3MOT: Monocular 3D Object Tracking with Selective State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at https://github.com/bytepioneerX/s3mot.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2504.15609.pdf' target='_blank'>https://arxiv.org/pdf/2504.15609.pdf</a></span>   <span><a href='https://github.com/LiYunfengLYF/SonarT165' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfeng Li, Bo Wang, Jiahao Wan, Xueyi Wu, Ye Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15609">SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater observation systems typically integrate optical cameras and imaging sonar systems. When underwater visibility is insufficient, only sonar systems can provide stable data, which necessitates exploration of the underwater acoustic object tracking (UAOT) task. Previous studies have explored traditional methods and Siamese networks for UAOT. However, the absence of a unified evaluation benchmark has significantly constrained the value of these methods. To alleviate this limitation, we propose the first large-scale UAOT benchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and 205K high-quality annotations. Experimental results demonstrate that SonarT165 reveals limitations in current state-of-the-art SOT trackers. To address these limitations, we propose STFTrack, an efficient framework for acoustic object tracking. It includes two novel modules, a multi-view template fusion module (MTFM) and an optimal trajectory correction module (OTCM). The MTFM module integrates multi-view feature of both the original image and the binary image of the dynamic template, and introduces a cross-attention-like layer to fuse the spatio-temporal target representations. The OTCM module introduces the acoustic-response-equivalent pixel property and proposes normalized pixel brightness response scores, thereby suppressing suboptimal matches caused by inaccurate Kalman filter prediction boxes. To further improve the model feature, STFTrack introduces a acoustic image enhancement method and a Frequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive experiments show the proposed STFTrack achieves state-of-the-art performance on the proposed benchmark. The code is available at https://github.com/LiYunfengLYF/SonarT165.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2504.14423.pdf' target='_blank'>https://arxiv.org/pdf/2504.14423.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Adversarial_Attack_Defense' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Chen, Xiao Wang, Haowen Wang, Bo Jiang, Lin Zhu, Dawei Zhang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14423">Adversarial Attack for RGB-Event based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking is a crucial research topic in the fields of computer vision and multi-modal fusion. Among various approaches, robust visual tracking that combines RGB frames with Event streams has attracted increasing attention from researchers. While striving for high accuracy and efficiency in tracking, it is also important to explore how to effectively conduct adversarial attacks and defenses on RGB-Event stream tracking algorithms, yet research in this area remains relatively scarce. To bridge this gap, in this paper, we propose a cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because of the diverse representations of Event streams, and given that Event voxels and frames are more commonly used, this paper will focus on these two representations for an in-depth study. Specifically, for the RGB-Event voxel, we first optimize the perturbation by adversarial loss to generate RGB frame adversarial examples. For discrete Event voxel representations, we propose a two-step attack strategy, more in detail, we first inject Event voxels into the target region as initialized adversarial examples, then, conduct a gradient-guided optimization by perturbing the spatial location of the Event voxels. For the RGB-Event frame based tracking, we optimize the cross-modal universal perturbation by integrating the gradient information from multimodal data. We evaluate the proposed approach against attacks on three widely used RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive experiments show that our method significantly reduces the performance of the tracker across numerous datasets in both unimodal and multimodal scenarios. The source code will be released on https://github.com/Event-AHU/Adversarial_Attack_Defense
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2504.10165.pdf' target='_blank'>https://arxiv.org/pdf/2504.10165.pdf</a></span>   <span><a href='https://dat-nguyenvn.github.io/WildLive/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nguyen Ngoc Dat, Tom Richardson, Matthew Watson, Kilian Meier, Jenna Kline, Sid Reid, Guy Maalouf, Duncan Hine, Majid Mirmehdi, Tilo Burghardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10165">WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Live tracking of wildlife via high-resolution video processing directly onboard drones is widely unexplored and most existing solutions rely on streaming video to ground stations to support navigation. Yet, both autonomous animal-reactive flight control beyond visual line of sight and/or mission-specific individual and behaviour recognition tasks rely to some degree on this capability. In response, we introduce WildLive - a near real-time animal detection and tracking framework for high-resolution imagery running directly onboard uncrewed aerial vehicles (UAVs). The system performs multi-animal detection and tracking at 17.81fps for HD and 7.53fps on 4K video streams suitable for operation during higher altitude flights to minimise animal disturbance. Our system is optimised for Jetson Orin AGX onboard hardware. It integrates the efficiency of sparse optical flow tracking and mission-specific sampling with device-optimised and proven YOLO-driven object detection and segmentation techniques. Essentially, computational resource is focused onto spatio-temporal regions of high uncertainty to significantly improve UAV processing speeds. Alongside, we introduce our WildLive dataset, which comprises 200K+ annotated animal instances across 19K+ frames from 4K UAV videos collected at the Ol Pejeta Conservancy in Kenya. All frames contain ground truth bounding boxes, segmentation masks, as well as individual tracklets and tracking point trajectories. We compare our system against current object tracking approaches including OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments with onboard hardware confirm that near real-time high-resolution wildlife tracking is possible on UAVs whilst maintaining high accuracy levels as needed for future navigational and mission-specific animal-centric operational autonomy. Our materials are available at: https://dat-nguyenvn.github.io/WildLive/
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2504.09195.pdf' target='_blank'>https://arxiv.org/pdf/2504.09195.pdf</a></span>   <span><a href='https://github.com/Tzoulio/ReferGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tzoulio Chamiti, Leandro Di Bella, Adrian Munteanu, Nikos Deligiannis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09195">ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple objects based on textual queries is a challenging task that requires linking language understanding with object association across frames. Previous works typically train the whole process end-to-end or integrate an additional referring text module into a multi-object tracker, but they both require supervised training and potentially struggle with generalization to open-set queries. In this work, we introduce ReferGPT, a novel zero-shot referring multi-object tracking framework. We provide a multi-modal large language model (MLLM) with spatial knowledge enabling it to generate 3D-aware captions. This enhances its descriptive capabilities and supports a more flexible referring vocabulary without training. We also propose a robust query-matching strategy, leveraging CLIP-based semantic encoding and fuzzy matching to associate MLLM generated captions with user queries. Extensive experiments on Refer-KITTI, Refer-KITTIv2 and Refer-KITTI+ demonstrate that ReferGPT achieves competitive performance against trained methods, showcasing its robustness and zero-shot capabilities in autonomous driving. The codes are available on https://github.com/Tzoulio/ReferGPT
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2504.07962.pdf' target='_blank'>https://arxiv.org/pdf/2504.07962.pdf</a></span>   <span><a href='https://glus-video.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07962">GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between "Ref" and "VOS": they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse "context frames" provides global information, while a stream of continuous "query frames" conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at https://glus-video.github.io/.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2504.04519.pdf' target='_blank'>https://arxiv.org/pdf/2504.04519.pdf</a></span>   <span><a href='https://github.com/TripleJoy/SAM2MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Jiang, Zelin Wang, Manqi Zhao, Yin Li, DongSheng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04519">SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segment Anything 2 (SAM2) enables robust single-object tracking using segmentation. To extend this to multi-object tracking (MOT), we propose SAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking by Detection or Tracking by Query, SAM2MOT directly generates tracking boxes from segmentation masks, reducing reliance on detection accuracy. SAM2MOT has two key advantages: zero-shot generalization, allowing it to work across datasets without fine-tuning, and strong object association, inherited from SAM2. To further improve performance, we integrate a trajectory manager system for precise object addition and removal, and a cross-object interaction module to handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show state-of-the-art results. Notably, SAM2MOT outperforms existing methods on DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT. Code is available at https://github.com/TripleJoy/SAM2MOT.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2504.01321.pdf' target='_blank'>https://arxiv.org/pdf/2504.01321.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Jialin Gao, Xin Sun, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01321">COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer has recently demonstrated great potential in improving vision-language (VL) tracking algorithms. However, most of the existing VL trackers rely on carefully designed mechanisms to perform the multi-stage multi-modal fusion. Additionally, direct multi-modal fusion without alignment ignores distribution discrepancy between modalities in feature space, potentially leading to suboptimal representations. In this work, we propose COST, a contrastive one-stage transformer fusion framework for VL tracking, aiming to learn semantically consistent and unified VL representations. Specifically, we introduce a contrastive alignment strategy that maximizes mutual information (MI) between a video and its corresponding language description. This enables effective cross-modal alignment, yielding semantically consistent features in the representation space. By leveraging a visual-linguistic transformer, we establish an efficient multi-modal fusion and reasoning mechanism, empirically demonstrating that a simple stack of transformer encoders effectively enables unified VL representations. Moreover, we contribute a newly collected VL tracking benchmark dataset for small object tracking, named VL-SOT500, with bounding boxes and language descriptions. Our dataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated to evaluating generic and high-speed small object tracking, respectively. Small object tracking is notoriously challenging due to weak appearance and limited features, and this dataset is, to the best of our knowledge, the first to explore the usage of language cues to enhance visual representation for small object tracking. Extensive experiments demonstrate that COST achieves state-of-the-art performance on five existing VL tracking datasets, as well as on our proposed VL-SOT500 dataset. Source codes and dataset will be made publicly available.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2504.00954.pdf' target='_blank'>https://arxiv.org/pdf/2504.00954.pdf</a></span>   <span><a href='https://github.com/BwLiu01/IDMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bangwei Liu, Yicheng Bao, Shaohui Lin, Xuhong Wang, Xin Tan, Yingchun Wang, Yuan Xie, Chaochao Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00954">IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal retrieval systems are becoming increasingly vital for cutting-edge AI technologies, such as embodied AI and AI-driven digital content industries. However, current multimodal retrieval tasks lack sufficient complexity and demonstrate limited practical application value. It spires us to design Instance-Driven Multimodal Image Retrieval (IDMR), a novel task that requires models to retrieve images containing the same instance as a query image while matching a text-described scenario. Unlike existing retrieval tasks focused on global image similarity or category-level matching, IDMR demands fine-grained instance-level consistency across diverse contexts. To benchmark this capability, we develop IDMR-bench using real-world object tracking and first-person video data. Addressing the scarcity of training data, we propose a cross-domain synthesis method that creates 557K training samples by cropping objects from standard detection datasets. Our Multimodal Large Language Model (MLLM) based retrieval model, trained on 1.2M samples, outperforms state-of-the-art approaches on both traditional benchmarks and our zero-shot IDMR-bench. Experimental results demonstrate previous models' limitations in instance-aware retrieval and highlight the potential of MLLM for advanced retrieval applications. The whole training dataset, codes and models, with wide ranges of sizes, are available at https://github.com/BwLiu01/IDMR.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2503.18338.pdf' target='_blank'>https://arxiv.org/pdf/2503.18338.pdf</a></span>   <span><a href='https://github.com/WenRuiCai/SPMTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenrui Cai, Qingjie Liu, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18338">SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture of Experts for Scalable Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most state-of-the-art trackers adopt one-stream paradigm, using a single Vision Transformer for joint feature extraction and relation modeling of template and search region images. However, relation modeling between different image patches exhibits significant variations. For instance, background regions dominated by target-irrelevant information require reduced attention allocation, while foreground, particularly boundary areas, need to be be emphasized. A single model may not effectively handle all kinds of relation modeling simultaneously. In this paper, we propose a novel tracker called SPMTrack based on mixture-of-experts tailored for visual tracking task (TMoE), combining the capability of multiple experts to handle diverse relation modeling more flexibly. Benefiting from TMoE, we extend relation modeling from image pairs to spatio-temporal context, further improving tracking accuracy with minimal increase in model parameters. Moreover, we employ TMoE as a parameter-efficient fine-tuning method, substantially reducing trainable parameters, which enables us to train SPMTrack of varying scales efficiently and preserve the generalization ability of pretrained models to achieve superior performance. We conduct experiments on seven datasets, and experimental results demonstrate that our method significantly outperforms current state-of-the-art trackers. The source code is available at https://github.com/WenRuiCai/SPMTrack.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2503.18282.pdf' target='_blank'>https://arxiv.org/pdf/2503.18282.pdf</a></span>   <span><a href='https://github.com/open-starlab/TrackID3x3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuhiro Yamada, Li Yin, Qingrui Hu, Ning Ding, Shunsuke Iwashita, Jun Ichikawa, Kiwamu Kotani, Calvin Yeung, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18282">TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with Identification and Pose Estimation in 3x3 Basketball Full-court Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking, player identification, and pose estimation are fundamental components of sports analytics, essential for analyzing player movements, performance, and tactical strategies. However, existing datasets and methodologies primarily target mainstream team sports such as soccer and conventional 5-on-5 basketball, often overlooking scenarios involving fixed-camera setups commonly used at amateur levels, less mainstream sports, or datasets that explicitly incorporate pose annotations. In this paper, we propose the TrackID3x3 dataset, the first publicly available comprehensive dataset specifically designed for multi-player tracking, player identification, and pose estimation in 3x3 basketball scenarios. The dataset comprises three distinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera footage), capturing diverse full-court camera perspectives and environments. We also introduce the Track-ID task, a simplified variant of the game state reconstruction task that excludes field detection and focuses exclusively on fixed-camera scenarios. To evaluate performance, we propose a baseline algorithm called Track-ID algorithm, tailored to assess tracking and identification quality. Furthermore, our benchmark experiments, utilizing recent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose estimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results and highlight remaining challenges. Our dataset and evaluation benchmarks provide a solid foundation for advancing automated analytics in 3x3 basketball. Dataset and code will be available at https://github.com/open-starlab/TrackID3x3.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2503.17699.pdf' target='_blank'>https://arxiv.org/pdf/2503.17699.pdf</a></span>   <span><a href='https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolin Qin, Tingfa Xu, Tianhao Li, Zhenxiang Chen, Tao Feng, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17699">MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>UAV tracking faces significant challenges in real-world scenarios, such as small-size targets and occlusions, which limit the performance of RGB-based trackers. Multispectral images (MSI), which capture additional spectral information, offer a promising solution to these challenges. However, progress in this field has been hindered by the lack of relevant datasets. To address this gap, we introduce the first large-scale Multispectral UAV Single Object Tracking dataset (MUST), which includes 250 video sequences spanning diverse environments and challenges, providing a comprehensive data foundation for multispectral UAV tracking. We also propose a novel tracking framework, UNTrack, which encodes unified spectral, spatial, and temporal features from spectrum prompts, initial templates, and sequential searches. UNTrack employs an asymmetric transformer with a spectral background eliminate mechanism for optimal relationship modeling and an encoder that continuously updates the spectrum prompt to refine tracking, improving both accuracy and efficiency. Extensive experiments show that our proposed UNTrack outperforms state-of-the-art UAV trackers. We believe our dataset and framework will drive future research in this area. The dataset is available on https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2503.12888.pdf' target='_blank'>https://arxiv.org/pdf/2503.12888.pdf</a></span>   <span><a href='https://github.com/ManOfStory/UncTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Yao, Yang Guo, Yanyang Yan, Wenqi Ren, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12888">UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based trackers have achieved promising success and become the dominant tracking paradigm due to their accuracy and efficiency. Despite the substantial progress, most of the existing approaches tackle object tracking as a deterministic coordinate regression problem, while the target localization uncertainty has been greatly overlooked, which hampers trackers' ability to maintain reliable target state prediction in challenging scenarios. To address this issue, we propose UncTrack, a novel uncertainty-aware transformer tracker that predicts the target localization uncertainty and incorporates this uncertainty information for accurate target state inference. Specifically, UncTrack utilizes a transformer encoder to perform feature interaction between template and search images. The output features are passed into an uncertainty-aware localization decoder (ULD) to coarsely predict the corner-based localization and the corresponding localization uncertainty. Then the localization uncertainty is sent into a prototype memory network (PMN) to excavate valuable historical information to identify whether the target state prediction is reliable or not. To enhance the template representation, the samples with high confidence are fed back into the prototype memory bank for memory updating, making the tracker more robust to challenging appearance variations. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods. Our code is available at https://github.com/ManOfStory/UncTrack.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2503.12562.pdf' target='_blank'>https://arxiv.org/pdf/2503.12562.pdf</a></span>   <span><a href='https://github.com/HELLORPG/HATReID-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruopeng Gao, Yuyao Wang, Chunxu Liu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12562">History-Aware Transformation of ReID Features for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of multiple object tracking (MOT) is to detect all objects in a video and bind them into multiple trajectories. Generally, this process is carried out in two steps: detecting objects and associating them across frames based on various cues and metrics. Many studies and applications adopt object appearance, also known as re-identification (ReID) features, for target matching through straightforward similarity calculation. However, we argue that this practice is overly naive and thus overlooks the unique characteristics of MOT tasks. Unlike regular re-identification tasks that strive to distinguish all potential targets in a general representation, multi-object tracking typically immerses itself in differentiating similar targets within the same video sequence. Therefore, we believe that seeking a more suitable feature representation space based on the different sample distributions of each sequence will enhance tracking performance. In this paper, we propose using history-aware transformations on ReID features to achieve more discriminative appearance representations. Specifically, we treat historical trajectory features as conditions and employ a tailored Fisher Linear Discriminant (FLD) to find a spatial projection matrix that maximizes the differentiation between different trajectories. Our extensive experiments reveal that this training-free projection can significantly boost feature-only trackers to achieve competitive, even superior tracking performance compared to state-of-the-art methods while also demonstrating impressive zero-shot transfer capabilities. This demonstrates the effectiveness of our proposal and further encourages future investigation into the importance and customization of ReID models in multiple object tracking. The code will be released at https://github.com/HELLORPG/HATReID-MOT.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2503.12527.pdf' target='_blank'>https://arxiv.org/pdf/2503.12527.pdf</a></span>   <span><a href='https://github.com/yiyscut/VIO-IPNet.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Yi, Kunqing Wang, Jinpu Zhang, Zhen Tan, Xiangke Wang, Hui Shen, Dewen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12527">A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The bias of low-cost Inertial Measurement Units (IMU) is a critical factor affecting the performance of Visual-Inertial Odometry (VIO). In particular, when visual tracking encounters errors, the optimized bias results may deviate significantly from the true values, adversely impacting the system's stability and localization precision. In this paper, we propose a novel plug-and-play framework featuring the Inertial Prior Network (IPNet), which is designed to accurately estimate IMU bias. Recognizing the substantial impact of initial bias errors in low-cost inertial devices on system performance, our network directly leverages raw IMU data to estimate the mean bias, eliminating the dependency on historical estimates in traditional recursive predictions and effectively preventing error propagation. Furthermore, we introduce an iterative approach to calculate the mean value of the bias for network training, addressing the lack of bias labels in many visual-inertial datasets. The framework is evaluated on two public datasets and one self-collected dataset. Extensive experiments demonstrate that our method significantly enhances both localization precision and robustness, with the ATE-RMSE metric improving on average by 46\%. The source code and video will be available at \textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2503.12006.pdf' target='_blank'>https://arxiv.org/pdf/2503.12006.pdf</a></span>   <span><a href='https://github.com/ShanZard/ROS-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Shan, Yang Liu, Lei Zhou, Cheng Yan, Heng Wang, Xia Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12006">ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of large-scale remote sensing video data underscores the importance of high-quality interactive segmentation. However, challenges such as small object sizes, ambiguous features, and limited generalization make it difficult for current methods to achieve this goal. In this work, we propose ROS-SAM, a method designed to achieve high-quality interactive segmentation while preserving generalization across diverse remote sensing data. The ROS-SAM is built upon three key innovations: 1) LoRA-based fine-tuning, which enables efficient domain adaptation while maintaining SAM's generalization ability, 2) Enhancement of deep network layers to improve the discriminability of extracted features, thereby reducing misclassifications, and 3) Integration of global context with local boundary details in the mask decoder to generate high-quality segmentation masks. Additionally, we design the data pipeline to ensure the model learns to better handle objects at varying scales during training while focusing on high-quality predictions during inference. Experiments on remote sensing video datasets show that the redesigned data pipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally, when evaluated on existing remote sensing object tracking datasets, ROS-SAM demonstrates impressive zero-shot capabilities, generating masks that closely resemble manual annotations. These results confirm ROS-SAM as a powerful tool for fine-grained segmentation in remote sensing applications. Code is available at https://github.com/ShanZard/ROS-SAM.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2503.10616.pdf' target='_blank'>https://arxiv.org/pdf/2503.10616.pdf</a></span>   <span><a href='https://github.com/jinyanglii/OVTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyang Li, En Yu, Sijia Chen, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10616">OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary multiple object tracking aims to generalize trackers to unseen categories during training, enabling their application across a variety of real-world scenarios. However, the existing open-vocabulary tracker is constrained by its framework structure, isolated frame-level perception, and insufficient modal interactions, which hinder its performance in open-vocabulary classification and tracking. In this paper, we propose OVTR (End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the first end-to-end open-vocabulary tracker that models motion, appearance, and category simultaneously. To achieve stable classification and continuous tracking, we design the CIP (Category Information Propagation) strategy, which establishes multiple high-level category information priors for subsequent frames. Additionally, we introduce a dual-branch structure for generalization capability and deep multimodal interaction, and incorporate protective strategies in the decoder to enhance performance. Experimental results show that our method surpasses previous trackers on the open-vocabulary MOT benchmark while also achieving faster inference speeds and significantly reducing preprocessing requirements. Moreover, the experiment transferring the model to another dataset demonstrates its strong adaptability. Models and code are released at https://github.com/jinyanglii/OVTR.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2503.08471.pdf' target='_blank'>https://arxiv.org/pdf/2503.08471.pdf</a></span>   <span><a href='https://github.com/Tsinghua-MARS-Lab/TrackOcc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoguang Chen, Kenan Li, Xiuyu Yang, Tao Jiang, Yiming Li, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08471">TrackOcc: Camera-based 4D Panoptic Occupancy Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehensive and consistent dynamic scene understanding from camera input is essential for advanced autonomous systems. Traditional camera-based perception tasks like 3D object tracking and semantic occupancy prediction lack either spatial comprehensiveness or temporal consistency. In this work, we introduce a brand-new task, Camera-based 4D Panoptic Occupancy Tracking, which simultaneously addresses panoptic occupancy segmentation and object tracking from camera-only input. Furthermore, we propose TrackOcc, a cutting-edge approach that processes image inputs in a streaming, end-to-end manner with 4D panoptic queries to address the proposed task. Leveraging the localization-aware loss, TrackOcc enhances the accuracy of 4D panoptic occupancy tracking without bells and whistles. Experimental results demonstrate that our method achieves state-of-the-art performance on the Waymo dataset. The source code will be released at https://github.com/Tsinghua-MARS-Lab/TrackOcc.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2503.06625.pdf' target='_blank'>https://arxiv.org/pdf/2503.06625.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/SGLATrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaocan Xue, Bineng Zhong, Qihua Liang, Yaozong Zheng, Ning Li, Yuanliang Xue, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06625">Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision transformers (ViTs) have emerged as a popular backbone for visual tracking. However, complete ViT architectures are too cumbersome to deploy for unmanned aerial vehicle (UAV) tracking which extremely emphasizes efficiency. In this study, we discover that many layers within lightweight ViT-based trackers tend to learn relatively redundant and repetitive target representations. Based on this observation, we propose a similarity-guided layer adaptation approach to optimize the structure of ViTs. Our approach dynamically disables a large number of representation-similar layers and selectively retains only a single optimal layer among them, aiming to achieve a better accuracy-speed trade-off. By incorporating this approach into existing ViTs, we tailor previously complete ViT architectures into an efficient similarity-guided layer-adaptive framework, namely SGLATrack, for real-time UAV tracking. Extensive experiments on six tracking benchmarks verify the effectiveness of the proposed approach, and show that our SGLATrack achieves a state-of-the-art real-time speed while maintaining competitive tracking precision. Codes and models are available at https://github.com/GXNU-ZhongLab/SGLATrack.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2503.04565.pdf' target='_blank'>https://arxiv.org/pdf/2503.04565.pdf</a></span>   <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04565">Omnidirectional Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic imagery, with its 360Â° field of view, offers comprehensive information to support Multi-Object Tracking (MOT) in capturing spatial and temporal relationships of surrounding objects. However, most MOT algorithms are tailored for pinhole images with limited views, impairing their effectiveness in panoramic settings. Additionally, panoramic image distortions, such as resolution loss, geometric deformation, and uneven lighting, hinder direct adaptation of existing MOT methods, leading to significant performance degradation. To address these challenges, we propose OmniTrack, an omnidirectional MOT framework that incorporates Tracklet Management to introduce temporal cues, FlexiTrack Instances for object localization and association, and the CircularStatE Module to alleviate image and geometric distortions. This integration enables tracking in panoramic field-of-view scenarios, even under rapid sensor motion. To mitigate the lack of panoramic MOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as panoramic fields of view, intense motion, and complex environments. Extensive experiments on the public JRDB dataset and the newly introduced QuadTrack benchmark demonstrate the state-of-the-art performance of the proposed framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the baseline by 6.81%. The established dataset and source code are available at https://github.com/xifen523/OmniTrack.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2503.04322.pdf' target='_blank'>https://arxiv.org/pdf/2503.04322.pdf</a></span>   <span><a href='https://github.com/LarsBredereke/object_tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lars Bredereke, Yale Hartmann, Tanja Schultz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04322">A Modular Pipeline for 3D Object Tracking Using RGB Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a key challenge of computer vision with various applications that all require different architectures. Most tracking systems have limitations such as constraining all movement to a 2D plane and they often track only one object. In this paper, we present a new modular pipeline that calculates 3D trajectories of multiple objects. It is adaptable to various settings where multiple time-synced and stationary cameras record moving objects, using off the shelf webcams. Our pipeline was tested on the Table Setting Dataset, where participants are recorded with various sensors as they set a table with tableware objects. We need to track these manipulated objects, using 6 rgb webcams. Challenges include: Detecting small objects in 9.874.699 camera frames, determining camera poses, discriminating between nearby and overlapping objects, temporary occlusions, and finally calculating a 3D trajectory using the right subset of an average of 11.12.456 pixel coordinates per 3-minute trial. We implement a robust pipeline that results in accurate trajectories with covariance of x,y,z-position as a confidence metric. It deals dynamically with appearing and disappearing objects, instantiating new Extended Kalman Filters. It scales to hundreds of table-setting trials with very little human annotation input, even with the camera poses of each trial unknown. The code is available at https://github.com/LarsBredereke/object_tracking
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2503.00516.pdf' target='_blank'>https://arxiv.org/pdf/2503.00516.pdf</a></span>   <span><a href='https://github.com/jiawen-zhu/AsymTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Zhu, Huayi Tang, Xin Chen, Xinying Wang, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00516">Two-stream Beats One-stream: Asymmetric Siamese Network for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient tracking has garnered attention for its ability to operate on resource-constrained platforms for real-world deployment beyond desktop GPUs. Current efficient trackers mainly follow precision-oriented trackers, adopting a one-stream framework with lightweight modules. However, blindly adhering to the one-stream paradigm may not be optimal, as incorporating template computation in every frame leads to redundancy, and pervasive semantic interaction between template and search region places stress on edge devices. In this work, we propose a novel asymmetric Siamese tracker named \textbf{AsymTrack} for efficient tracking. AsymTrack disentangles template and search streams into separate branches, with template computing only once during initialization to generate modulation signals. Building on this architecture, we devise an efficient template modulation mechanism to unidirectional inject crucial cues into the search features, and design an object perception enhancement module that integrates abstract semantics and local details to overcome the limited representation in lightweight tracker. Extensive experiments demonstrate that AsymTrack offers superior speed-precision trade-offs across different platforms compared to the current state-of-the-arts. For instance, AsymTrack-T achieves 60.8\% AUC on LaSOT and 224/81/84 FPS on GPU/CPU/AGX, surpassing HiT-Tiny by 6.0\% AUC with higher speeds. The code is available at https://github.com/jiawen-zhu/AsymTrack.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2502.20111.pdf' target='_blank'>https://arxiv.org/pdf/2502.20111.pdf</a></span>   <span><a href='https://mii-laboratory.github.io/MITracker/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20111">MITracker: Multi-View Integration for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a bird's eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance. The code and the new dataset will be available at https://mii-laboratory.github.io/MITracker/.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2502.18220.pdf' target='_blank'>https://arxiv.org/pdf/2502.18220.pdf</a></span>   <span><a href='https://github.com/wanghe/UASTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>He Wang, Tianyang Xu, Zhangyong Tang, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18220">UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal tracking is essential in single-object tracking (SOT), as different sensor types contribute unique capabilities to overcome challenges caused by variations in object appearance. However, existing unified RGB-X trackers (X represents depth, event, or thermal modality) either rely on the task-specific training strategy for individual RGB-X image pairs or fail to address the critical importance of modality-adaptive perception in real-world applications. In this work, we propose UASTrack, a unified adaptive selection framework that facilitates both model and parameter unification, as well as adaptive modality discrimination across various multi-modal tracking tasks. To achieve modality-adaptive perception in joint RGB-X pairs, we design a Discriminative Auto-Selector (DAS) capable of identifying modality labels, thereby distinguishing the data distributions of auxiliary modalities. Furthermore, we propose a Task-Customized Optimization Adapter (TCOA) tailored to various modalities in the latent space. This strategy effectively filters noise redundancy and mitigates background interference based on the specific characteristics of each modality. Extensive comparisons conducted on five benchmarks including LasHeR, GTOT, RGBT234, VisEvent, and DepthTrack, covering RGB-T, RGB-E, and RGB-D tracking scenarios, demonstrate our innovative approach achieves comparative performance by introducing only additional training parameters of 1.87M and flops of 1.95G. The code will be available at https://github.com/wanghe/UASTrack.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2502.16809.pdf' target='_blank'>https://arxiv.org/pdf/2502.16809.pdf</a></span>   <span><a href='https://github.com/ZJZhao123/CRTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijing Zhao, Jianlong Yu, Lin Zhang, Shunli Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16809">CRTrack: Low-Light Semi-Supervised Multi-object Tracking Based on Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking under low-light environments is prevalent in real life. Recent years have seen rapid development in the field of multi-object tracking. However, due to the lack of datasets and the high cost of annotations, multi-object tracking under low-light environments remains a persistent challenge. In this paper, we focus on multi-object tracking under low-light conditions. To address the issues of limited data and the lack of dataset, we first constructed a low-light multi-object tracking dataset (LLMOT). This dataset comprises data from MOT17 that has been enhanced for nighttime conditions as well as multiple unannotated low-light videos. Subsequently, to tackle the high annotation costs and address the issue of image quality degradation, we propose a semi-supervised multi-object tracking method based on consistency regularization named CRTrack. First, we calibrate a consistent adaptive sampling assignment to replace the static IoU-based strategy, enabling the semi-supervised tracking method to resist noisy pseudo-bounding boxes. Then, we design a adaptive semi-supervised network update method, which effectively leverages unannotated data to enhance model performance. Dataset and Code: https://github.com/ZJZhao123/CRTrack.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2502.09672.pdf' target='_blank'>https://arxiv.org/pdf/2502.09672.pdf</a></span>   <span><a href='https://github.com/Ap01lo/IMM-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohong Liu, Xulong Zhao, Gang Liu, Zili Wu, Tao Wang, Lei Meng, Yuhan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09672">IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT) provides the trajectories of surrounding objects, assisting robots or vehicles in smarter path planning and obstacle avoidance. Existing 3D MOT methods based on the Tracking-by-Detection framework typically use a single motion model to track an object throughout its entire tracking process. However, objects may change their motion patterns due to variations in the surrounding environment. In this paper, we introduce the Interacting Multiple Model filter in IMM-MOT, which accurately fits the complex motion patterns of individual objects, overcoming the limitation of single-model tracking in existing approaches. In addition, we incorporate a Damping Window mechanism into the trajectory lifecycle management, leveraging the continuous association status of trajectories to control their creation and termination, reducing the occurrence of overlooked low-confidence true targets. Furthermore, we propose the Distance-Based Score Enhancement module, which enhances the differentiation between false positives and true positives by adjusting detection scores, thereby improving the effectiveness of the Score Filter. On the NuScenes Val dataset, IMM-MOT outperforms most other single-modal models using 3D point clouds, achieving an AMOTA of 73.8%. Our project is available at https://github.com/Ap01lo/IMM-MOT.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2502.05574.pdf' target='_blank'>https://arxiv.org/pdf/2502.05574.pdf</a></span>   <span><a href='https://github.com/Event-AHU/EventVOT_Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiao Wang, Xiao Wang, Chao Wang, Liye Jin, Lin Zhu, Bo Jiang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05574">Event Stream-based Visual Object Tracking: HDETrack V2 and A High-Definition Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We then introduce a novel hierarchical knowledge distillation strategy that incorporates the similarity matrix, feature representation, and response map-based distillation to guide the learning of the student Transformer network. We also enhance the model's ability to capture temporal dependencies by applying the temporal Fourier transform to establish temporal relationships between video frames. We adapt the network model to specific target objects during testing via a newly proposed test-time tuning strategy to achieve high performance and flexibility in target tracking. Recognizing the limitations of existing event-based tracking datasets, which are predominantly low-resolution, we propose EventVOT, the first large-scale high-resolution event-based tracking dataset. It comprises 1141 videos spanning diverse categories such as pedestrians, vehicles, UAVs, ping pong, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, FELT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. Both the benchmark dataset and source code have been released on https://github.com/Event-AHU/EventVOT_Benchmark
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2501.12386.pdf' target='_blank'>https://arxiv.org/pdf/2501.12386.pdf</a></span>   <span><a href='https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12386">InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to improve the performance of video multimodal large language models (MLLM) via long and rich context (LRC) modeling. As a result, we develop a new version of InternVideo2.5 with a focus on enhancing the original MLLMs' ability to perceive fine-grained details and capture long-form temporal structure in videos. Specifically, our approach incorporates dense vision task annotations into MLLMs using direct preference optimization and develops compact spatiotemporal representations through adaptive hierarchical token compression. Experimental results demonstrate this unique design of LRC greatly improves the results of video MLLM in mainstream video understanding benchmarks (short & long), enabling the MLLM to memorize significantly longer video inputs (at least 6x longer than the original), and master specialized vision capabilities like object tracking and segmentation. Our work highlights the importance of multimodal context richness (length and fineness) in empowering MLLM's innate abilites (focus and memory), providing new insights for future research on video MLLM. Code and models are available at https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2501.11288.pdf' target='_blank'>https://arxiv.org/pdf/2501.11288.pdf</a></span>   <span><a href='https://github.com/Wangyc2000/PD_SORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanchao Wang, Dawei Zhang, Run Li, Zhonglong Zheng, Minglu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11288">PD-SORT: Occlusion-Robust Multi-Object Tracking Using Pseudo-Depth Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a rising topic in video processing technologies and has important application value in consumer electronics. Currently, tracking-by-detection (TBD) is the dominant paradigm for MOT, which performs target detection and association frame by frame. However, the association performance of TBD methods degrades in complex scenes with heavy occlusions, which hinders the application of such methods in real-world scenarios.To this end, we incorporate pseudo-depth cues to enhance the association performance and propose Pseudo-Depth SORT (PD-SORT). First, we extend the Kalman filter state vector with pseudo-depth states. Second, we introduce a novel depth volume IoU (DVIoU) by combining the conventional 2D IoU with pseudo-depth. Furthermore, we develop a quantized pseudo-depth measurement (QPDM) strategy for more robust data association. Besides, we also integrate camera motion compensation (CMC) to handle dynamic camera situations. With the above designs, PD-SORT significantly alleviates the occlusion-induced ambiguous associations and achieves leading performances on DanceTrack, MOT17, and MOT20. Note that the improvement is especially obvious on DanceTrack, where objects show complex motions, similar appearances, and frequent occlusions. The code is available at https://github.com/Wangyc2000/PD_SORT.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2501.07554.pdf' target='_blank'>https://arxiv.org/pdf/2501.07554.pdf</a></span>   <span><a href='https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Varun Biyyala, Bharat Chanderprakash Kathuria, Jialu Li, Youshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07554">SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video editing models have advanced significantly, but evaluating their performance remains challenging. Traditional metrics, such as CLIP text and image scores, often fall short: text scores are limited by inadequate training data and hierarchical dependencies, while image scores fail to assess temporal consistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation Metric), a novel evaluation framework that leverages modern Vision-Language Models (VLMs), Object Detection, and Temporal Consistency checks. SST-EM comprises four components: (1) semantic extraction from frames using a VLM, (2) primary object tracking with Object Detection, (3) focused object refinement via an LLM agent, and (4) temporal consistency assessment using a Vision Transformer (ViT). These components are integrated into a unified metric with weights derived from human evaluations and regression analysis. The name SST-EM reflects its focus on Semantic, Spatial, and Temporal aspects of video evaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and temporal smoothness in video editing. The source code is available in the \textbf{\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub Repository}}.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2501.07360.pdf' target='_blank'>https://arxiv.org/pdf/2501.07360.pdf</a></span>   <span><a href='https://github.com/timbervision/timbervision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Steininger, Julia Simon, Andreas Trondl, Markus Murschitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07360">TimberVision: A Multi-Task Dataset and Framework for Log-Component Segmentation and Tracking in Autonomous Forestry Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Timber represents an increasingly valuable and versatile resource. However, forestry operations such as harvesting, handling and measuring logs still require substantial human labor in remote environments posing significant safety risks. Progressively automating these tasks has the potential of increasing their efficiency as well as safety, but requires an accurate detection of individual logs as well as live trees and their context. Although initial approaches have been proposed for this challenging application domain, specialized data and algorithms are still too scarce to develop robust solutions. To mitigate this gap, we introduce the TimberVision dataset, consisting of more than 2k annotated RGB images containing a total of 51k trunk components including cut and lateral surfaces, thereby surpassing any existing dataset in this domain in terms of both quantity and detail by a large margin. Based on this data, we conduct a series of ablation experiments for oriented object detection and instance segmentation and evaluate the influence of multiple scene parameters on model performance. We introduce a generic framework to fuse the components detected by our models for both tasks into unified trunk representations. Furthermore, we automatically derive geometric properties and apply multi-object tracking to further enhance robustness. Our detection and tracking approach provides highly descriptive and accurate trunk representations solely from RGB image data, even under challenging environmental conditions. Our solution is suitable for a wide range of application scenarios and can be readily combined with other sensor modalities.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2501.05453.pdf' target='_blank'>https://arxiv.org/pdf/2501.05453.pdf</a></span>   <span><a href='https://brjathu.github.io/toto/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jathushan Rajasegaran, Ilija Radosavovic, Rahul Ravishankar, Yossi Gandelsman, Christoph Feichtenhofer, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05453">An Empirical Study of Autoregressive Pre-training from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2501.01275.pdf' target='_blank'>https://arxiv.org/pdf/2501.01275.pdf</a></span>   <span><a href='https://github.com/leandro-svg/HybridTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leandro Di Bella, Yangxintong Lyu, Bruno Cornelis, Adrian Munteanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01275">HybridTrack: A Hybrid Approach for Robust Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of Advanced Driver Assistance Systems (ADAS) has increased the need for robust and generalizable algorithms for multi-object tracking. Traditional statistical model-based tracking methods rely on predefined motion models and assumptions about system noise distributions. Although computationally efficient, they often lack adaptability to varying traffic scenarios and require extensive manual design and parameter tuning. To address these issues, we propose a novel 3D multi-object tracking approach for vehicles, HybridTrack, which integrates a data-driven Kalman Filter (KF) within a tracking-by-detection paradigm. In particular, it learns the transition residual and Kalman gain directly from data, which eliminates the need for manual motion and stochastic parameter modeling. Validated on the real-world KITTI dataset, HybridTrack achieves 82.72% HOTA accuracy, significantly outperforming state-of-the-art methods. We also evaluate our method under different configurations, achieving the fastest processing speed of 112 FPS. Consequently, HybridTrack eliminates the dependency on scene-specific designs while improving performance and maintaining real-time efficiency. The code is publicly available at: https://github.com/leandro-svg/HybridTrack.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2412.20002.pdf' target='_blank'>https://arxiv.org/pdf/2412.20002.pdf</a></span>   <span><a href='https://github.com/wuyou3474/AVTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>You Wu, Yongxin Li, Mengyuan Liu, Xucheng Wang, Xiangyang Yang, Hengzhou Ye, Dan Zeng, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20002">Learning an Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based models have improved visual tracking, but most still cannot run in real time on resource-limited devices, especially for unmanned aerial vehicle (UAV) tracking. To achieve a better balance between performance and efficiency, we propose AVTrack, an adaptive computation tracking framework that adaptively activates transformer blocks through an Activation Module (AM), which dynamically optimizes the ViT architecture by selectively engaging relevant components. To address extreme viewpoint variations, we propose to learn view-invariant representations via mutual information (MI) maximization. In addition, we propose AVTrack-MD, an enhanced tracker incorporating a novel MI maximization-based multi-teacher knowledge distillation framework. Leveraging multiple off-the-shelf AVTrack models as teachers, we maximize the MI between their aggregated softened features and the corresponding softened feature of the student model, improving the generalization and performance of the student, especially under noisy conditions. Extensive experiments show that AVTrack-MD achieves performance comparable to AVTrack's performance while reducing model complexity and boosting average tracking speed by over 17\%. Codes is available at: https://github.com/wuyou3474/AVTrack.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2412.17807.pdf' target='_blank'>https://arxiv.org/pdf/2412.17807.pdf</a></span>   <span><a href='https://github.com/chen-si-jia/CRMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Chen, En Yu, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17807">Cross-View Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) is an important topic in the current tracking field. Its task form is to guide the tracker to track objects that match the language description. Current research mainly focuses on referring multi-object tracking under single-view, which refers to a view sequence or multiple unrelated view sequences. However, in the single-view, some appearances of objects are easily invisible, resulting in incorrect matching of objects with the language description. In this work, we propose a new task, called Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the cross-view to obtain the appearances of objects from multiple views, avoiding the problem of the invisible appearances of objects in RMOT task. CRMOT is a more challenging task of accurately tracking the objects that match the language description and maintaining the identity consistency of objects in each cross-view. To advance CRMOT task, we construct a cross-view referring multi-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named CRTrack. Specifically, it provides 13 different scenes and 221 language descriptions. Furthermore, we propose an end-to-end cross-view referring multi-object tracking method, named CRTracker. Extensive experiments on the CRTrack benchmark verify the effectiveness of our method. The dataset and code are available at https://github.com/chen-si-jia/CRMOT.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2412.15691.pdf' target='_blank'>https://arxiv.org/pdf/2412.15691.pdf</a></span>   <span><a href='https://github.com/NJU-PCALab/STTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiantao Hu, Ying Tai, Xu Zhao, Chen Zhao, Zhenyu Zhang, Jun Li, Bineng Zhong, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15691">Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal tracking has garnered widespread attention as a result of its ability to effectively address the inherent limitations of traditional RGB tracking. However, existing multimodal trackers mainly focus on the fusion and enhancement of spatial features or merely leverage the sparse temporal relationships between video frames. These approaches do not fully exploit the temporal correlations in multimodal videos, making it difficult to capture the dynamic changes and motion information of targets in complex scenarios. To alleviate this problem, we propose a unified multimodal spatial-temporal tracking approach named STTrack. In contrast to previous paradigms that solely relied on updating reference information, we introduced a temporal state generator (TSG) that continuously generates a sequence of tokens containing multimodal temporal information. These temporal information tokens are used to guide the localization of the target in the next time state, establish long-range contextual relationships between video frames, and capture the temporal trajectory of the target. Furthermore, at the spatial level, we introduced the mamba fusion and background suppression interactive (BSI) modules. These modules establish a dual-stage mechanism for coordinating information interaction and fusion between modalities. Extensive comparisons on five benchmark datasets illustrate that STTrack achieves state-of-the-art performance across various multimodal tracking scenarios. Code is available at: https://github.com/NJU-PCALab/STTrack.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2412.15212.pdf' target='_blank'>https://arxiv.org/pdf/2412.15212.pdf</a></span>   <span><a href='https://github.com/google-deepmind/representations4d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro VÃ©lez, Luisa PolanÃ­a, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica PÄtrÄucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15212">Scaling 4D Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling has not yet been convincingly demonstrated for pure self-supervised learning from video. However, prior work has focused evaluations on semantic-related tasks $\unicode{x2013}$ action classification, ImageNet classification, etc. In this paper we focus on evaluating self-supervised learning on non-semantic vision tasks that are more spatial (3D) and temporal (+1D = 4D), such as camera pose estimation, point and object tracking, and depth estimation. We show that by learning from very large video datasets, masked auto-encoding (MAE) with transformer video models actually scales, consistently improving performance on these 4D tasks, as model size increases from 20M all the way to the largest by far reported self-supervised video model $\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with many recent image and video models demonstrates the benefits of scaling 4D representations. Pretrained models are available at https://github.com/google-deepmind/representations4d .
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2412.11023.pdf' target='_blank'>https://arxiv.org/pdf/2412.11023.pdf</a></span>   <span><a href='https://github.com/kangben258/MCITrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Kang, Xin Chen, Simiao Lai, Yang Liu, Yi Liu, Dong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11023">Exploring Enhanced Contextual Information for Video-Level Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contextual information at the video level has become increasingly crucial for visual object tracking. However, existing methods typically use only a few tokens to convey this information, which can lead to information loss and limit their ability to fully capture the context. To address this issue, we propose a new video-level visual object tracking framework called MCITrack. It leverages Mamba's hidden states to continuously record and transmit extensive contextual information throughout the video stream, resulting in more robust object tracking. The core component of MCITrack is the Contextual Information Fusion module, which consists of the mamba layer and the cross-attention layer. The mamba layer stores historical contextual information, while the cross-attention layer integrates this information into the current visual features of each backbone block. This module enhances the model's ability to capture and utilize contextual information at multiple levels through deep integration with the backbone. Experiments demonstrate that MCITrack achieves competitive performance across numerous benchmarks. For instance, it gets 76.6% AUC on LaSOT and 80.0% AO on GOT-10k, establishing a new state-of-the-art performance. Code and models are available at https://github.com/kangben258/MCITrack.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2412.10861.pdf' target='_blank'>https://arxiv.org/pdf/2412.10861.pdf</a></span>   <span><a href='https://github.com/xuqingyu26/HGTMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyu Xu, Longguang Wang, Weidong Sheng, Yingqian Wang, Chao Xiao, Chao Ma, Wei An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10861">Heterogeneous Graph Transformer for Multiple Tiny Object Tracking in RGB-T Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple tiny objects is highly challenging due to their weak appearance and limited features. Existing multi-object tracking algorithms generally focus on single-modality scenes, and overlook the complementary characteristics of tiny objects captured by multiple remote sensors. To enhance tracking performance by integrating complementary information from multiple sources, we propose a novel framework called {HGT-Track (Heterogeneous Graph Transformer based Multi-Tiny-Object Tracking)}. Specifically, we first employ a Transformer-based encoder to embed images from different modalities. Subsequently, we utilize Heterogeneous Graph Transformer to aggregate spatial and temporal information from multiple modalities to generate detection and tracking features. Additionally, we introduce a target re-detection module (ReDet) to ensure tracklet continuity by maintaining consistency across different modalities. Furthermore, this paper introduces the first benchmark VT-Tiny-MOT (Visible-Thermal Tiny Multi-Object Tracking) for RGB-T fused multiple tiny object tracking. Extensive experiments are conducted on VT-Tiny-MOT, and the results have demonstrated the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of MOTA (Multiple-Object Tracking Accuracy) and ID-F1 score. The code and dataset will be made available at https://github.com/xuqingyu26/HGTMT.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2412.09617.pdf' target='_blank'>https://arxiv.org/pdf/2412.09617.pdf</a></span>   <span><a href='https://joehjhuang.github.io/normalflow' target='_blank'>  GitHub</a></span> <span><a href='https://joehjhuang.github.io/normalflow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung-Jui Huang, Michael Kaess, Wenzhen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09617">NormalFlow: Fast, Robust, and Accurate Contact-based Object 6DoF Pose Tracking with Vision-based Tactile Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing is crucial for robots aiming to achieve human-level dexterity. Among tactile-dependent skills, tactile-based object tracking serves as the cornerstone for many tasks, including manipulation, in-hand manipulation, and 3D reconstruction. In this work, we introduce NormalFlow, a fast, robust, and real-time tactile-based 6DoF tracking algorithm. Leveraging the precise surface normal estimation of vision-based tactile sensors, NormalFlow determines object movements by minimizing discrepancies between the tactile-derived surface normals. Our results show that NormalFlow consistently outperforms competitive baselines and can track low-texture objects like table surfaces. For long-horizon tracking, we demonstrate when rolling the sensor around a bead for 360 degrees, NormalFlow maintains a rotational tracking error of 2.5 degrees. Additionally, we present state-of-the-art tactile-based 3D reconstruction results, showcasing the high accuracy of NormalFlow. We believe NormalFlow unlocks new possibilities for high-precision perception and manipulation tasks that involve interacting with objects using hands. The video demo, code, and dataset are available on our website: https://joehjhuang.github.io/normalflow.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2412.05548.pdf' target='_blank'>https://arxiv.org/pdf/2412.05548.pdf</a></span>   <span><a href='https://lolrudy.github.io/No3DTrackSG/' target='_blank'>  GitHub</a></span> <span><a href='https://lolrudy.github.io/No3DTrackSG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruida Zhang, Chengxi Li, Chenyangguang Zhang, Xingyu Liu, Haili Yuan, Yanyan Li, Xiangyang Ji, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05548">Street Gaussians without 3D Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic scene reconstruction in driving scenarios poses significant challenges due to fast-moving objects. Most existing methods rely on labor-intensive manual labeling of object poses to reconstruct dynamic objects in canonical space and move them based on these poses during rendering. While some approaches attempt to use 3D object trackers to replace manual annotations, the limited generalization of 3D trackers -- caused by the scarcity of large-scale 3D datasets -- results in inferior reconstructions in real-world settings. In contrast, 2D foundation models demonstrate strong generalization capabilities. To eliminate the reliance on 3D trackers and enhance robustness across diverse environments, we propose a stable object tracking module by leveraging associations from 2D deep trackers within a 3D object fusion strategy. We address inevitable tracking errors by further introducing a motion learning strategy in an implicit feature space that autonomously corrects trajectory errors and recovers missed detections. Experimental results on Waymo-NOTR and KITTI show that our method outperforms existing approaches. Our code will be released on https://lolrudy.github.io/No3DTrackSG/.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2412.04945.pdf' target='_blank'>https://arxiv.org/pdf/2412.04945.pdf</a></span>   <span><a href='https://github.com/mschwimmbeck/HOLa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Schwimmbeck, Serouj Khajarian, Konstantin Holzapfel, Johannes Schmidt, Stefanie Remmele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04945">HOLa: HoloLens Object Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of medical Augmented Reality (AR) applications, object tracking is a key challenge and requires a significant amount of annotation masks. As segmentation foundation models like the Segment Anything Model (SAM) begin to emerge, zero-shot segmentation requires only minimal human participation obtaining high-quality object masks. We introduce a HoloLens-Object-Labeling (HOLa) Unity and Python application based on the SAM-Track algorithm that offers fully automatic single object annotation for HoloLens 2 while requiring minimal human participation. HOLa does not have to be adjusted to a specific image appearance and could thus alleviate AR research in any application field. We evaluate HOLa for different degrees of image complexity in open liver surgery and in medical phantom experiments. Using HOLa for image annotation can increase the labeling speed by more than 500 times while providing Dice scores between 0.875 and 0.982, which are comparable to human annotators. Our code is publicly available at: https://github.com/mschwimmbeck/HOLa
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2412.03512.pdf' target='_blank'>https://arxiv.org/pdf/2412.03512.pdf</a></span>   <span><a href='https://compvis.github.io/distilldift' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank Fundel, Johannes Schusterbauer, Vincent Tao Hu, BjÃ¶rn Ommer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03512">Distillation of Diffusion Features for Semantic Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic correspondence, the task of determining relationships between different parts of images, underpins various applications including 3D reconstruction, image-to-image translation, object tracking, and visual place recognition. Recent studies have begun to explore representations learned in large generative image models for semantic correspondence, demonstrating promising results. Building on this progress, current state-of-the-art methods rely on combining multiple large models, resulting in high computational demands and reduced efficiency. In this work, we address this challenge by proposing a more computationally efficient approach. We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency. We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost. Furthermore, we demonstrate that by incorporating 3D data, we are able to further improve performance, without the need for human-annotated correspondences. Overall, our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications, such as semantic video correspondence. Our code and weights are publicly available on our project page.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2412.02129.pdf' target='_blank'>https://arxiv.org/pdf/2412.02129.pdf</a></span>   <span><a href='https://github.com/ailovejinx/GSOT3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Jiao, Yunhao Li, Junhua Ding, Qing Yang, Song Fu, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02129">GSOT3D: Towards Generic 3D Single Object Tracking in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel benchmark, GSOT3D, that aims at facilitating development of generic 3D single object tracking (SOT) in the wild. Specifically, GSOT3D offers 620 sequences with 123K frames, and covers a wide selection of 54 object categories. Each sequence is offered with multiple modalities, including the point cloud (PC), RGB image, and depth. This allows GSOT3D to support various 3D tracking tasks, such as single-modal 3D SOT on PC and multi-modal 3D SOT on RGB-PC or RGB-D, and thus greatly broadens research directions for 3D object tracking. To provide highquality per-frame 3D annotations, all sequences are labeled manually with multiple rounds of meticulous inspection and refinement. To our best knowledge, GSOT3D is the largest benchmark dedicated to various generic 3D object tracking tasks. To understand how existing 3D trackers perform and to provide comparisons for future research on GSOT3D, we assess eight representative point cloud-based tracking models. Our evaluation results exhibit that these models heavily degrade on GSOT3D, and more efforts are required for robust and generic 3D object tracking. Besides, to encourage future research, we present a simple yet effective generic 3D tracker, named PROT3D, that localizes the target object via a progressive spatial-temporal network and outperforms all current solutions by a large margin. By releasing GSOT3D, we expect to advance further 3D tracking in future research and applications. Our benchmark and model as well as the evaluation results will be publicly released at our webpage https://github.com/ailovejinx/GSOT3D.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2412.01147.pdf' target='_blank'>https://arxiv.org/pdf/2412.01147.pdf</a></span>   <span><a href='https://uark-aicv.github.io/A2VIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Tran, Thang Pham, Winston Bounsavy, Tri Nguyen, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01147">A2VIS: Amodal-Aware Approach to Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Handling occlusion remains a significant challenge for video instance-level tasks like Multiple Object Tracking (MOT) and Video Instance Segmentation (VIS). In this paper, we propose a novel framework, Amodal-Aware Video Instance Segmentation (A2VIS), which incorporates amodal representations to achieve a reliable and comprehensive understanding of both visible and occluded parts of objects in a video. The key intuition is that awareness of amodal segmentation through spatiotemporal dimension enables a stable stream of object information. In scenarios where objects are partially or completely hidden from view, amodal segmentation offers more consistency and less dramatic changes along the temporal axis compared to visible segmentation. Hence, both amodal and visible information from all clips can be integrated into one global instance prototype. To effectively address the challenge of video amodal segmentation, we introduce the spatiotemporal-prior Amodal Mask Head, which leverages visible information intra clips while extracting amodal characteristics inter clips. Through extensive experiments and ablation studies, we show that A2VIS excels in both MOT and VIS tasks in identifying and tracking object instances with a keen understanding of their full shape.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2412.01132.pdf' target='_blank'>https://arxiv.org/pdf/2412.01132.pdf</a></span>   <span><a href='https://github.com/joe-rabbit/VideoQA_Pilot_Study' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Raj Vishal, Divesh Basina, Aarya Choudhary, Bharatesh Chakravarthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01132">Eyes on the Road: State-of-the-Art Video Question Answering Models Assessment for Traffic Monitoring Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video question answering (VideoQA) offer promising applications, especially in traffic monitoring, where efficient video interpretation is critical. Within ITS, answering complex, real-time queries like "How many red cars passed in the last 10 minutes?" or "Was there an incident between 3:00 PM and 3:05 PM?" enhances situational awareness and decision-making. Despite progress in vision-language models, VideoQA remains challenging, especially in dynamic environments involving multiple objects and intricate spatiotemporal relationships. This study evaluates state-of-the-art VideoQA models using non-benchmark synthetic and real-world traffic sequences. The framework leverages GPT-4o to assess accuracy, relevance, and consistency across basic detection, temporal reasoning, and decomposition queries. VideoLLaMA-2 excelled with 57% accuracy, particularly in compositional reasoning and consistent answers. However, all models, including VideoLLaMA-2, faced limitations in multi-object tracking, temporal coherence, and complex scene interpretation, highlighting gaps in current architectures. These findings underscore VideoQA's potential in traffic monitoring but also emphasize the need for improvements in multi-object tracking, temporal reasoning, and compositional capabilities. Enhancing these areas could make VideoQA indispensable for incident detection, traffic flow management, and responsive urban planning. The study's code and framework are open-sourced for further exploration: https://github.com/joe-rabbit/VideoQA_Pilot_Study
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2411.18855.pdf' target='_blank'>https://arxiv.org/pdf/2411.18855.pdf</a></span>   <span><a href='https://wvuvl.github.io/SiamABC/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ram Zaveri, Shivang Patel, Yu Gu, Gianfranco Doretto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18855">Improving Accuracy and Generalization for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient visual trackers overfit to their training distributions and lack generalization abilities, resulting in them performing well on their respective in-distribution (ID) test sets and not as well on out-of-distribution (OOD) sequences, imposing limitations to their deployment in-the-wild under constrained resources. We introduce SiamABC, a highly efficient Siamese tracker that significantly improves tracking performance, even on OOD sequences. SiamABC takes advantage of new architectural designs in the way it bridges the dynamic variability of the target, and of new losses for training. Also, it directly addresses OOD tracking generalization by including a fast backward-free dynamic test-time adaptation method that continuously adapts the model according to the dynamic visual changes of the target. Our extensive experiments suggest that SiamABC shows remarkable performance gains in OOD sets while maintaining accurate performance on the ID benchmarks. SiamABC outperforms MixFormerV2-S by 7.6\% on the OOD AVisT benchmark while being 3x faster (100 FPS) on a CPU. Our code and models are available at https://wvuvl.github.io/SiamABC/.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2411.17576.pdf' target='_blank'>https://arxiv.org/pdf/2411.17576.pdf</a></span>   <span><a href='https://github.com/jovanavidenovic/DAM4SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovana Videnovic, Alan Lukezic, Matej Kristan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17576">A Distractor-Aware Memory for Visual Object Tracking with SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Memory-based trackers are video object segmentation methods that form the target model by concatenating recently tracked frames into a memory buffer and localize the target by attending the current image to the buffered frames. While already achieving top performance on many benchmarks, it was the recent release of SAM2 that placed memory-based trackers into focus of the visual object tracking community. Nevertheless, modern trackers still struggle in the presence of distractors. We argue that a more sophisticated memory model is required, and propose a new distractor-aware memory model for SAM2 and an introspection-based update strategy that jointly addresses the segmentation accuracy as well as tracking robustness. The resulting tracker is denoted as SAM2.1++. We also propose a new distractor-distilled DiDi dataset to study the distractor problem better. SAM2.1++ outperforms SAM2.1 and related SAM memory extensions on seven benchmarks and sets a solid new state-of-the-art on six of them.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2411.15761.pdf' target='_blank'>https://arxiv.org/pdf/2411.15761.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15761">MambaTrack: Exploiting Dual-Enhancement for Night UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Night unmanned aerial vehicle (UAV) tracking is impeded by the challenges of poor illumination, with previous daylight-optimized methods demonstrating suboptimal performance in low-light conditions, limiting the utility of UAV applications. To this end, we propose an efficient mamba-based tracker, leveraging dual enhancement techniques to boost night UAV tracking. The mamba-based low-light enhancer, equipped with an illumination estimator and a damage restorer, achieves global image enhancement while preserving the details and structure of low-light images. Additionally, we advance a cross-modal mamba network to achieve efficient interactive learning between vision and language modalities. Extensive experiments showcase that our method achieves advanced performance and exhibits significantly improved computation and memory efficiency. For instance, our method is 2.8$\times$ faster than CiteTracker and reduces 50.2$\%$ GPU memory. Our codes are available at \url{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2411.12943.pdf' target='_blank'>https://arxiv.org/pdf/2411.12943.pdf</a></span>   <span><a href='https://github.com/wassimea/thermalMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wassim El Ahmar, Dhanvin Kolhatkar, Farzan Nowruzi, Robert Laganiere
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12943">Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) in thermal imaging presents unique challenges due to the lack of visual features and the complexity of motion patterns. This paper introduces an innovative approach to improve MOT in the thermal domain by developing a novel box association method that utilizes both thermal object identity and motion similarity. Our method merges thermal feature sparsity and dynamic object tracking, enabling more accurate and robust MOT performance. Additionally, we present a new dataset comprised of a large-scale collection of thermal and RGB images captured in diverse urban environments, serving as both a benchmark for our method and a new resource for thermal imaging. We conduct extensive experiments to demonstrate the superiority of our approach over existing methods, showing significant improvements in tracking accuracy and robustness under various conditions. Our findings suggest that incorporating thermal identity with motion data enhances MOT performance. The newly collected dataset and source code is available at https://github.com/wassimea/thermalMOT
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2411.11922.pdf' target='_blank'>https://arxiv.org/pdf/2411.11922.pdf</a></span>   <span><a href='https://yangchris11.github.io/samurai/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11922">SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of memories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incorporating temporal motion cues with the proposed motion-aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, showcasing its ability to generalize without fine-tuning. In evaluations, SAMURAI achieves significant improvements in success rate and precision over existing trackers, with a 7.1% AUC gain on LaSOT$_{\text{ext}}$ and a 3.5% AO gain on GOT-10k. Moreover, it achieves competitive results compared to fully supervised methods on LaSOT, underscoring its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2411.10564.pdf' target='_blank'>https://arxiv.org/pdf/2411.10564.pdf</a></span>   <span><a href='https://github.com/MahmudulHasan11085/Vision-Eagle-Attention.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmudul Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10564">Vision Eagle Attention: a new lens for advancing image classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In computer vision tasks, the ability to focus on relevant regions within an image is crucial for improving model performance, particularly when key features are small, subtle, or spatially dispersed. Convolutional neural networks (CNNs) typically treat all regions of an image equally, which can lead to inefficient feature extraction. To address this challenge, I have introduced Vision Eagle Attention, a novel attention mechanism that enhances visual feature extraction using convolutional spatial attention. The model applies convolution to capture local spatial features and generates an attention map that selectively emphasizes the most informative regions of the image. This attention mechanism enables the model to focus on discriminative features while suppressing irrelevant background information. I have integrated Vision Eagle Attention into a lightweight ResNet-18 architecture, demonstrating that this combination results in an efficient and powerful model. I have evaluated the performance of the proposed model on three widely used benchmark datasets: FashionMNIST, Intel Image Classification, and OracleMNIST, with a primary focus on image classification. Experimental results show that the proposed approach improves classification accuracy. Additionally, this method has the potential to be extended to other vision tasks, such as object detection, segmentation, and visual tracking, offering a computationally efficient solution for a wide range of vision-based applications. Code is available at: https://github.com/MahmudulHasan11085/Vision-Eagle-Attention.git
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2411.09551.pdf' target='_blank'>https://arxiv.org/pdf/2411.09551.pdf</a></span>   <span><a href='https://github.com/serycjon/MFTIQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Serych, Michal Neoral, Jiri Matas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09551">MFTIQ: Multi-Flow Tracker with Independent Matching Quality Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present MFTIQ, a novel dense long-term tracking model that advances the Multi-Flow Tracker (MFT) framework to address challenges in point-level visual tracking in video sequences. MFTIQ builds upon the flow-chaining concepts of MFT, integrating an Independent Quality (IQ) module that separates correspondence quality estimation from optical flow computations. This decoupling significantly enhances the accuracy and flexibility of the tracking process, allowing MFTIQ to maintain reliable trajectory predictions even in scenarios of prolonged occlusions and complex dynamics. Designed to be "plug-and-play", MFTIQ can be employed with any off-the-shelf optical flow method without the need for fine-tuning or architectural modifications. Experimental validations on the TAP-Vid Davis dataset show that MFTIQ with RoMa optical flow not only surpasses MFT but also performs comparably to state-of-the-art trackers while having substantially faster processing speed. Code and models available at https://github.com/serycjon/MFTIQ .
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2411.08216.pdf' target='_blank'>https://arxiv.org/pdf/2411.08216.pdf</a></span>   <span><a href='https://github.com/sjc042/gta-link.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Sun, Hsiang-Wei Huang, Cheng-Yen Yang, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08216">GTA: Global Tracklet Association for Multi-Object Tracking in Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking in sports scenarios has become one of the focal points in computer vision, experiencing significant advancements through the integration of deep learning techniques. Despite these breakthroughs, challenges remain, such as accurately re-identifying players upon re-entry into the scene and minimizing ID switches. In this paper, we propose an appearance-based global tracklet association algorithm designed to enhance tracking performance by splitting tracklets containing multiple identities and connecting tracklets seemingly from the same identity. This method can serve as a plug-and-play refinement tool for any multi-object tracker to further boost their performance. The proposed method achieved a new state-of-the-art performance on the SportsMOT dataset with HOTA score of 81.04%. Similarly, on the SoccerNet dataset, our method enhanced multiple trackers' performance, consistently increasing the HOTA score from 79.41% to 83.11%. These significant and consistent improvements across different trackers and datasets underscore our proposed method's potential impact on the application of sports player tracking. We open-source our project codebase at https://github.com/sjc042/gta-link.git.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2411.06378.pdf' target='_blank'>https://arxiv.org/pdf/2411.06378.pdf</a></span>   <span><a href='https://github.com/hwcao17/pkf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwen Cao, George J. Pappas, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06378">PKF: Probabilistic Data Association Kalman Filter for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we derive a new Kalman filter with probabilistic data association between measurements and states. We formulate a variational inference problem to approximate the posterior density of the state conditioned on the measurement data. We view the unknown data association as a latent variable and apply Expectation Maximization (EM) to obtain a filter with update step in the same form as the Kalman filter but with expanded measurement vector of all potential associations. We show that the association probabilities can be computed as permanents of matrices with measurement likelihood entries. We also propose an ambiguity check that associates only a subset of ambiguous measurements and states probabilistically, thus reducing the association time and preventing low-probability measurements from harming the estimation accuracy. Experiments in simulation show that our filter achieves lower tracking errors than the well-established joint probabilistic data association filter (JPDAF), while running at comparable rate. We also demonstrate the effectiveness of our filter in multi-object tracking (MOT) on multiple real-world datasets, including MOT17, MOT20, and DanceTrack. We achieve better higher order tracking accuracy (HOTA) than previous Kalman-filter methods and remain real-time. Associating only bounding boxes without deep features or velocities, our method ranks top-10 on both MOT17 and MOT20 in terms of HOTA. Given offline detections, our algorithm tracks at 250+ fps on a single laptop CPU. Code is available at https://github.com/hwcao17/pkf.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2410.20421.pdf' target='_blank'>https://arxiv.org/pdf/2410.20421.pdf</a></span>   <span><a href='https://github.com/LiuYuML/NV-VOT211' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Arif Mahmood, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20421">NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many current visual object tracking benchmarks such as OTB100, NfS, UAV123, LaSOT, and GOT-10K, predominantly contain day-time scenarios while the challenges posed by the night-time has been less investigated. It is primarily because of the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. To this end, this paper presents NT-VOT211, a new benchmark tailored for evaluating visual object tracking algorithms in the challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. To the best of our knowledge, it is the largest night-time tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and distractors inherent to night-time tracking scenarios. Through a comprehensive analysis of results obtained from 42 diverse tracking algorithms on NT-VOT211, we uncover the strengths and limitations of these algorithms, highlighting opportunities for enhancements in visual object tracking, particularly in environments with suboptimal lighting. Besides, a leaderboard for revealing performance rankings, annotation tools, comprehensive meta-information and all the necessary code for reproducibility of results is made publicly available. We believe that our NT-VOT211 benchmark will not only be instrumental in facilitating field deployment of VOT algorithms, but will also help VOT enhancements and it will unlock new real-world tracking applications. Our dataset and other assets can be found at: {https://github.com/LiuYuML/NV-VOT211.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2410.20395.pdf' target='_blank'>https://arxiv.org/pdf/2410.20395.pdf</a></span>   <span><a href='https://github.com/LiuYuML/Depth-Attention' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Arif Mahmood, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20395">Depth Attention for Robust RGB Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB video object tracking is a fundamental task in computer vision. Its effectiveness can be improved using depth information, particularly for handling motion-blurred target. However, depth information is often missing in commonly used tracking benchmarks. In this work, we propose a new framework that leverages monocular depth estimation to counter the challenges of tracking targets that are out of view or affected by motion blur in RGB video sequences. Specifically, our work introduces following contributions. To the best of our knowledge, we are the first to propose a depth attention mechanism and to formulate a simple framework that allows seamlessly integration of depth information with state of the art tracking algorithms, without RGB-D cameras, elevating accuracy and robustness. We provide extensive experiments on six challenging tracking benchmarks. Our results demonstrate that our approach provides consistent gains over several strong baselines and achieves new SOTA performance. We believe that our method will open up new possibilities for more sophisticated VOT solutions in real-world scenarios. Our code and models are publicly released: https://github.com/LiuYuML/Depth-Attention.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2410.17856.pdf' target='_blank'>https://arxiv.org/pdf/2410.17856.pdf</a></span>   <span><a href='https://craftjarvis.github.io/ROCKET-1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17856">ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. One critical issue is bridging the gap between discrete entities in low-level observations and the abstract concepts required for effective planning. A common solution is building hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language. However, language suffers from the inability to communicate detailed spatial information. We propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from past observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, supported by real-time object tracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to tackle complex tasks that demand spatial reasoning. Experiments in Minecraft show that our approach enables agents to achieve previously unattainable tasks, with a $\mathbf{76}\%$ absolute improvement in open-world interaction performance. Codes and demos are now available on the project page: https://craftjarvis.github.io/ROCKET-1.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2410.17534.pdf' target='_blank'>https://arxiv.org/pdf/2410.17534.pdf</a></span>   <span><a href='https://github.com/Coo1Sea/OVT-B-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiji Liang, Ruize Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17534">OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary object perception has become an important topic in artificial intelligence, which aims to identify objects with novel classes that have not been seen during training. Under this setting, open-vocabulary object detection (OVD) in a single image has been studied in many literature. However, open-vocabulary object tracking (OVT) from a video has been studied less, and one reason is the shortage of benchmarks. In this work, we have built a new large-scale benchmark for open-vocabulary multi-object tracking namely OVT-B. OVT-B contains 1,048 categories of objects and 1,973 videos with 637,608 bounding box annotations, which is much larger than the sole open-vocabulary tracking dataset, i.e., OVTAO-val dataset (200+ categories, 900+ videos). The proposed OVT-B can be used as a new benchmark to pave the way for OVT research. We also develop a simple yet effective baseline method for OVT. It integrates the motion features for object tracking, which is an important feature for MOT but is ignored in previous OVT methods. Experimental results have verified the usefulness of the proposed benchmark and the effectiveness of our method. We have released the benchmark to the public at https://github.com/Coo1Sea/OVT-B-Dataset.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2410.14977.pdf' target='_blank'>https://arxiv.org/pdf/2410.14977.pdf</a></span>   <span><a href='https://github.com/linh-gist/ms-glmb-nuScenes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Van Ma, Muhammad Ishfaq Hussain, Kin-Choong Yow, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14977">3D Multi-Object Tracking Employing MS-GLMB Filter for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The MS-GLMB filter offers a robust framework for tracking multiple objects through the use of multi-sensor data. Building on this, the MV-GLMB and MV-GLMB-AB filters enhance the MS-GLMB capabilities by employing cameras for 3D multi-sensor multi-object tracking, effectively addressing occlusions. However, both filters depend on overlapping fields of view from the cameras to combine complementary information. In this paper, we introduce an improved approach that integrates an additional sensor, such as LiDAR, into the MS-GLMB framework for 3D multi-object tracking. Specifically, we present a new LiDAR measurement model, along with a multi-camera and LiDAR multi-object measurement model. Our experimental results demonstrate a significant improvement in tracking performance compared to existing MS-GLMB-based methods. Importantly, our method eliminates the need for overlapping fields of view, broadening the applicability of the MS-GLMB filter. Our source code for nuScenes dataset is available at https://github.com/linh-gist/ms-glmb-nuScenes.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2410.11125.pdf' target='_blank'>https://arxiv.org/pdf/2410.11125.pdf</a></span>   <span><a href='https://huiyegit.github.io/UAV3D_Benchmark/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Ye, Rajshekhar Sunderraman, Shihao Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11125">UAV3D: A Large-scale 3D Perception Benchmark for Unmanned Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs), equipped with cameras, are employed in numerous applications, including aerial photography, surveillance, and agriculture. In these applications, robust object detection and tracking are essential for the effective deployment of UAVs. However, existing benchmarks for UAV applications are mainly designed for traditional 2D perception tasks, restricting the development of real-world applications that require a 3D understanding of the environment. Furthermore, despite recent advancements in single-UAV perception, limited views of a single UAV platform significantly constrain its perception capabilities over long distances or in occluded areas. To address these challenges, we introduce UAV3D, a benchmark designed to advance research in both 3D and collaborative 3D perception tasks with UAVs. UAV3D comprises 1,000 scenes, each of which has 20 frames with fully annotated 3D bounding boxes on vehicles. We provide the benchmark for four 3D perception tasks: single-UAV 3D object detection, single-UAV object tracking, collaborative-UAV 3D object detection, and collaborative-UAV object tracking. Our dataset and code are available at https://huiyegit.github.io/UAV3D_Benchmark/.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2410.10409.pdf' target='_blank'>https://arxiv.org/pdf/2410.10409.pdf</a></span>   <span><a href='https://github.com/mzahana/SMART-TRACK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Khaled Gabr, Mohamed Abdelkader, Imen Jarraya, Abdullah AlMusalami, Anis Koubaa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10409">SMART-TRACK: A Novel Kalman Filter-Guided Sensor Fusion For Robust UAV Object Tracking in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of sensor fusion and state estimation for object detection and localization, ensuring accurate tracking in dynamic environments poses significant challenges. Traditional methods like the Kalman Filter (KF) often fail when measurements are intermittent, leading to rapid divergence in state estimations. To address this, we introduce SMART (Sensor Measurement Augmentation and Reacquisition Tracker), a novel approach that leverages high-frequency state estimates from the KF to guide the search for new measurements, maintaining tracking continuity even when direct measurements falter. This is crucial for dynamic environments where traditional methods struggle. Our contributions include: 1) Versatile Measurement Augmentation Using KF Feedback: We implement a versatile measurement augmentation system that serves as a backup when primary object detectors fail intermittently. This system is adaptable to various sensors, demonstrated using depth cameras where KF's 3D predictions are projected into 2D depth image coordinates, integrating nonlinear covariance propagation techniques simplified to first-order approximations. 2) Open-source ROS2 Implementation: We provide an open-source ROS2 implementation of the SMART-TRACK framework, validated in a realistic simulation environment using Gazebo and ROS2, fostering broader adaptation and further research. Our results showcase significant enhancements in tracking stability, with estimation RMSE as low as 0.04 m during measurement disruptions, advancing the robustness of UAV tracking and expanding the potential for reliable autonomous UAV operations in complex scenarios. The implementation is available at https://github.com/mzahana/SMART-TRACK.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2410.09243.pdf' target='_blank'>https://arxiv.org/pdf/2410.09243.pdf</a></span>   <span><a href='https://UARK-AICV.github.io/G2MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duy Le Dinh Anh, Kim Hoang Tran, Quang-Thuc Nguyen, Ngan Hoang Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09243">Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent progress, Multi-Object Tracking (MOT) continues to face significant challenges, particularly its dependence on prior knowledge and predefined categories, complicating the tracking of unfamiliar objects. Generic Multiple Object Tracking (GMOT) emerges as a promising solution, requiring less prior information. Nevertheless, existing GMOT methods, primarily designed as OneShot-GMOT, rely heavily on initial bounding boxes and often struggle with variations in viewpoint, lighting, occlusion, and scale. To overcome the limitations inherent in both MOT and GMOT when it comes to tracking objects with specific generic attributes, we introduce Grounded-GMOT, an innovative tracking paradigm that enables users to track multiple generic objects in videos through natural language descriptors.
  Our contributions begin with the introduction of the G2MOT dataset, which includes a collection of videos featuring a wide variety of generic objects, each accompanied by detailed textual descriptions of their attributes. Following this, we propose a novel tracking method, KAM-SORT, which not only effectively integrates visual appearance with motion cues but also enhances the Kalman filter. KAM-SORT proves particularly advantageous when dealing with objects of high visual similarity from the same generic category in GMOT scenarios. Through comprehensive experiments, we demonstrate that Grounded-GMOT outperforms existing OneShot-GMOT approaches. Additionally, our extensive comparisons between various trackers highlight KAM-SORT's efficacy in GMOT, further establishing its significance in the field. Project page: https://UARK-AICV.github.io/G2MOT. The source code and dataset will be made publicly available.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2410.04250.pdf' target='_blank'>https://arxiv.org/pdf/2410.04250.pdf</a></span>   <span><a href='https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/leggedrobotics/rsl_panoptic_mapping' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Terenzi, Julian Nubert, Pol Eyschen, Pascal Roth, Simin Fei, Edo Jelavic, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04250">ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Construction sites are challenging environments for autonomous systems due to their unstructured nature and the presence of dynamic actors, such as workers and machinery. This work presents a comprehensive panoptic scene understanding solution designed to handle the complexities of such environments by integrating 2D panoptic segmentation with 3D LiDAR mapping. Our system generates detailed environmental representations in real-time by combining semantic and geometric data, supported by Kalman Filter-based tracking for dynamic object detection. We introduce a fine-tuning method that adapts large pre-trained panoptic segmentation models for construction site applications using a limited number of domain-specific samples. For this use case, we release a first-of-its-kind dataset of 502 hand-labeled sample images with panoptic annotations from construction sites. In addition, we propose a dynamic panoptic mapping technique that enhances scene understanding in unstructured environments. As a case study, we demonstrate the system's application for autonomous navigation, utilizing real-time RRT* for reactive path planning in dynamic scenarios. The dataset (https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/) and code (https://github.com/leggedrobotics/rsl_panoptic_mapping) for training and deployment are publicly available to support future research.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2410.02249.pdf' target='_blank'>https://arxiv.org/pdf/2410.02249.pdf</a></span>   <span><a href='https://github.com/AndyCao1125/SpikeSlicer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Cao, Mingyuan Sun, Ziqing Wang, Hao Cheng, Qiang Zhang, Shibo Zhou, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02249">Spiking Neural Network as Adaptive Event Stream Slicer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event-based cameras are attracting significant interest as they provide rich edge information, high dynamic range, and high temporal resolution. Many state-of-the-art event-based algorithms rely on splitting the events into fixed groups, resulting in the omission of crucial temporal information, particularly when dealing with diverse motion scenarios (\eg, high/low speed).In this work, we propose SpikeSlicer, a novel-designed plug-and-play event processing method capable of splitting events stream adaptively.SpikeSlicer utilizes a low-energy spiking neural network (SNN) to trigger event slicing. To guide the SNN to fire spikes at optimal time steps, we propose the Spiking Position-aware Loss (SPA-Loss) to modulate the neuron's state. Additionally, we develop a Feedback-Update training strategy that refines the slicing decisions using feedback from the downstream artificial neural network (ANN). Extensive experiments demonstrate that our method yields significant performance improvements in event-based object tracking and recognition. Notably, SpikeSlicer provides a brand-new SNN-ANN cooperation paradigm, where the SNN acts as an efficient, low-energy data processor to assist the ANN in improving downstream performance, injecting new perspectives and potential avenues of exploration. Our code is available at https://github.com/AndyCao1125/SpikeSlicer.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2409.19603.pdf' target='_blank'>https://arxiv.org/pdf/2409.19603.pdf</a></span>   <span><a href='https://github.com/showlab/VideoLISA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19603">One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2409.17564.pdf' target='_blank'>https://arxiv.org/pdf/2409.17564.pdf</a></span>   <span><a href='https://github.com/LingyiHongfd/CompressTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyi Hong, Jinglun Li, Xinyu Zhou, Shilin Yan, Pinxue Guo, Kaixun Jiang, Zhaoyu Chen, Shuyong Gao, Runze Li, Xingdong Sheng, Wei Zhang, Hong Lu, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17564">General Compression Framework for Efficient Transformer Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous works have attempted to improve tracking efficiency through lightweight architecture design or knowledge distillation from teacher models to compact student trackers. However, these solutions often sacrifice accuracy for speed to a great extent, and also have the problems of complex training process and structural limitations. Thus, we propose a general model compression framework for efficient transformer object tracking, named CompressTracker, to reduce model size while preserving tracking accuracy. Our approach features a novel stage division strategy that segments the transformer layers of the teacher model into distinct stages to break the limitation of model structure. Additionally, we also design a unique replacement training technique that randomly substitutes specific stages in the student model with those from the teacher model, as opposed to training the student model in isolation. Replacement training enhances the student model's ability to replicate the teacher model's behavior and simplifies the training process. To further forcing student model to emulate teacher model, we incorporate prediction guidance and stage-wise feature mimicking to provide additional supervision during the teacher model's compression process. CompressTracker is structurally agnostic, making it compatible with any transformer architecture. We conduct a series of experiment to verify the effectiveness and generalizability of our CompressTracker. Our CompressTracker-SUTrack, compressed from SUTrack, retains about 99 performance on LaSOT (72.2 AUC) while achieves 2.42x speed up. Code is available at https://github.com/LingyiHongfd/CompressTracker.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2409.16902.pdf' target='_blank'>https://arxiv.org/pdf/2409.16902.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Guanjie Huang, Zhipeng Zhang, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16902">Underwater Camouflaged Object Tracking Meets Vision-Language SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale datasets. However, these datasets have primarily focused on open-air scenarios and have largely overlooked underwater animal tracking-especially the complex challenges posed by camouflaged marine animals. To bridge this gap, we take a step forward by proposing the first large-scale multi-modal underwater camouflaged object tracking dataset, namely UW-COT220. Based on the proposed dataset, this work first comprehensively evaluates current advanced visual object tracking methods, including SAM- and SAM2-based trackers, in challenging underwater environments, \eg, coral reefs. Our findings highlight the improvements of SAM2 over SAM, demonstrating its enhanced ability to handle the complexities of underwater camouflaged objects. Furthermore, we propose a novel vision-language tracking framework called VL-SAM2, based on the video foundation model SAM2. Extensive experimental results demonstrate that the proposed VL-SAM2 achieves state-of-the-art performance across underwater and open-air object tracking datasets. The dataset and codes are available at~{\color{magenta}{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}}.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2409.16834.pdf' target='_blank'>https://arxiv.org/pdf/2409.16834.pdf</a></span>   <span><a href='https://github.com/vision4robotics/CGDenoiser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Wang, Changhong Fu, Kunhan Lu, Liangliang Yao, Haobo Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16834">Conditional Generative Denoiser for Nighttime UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art (SOTA) visual object tracking methods have significantly enhanced the autonomy of unmanned aerial vehicles (UAVs). However, in low-light conditions, the presence of irregular real noise from the environments severely degrades the performance of these SOTA methods. Moreover, existing SOTA denoising techniques often fail to meet the real-time processing requirements when deployed as plug-and-play denoisers for UAV tracking. To address this challenge, this work proposes a novel conditional generative denoiser (CGDenoiser), which breaks free from the limitations of traditional deterministic paradigms and generates the noise conditioning on the input, subsequently removing it. To better align the input dimensions and accelerate inference, a novel nested residual Transformer conditionalizer is developed. Furthermore, an innovative multi-kernel conditional refiner is designed to pertinently refine the denoised output. Extensive experiments show that CGDenoiser promotes the tracking precision of the SOTA tracker by 18.18\% on DarkTrack2021 whereas working 5.8 times faster than the second well-performed denoiser. Real-world tests with complex challenges also prove the effectiveness and practicality of CGDenoiser. Code, video demo and supplementary proof for CGDenoier are now available at: \url{https://github.com/vision4robotics/CGDenoiser}.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2409.16652.pdf' target='_blank'>https://arxiv.org/pdf/2409.16652.pdf</a></span>   <span><a href='https://github.com/vision4robotics/PRL-Track' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changhong Fu, Xiang Lei, Haobo Zuo, Liangliang Yao, Guangze Zheng, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16652">Progressive Representation Learning for Real-Time UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has significantly promoted autonomous applications for unmanned aerial vehicles (UAVs). However, learning robust object representations for UAV tracking is especially challenging in complex dynamic environments, when confronted with aspect ratio change and occlusion. These challenges severely alter the original information of the object. To handle the above issues, this work proposes a novel progressive representation learning framework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided into coarse representation learning and fine representation learning. For coarse representation learning, two innovative regulators, which rely on appearance and semantic information, are designed to mitigate appearance interference and capture semantic information. Furthermore, for fine representation learning, a new hierarchical modeling generator is developed to intertwine coarse object representations. Exhaustive experiments demonstrate that the proposed PRL-Track delivers exceptional performance on three authoritative UAV tracking benchmarks. Real-world tests indicate that the proposed PRL-Track realizes superior tracking performance with 42.6 frames per second on the typical UAV platform equipped with an edge smart camera. The code, model, and demo videos are available at \url{https://github.com/vision4robotics/PRL-Track}.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2409.16631.pdf' target='_blank'>https://arxiv.org/pdf/2409.16631.pdf</a></span>   <span><a href='https://github.com/vision4robotics/LDEnhancer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangliang Yao, Changhong Fu, Yiheng Wang, Haobo Zuo, Kunhan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16631">Enhancing Nighttime UAV Tracking with Light Distribution Suppression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has boosted extensive intelligent applications for unmanned aerial vehicles (UAVs). However, the state-of-the-art (SOTA) enhancers for nighttime UAV tracking always neglect the uneven light distribution in low-light images, inevitably leading to excessive enhancement in scenarios with complex illumination. To address these issues, this work proposes a novel enhancer, i.e., LDEnhancer, enhancing nighttime UAV tracking with light distribution suppression. Specifically, a novel image content refinement module is developed to decompose the light distribution information and image content information in the feature space, allowing for the targeted enhancement of the image content information. Then this work designs a new light distribution generation module to capture light distribution effectively. The features with light distribution information and image content information are fed into the different parameter estimation modules, respectively, for the parameter map prediction. Finally, leveraging two parameter maps, an innovative interweave iteration adjustment is proposed for the collaborative pixel-wise adjustment of low-light images. Additionally, a challenging nighttime UAV tracking dataset with uneven light distribution, namely NAT2024-2, is constructed to provide a comprehensive evaluation, which contains 40 challenging sequences with over 74K frames in total. Experimental results on the authoritative UAV benchmarks and the proposed NAT2024-2 demonstrate that LDEnhancer outperforms other SOTA low-light enhancers for nighttime UAV tracking. Furthermore, real-world tests on a typical UAV platform with an NVIDIA Orin NX confirm the practicality and efficiency of LDEnhancer. The code is available at https://github.com/vision4robotics/LDEnhancer.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2409.16149.pdf' target='_blank'>https://arxiv.org/pdf/2409.16149.pdf</a></span>   <span><a href='https://github.com/megvii-research/MCTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyang Wang, Shouzheng Qi, Jieyou Zhao, Hangning Zhou, Siyu Zhang, Guoan Wang, Kai Tu, Songlin Guo, Jianbo Zhao, Jian Li, Mu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16149">MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MCTrack, a new 3D multi-object tracking method that achieves state-of-the-art (SOTA) performance across KITTI, nuScenes, and Waymo datasets. Addressing the gap in existing tracking paradigms, which often perform well on specific datasets but lack generalizability, MCTrack offers a unified solution. Additionally, we have standardized the format of perceptual results across various datasets, termed BaseVersion, facilitating researchers in the field of multi-object tracking (MOT) to concentrate on the core algorithmic development without the undue burden of data preprocessing. Finally, recognizing the limitations of current evaluation metrics, we propose a novel set that assesses motion information output, such as velocity and acceleration, crucial for downstream tasks. The source codes of the proposed method are available at this link: https://github.com/megvii-research/MCTrack}{https://github.com/megvii-research/MCTrack
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2409.12159.pdf' target='_blank'>https://arxiv.org/pdf/2409.12159.pdf</a></span>   <span><a href='https://github.com/Walleclipse/WeHelp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abulikemu Abuduweili, Alice Wu, Tianhao Wei, Weiye Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12159">WeHelp: A Shared Autonomy System for Wheelchair Users</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a large population of wheelchair users. Most of the wheelchair users need help with daily tasks. However, according to recent reports, their needs are not properly satisfied due to the lack of caregivers. Therefore, in this project, we develop WeHelp, a shared autonomy system aimed for wheelchair users. A robot with a WeHelp system has three modes, following mode, remote control mode and tele-operation mode. In the following mode, the robot follows the wheelchair user automatically via visual tracking. The wheelchair user can ask the robot to follow them from behind, by the left or by the right. When the wheelchair user asks for help, the robot will recognize the command via speech recognition, and then switch to the teleoperation mode or remote control mode. In the teleoperation mode, the wheelchair user takes over the robot with a joy stick and controls the robot to complete some complex tasks for their needs, such as opening doors, moving obstacles on the way, reaching objects on a high shelf or on the low ground, etc. In the remote control mode, a remote assistant takes over the robot and helps the wheelchair user complete some complex tasks for their needs. Our evaluation shows that the pipeline is useful and practical for wheelchair users. Source code and demo of the paper are available at \url{https://github.com/Walleclipse/WeHelp}.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2409.11235.pdf' target='_blank'>https://arxiv.org/pdf/2409.11235.pdf</a></span>   <span><a href='https://github.com/siyuanliii/SLAck' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/siyuanliii/SLAck' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Li, Lei Ke, Yung-Hsu Yang, Luigi Piccinelli, Mattia SegÃ¹, Martin Danelljan, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11235">SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers to novel categories not in the training set. Currently, the best-performing methods are mainly based on pure appearance matching. Due to the complexity of motion patterns in the large-vocabulary scenarios and unstable classification of the novel objects, the motion and semantics cues are either ignored or applied based on heuristics in the final matching steps by existing methods. In this paper, we present a unified framework SLAck that jointly considers semantics, location, and appearance priors in the early steps of association and learns how to integrate all valuable information through a lightweight spatial and temporal object graph. Our method eliminates complex post-processing heuristics for fusing different cues and boosts the association performance significantly for large-scale open-vocabulary tracking. Without bells and whistles, we outperform previous state-of-the-art methods for novel classes tracking on the open-vocabulary MOT and TAO TETA benchmarks. Our code is available at \href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2409.11234.pdf' target='_blank'>https://arxiv.org/pdf/2409.11234.pdf</a></span>   <span><a href='https://github.com/ydhcg-BoBo/STCMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Ma, Chuanming Tang, Fei Wu, Can Zhao, Jianlin Zhang, Zhiyong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11234">STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos is important for diverse applications in computer vision. Current MOT trackers rely on accurate object detection results and precise matching of target reidentification (ReID). These methods focus on optimizing target spatial attributes while overlooking temporal cues in modelling object relationships, especially for challenging tracking conditions such as object deformation and blurring, etc. To address the above-mentioned issues, we propose a novel Spatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), which utilizes historical embedding features to model the representation of ReID and detection features in a sequential order. Concretely, a temporal embedding boosting module is introduced to enhance the discriminability of individual embedding based on adjacent frame cooperation. While the trajectory embedding is then propagated by a temporal detection refinement module to mine salient target locations in the temporal field. Extensive experiments on the VisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a new state-of-the-art performance in MOTA and IDF1 metrics. The source codes are released at https://github.com/ydhcg-BoBo/STCMOT.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2409.09293.pdf' target='_blank'>https://arxiv.org/pdf/2409.09293.pdf</a></span>   <span><a href='https://github.com/balabooooo/AED' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zimeng Fang, Chao Liang, Xue Zhou, Shuyuan Zhu, Xi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09293">Associate Everything Detected: Facilitating Tracking-by-Detection to the Unknown</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) emerges as a pivotal and highly promising branch in the field of computer vision. Classical closed-vocabulary MOT (CV-MOT) methods aim to track objects of predefined categories. Recently, some open-vocabulary MOT (OV-MOT) methods have successfully addressed the problem of tracking unknown categories. However, we found that the CV-MOT and OV-MOT methods each struggle to excel in the tasks of the other. In this paper, we present a unified framework, Associate Everything Detected (AED), that simultaneously tackles CV-MOT and OV-MOT by integrating with any off-the-shelf detector and supports unknown categories. Different from existing tracking-by-detection MOT methods, AED gets rid of prior knowledge (e.g. motion cues) and relies solely on highly robust feature learning to handle complex trajectories in OV-MOT tasks while keeping excellent performance in CV-MOT tasks. Specifically, we model the association task as a similarity decoding problem and propose a sim-decoder with an association-centric learning mechanism. The sim-decoder calculates similarities in three aspects: spatial, temporal, and cross-clip. Subsequently, association-centric learning leverages these threefold similarities to ensure that the extracted features are appropriate for continuous tracking and robust enough to generalize to unknown categories. Compared with existing powerful OV-MOT and CV-MOT methods, AED achieves superior performance on TAO, SportsMOT, and DanceTrack without any prior knowledge. Our code is available at https://github.com/balabooooo/AED.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2409.06617.pdf' target='_blank'>https://arxiv.org/pdf/2409.06617.pdf</a></span>   <span><a href='https://github.com/emirhanbayar/Fast-Deep-OC-SORT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/emirhanbayar/Fast-StrongSORT,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Emirhan Bayar, Cemal Aker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06617">When to Extract ReID Features: A Selective Approach for Improved Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting and matching Re-Identification (ReID) features is used by many state-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularly effective against frequent and long-term occlusions. While end-to-end object detection and tracking have been the main focus of recent research, they have yet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus, from an application standpoint, methods with separate detection and embedding remain the best option for accuracy, modularity, and ease of implementation, though they are impractical for edge devices due to the overhead involved. In this paper, we investigate a selective approach to minimize the overhead of feature extraction while preserving accuracy, modularity, and ease of implementation. This approach can be integrated into various SOTA methods. We demonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT. Experiments on MOT17, MOT20, and DanceTrack datasets show that our mechanism retains the advantages of feature extraction during occlusions while significantly reducing runtime. Additionally, it improves accuracy by preventing confusion in the feature-matching stage, particularly in cases of deformation and appearance similarity, which are common in DanceTrack. https://github.com/emirhanbayar/Fast-StrongSORT, https://github.com/emirhanbayar/Fast-Deep-OC-SORT
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2409.04187.pdf' target='_blank'>https://arxiv.org/pdf/2409.04187.pdf</a></span>   <span><a href='https://github.com/Jumabek/LITE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jumabek Alikhanov, Dilshod Obidov, Hakil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04187">LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID Feature Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is introduced as a novel multi-object tracking (MOT) approach. It enhances ReID-based trackers by eliminating inference, pre-processing, post-processing, and ReID model training costs. LITE uses real-time appearance features without compromising speed. By integrating appearance feature extraction directly into the tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE demonstrates significant performance improvements. The simplest implementation of LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS on the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four times faster on the more crowded MOT20 dataset, while maintaining similar accuracy. Additionally, a new evaluation framework for tracking-by-detection approaches reveals that conventional trackers like DeepSORT remain competitive with modern state-of-the-art trackers when evaluated under fair conditions. The code will be available post-publication at https://github.com/Jumabek/LITE.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2409.02490.pdf' target='_blank'>https://arxiv.org/pdf/2409.02490.pdf</a></span>   <span><a href='https://fsoft-aic.github.io/TP-GMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duy Le Dinh Anh, Kim Hoang Tran, Ngan Hoang Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02490">TP-GMOT: Tracking Generic Multiple Object by Textual Prompt with Motion-Appearance Cost (MAC) SORT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multi-Object Tracking (MOT) has made substantial advancements, it is limited by heavy reliance on prior knowledge and limited to predefined categories. In contrast, Generic Multiple Object Tracking (GMOT), tracking multiple objects with similar appearance, requires less prior information about the targets but faces challenges with variants like viewpoint, lighting, occlusion, and resolution. Our contributions commence with the introduction of the \textbf{\text{Refer-GMOT dataset}} a collection of videos, each accompanied by fine-grained textual descriptions of their attributes. Subsequently, we introduce a novel text prompt-based open-vocabulary GMOT framework, called \textbf{\text{TP-GMOT}}, which can track never-seen object categories with zero training examples. Within \text{TP-GMOT} framework, we introduce two novel components: (i) {\textbf{\text{TP-OD}}, an object detection by a textual prompt}, for accurately detecting unseen objects with specific characteristics. (ii) Motion-Appearance Cost SORT \textbf{\text{MAC-SORT}}, a novel object association approach that adeptly integrates motion and appearance-based matching strategies to tackle the complex task of tracking multiple generic objects with high similarity. Our contributions are benchmarked on the \text{Refer-GMOT} dataset for GMOT task. Additionally, to assess the generalizability of the proposed \text{TP-GMOT} framework and the effectiveness of \text{MAC-SORT} tracker, we conduct ablation studies on the DanceTrack and MOT20 datasets for the MOT task. Our dataset, code, and models will be publicly available at: https://fsoft-aic.github.io/TP-GMOT
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2409.00487.pdf' target='_blank'>https://arxiv.org/pdf/2409.00487.pdf</a></span>   <span><a href='https://github.com/Xavier-Lin/TrackSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Hu, Run Luo, Zelin Liu, Cheng Wang, Wenyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00487">TrackSSM: A General Motion Predictor by State-Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal motion modeling has always been a key component in multiple object tracking (MOT) which can ensure smooth trajectory movement and provide accurate positional information to enhance association precision. However, current motion models struggle to be both efficient and effective across different application scenarios. To this end, we propose TrackSSM inspired by the recently popular state space models (SSM), a unified encoder-decoder motion framework that uses data-dependent state space model to perform temporal motion of trajectories. Specifically, we propose Flow-SSM, a module that utilizes the position and motion information from historical trajectories to guide the temporal state transition of object bounding boxes. Based on Flow-SSM, we design a flow decoder. It is composed of a cascaded motion decoding module employing Flow-SSM, which can use the encoded flow information to complete the temporal position prediction of trajectories. Additionally, we propose a Step-by-Step Linear (S$^2$L) training strategy. By performing linear interpolation between the positions of the object in the previous frame and the current frame, we construct the pseudo labels of step-by-step linear training, ensuring that the trajectory flow information can better guide the object bounding box in completing temporal transitions. TrackSSM utilizes a simple Mamba-Block to build a motion encoder for historical trajectories, forming a temporal motion model with an encoder-decoder structure in conjunction with the flow decoder. TrackSSM is applicable to various tracking scenarios and achieves excellent tracking performance across multiple benchmarks, further extending the potential of SSM-like temporal motion models in multi-object tracking tasks. Code and models are publicly available at \url{https://github.com/Xavier-Lin/TrackSSM}.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2408.15548.pdf' target='_blank'>https://arxiv.org/pdf/2408.15548.pdf</a></span>   <span><a href='https://github.com/Tankowa/ConsistencyTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lifan Jiang, Zhihui Wang, Siqi Yin, Guangxiao Ma, Peng Zhang, Boxi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15548">ConsistencyTrack: A Robust Multi-Object Tracker with a Generation Strategy of Consistency Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a critical technology in computer vision, designed to detect multiple targets in video sequences and assign each target a unique ID per frame. Existed MOT methods excel at accurately tracking multiple objects in real-time across various scenarios. However, these methods still face challenges such as poor noise resistance and frequent ID switches. In this research, we propose a novel ConsistencyTrack, joint detection and tracking(JDT) framework that formulates detection and association as a denoising diffusion process on perturbed bounding boxes. This progressive denoising strategy significantly improves the model's noise resistance. During the training phase, paired object boxes within two adjacent frames are diffused from ground-truth boxes to a random distribution, and then the model learns to detect and track by reversing this process. In inference, the model refines randomly generated boxes into detection and tracking results through minimal denoising steps. ConsistencyTrack also introduces an innovative target association strategy to address target occlusion. Experiments on the MOT17 and DanceTrack datasets demonstrate that ConsistencyTrack outperforms other compared methods, especially better than DiffusionTrack in inference speed and other performance metrics. Our code is available at https://github.com/Tankowa/ConsistencyTrack.
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2408.13877.pdf' target='_blank'>https://arxiv.org/pdf/2408.13877.pdf</a></span>   <span><a href='https://github.com/openat25/HIPTrack-MLS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Guo, Pengzhi Zhong, Hao Zhang, Defeng Huang, Huikai Shao, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13877">Camouflaged Object Tracking: A Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual tracking has seen remarkable advancements, largely driven by the availability of large-scale training datasets that have enabled the development of highly accurate and robust algorithms. While significant progress has been made in tracking general objects, research on more challenging scenarios, such as tracking camouflaged objects, remains limited. Camouflaged objects, which blend seamlessly with their surroundings or other objects, present unique challenges for detection and tracking in complex environments. This challenge is particularly critical in applications such as military, security, agriculture, and marine monitoring, where precise tracking of camouflaged objects is essential. To address this gap, we introduce the Camouflaged Object Tracking Dataset (COTD), a specialized benchmark designed specifically for evaluating camouflaged object tracking methods. The COTD dataset comprises 200 sequences and approximately 80,000 frames, each annotated with detailed bounding boxes. Our evaluation of 20 existing tracking algorithms reveals significant deficiencies in their performance with camouflaged objects. To address these issues, we propose a novel tracking framework, HiPTrack-MLS, which demonstrates promising results in improving tracking performance for camouflaged objects. COTD and code are avialable at https://github.com/openat25/HIPTrack-MLS.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2408.13003.pdf' target='_blank'>https://arxiv.org/pdf/2408.13003.pdf</a></span>   <span><a href='https://github.com/vukasin-stanojevic/BoostTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>VukaÅ¡in StanojeviÄ, Branimir TodoroviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13003">BoostTrack++: using tracklet information to detect more objects in multiple object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) depends heavily on selection of true positive detected bounding boxes. However, this aspect of the problem is mostly overlooked or mitigated by employing two-stage association and utilizing low confidence detections in the second stage. Recently proposed BoostTrack attempts to avoid the drawbacks of multiple stage association approach and use low-confidence detections by applying detection confidence boosting. In this paper, we identify the limitations of the confidence boost used in BoostTrack and propose a method to improve its performance. To construct a richer similarity measure and enable a better selection of true positive detections, we propose to use a combination of shape, Mahalanobis distance and novel soft BIoU similarity. We propose a soft detection confidence boost technique which calculates new confidence scores based on the similarity measure and the previous confidence scores, and we introduce varying similarity threshold to account for lower similarity measure between detections and tracklets which are not regularly updated. The proposed additions are mutually independent and can be used in any MOT algorithm.
  Combined with the BoostTrack+ baseline, our method achieves near state of the art results on the MOT17 dataset and new state of the art HOTA and IDF1 scores on the MOT20 dataset.
  The source code is available at: https://github.com/vukasin-stanojevic/BoostTrack .
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2408.11571.pdf' target='_blank'>https://arxiv.org/pdf/2408.11571.pdf</a></span>   <span><a href='https://github.com/CellTrackingChallenge/py-ctcmetrics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Timo Kaiser, Vladimir Ulman, Bodo Rosenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11571">CHOTA: A Higher Order Accuracy Metric for Cell Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evaluation of cell tracking results steers the development of tracking methods, significantly impacting biomedical research. This is quantitatively achieved by means of evaluation metrics. Unfortunately, current metrics favor local correctness and weakly reward global coherence, impeding high-level biological analysis. To also foster global coherence, we propose the CHOTA metric (Cell-specific Higher Order Tracking Accuracy) which unifies the evaluation of all relevant aspects of cell tracking: cell detections and local associations, global coherence, and lineage tracking. We achieve this by introducing a new definition of the term 'trajectory' that includes the entire cell lineage and by including this into the well-established HOTA metric from general multiple object tracking. Furthermore, we provide a detailed survey of contemporary cell tracking metrics to compare our novel CHOTA metric and to show its advantages. All metrics are extensively evaluated on state-of-the-art real-data cell tracking results and synthetic results that simulate specific tracking errors. We show that CHOTA is sensitive to all tracking errors and gives a good indication of the biologically relevant capability of a method to reconstruct the full lineage of cells. It introduces a robust and comprehensive alternative to the currently used metrics in cell tracking. Python code is available at https://github.com/CellTrackingChallenge/py-ctcmetrics .
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2408.11463.pdf' target='_blank'>https://arxiv.org/pdf/2408.11463.pdf</a></span>   <span><a href='https://github.com/OpenCodeGithub/H-DCPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengzhi Zhong, Xiaoyu Guo, Defeng Huang, Xiaojun Peng, Yian Li, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11463">Low-Light Object Tracking: A Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the field of visual tracking has made significant progress with the application of large-scale training datasets. These datasets have supported the development of sophisticated algorithms, enhancing the accuracy and stability of visual object tracking. However, most research has primarily focused on favorable illumination circumstances, neglecting the challenges of tracking in low-ligh environments. In low-light scenes, lighting may change dramatically, targets may lack distinct texture features, and in some scenarios, targets may not be directly observable. These factors can lead to a severe decline in tracking performance. To address this issue, we introduce LLOT, a benchmark specifically designed for Low-Light Object Tracking. LLOT comprises 269 challenging sequences with a total of over 132K frames, each carefully annotated with bounding boxes. This specially designed dataset aims to promote innovation and advancement in object tracking techniques for low-light conditions, addressing challenges not adequately covered by existing benchmarks. To assess the performance of existing methods on LLOT, we conducted extensive tests on 39 state-of-the-art tracking algorithms. The results highlight a considerable gap in low-light tracking performance. In response, we propose H-DCPT, a novel tracker that incorporates historical and darkness clue prompts to set a stronger baseline. H-DCPT outperformed all 39 evaluated methods in our experiments, demonstrating significant improvements. We hope that our benchmark and H-DCPT will stimulate the development of novel and accurate methods for tracking objects in low-light conditions. The LLOT and code are available at https://github.com/OpenCodeGithub/H-DCPT.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2408.10487.pdf' target='_blank'>https://arxiv.org/pdf/2408.10487.pdf</a></span>   <span><a href='https://github.com/Event-AHU/MambaEVT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Chao wang, Shiao Wang, Xixi Wang, Zhicheng Zhao, Lin Zhu, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10487">MambaEVT: Event Stream based Visual Object Tracking using State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event camera-based visual tracking has drawn more and more attention in recent years due to the unique imaging principle and advantages of low energy consumption, high dynamic range, and dense temporal resolution. Current event-based tracking algorithms are gradually hitting their performance bottlenecks, due to the utilization of vision Transformer and the static template for target object localization. In this paper, we propose a novel Mamba-based visual tracking framework that adopts the state space model with linear complexity as a backbone network. The search regions and target template are fed into the vision Mamba network for simultaneous feature extraction and interaction. The output tokens of search regions will be fed into the tracking head for target localization. More importantly, we consider introducing a dynamic template update strategy into the tracking framework using the Memory Mamba network. By considering the diversity of samples in the target template library and making appropriate adjustments to the template memory module, a more effective dynamic template can be integrated. The effective combination of dynamic and static templates allows our Mamba-based tracking algorithm to achieve a good balance between accuracy and computational cost on multiple large-scale datasets, including EventVOT, VisEvent, and FE240hz. The source code will be released on https://github.com/Event-AHU/MambaEVT
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2408.07605.pdf' target='_blank'>https://arxiv.org/pdf/2408.07605.pdf</a></span>   <span><a href='https://panacea-ad.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Wen, Yucheng Zhao, Yingfei Liu, Binyuan Huang, Fan Jia, Yanhui Wang, Chi Zhang, Tiancai Wang, Xiaoyan Sun, Xiangyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07605">Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of autonomous driving increasingly demands high-quality annotated video training data. In this paper, we propose Panacea+, a powerful and universally applicable framework for generating video data in driving scenes. Built upon the foundation of our previous work, Panacea, Panacea+ adopts a multi-view appearance noise prior mechanism and a super-resolution module for enhanced consistency and increased resolution. Extensive experiments show that the generated video samples from Panacea+ greatly benefit a wide range of tasks on different datasets, including 3D object tracking, 3D object detection, and lane detection tasks on the nuScenes and Argoverse 2 dataset. These results strongly prove Panacea+ to be a valuable data generation framework for autonomous driving.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2408.06190.pdf' target='_blank'>https://arxiv.org/pdf/2408.06190.pdf</a></span>   <span><a href='https://meyerls.github.io/fruit_nerf/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Meyer, Andreas Gilson, Ute Schmid, Marc Stamminger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06190">FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FruitNeRF, a unified novel fruit counting framework that leverages state-of-the-art view synthesis methods to count any fruit type directly in 3D. Our framework takes an unordered set of posed images captured by a monocular camera and segments fruit in each image. To make our system independent of the fruit type, we employ a foundation model that generates binary segmentation masks for any fruit. Utilizing both modalities, RGB and semantic, we train a semantic neural radiance field. Through uniform volume sampling of the implicit Fruit Field, we obtain fruit-only point clouds. By applying cascaded clustering on the extracted point cloud, our approach achieves precise fruit count.The use of neural radiance fields provides significant advantages over conventional methods such as object tracking or optical flow, as the counting itself is lifted into 3D. Our method prevents double counting fruit and avoids counting irrelevant fruit.We evaluate our methodology using both real-world and synthetic datasets. The real-world dataset consists of three apple trees with manually counted ground truths, a benchmark apple dataset with one row and ground truth fruit location, while the synthetic dataset comprises various fruit types including apple, plum, lemon, pear, peach, and mango.Additionally, we assess the performance of fruit counting using the foundation model compared to a U-Net.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2408.01688.pdf' target='_blank'>https://arxiv.org/pdf/2408.01688.pdf</a></span>   <span><a href='https://github.com/HDU-VRLab/SiamMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Yang, Yingqi Deng, Jing Zhang, Hongjie Gu, Zhekang Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01688">SiamMo: Siamese Motion-Centric 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode object size priors into motion estimation. SiamMo is a purely motion-centric tracker that eliminates the need for additional processes like segmentation and box refinement. Without whistles and bells, SiamMo not only surpasses state-of-the-art methods across multiple benchmarks but also demonstrates exceptional robustness in challenging scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\% precision while maintaining a high inference speed of 108 FPS. The code will be released at https://github.com/HDU-VRLab/SiamMo.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2408.00969.pdf' target='_blank'>https://arxiv.org/pdf/2408.00969.pdf</a></span>   <span><a href='https://github.com/wqw123wqw/PFTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yabin Zhu, Qianwu Wang, Chenglong Li, Jin Tang, Zhixiang Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00969">Visible-Thermal Multiple Object Tracking: Large-scale Video Dataset and Progressive Fusion Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complementary benefits from visible and thermal infrared data are widely utilized in various computer vision task, such as visual tracking, semantic segmentation and object detection, but rarely explored in Multiple Object Tracking (MOT). In this work, we contribute a large-scale Visible-Thermal video benchmark for MOT, called VT-MOT. VT-MOT has the following main advantages. 1) The data is large scale and high diversity. VT-MOT includes 582 video sequence pairs, 401k frame pairs from surveillance, drone, and handheld platforms. 2) The cross-modal alignment is highly accurate. We invite several professionals to perform both spatial and temporal alignment frame by frame. 3) The annotation is dense and high-quality. VT-MOT has 3.99 million annotation boxes annotated and double-checked by professionals, including heavy occlusion and object re-acquisition (object disappear and reappear) challenges. To provide a strong baseline, we design a simple yet effective tracking framework, which effectively fuses temporal information and complementary information of two modalities in a progressive manner, for robust visible-thermal MOT. A comprehensive experiment are conducted on VT-MOT and the results prove the superiority and effectiveness of the proposed method compared with state-of-the-art methods. From the evaluation results and analysis, we specify several potential future directions for visible-thermal MOT. The project is released in https://github.com/wqw123wqw/PFTrack.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2408.00874.pdf' target='_blank'>https://arxiv.org/pdf/2408.00874.pdf</a></span>   <span><a href='https://supermedintel.github.io/Medical-SAM2/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayuan Zhu, Abdullah Hamdi, Yunli Qi, Yueming Jin, Junde Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00874">Medical SAM 2: Segment medical images as video via Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation plays a pivotal role in clinical diagnostics and treatment planning, yet existing models often face challenges in generalization and in handling both 2D and 3D data uniformly. In this paper, we introduce Medical SAM 2 (MedSAM-2), a generalized auto-tracking model for universal 2D and 3D medical image segmentation. The core concept is to leverage the Segment Anything Model 2 (SAM2) pipeline to treat all 2D and 3D medical segmentation tasks as a video object tracking problem. To put it into practice, we propose a novel \emph{self-sorting memory bank} mechanism that dynamically selects informative embeddings based on confidence and dissimilarity, regardless of temporal order. This mechanism not only significantly improves performance in 3D medical image segmentation but also unlocks a \emph{One-Prompt Segmentation} capability for 2D images, allowing segmentation across multiple images from a single prompt without temporal relationships. We evaluated MedSAM-2 on five 2D tasks and nine 3D tasks, including white blood cells, optic cups, retinal vessels, mandibles, coronary arteries, kidney tumors, liver tumors, breast cancer, nasopharynx cancer, vestibular schwannoma, mediastinal lymph nodules, cerebral artery, inferior alveolar nerve, and abdominal organs, comparing it against state-of-the-art (SOTA) models in task-tailored, general and interactive segmentation settings. Our findings demonstrate that MedSAM-2 surpasses a wide range of existing models and updates new SOTA on several benchmarks. The code is released on the project page: https://supermedintel.github.io/Medical-SAM2/.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2407.15199.pdf' target='_blank'>https://arxiv.org/pdf/2407.15199.pdf</a></span>   <span><a href='https://github.com/SpaceTimeLab/360_object_tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingwei Guo, Yitai Cheng, Meihui Wang, Ilya Ilyankou, Natchapon Jongwiriyanurak, Xiaowei Gao, Nicola Christie, James Haworth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15199">Multiple Object Detection and Tracking in Panoramic Videos for Cycling Safety Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cyclists face a disproportionate risk of injury, yet conventional crash records are too limited to reconstruct the circumstances of incidents or to diagnose risk at the finer spatial and temporal detail needed for targeted interventions. Recently, naturalistic studies have gained traction as a way to capture the complex behavioural and infrastructural factors that contribute to crashes. These approaches typically involve the collection and analysis of video data. A video promising format is panoramic video, which can record 360-degree views around a rider. However, its use is limited by severe distortions, large numbers of small objects and boundary continuity. This study addresses these challenges by proposing a novel three-step framework: (1) enhancing object detection accuracy on panoramic imagery by segmenting and projecting the original 360-degree images into four perspective sub-images, thus reducing distortion; (2) modifying multi-object tracking models to incorporate boundary continuity and object category information for improved tracking consistency; and (3) validating the proposed approach through a real-world application focused on detecting overtaking manoeuvres by vehicles around cyclists. The methodology is evaluated using panoramic videos recorded by cyclists on London's roadways under diverse conditions. Experimental results demonstrate notable improvements over baseline methods, achieving higher average precision across varying image resolutions. Moreover, the enhanced tracking approach yields a 3.0% increase in multi-object tracking accuracy and a 4.6% improvement in identification F-score. The overtaking detection task achieves a high F-score of 0.81, illustrating the practical effectiveness of the proposed method in real-world cycling safety scenarios. The code is available on GitHub (https://github.com/SpaceTimeLab/360_object_tracking) to ensure reproducibility.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2407.14086.pdf' target='_blank'>https://arxiv.org/pdf/2407.14086.pdf</a></span>   <span><a href='https://github.com/yfzhang1214/TCBTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfei Zhang, Chao Liang, Jin Gao, Zhipeng Zhang, Weiming Hu, Stephen Maybank, Xue Zhou, Liang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14086">Temporal Correlation Meets Embedding: Towards a 2nd Generation of JDE-based Real-Time Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint Detection and Embedding (JDE) trackers have demonstrated excellent performance in Multi-Object Tracking (MOT) tasks by incorporating the extraction of appearance features as auxiliary tasks through embedding Re-Identification task (ReID) into the detector, achieving a balance between inference speed and tracking performance. However, solving the competition between the detector and the feature extractor has always been a challenge. Meanwhile, the issue of directly embedding the ReID task into MOT has remained unresolved. The lack of high discriminability in appearance features results in their limited utility. In this paper, a new learning approach using cross-correlation to capture temporal information of objects is proposed. The feature extraction network is no longer trained solely on appearance features from each frame but learns richer motion features by utilizing feature heatmaps from consecutive frames, which addresses the challenge of inter-class feature similarity. Furthermore, our learning approach is applied to a more lightweight feature extraction network, and treat the feature matching scores as strong cues rather than auxiliary cues, with an appropriate weight calculation to reflect the compatibility between our obtained features and the MOT task. Our tracker, named TCBTrack, achieves state-of-the-art performance on multiple public benchmarks, i.e., MOT17, MOT20, and DanceTrack datasets. Specifically, on the DanceTrack test set, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA, making it the best online tracker capable of achieving real-time performance. Comparative evaluations with other trackers prove that our tracker achieves the best balance between speed, robustness and accuracy. Code is available at https://github.com/yfzhang1214/TCBTrack.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2407.10151.pdf' target='_blank'>https://arxiv.org/pdf/2407.10151.pdf</a></span>   <span><a href='https://github.com/lorenzovaquero/BUSCA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lorenzovaquero/BUSCA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Vaquero, Yihong Xu, Xavier Alameda-Pineda, Victor M. Brea, Manuel Mucientes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10151">Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) endeavors to precisely estimate the positions and identities of multiple objects over time. The prevailing approach, tracking-by-detection (TbD), first detects objects and then links detections, resulting in a simple yet effective method. However, contemporary detectors may occasionally miss some objects in certain frames, causing trackers to cease tracking prematurely. To tackle this issue, we propose BUSCA, meaning `to search', a versatile framework compatible with any online TbD system, enhancing its ability to persistently track those objects missed by the detector, primarily due to occlusions. Remarkably, this is accomplished without modifying past tracking results or accessing future frames, i.e., in a fully online manner. BUSCA generates proposals based on neighboring tracks, motion, and learned tokens. Utilizing a decision Transformer that integrates multimodal visual and spatiotemporal information, it addresses the object-proposal association as a multi-choice question-answering task. BUSCA is trained independently of the underlying tracker, solely on synthetic data, without requiring fine-tuning. Through BUSCA, we showcase consistent performance enhancements across five different trackers and establish a new state-of-the-art baseline across three different benchmarks. Code available at: https://github.com/lorenzovaquero/BUSCA.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2407.08872.pdf' target='_blank'>https://arxiv.org/pdf/2407.08872.pdf</a></span>   <span><a href='https://github.com/linh-gist/mv-glmb-ab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Van Ma, Tran Thien Dat Nguyen, Changbeom Shim, Du Yong Kim, Namkoo Ha, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08872">Visual Multi-Object Tracking with Re-Identification and Occlusion Handling using Labeled Random Finite Sets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes an online visual multi-object tracking (MOT) algorithm that resolves object appearance-reappearance and occlusion. Our solution is based on the labeled random finite set (LRFS) filtering approach, which in principle, addresses disappearance, appearance, reappearance, and occlusion via a single Bayesian recursion. However, in practice, existing numerical approximations cause reappearing objects to be initialized as new tracks, especially after long periods of being undetected. In occlusion handling, the filter's efficacy is dictated by trade-offs between the sophistication of the occlusion model and computational demand. Our contribution is a novel modeling method that exploits object features to address reappearing objects whilst maintaining a linear complexity in the number of detections. Moreover, to improve the filter's occlusion handling, we propose a fuzzy detection model that takes into consideration the overlapping areas between tracks and their sizes. We also develop a fast version of the filter to further reduce the computational time. The source code is publicly available at https://github.com/linh-gist/mv-glmb-ab.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2407.05383.pdf' target='_blank'>https://arxiv.org/pdf/2407.05383.pdf</a></span>   <span><a href='https://github.com/wuyou3474/BDTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>You Wu, Xucheng Wang, Dan Zeng, Hengzhou Ye, Xiaolan Xie, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05383">Learning Motion Blur Robust Vision Transformers for Real-Time UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicle (UAV) tracking is critical for applications like surveillance, search-and-rescue, and autonomous navigation. However, the high-speed movement of UAVs and targets introduces unique challenges, including real-time processing demands and severe motion blur, which degrade the performance of existing generic trackers. While single-stream vision transformer (ViT) architectures have shown promise in visual tracking, their computational inefficiency and lack of UAV-specific optimizations limit their practicality in this domain. In this paper, we boost the efficiency of this framework by tailoring it into an adaptive computation framework that dynamically exits Transformer blocks for real-time UAV tracking. The motivation behind this is that tracking tasks with fewer challenges can be adequately addressed using low-level feature representations. Simpler tasks can often be handled with less demanding, lower-level features. This approach allows the model use computational resources more efficiently by focusing on complex tasks and conserving resources for easier ones. Another significant enhancement introduced in this paper is the improved effectiveness of ViTs in handling motion blur, a common issue in UAV tracking caused by the fast movements of either the UAV, the tracked objects, or both. This is achieved by acquiring motion blur robust representations through enforcing invariance in the feature representation of the target with respect to simulated motion blur. We refer to our proposed approach as BDTrack. Extensive experiments conducted on four tracking benchmarks validate the effectiveness and versatility of our approach, demonstrating its potential as a practical and effective approach for real-time UAV tracking. Code is released at: https://github.com/wuyou3474/BDTrack.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2407.05238.pdf' target='_blank'>https://arxiv.org/pdf/2407.05238.pdf</a></span>   <span><a href='https://github.com/haooozi/P2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Nie, Fei Xie, Sifan Zhou, Xueyi Zhou, Dong-Kyu Chae, Zhiwei He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05238">P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) methods based on appearance matching has long suffered from insufficient appearance information incurred by incomplete, textureless and semantically deficient LiDAR point clouds. While motion paradigm exploits motion cues instead of appearance matching for tracking, it incurs complex multi-stage processing and segmentation module. In this paper, we first provide in-depth explorations on motion paradigm, which proves that (\textbf{i}) it is feasible to directly infer target relative motion from point clouds across consecutive frames; (\textbf{ii}) fine-grained information comparison between consecutive point clouds facilitates target motion modeling. We thereby propose to perform part-to-part motion modeling for consecutive point clouds and introduce a novel tracking framework, termed \textbf{P2P}. The novel framework fuses each corresponding part information between consecutive point clouds, effectively exploring detailed information changes and thus modeling accurate target-related motion cues. Following this framework, we present P2P-point and P2P-voxel models, incorporating implicit and explicit part-to-part motion modeling by point- and voxel-based representation, respectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art performance ($\sim$\textbf{89\%}, \textbf{72\%} and \textbf{63\%} precision on KITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same point-based representation, P2P-point outperforms the previous motion tracker M$^2$Track by \textbf{3.3\%} and \textbf{6.7\%} on the KITTI and NuScenes, while running at a considerably high speed of \textbf{107 Fps} on a single RTX3090 GPU. The source code and pre-trained models are available at https://github.com/haooozi/P2P.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2407.05235.pdf' target='_blank'>https://arxiv.org/pdf/2407.05235.pdf</a></span>   <span><a href='https://github.com/OpenCodeGithub/HIP-HaTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Guo, Pengzhi Zhong, Lizhi Lin, Hao Zhang, Ling Huang, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05235">Tracking Reflected Objects: A Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual tracking has advanced significantly in recent years, mainly due to the availability of large-scale training datasets. These datasets have enabled the development of numerous algorithms that can track objects with high accuracy and robustness.However, the majority of current research has been directed towards tracking generic objects, with less emphasis on more specialized and challenging scenarios. One such challenging scenario involves tracking reflected objects. Reflections can significantly distort the appearance of objects, creating ambiguous visual cues that complicate the tracking process. This issue is particularly pertinent in applications such as autonomous driving, security, smart homes, and industrial production, where accurately tracking objects reflected in surfaces like mirrors or glass is crucial. To address this gap, we introduce TRO, a benchmark specifically for Tracking Reflected Objects. TRO includes 200 sequences with around 70,000 frames, each carefully annotated with bounding boxes. This dataset aims to encourage the development of new, accurate methods for tracking reflected objects, which present unique challenges not sufficiently covered by existing benchmarks. We evaluated 20 state-of-the-art trackers and found that they struggle with the complexities of reflections. To provide a stronger baseline, we propose a new tracker, HiP-HaTrack, which uses hierarchical features to improve performance, significantly outperforming existing algorithms. We believe our benchmark, evaluation, and HiP-HaTrack will inspire further research and applications in tracking reflected objects. The TRO and code are available at https://github.com/OpenCodeGithub/HIP-HaTrack.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2406.16837.pdf' target='_blank'>https://arxiv.org/pdf/2406.16837.pdf</a></span>   <span><a href='https://github.com/MIT-SPARK/certifiable_tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Shaikewitz, Samuel Ubellacker, Luca Carlone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16837">A Certifiable Algorithm for Simultaneous Shape Estimation and Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Applications from manipulation to autonomous vehicles rely on robust and general object tracking to safely perform tasks in dynamic environments. We propose the first certifiably optimal category-level approach for simultaneous shape estimation and pose tracking of an object of known category (e.g. a car). Our approach uses 3D semantic keypoint measurements extracted from an RGB-D image sequence, and phrases the estimation as a fixed-lag smoothing problem. Temporal constraints enforce the object's rigidity (fixed shape) and smooth motion according to a constant-twist motion model. The solutions to this problem are the estimates of the object's state (poses, velocities) and shape (paramaterized according to the active shape model) over the smoothing horizon. Our key contribution is to show that despite the non-convexity of the fixed-lag smoothing problem, we can solve it to certifiable optimality using a small-size semidefinite relaxation. We also present a fast outlier rejection scheme that filters out incorrect keypoint detections with shape and time compatibility tests, and wrap our certifiable solver in a graduated non-convexity scheme. We evaluate the proposed approach on synthetic and real data, showcasing its performance in a table-top manipulation scenario and a drone-based vehicle tracking application.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2406.13271.pdf' target='_blank'>https://arxiv.org/pdf/2406.13271.pdf</a></span>   <span><a href='https://github.com/dyhBUPT/HIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Du, Zhicheng Zhao, Fei Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13271">Hierarchical IoU Tracking based on Interval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) aims to detect and associate all targets of given classes across frames. Current dominant solutions, e.g. ByteTrack and StrongSORT++, follow the hybrid pipeline, which first accomplish most of the associations in an online manner, and then refine the results using offline tricks such as interpolation and global link. While this paradigm offers flexibility in application, the disjoint design between the two stages results in suboptimal performance. In this paper, we propose the Hierarchical IoU Tracking framework, dubbed HIT, which achieves unified hierarchical tracking by utilizing tracklet intervals as priors. To ensure the conciseness, only IoU is utilized for association, while discarding the heavy appearance models, tricky auxiliary cues, and learning-based association modules. We further identify three inconsistency issues regarding target size, camera movement and hierarchical cues, and design corresponding solutions to guarantee the reliability of associations. Though its simplicity, our method achieves promising performance on four datasets, i.e., MOT17, KITTI, DanceTrack and VisDrone, providing a strong baseline for future tracking method design. Moreover, we experiment on seven trackers and prove that HIT can be seamlessly integrated with other solutions, whether they are motion-based, appearance-based or learning-based. Our codes will be released at https://github.com/dyhBUPT/HIT.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2406.09598.pdf' target='_blank'>https://arxiv.org/pdf/2406.09598.pdf</a></span>   <span><a href='https://facebookresearch.github.io/hot3d/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Fan Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09598">Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB/monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects. In addition to simple pick-up/observe/put-down actions, HOT3D contains scenarios resembling typical actions in a kitchen, office, and living room environment. The dataset is recorded by two head-mounted devices from Meta: Project Aria, a research prototype of light-weight AR/AI glasses, and Quest 3, a production VR headset sold in millions of units. Ground-truth poses were obtained by a professional motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. We aim to accelerate research on egocentric hand-object interaction by making the HOT3D dataset publicly available and by co-organizing public challenges on the dataset at ECCV 2024. The dataset can be downloaded from the project website: https://facebookresearch.github.io/hot3d/.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2406.08324.pdf' target='_blank'>https://arxiv.org/pdf/2406.08324.pdf</a></span>   <span><a href='https://github.com/Nathan-Li123/LaMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Xiaoqiong Liu, Luke Liu, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08324">LaMOT: Language-Guided Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language MOT is a crucial tracking problem and has drawn increasing attention recently. It aims to track objects based on human language commands, replacing the traditional use of templates or pre-set information from training sets in conventional tracking tasks. Despite various efforts, a key challenge lies in the lack of a clear understanding of why language is used for tracking, which hinders further development in this field. In this paper, we address this challenge by introducing Language-Guided MOT, a unified task framework, along with a corresponding large-scale benchmark, termed LaMOT, which encompasses diverse scenarios and language descriptions. Specially, LaMOT comprises 1,660 sequences from 4 different datasets and aims to unify various Vision-Language MOT tasks while providing a standardized evaluation platform. To ensure high-quality annotations, we manually assign appropriate descriptive texts to each target in every video and conduct careful inspection and correction. To the best of our knowledge, LaMOT is the first benchmark dedicated to Language-Guided MOT. Additionally, we propose a simple yet effective tracker, termed LaMOTer. By establishing a unified task framework, providing challenging benchmarks, and offering insights for future algorithm design and evaluation, we expect to contribute to the advancement of research in Vision-Language MOT. We will release the data at https://github.com/Nathan-Li123/LaMOT.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2406.08037.pdf' target='_blank'>https://arxiv.org/pdf/2406.08037.pdf</a></span>   <span><a href='https://github.com/xyyang317/ABTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyang Yang, Dan Zeng, Xucheng Wang, You Wu, Hengzhou Ye, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08037">Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we introduce a novel ViT pruning method to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: https://github.com/xyyang317/ABTrack.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2406.05039.pdf' target='_blank'>https://arxiv.org/pdf/2406.05039.pdf</a></span>   <span><a href='https://github.com/zyn213/TempRMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yani Zhang, Dongming Wu, Wencheng Han, Xingping Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05039">Bootstrapping Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) aims at detecting and tracking multiple objects following human instruction represented by a natural language expression. Existing RMOT benchmarks are usually formulated through manual annotations, integrated with static regulations. This approach results in a dearth of notable diversity and a constrained scope of implementation. In this work, our key idea is to bootstrap the task of referring multi-object tracking by introducing discriminative language words as much as possible. In specific, we first develop Refer-KITTI into a large-scale dataset, named Refer-KITTI-V2. It starts with 2,719 manual annotations, addressing the issue of class imbalance and introducing more keywords to make it closer to real-world scenarios compared to Refer-KITTI. They are further expanded to a total of 9,758 annotations by prompting large language models, which create 617 different words, surpassing previous RMOT benchmarks. In addition, the end-to-end framework in RMOT is also bootstrapped by a simple yet elegant temporal advancement strategy, which achieves better performance than previous approaches. The source code and dataset is available at https://github.com/zyn213/TempRMOT.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2406.04844.pdf' target='_blank'>https://arxiv.org/pdf/2406.04844.pdf</a></span>   <span><a href='https://github.com/WesLee88524/LG-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Li, Jiale Cao, Muzammal Naseer, Yu Zhu, Jinqiu Sun, Yanning Zhang, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04844">Multi-Granularity Language-Guided Training for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing multi-object tracking methods typically learn visual tracking features via maximizing dis-similarities of different instances and minimizing similarities of the same instance. While such a feature learning scheme achieves promising performance, learning discriminative features solely based on visual information is challenging especially in case of environmental interference such as occlusion, blur and domain variance. In this work, we argue that multi-modal language-driven features provide complementary information to classical visual features, thereby aiding in improving the robustness to such environmental interference. To this end, we propose a new multi-object tracking framework, named LG-MOT, that explicitly leverages language information at different levels of granularity (scene-and instance-level) and combines it with standard visual features to obtain discriminative representations. To develop LG-MOT, we annotate existing MOT datasets with scene-and instance-level language descriptions. We then encode both instance-and scene-level language information into high-dimensional embeddings, which are utilized to guide the visual features during training. At inference, our LG-MOT uses the standard visual features without relying on annotated language descriptions. Extensive experiments on three benchmarks, MOT17, DanceTrack and SportsMOT, reveal the merits of the proposed contributions leading to state-of-the-art performance. On the DanceTrack test set, our LG-MOT achieves an absolute gain of 2.2\% in terms of target object association (IDF1 score), compared to the baseline using only visual features. Further, our LG-MOT exhibits strong cross-domain generalizability. The dataset and code will be available at https://github.com/WesLee88524/LG-MOT.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2406.04221.pdf' target='_blank'>https://arxiv.org/pdf/2406.04221.pdf</a></span>   <span><a href='https://github.com/siyuanliii/masa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04221">Matching Anything by Segmenting Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT). Current methods predominantly rely on labeled domain-specific video datasets, which limits the cross-domain generalization of learned similarity embeddings. We propose MASA, a novel method for robust instance association learning, capable of matching any objects within videos across diverse domains without tracking labels. Leveraging the rich object segmentation from the Segment Anything Model (SAM), MASA learns instance-level correspondence through exhaustive data transformations. We treat the SAM outputs as dense object region proposals and learn to match those regions from a vast image collection. We further design a universal MASA adapter which can work in tandem with foundational segmentation or detection models and enable them to track any detected objects. Those combinations present strong zero-shot tracking ability in complex domains. Extensive tests on multiple challenging MOT and MOTS benchmarks indicate that the proposed method, using only unlabeled static images, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain video sequences, in zero-shot association. Project Page: https://matchinganything.github.io/
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2406.01765.pdf' target='_blank'>https://arxiv.org/pdf/2406.01765.pdf</a></span>   <span><a href='https://github.com/fatemehN/ReproducibilityStudy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Nourilenjan Nokabadi, Jean-FranÃ§ois Lalonde, Christian GagnÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01765">Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>New transformer networks have been integrated into object tracking pipelines and have demonstrated strong performance on the latest benchmarks. This paper focuses on understanding how transformer trackers behave under adversarial attacks and how different attacks perform on tracking datasets as their parameters change. We conducted a series of experiments to evaluate the effectiveness of existing adversarial attacks on object trackers with transformer and non-transformer backbones. We experimented on 7 different trackers, including 3 that are transformer-based, and 4 which leverage other architectures. These trackers are tested against 4 recent attack methods to assess their performance and robustness on VOT2022ST, UAV123 and GOT10k datasets. Our empirical study focuses on evaluating adversarial robustness of object trackers based on bounding box versus binary mask predictions, and attack methods at different levels of perturbations. Interestingly, our study found that altering the perturbation level may not significantly affect the overall object tracking results after the attack. Similarly, the sparsity and imperceptibility of the attack perturbations may remain stable against perturbation level shifts. By applying a specific attack on all transformer trackers, we show that new transformer trackers having a stronger cross-attention modeling achieve a greater adversarial robustness on tracking datasets, such as VOT2022ST and GOT10k. Our results also indicate the necessity for new attack methods to effectively tackle the latest types of transformer trackers. The codes necessary to reproduce this study are available at https://github.com/fatemehN/ReproducibilityStudy.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2406.00429.pdf' target='_blank'>https://arxiv.org/pdf/2406.00429.pdf</a></span>   <span><a href='https://github.com/qinzheng2000/GeneralTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Le Wang, Sanping Zhou, Panpan Fu, Gang Hua, Wei Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00429">Towards Generalizable Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking MOT encompasses various tracking scenarios, each characterized by unique traits. Effective trackers should demonstrate a high degree of generalizability across diverse scenarios. However, existing trackers struggle to accommodate all aspects or necessitate hypothesis and experimentation to customize the association information motion and or appearance for a given scenario, leading to narrowly tailored solutions with limited generalizability. In this paper, we investigate the factors that influence trackers generalization to different scenarios and concretize them into a set of tracking scenario attributes to guide the design of more generalizable trackers. Furthermore, we propose a point-wise to instance-wise relation framework for MOT, i.e., GeneralTrack, which can generalize across diverse scenarios while eliminating the need to balance motion and appearance. Thanks to its superior generalizability, our proposed GeneralTrack achieves state-of-the-art performance on multiple benchmarks and demonstrates the potential for domain generalization. https://github.com/qinzheng2000/GeneralTrack.git
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2405.19818.pdf' target='_blank'>https://arxiv.org/pdf/2405.19818.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Guanjie Huang, Hao Wen, Xi Zhou, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19818">WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater object tracking (UOT) is a foundational task for identifying and tracing submerged entities in underwater video sequences. However, current UOT datasets suffer from limitations in scale, diversity of target categories and scenarios covered, hindering the training and evaluation of modern tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, \ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \eg, UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \eg, underwater vision-language tracking. Most existing trackers are tailored for open-air environments, leading to performance degradation when applied to UOT due to domain gaps. Retraining and fine-tuning these trackers are challenging due to sample imbalances and limited real-world underwater datasets. To tackle these challenges, we propose a novel omni-knowledge distillation framework based on WebUOT-1M, incorporating various strategies to guide the learning of the student Transformer. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. Furthermore, we comprehensively evaluate WebUOT-1M using 30 deep trackers, showcasing its value as a benchmark for UOT research by presenting new challenges and opportunities for future studies. The complete dataset, codes and tracking results, will be made publicly available.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2405.19321.pdf' target='_blank'>https://arxiv.org/pdf/2405.19321.pdf</a></span>   <span><a href='https://isaaclabe.github.io/DGD-Website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Labe, Noam Issachar, Itai Lang, Sagie Benaim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19321">DGD: Dynamic 3D Gaussians Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input. Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics. This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt. To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation. Our representation is optimized over time with both color and semantic information. Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene. We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes. Our project webpage is available on https://isaaclabe.github.io/DGD-Website/
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2405.18606.pdf' target='_blank'>https://arxiv.org/pdf/2405.18606.pdf</a></span>   <span><a href='https://github.com/linh-gist/mv-glmb-ab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Van Ma, Tran Thien Dat Nguyen, Ba-Ngu Vo, Hyunsung Jang, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18606">Track Initialization and Re-Identification for~3D Multi-View Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a 3D multi-object tracking (MOT) solution using only 2D detections from monocular cameras, which automatically initiates/terminates tracks as well as resolves track appearance-reappearance and occlusions. Moreover, this approach does not require detector retraining when cameras are reconfigured but only the camera matrices of reconfigured cameras need to be updated. Our approach is based on a Bayesian multi-object formulation that integrates track initiation/termination, re-identification, occlusion handling, and data association into a single Bayes filtering recursion. However, the exact filter that utilizes all these functionalities is numerically intractable due to the exponentially growing number of terms in the (multi-object) filtering density, while existing approximations trade-off some of these functionalities for speed. To this end, we develop a more efficient approximation suitable for online MOT by incorporating object features and kinematics into the measurement model, which improves data association and subsequently reduces the number of terms. Specifically, we exploit the 2D detections and extracted features from multiple cameras to provide a better approximation of the multi-object filtering density to realize the track initiation/termination and re-identification functionalities. Further, incorporating a tractable geometric occlusion model based on 2D projections of 3D objects on the camera planes realizes the occlusion handling functionality of the filter. Evaluation of the proposed solution on challenging datasets demonstrates significant improvements and robustness when camera configurations change on-the-fly, compared to existing multi-view MOT solutions. The source code is publicly available at https://github.com/linh-gist/mv-glmb-ab.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2405.17773.pdf' target='_blank'>https://arxiv.org/pdf/2405.17773.pdf</a></span>   <span><a href='https://github.com/supertyd/XTrack/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuedong Tan, Zongwei Wu, Yuqian Fu, Zhuyun Zhou, Guolei Sun, Eduard Zamfi, Chao Ma, Danda Pani Paudel, Luc Van Gool, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17773">XTrack: Multimodal Training Boosts RGB-X Video Object Trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal sensing has proven valuable for visual tracking, as different sensor types offer unique strengths in handling one specific challenging scene where object appearance varies. While a generalist model capable of leveraging all modalities would be ideal, development is hindered by data sparsity, typically in practice, only one modality is available at a time. Therefore, it is crucial to ensure and achieve that knowledge gained from multimodal sensing -- such as identifying relevant features and regions -- is effectively shared, even when certain modalities are unavailable at inference. We venture with a simple assumption: similar samples across different modalities have more knowledge to share than otherwise. To implement this, we employ a ``weak" classifier tasked with distinguishing between modalities. More specifically, if the classifier ``fails" to accurately identify the modality of the given sample, this signals an opportunity for cross-modal knowledge sharing. Intuitively, knowledge transfer is facilitated whenever a sample from one modality is sufficiently close and aligned with another. Technically, we achieve this by routing samples from one modality to the expert of the others, within a mixture-of-experts framework designed for multimodal video object tracking. During the inference, the expert of the respective modality is chosen, which we show to benefit from the multimodal knowledge available during training, thanks to the proposed method. Through the exhaustive experiments that use only paired RGB-E, RGB-D, and RGB-T during training, we showcase the benefit of the proposed method for RGB-X tracker during inference, with an average +3\% precision improvement over the current SOTA. Our source code is publicly available at https://github.com/supertyd/XTrack/tree/main.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2405.15700.pdf' target='_blank'>https://arxiv.org/pdf/2405.15700.pdf</a></span>   <span><a href='https://github.com/weigertlab/trackastra' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Gallusser, Martin Weigert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15700">Trackastra: Transformer-based cell tracking for live-cell microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell tracking is a ubiquitous image analysis task in live-cell microscopy. Unlike multiple object tracking (MOT) for natural images, cell tracking typically involves hundreds of similar-looking objects that can divide in each frame, making it a particularly challenging problem. Current state-of-the-art approaches follow the tracking-by-detection paradigm, i.e. first all cells are detected per frame and successively linked in a second step to form biologically consistent cell tracks. Linking is commonly solved via discrete optimization methods, which require manual tuning of hyperparameters for each dataset and are therefore cumbersome to use in practice. Here we propose Trackastra, a general purpose cell tracking approach that uses a simple transformer architecture to directly learn pairwise associations of cells within a temporal window from annotated data. Importantly, unlike existing transformer-based MOT pipelines, our learning architecture also accounts for dividing objects such as cells and allows for accurate tracking even with simple greedy linking, thus making strides towards removing the requirement for a complex linking step. The proposed architecture operates on the full spatio-temporal context of detections within a time window by avoiding the computational burden of processing dense images. We show that our tracking approach performs on par with or better than highly tuned state-of-the-art cell tracking algorithms for various biological datasets, such as bacteria, cell cultures and fluorescent particles. We provide code at https://github.com/weigertlab/trackastra.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2405.14200.pdf' target='_blank'>https://arxiv.org/pdf/2405.14200.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14200">Awesome Multi-modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal object tracking (MMOT) is an emerging field that combines data from various modalities, \eg vision (RGB), depth, thermal infrared, event, language and audio, to estimate the state of an arbitrary object in a video sequence. It is of great significance for many applications such as autonomous driving and intelligent surveillance. In recent years, MMOT has received more and more attention. However, existing MMOT algorithms mainly focus on two modalities (\eg RGB+depth, RGB+thermal infrared, and RGB+language). To leverage more modalities, some recent efforts have been made to learn a unified visual object tracking model for any modality. Additionally, some large-scale multi-modal tracking benchmarks have been established by simultaneously providing more than two modalities, such as vision-language-audio (\eg WebUAV-3M) and vision-depth-language (\eg UniMod1K). To track the latest progress in MMOT, we conduct a comprehensive investigation in this report. Specifically, we first divide existing MMOT tasks into five main categories, \ie RGBL tracking, RGBE tracking, RGBD tracking, RGBT tracking, and miscellaneous (RGB+X), where X can be any modality, such as language, depth, and event. Then, we analyze and summarize each MMOT task, focusing on widely used datasets and mainstream tracking algorithms based on their technical paradigms (\eg self-supervised learning, prompt learning, knowledge distillation, generative models, and state space models). Finally, we maintain a continuously updated paper list for MMOT at https://github.com/983632847/Awesome-Multimodal-Object-Tracking.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2405.14119.pdf' target='_blank'>https://arxiv.org/pdf/2405.14119.pdf</a></span>   <span><a href='https://github.com/chongweiliu/PuTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongwei Liu, Haojie Li, Zhihui Wang, Rui Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14119">Is a Pure Transformer Effective for Separated and Online Multi-Object Tracking?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Multi-Object Tracking (MOT) have demonstrated significant success in short-term association within the separated tracking-by-detection online paradigm. However, long-term tracking remains challenging. While graph-based approaches address this by modeling trajectories as global graphs, these methods are unsuitable for real-time applications due to their non-online nature. In this paper, we review the concept of trajectory graphs and propose a novel perspective by representing them as directed acyclic graphs. This representation can be described using frame-ordered object sequences and binary adjacency matrices. We observe that this structure naturally aligns with Transformer attention mechanisms, enabling us to model the association problem using a classic Transformer architecture. Based on this insight, we introduce a concise Pure Transformer (PuTR) to validate the effectiveness of Transformer in unifying short- and long-term tracking for separated online MOT. Extensive experiments on four diverse datasets (SportsMOT, DanceTrack, MOT17, and MOT20) demonstrate that PuTR effectively establishes a solid baseline compared to existing foundational online methods while exhibiting superior domain adaptation capabilities. Furthermore, the separated nature enables efficient training and inference, making it suitable for practical applications. Implementation code and trained models are available at https://github.com/chongweiliu/PuTR .
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2405.05210.pdf' target='_blank'>https://arxiv.org/pdf/2405.05210.pdf</a></span>   <span><a href='https://github.com/mit-acl/tcaff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mason B. Peterson, Parker C. Lusk, Antonio Avila, Jonathan P. How
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05210">TCAFF: Temporal Consistency for Robot Frame Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of collaborative robotics, the ability to communicate spatial information like planned trajectories and shared environment information is crucial. When no global position information is available (e.g., indoor or GPS-denied environments), agents must align their coordinate frames before shared spatial information can be properly expressed and interpreted. Coordinate frame alignment is particularly difficult when robots have no initial alignment and are affected by odometry drift. To this end, we develop a novel multiple hypothesis algorithm, called TCAFF, for aligning the coordinate frames of neighboring robots. TCAFF considers potential alignments from associating sparse open-set object maps and leverages temporal consistency to determine an initial alignment and correct for drift, all without any initial knowledge of neighboring robot poses. We demonstrate TCAFF being used for frame alignment in a collaborative object tracking application on a team of four robots tracking six pedestrians and show that TCAFF enables robots to achieve a tracking accuracy similar to that of a system with ground truth localization. The code and hardware dataset are available at https://github.com/mit-acl/tcaff.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2405.05004.pdf' target='_blank'>https://arxiv.org/pdf/2405.05004.pdf</a></span>   <span><a href='https://github.com/SSSpc333/TENet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Shao, Tianyang Xu, Zhangyong Tang, Linze Li, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05004">TENet: Targetness Entanglement Incorporating with Multi-Scale Pooling and Mutually-Guided Fusion for RGB-E Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is currently strong interest in improving visual object tracking by augmenting the RGB modality with the output of a visual event camera that is particularly informative about the scene motion. However, existing approaches perform event feature extraction for RGB-E tracking using traditional appearance models, which have been optimised for RGB only tracking, without adapting it for the intrinsic characteristics of the event data. To address this problem, we propose an Event backbone (Pooler), designed to obtain a high-quality feature representation that is cognisant of the innate characteristics of the event data, namely its sparsity. In particular, Multi-Scale Pooling is introduced to capture all the motion feature trends within event data through the utilisation of diverse pooling kernel sizes. The association between the derived RGB and event representations is established by an innovative module performing adaptive Mutually Guided Fusion (MGF). Extensive experimental results show that our method significantly outperforms state-of-the-art trackers on two widely used RGB-E tracking datasets, including VisEvent and COESOT, where the precision and success rates on COESOT are improved by 4.9% and 5.2%, respectively. Our code will be available at https://github.com/SSSpc333/TENet.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2404.13868.pdf' target='_blank'>https://arxiv.org/pdf/2404.13868.pdf</a></span>   <span><a href='https://atomscott.github.io/TeamTrack/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Atom Scott, Ikuma Uchida, Ning Ding, Rikuhei Umemoto, Rory Bunker, Ren Kobayashi, Takeshi Koyama, Masaki Onishi, Yoshinari Kameda, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13868">TeamTrack: A Dataset for Multi-Sport Multi-Object Tracking in Full-pitch Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a critical and challenging task in computer vision, particularly in situations involving objects with similar appearances but diverse movements, as seen in team sports. Current methods, largely reliant on object detection and appearance, often fail to track targets in such complex scenarios accurately. This limitation is further exacerbated by the lack of comprehensive and diverse datasets covering the full view of sports pitches. Addressing these issues, we introduce TeamTrack, a pioneering benchmark dataset specifically designed for MOT in sports. TeamTrack is an extensive collection of full-pitch video data from various sports, including soccer, basketball, and handball. Furthermore, we perform a comprehensive analysis and benchmarking effort to underscore TeamTrack's utility and potential impact. Our work signifies a crucial step forward, promising to elevate the precision and effectiveness of MOT in complex, dynamic settings such as team sports. The dataset, project code and competition is released at: https://atomscott.github.io/TeamTrack/.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2404.07977.pdf' target='_blank'>https://arxiv.org/pdf/2404.07977.pdf</a></span>   <span><a href='https://weijielyu.github.io/Gaga' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07977">Gaga: Group Any Gaussians via 3D-aware Memory Bank</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Gaga, a framework that reconstructs and segments open-world 3D scenes by leveraging inconsistent 2D masks predicted by zero-shot class-agnostic segmentation models. Contrasted to prior 3D scene segmentation approaches that rely on video object tracking or contrastive learning methods, Gaga utilizes spatial information and effectively associates object masks across diverse camera poses through a novel 3D-aware memory bank. By eliminating the assumption of continuous view changes in training images, Gaga demonstrates robustness to variations in camera poses, particularly beneficial for sparsely sampled images, ensuring precise mask label consistency. Furthermore, Gaga accommodates 2D segmentation masks from diverse sources and demonstrates robust performance with different open-world zero-shot class-agnostic segmentation models, significantly enhancing its versatility. Extensive qualitative and quantitative evaluations demonstrate that Gaga performs favorably against state-of-the-art methods, emphasizing its potential for real-world applications such as 3D scene understanding and manipulation.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2404.07553.pdf' target='_blank'>https://arxiv.org/pdf/2404.07553.pdf</a></span>   <span><a href='https://github.com/gitmehrdad/SFSORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>M. M. Morsali, Z. Sharifi, F. Fallah, S. Hashembeiki, H. Mohammadzade, S. Bagheri Shouraki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07553">SFSORT: Scene Features-based Simple Online Real-Time Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces SFSORT, the world's fastest multi-object tracking system based on experiments conducted on MOT Challenge datasets. To achieve an accurate and computationally efficient tracker, this paper employs a tracking-by-detection method, following the online real-time tracking approach established in prior literature. By introducing a novel cost function called the Bounding Box Similarity Index, this work eliminates the Kalman Filter, leading to reduced computational requirements. Additionally, this paper demonstrates the impact of scene features on enhancing object-track association and improving track post-processing. Using a 2.2 GHz Intel Xeon CPU, the proposed method achieves an HOTA of 61.7\% with a processing speed of 2242 Hz on the MOT17 dataset and an HOTA of 60.9\% with a processing speed of 304 Hz on the MOT20 dataset. The tracker's source code, fine-tuned object detection model, and tutorials are available at \url{https://github.com/gitmehrdad/SFSORT}.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2404.05518.pdf' target='_blank'>https://arxiv.org/pdf/2404.05518.pdf</a></span>   <span><a href='https://github.com/JackWoo0831/DepthMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiapeng Wu, Yichen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05518">DepthMOT: Depth Cues Lead to a Strong Multi-Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately distinguishing each object is a fundamental goal of Multi-object tracking (MOT) algorithms. However, achieving this goal still remains challenging, primarily due to: (i) For crowded scenes with occluded objects, the high overlap of object bounding boxes leads to confusion among closely located objects. Nevertheless, humans naturally perceive the depth of elements in a scene when observing 2D videos. Inspired by this, even though the bounding boxes of objects are close on the camera plane, we can differentiate them in the depth dimension, thereby establishing a 3D perception of the objects. (ii) For videos with rapidly irregular camera motion, abrupt changes in object positions can result in ID switches. However, if the camera pose are known, we can compensate for the errors in linear motion models. In this paper, we propose \textit{DepthMOT}, which achieves: (i) detecting and estimating scene depth map \textit{end-to-end}, (ii) compensating the irregular camera motion by camera pose estimation. Extensive experiments demonstrate the superior performance of DepthMOT in VisDrone-MOT and UAVDT datasets. The code will be available at \url{https://github.com/JackWoo0831/DepthMOT}.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2404.03110.pdf' target='_blank'>https://arxiv.org/pdf/2404.03110.pdf</a></span>   <span><a href='https://github.com/noyzzz/EMAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Mahdian, Mohammad Jani, Amir M. Soufi Enayati, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03110">Ego-Motion Aware Target Prediction Module for Robust Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories. Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target. Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model. These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections. Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories. In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models. Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter. This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model. We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT. In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively. At the same time, it elevates other performance metrics such as HOTA by more than 5%. Our source code is available at https://github.com/noyzzz/EMAP.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2403.17935.pdf' target='_blank'>https://arxiv.org/pdf/2403.17935.pdf</a></span>   <span><a href='https://github.com/wangjk666/OmniVid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17935">OmniVid: A Generative Framework for Universal Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution. Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats. In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as GPT-3, with extensive training corpora. Inspired by this, we seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens. In this way, a variety of video tasks could be formulated as video-grounded token generation. This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video question answering, and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework. Through comprehensive experiments, we demonstrate such a simple and straightforward idea is quite effective and can achieve state-of-the-art or competitive results on seven video benchmarks, providing a novel perspective for more universal video understanding. Code is available at https://github.com/wangjk666/OmniVid.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2403.16848.pdf' target='_blank'>https://arxiv.org/pdf/2403.16848.pdf</a></span>   <span><a href='https://github.com/MCG-NJU/MOTIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruopeng Gao, Ji Qi, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16848">Multiple Object Tracking as ID Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) has been a long-standing challenge in video understanding. A natural and intuitive approach is to split this task into two parts: object detection and association. Most mainstream methods employ meticulously crafted heuristic techniques to maintain trajectory information and compute cost matrices for object matching. Although these methods can achieve notable tracking performance, they often require a series of elaborate handcrafted modifications while facing complicated scenarios. We believe that manually assumed priors limit the method's adaptability and flexibility in learning optimal tracking capabilities from domain-specific data. Therefore, we introduce a new perspective that treats Multiple Object Tracking as an in-context ID Prediction task, transforming the aforementioned object association into an end-to-end trainable task. Based on this, we propose a simple yet effective method termed MOTIP. Given a set of trajectories carried with ID information, MOTIP directly decodes the ID labels for current detections to accomplish the association process. Without using tailored or sophisticated architectures, our method achieves state-of-the-art results across multiple benchmarks by solely leveraging object-level features as tracking cues. The simplicity and impressive results of MOTIP leave substantial room for future advancements, thereby making it a promising baseline for subsequent research. Our code and checkpoints are released at https://github.com/MCG-NJU/MOTIP.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2403.16558.pdf' target='_blank'>https://arxiv.org/pdf/2403.16558.pdf</a></span>   <span><a href='https://github.com/Hon-Wong/Elysium' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, Can Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16558">Elysium: Exploring Object-level Perception in Videos via MLLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied. This lack of exploration is primarily due to two key challenges. Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships. Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden. To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset supported for three tasks: Single Object Tracking (SOT), Referring Single Object Tracking (RSOT), and Video Referring Expression Generation (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions. Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge. Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that attempts to conduct object-level tasks in videos without requiring any additional plug-in or expert models. All codes and datasets are available at https://github.com/Hon-Wong/Elysium.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2403.16002.pdf' target='_blank'>https://arxiv.org/pdf/2403.16002.pdf</a></span>   <span><a href='https://github.com/hoqolo/SDSTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojun Hou, Jiazheng Xing, Yijie Qian, Yaowei Guo, Shuo Xin, Junhao Chen, Kai Tang, Mengmeng Wang, Zhengkai Jiang, Liang Liu, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16002">SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Visual Object Tracking (VOT) has recently gained significant attention due to its robustness. Early research focused on fully fine-tuning RGB-based trackers, which was inefficient and lacked generalized representation due to the scarcity of multimodal data. Therefore, recent studies have utilized prompt tuning to transfer pre-trained RGB-based trackers to multimodal data. However, the modality gap limits pre-trained knowledge recall, and the dominance of the RGB modality persists, preventing the full utilization of information from other modalities. To address these issues, we propose a novel symmetric multimodal tracking framework called SDSTrack. We introduce lightweight adaptation for efficient fine-tuning, which directly transfers the feature extraction ability from RGB to other domains with a small number of trainable parameters and integrates multimodal features in a balanced, symmetric manner. Furthermore, we design a complementary masked patch distillation strategy to enhance the robustness of trackers in complex environments, such as extreme weather, poor imaging, and sensor failure. Extensive experiments demonstrate that SDSTrack outperforms state-of-the-art methods in various multimodal tracking scenarios, including RGB+Depth, RGB+Thermal, and RGB+Event tracking, and exhibits impressive results in extreme conditions. Our source code is available at https://github.com/hoqolo/SDSTrack.
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2403.15712.pdf' target='_blank'>https://arxiv.org/pdf/2403.15712.pdf</a></span>   <span><a href='https://github.com/PholyPeng/PNAS-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chensheng Peng, Zhaoyu Zeng, Jinling Gao, Jundong Zhou, Masayoshi Tomizuka, Xinbing Wang, Chenghu Zhou, Nanyang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15712">PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking is a critical task in autonomous driving. Existing works primarily focus on the heuristic design of neural networks to obtain high accuracy. As tracking accuracy improves, however, neural networks become increasingly complex, posing challenges for their practical application in real driving scenarios due to the high level of latency. In this paper, we explore the use of the neural architecture search (NAS) methods to search for efficient architectures for tracking, aiming for low real-time latency while maintaining relatively high accuracy. Another challenge for object tracking is the unreliability of a single sensor, therefore, we propose a multi-modal framework to improve the robustness. Experiments demonstrate that our algorithm can run on edge devices within lower latency constraints, thus greatly reducing the computational requirements for multi-modal object tracking while keeping lower latency.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2403.15313.pdf' target='_blank'>https://arxiv.org/pdf/2403.15313.pdf</a></span>   <span><a href='https://github.com/ETH-PBL/CR3DT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas KÃ¼hne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15313">CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor. Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications. The code is available at: https://github.com/ETH-PBL/CR3DT.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2403.15245.pdf' target='_blank'>https://arxiv.org/pdf/2403.15245.pdf</a></span>   <span><a href='https://github.com/intell-sci-comput/STATM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Li, Pu Ren, Yang Liu, Hao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15245">Reasoning-Enhanced Object-Centric Learning for Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatiotemporal attention computations and fusion. Our experimental results on various datasets indicate that the STATM module can significantly enhance the capabilities of multiple state-of-the-art object-centric learning models for video. Moreover, as a predictive model, the STATM module also performs well in downstream prediction and Visual Question Answering (VQA) tasks. We will release our codes and data at https://github.com/intell-sci-comput/STATM.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2403.13443.pdf' target='_blank'>https://arxiv.org/pdf/2403.13443.pdf</a></span>   <span><a href='https://github.com/lixiaoyu2000/FastPoly' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Li, Dedong Liu, Yitao Wu, Xian Wu, Lijun Zhao, Jinghan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13443">Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT) captures stable and comprehensive motion states of surrounding obstacles, essential for robotic perception. However, current 3D trackers face issues with accuracy and latency consistency. In this paper, we propose Fast-Poly, a fast and effective filter-based method for 3D MOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object rotational anisotropy in 3D space, enhances local computation densification, and leverages parallelization technique, improving inference speed and precision. Fast-Poly is extensively tested on two large-scale tracking benchmarks with Python implementation. On the nuScenes dataset, Fast-Poly achieves new state-of-the-art performance with 75.8% AMOTA among all methods and can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly exhibits competitive accuracy with 63.6% MOTA and impressive inference speed (35.5 FPS). The source code is publicly available at https://github.com/lixiaoyu2000/FastPoly.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2403.12573.pdf' target='_blank'>https://arxiv.org/pdf/2403.12573.pdf</a></span>   <span><a href='https://github.com/tteepe/TrackTacular' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Torben Teepe, Philipp Wolters, Johannes Gilg, Fabian Herzog, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12573">Lifting Multi-View Detection and Tracking to the Bird's Eye View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Taking advantage of multi-view aggregation presents a promising solution to tackle challenges such as occlusion and missed detection in multi-object tracking and detection. Recent advancements in multi-view detection and 3D object recognition have significantly improved performance by strategically projecting all views onto the ground plane and conducting detection analysis from a Bird's Eye View. In this paper, we compare modern lifting methods, both parameter-free and parameterized, to multi-view aggregation. Additionally, we present an architecture that aggregates the features of multiple times steps to learn robust detection and combines appearance- and motion-based cues for tracking. Most current tracking approaches either focus on pedestrians or vehicles. In our work, we combine both branches and add new challenges to multi-view detection with cross-scene setups. Our method generalizes to three public datasets across two domains: (1) pedestrian: Wildtrack and MultiviewX, and (2) roadside perception: Synthehicle, achieving state-of-the-art performance in detection and tracking. https://github.com/tteepe/TrackTacular
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2403.12504.pdf' target='_blank'>https://arxiv.org/pdf/2403.12504.pdf</a></span>   <span><a href='https://github.com/Franky-X/FVON-TPN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoran Xiong, Guoqing Liu, Qi Wu, Songpengcheng Xia, Tong Hua, Kehui Ma, Zhen Sun, Yan Xiang, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12504">TON-VIO: Online Time Offset Modeling Networks for Robust Temporal Alignment in High Dynamic Motion VIO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal misalignment (time offset) between sensors is common in low cost visual-inertial odometry (VIO) systems. Such temporal misalignment introduces inconsistent constraints for state estimation, leading to a significant positioning drift especially in high dynamic motion scenarios. In this article, we focus on online temporal calibration to reduce the positioning drift caused by the time offset for high dynamic motion VIO. For the time offset observation model, most existing methods rely on accurate state estimation or stable visual tracking. For the prediction model, current methods oversimplify the time offset as a constant value with white Gaussian noise. However, these ideal conditions are seldom satisfied in real high dynamic scenarios, resulting in the poor performance. In this paper, we introduce online time offset modeling networks (TON) to enhance real-time temporal calibration. TON improves the accuracy of time offset observation and prediction modeling. Specifically, for observation modeling, we propose feature velocity observation networks to enhance velocity computation for features in unstable visual tracking conditions. For prediction modeling, we present time offset prediction networks to learn its evolution pattern. To highlight the effectiveness of our method, we integrate the proposed TON into both optimization-based and filter-based VIO systems. Simulation and real-world experiments are conducted to demonstrate the enhanced performance of our approach. Additionally, to contribute to the VIO community, we will open-source the code of our method on: https://github.com/Franky-X/FVON-TPN.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2403.11186.pdf' target='_blank'>https://arxiv.org/pdf/2403.11186.pdf</a></span>   <span><a href='https://george-zhuang.github.io/nettrack/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangze Zheng, Shijie Lin, Haobo Zuo, Changhong Fu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11186">NetTrack: Tracking Highly Dynamic Objects with a Net</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complex dynamicity of open-world objects presents non-negligible challenges for multi-object tracking (MOT), often manifested as severe deformations, fast motion, and occlusions. Most methods that solely depend on coarse-grained object cues, such as boxes and the overall appearance of the object, are susceptible to degradation due to distorted internal relationships of dynamic objects. To address this problem, this work proposes NetTrack, an efficient, generic, and affordable tracking framework to introduce fine-grained learning that is robust to dynamicity. Specifically, NetTrack constructs a dynamicity-aware association with a fine-grained Net, leveraging point-level visual cues. Correspondingly, a fine-grained sampler and matching method have been incorporated. Furthermore, NetTrack learns object-text correspondence for fine-grained localization. To evaluate MOT in extremely dynamic open-world scenarios, a bird flock tracking (BFT) dataset is constructed, which exhibits high dynamicity with diverse species and open-world scenarios. Comprehensive evaluation on BFT validates the effectiveness of fine-grained learning on object dynamicity, and thorough transfer experiments on challenging open-world benchmarks, i.e., TAO, TAO-OW, AnimalTrack, and GMOT-40, validate the strong generalization ability of NetTrack even without finetuning. Project page: https://george-zhuang.github.io/nettrack/.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2403.10425.pdf' target='_blank'>https://arxiv.org/pdf/2403.10425.pdf</a></span>   <span><a href='https://github.com/neufieldrobotics/NeuFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyong Zhang, Huaizu Jiang, Hanumant Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10425">NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision. While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs. In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns. The architecture follows a global-to-local scheme. Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different computing platforms. We achieve a notable 10x-80x speedup compared to several state-of-the-art methods, while maintaining comparable accuracy. Our approach achieves around 30 FPS on edge computing platforms, which represents a significant breakthrough in deploying complex computer vision tasks such as SLAM on small robots like drones. The full training and evaluation code is available at https://github.com/neufieldrobotics/NeuFlow.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2403.05839.pdf' target='_blank'>https://arxiv.org/pdf/2403.05839.pdf</a></span>   <span><a href='https://github.com/Event-AHU/FELT_SOT_Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Xufeng Lou, Shiao Wang, Ju Huang, Lan Chen, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05839">Long-Term Visual Object Tracking with Event Cameras: An Associative Memory Augmented Tracker and A Benchmark Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing event stream based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear. In this paper, we first propose a new long-term, large-scale frame-event visual object tracking dataset, termed FELT. It contains 1,044 long-term videos that involve 1.9 million RGB frames and event stream pairs, 60 different target objects, and 14 challenging attributes. To build a solid benchmark, we retrain and evaluate 21 baseline trackers on our dataset for future work to compare. In addition, we propose a novel Associative Memory Transformer based RGB-Event long-term visual tracker, termed AMTTrack. It follows a one-stream tracking framework and aggregates the multi-scale RGB/event template and search tokens effectively via the Hopfield retrieval layer. The framework also embodies another aspect of associative memory by maintaining dynamic template representations through an associative memory update scheme, which addresses the appearance variation in long-term tracking. Extensive experiments on FELT, FE108, VisEvent, and COESOT datasets fully validated the effectiveness of our proposed tracker. Both the dataset and source code will be released on https://github.com/Event-AHU/FELT_SOT_Benchmark
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2403.05231.pdf' target='_blank'>https://arxiv.org/pdf/2403.05231.pdf</a></span>   <span><a href='https://github.com/LitingLin/LoRAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liting Lin, Heng Fan, Zhipeng Zhang, Yaowei Wang, Yong Xu, Haibin Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05231">Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the Parameter-Efficient Fine-Tuning (PEFT) in large language models, we propose LoRAT, a method that unveils the power of large ViT model for tracking within laboratory-level resources. The essence of our work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. However, unique challenges and potential domain gaps make this transfer not as easy as the first intuition. Firstly, a transformer-based tracker constructs unshared position embedding for template and search image. This poses a challenge for the transfer of LoRA, usually requiring consistency in the design when applied to the pre-trained backbone, to downstream tasks. Secondly, the inductive bias inherent in convolutional heads diminishes the effectiveness of parameter-efficient fine-tuning in tracking models. To overcome these limitations, we first decouple the position embeddings in transformer-based trackers into shared spatial ones and independent type ones. The shared embeddings, which describe the absolute coordinates of multi-resolution images (namely, the template and search images), are inherited from the pre-trained backbones. In contrast, the independent embeddings indicate the sources of each token and are learned from scratch. Furthermore, we design an anchor-free head solely based on MLP to adapt PETR, enabling better performance with less computational overhead. With our design, 1) it becomes practical to train trackers with the ViT-g backbone on GPUs with only memory of 25.8GB (batch size of 16); 2) we reduce the training time of the L-224 variant from 35.0 to 10.8 GPU hours; 3) we improve the LaSOT SUC score from 0.703 to 0.742 with the L-224 variant; 4) we fast the inference speed of the L-224 variant from 52 to 119 FPS. Code and models are available at https://github.com/LitingLin/LoRAT.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2403.05146.pdf' target='_blank'>https://arxiv.org/pdf/2403.05146.pdf</a></span>   <span><a href='https://github.com/PieceZhang/MotionDCTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Zhang, Kim Yan, Chun Ping Lam, Chengyu Fang, Wenxuan Xie, Yufu Qiu, Raymond Shing-Yan Tang, Shing Shin Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05146">Motion-Guided Dual-Camera Tracker for Endoscope Tracking and Motion Analysis in a Mechanical Gastric Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flexible endoscope motion tracking and analysis in mechanical simulators have proven useful for endoscopy training. Common motion tracking methods based on electromagnetic tracker are however limited by their high cost and material susceptibility. In this work, the motion-guided dual-camera vision tracker is proposed to provide robust and accurate tracking of the endoscope tip's 3D position. The tracker addresses several unique challenges of tracking flexible endoscope tip inside a dynamic, life-sized mechanical simulator. To address the appearance variation and keep dual-camera tracking consistency, the cross-camera mutual template strategy (CMT) is proposed by introducing dynamic transient mutual templates. To alleviate large occlusion and light-induced distortion, the Mamba-based motion-guided prediction head (MMH) is presented to aggregate historical motion with visual tracking. The proposed tracker achieves superior performance against state-of-the-art vision trackers, achieving 42% and 72% improvements against the second-best method in average error and maximum error. Further motion analysis involving novice and expert endoscopists also shows that the tip 3D motion provided by the proposed tracker enables more reliable motion analysis and more substantial differentiation between different expertise levels, compared with other trackers. Project page: https://github.com/PieceZhang/MotionDCTrack
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2403.05021.pdf' target='_blank'>https://arxiv.org/pdf/2403.05021.pdf</a></span>   <span><a href='https://github.com/Nathan-Li123/SMOTer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Qin Li, Hao Wang, Xue Ma, Jiali Yao, Shaohua Dong, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05021">Beyond MOT: Semantic Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multi-object tracking (MOT) aims to predict trajectories of targets (i.e., ''where'') in videos. Yet, knowing merely ''where'' is insufficient in many crucial applications. In comparison, semantic understanding such as fine-grained behaviors, interactions, and overall summarized captions (i.e., ''what'') from videos, associated with ''where'', is highly-desired for comprehensive video analysis. Thus motivated, we introduce Semantic Multi-Object Tracking (SMOT), that aims to estimate object trajectories and meanwhile understand semantic details of associated trajectories including instance captions, instance interactions, and overall video captions, integrating ''where'' and ''what'' for tracking. In order to foster the exploration of SMOT, we propose BenSMOT, a large-scale Benchmark for Semantic MOT. Specifically, BenSMOT comprises 3,292 videos with 151K frames, covering various scenarios for semantic tracking of humans. BenSMOT provides annotations for the trajectories of targets, along with associated instance captions in natural language, instance interactions, and overall caption for each video sequence. To our best knowledge, BenSMOT is the first publicly available benchmark for SMOT. Besides, to encourage future research, we present a novel tracker named SMOTer, which is specially designed and end-to-end trained for SMOT, showing promising performance. By releasing BenSMOT, we expect to go beyond conventional MOT by predicting ''where'' and ''what'' for SMOT, opening up a new direction in tracking for video understanding. We will release BenSMOT and SMOTer at https://github.com/Nathan-Li123/SMOTer.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2403.04700.pdf' target='_blank'>https://arxiv.org/pdf/2403.04700.pdf</a></span>   <span><a href='https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Chen, En Yu, Jinyang Li, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04700">Delving into the Trajectory Long-tail Distribution for Muti-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations. Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques. Yet, there has been a lack of thorough examination concerning the nature of tracking data it self. In this study, we pioneer an exploration into the distribution patterns of tracking data and identify a pronounced long-tail distribution issue within existing MOT datasets. We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as ``pedestrians trajectory long-tail distribution''. Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution. Specifically, we propose two data augmentation strategies, including Stationary Camera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID. SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use diffusion model to change the background of the scene. GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually. Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance. The code is available at https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2403.03493.pdf' target='_blank'>https://arxiv.org/pdf/2403.03493.pdf</a></span>   <span><a href='https://github.com/HengLan/VastTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, Zhipeng Zhang, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03493">VastTrack: Vast Category Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel benchmark, dubbed VastTrack, towards facilitating the development of more general visual tracking via encompassing abundant classes and videos. VastTrack possesses several attractive properties: (1) Vast Object Category. In particular, it covers target objects from 2,115 classes, largely surpassing object categories of existing popular benchmarks (e.g., GOT-10k with 563 classes and LaSOT with 70 categories). With such vast object classes, we expect to learn more general object tracking. (2) Larger scale. Compared with current benchmarks, VastTrack offers 50,610 sequences with 4.2 million frames, which makes it to date the largest benchmark regarding the number of videos, and thus could benefit training even more powerful visual trackers in the deep learning era. (3) Rich Annotation. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions for the videos. The rich annotations of VastTrack enables development of both the vision-only and the vision-language tracking. To ensure precise annotation, all videos are manually labeled with multiple rounds of careful inspection and refinement. To understand performance of existing trackers and to provide baselines for future comparison, we extensively assess 25 representative trackers. The results, not surprisingly, show significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are required to improve general tracking. Our VastTrack and all the evaluation results will be made publicly available https://github.com/HengLan/VastTrack.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2402.18302.pdf' target='_blank'>https://arxiv.org/pdf/2402.18302.pdf</a></span>   <span><a href='https://github.com/lab206/EchoTrack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lab206/EchoTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Lin, Jiajun Chen, Kunyu Peng, Xuan He, Zhiyong Li, Rainer Stiefelhagen, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18302">EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains. Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively. Aside from the architectural design, we establish the first set of large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD. Extensive experiments on the established benchmarks demonstrate the effectiveness of the proposed EchoTrack and its components. The source code and datasets are available at https://github.com/lab206/EchoTrack.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2402.16249.pdf' target='_blank'>https://arxiv.org/pdf/2402.16249.pdf</a></span>   <span><a href='https://github.com/aron-lin/seqtrack3d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Lin, Zhiheng Li, Yubo Cui, Zheng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16249">SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in the scenes with sparse points. To break through this limitation, we introduce Sequence-to-Sequence tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel method ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset. The code will be made public at https://github.com/aron-lin/seqtrack3d.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2402.14136.pdf' target='_blank'>https://arxiv.org/pdf/2402.14136.pdf</a></span>   <span><a href='https://github.com/nesl/GDTM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ho Lyun Jeong, Ziqi Wang, Colin Samplawski, Jason Wu, Shiwei Fang, Lance M. Kaplan, Deepak Ganesan, Benjamin Marlin, Mani Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14136">GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2402.12303.pdf' target='_blank'>https://arxiv.org/pdf/2402.12303.pdf</a></span>   <span><a href='https://github.com/TRAILab/UncertaintyTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Won Lee, Steven L. Waslander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12303">UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2402.08437.pdf' target='_blank'>https://arxiv.org/pdf/2402.08437.pdf</a></span>   <span><a href='https://github.com/CVLABLUMS/CVGL-Camera-Calibration' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Waleed, Abdul Rauf, Murtaza Taj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08437">Camera Calibration through Geometric Constraints from Rotation and Projection Matrices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The process of camera calibration involves estimating the intrinsic and extrinsic parameters, which are essential for accurately performing tasks such as 3D reconstruction, object tracking and augmented reality. In this work, we propose a novel constraints-based loss for measuring the intrinsic (focal length: $(f_x, f_y)$ and principal point: $(p_x, p_y)$) and extrinsic (baseline: ($b$), disparity: ($d$), translation: $(t_x, t_y, t_z)$, and rotation specifically pitch: $(Î¸_p)$) camera parameters. Our novel constraints are based on geometric properties inherent in the camera model, including the anatomy of the projection matrix (vanishing points, image of world origin, axis planes) and the orthonormality of the rotation matrix. Thus we proposed a novel Unsupervised Geometric Constraint Loss (UGCL) via a multitask learning framework. Our methodology is a hybrid approach that employs the learning power of a neural network to estimate the desired parameters along with the underlying mathematical properties inherent in the camera projection matrix. This distinctive approach not only enhances the interpretability of the model but also facilitates a more informed learning process. Additionally, we introduce a new CVGL Camera Calibration dataset, featuring over 900 configurations of camera parameters, incorporating 63,600 image pairs that closely mirror real-world conditions. By training and testing on both synthetic and real-world datasets, our proposed approach demonstrates improvements across all parameters when compared to the state-of-the-art (SOTA) benchmarks. The code and the updated dataset can be found here: https://github.com/CVLABLUMS/CVGL-Camera-Calibration
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2402.02574.pdf' target='_blank'>https://arxiv.org/pdf/2402.02574.pdf</a></span>   <span><a href='https://github.com/guanxiongsun/vfe.pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanxiong Sun, Chi Wang, Zhaoyu Zhang, Jiankang Deng, Stefanos Zafeiriou, Yang Hua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02574">Spatio-temporal Prompting Network for Robust Video Feature Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Frame quality deterioration is one of the main challenges in the field of video understanding. To compensate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information. However, these integration modules are heavy and complex. Furthermore, each integration module is specifically tailored for its target task, making it difficult to generalise to multiple tasks. In this paper, we present a neat and unified framework, called Spatio-Temporal Prompting Network (STPN). It can efficiently extract robust and accurate video features by dynamically adjusting the input features in the backbone network. Specifically, STPN predicts several video prompts containing spatio-temporal information of neighbour frames. Then, these video prompts are prepended to the patch embeddings of the current frame as the updated input for video feature extraction. Moreover, STPN is easy to generalise to various video tasks because it does not contain task-specific modules. Without bells and whistles, STPN achieves state-of-the-art performance on three widely-used datasets for different video understanding tasks, i.e., ImageNetVID for video object detection, YouTubeVIS for video instance segmentation, and GOT-10k for visual object tracking. Code is available at https://github.com/guanxiongsun/vfe.pytorch.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2401.16618.pdf' target='_blank'>https://arxiv.org/pdf/2401.16618.pdf</a></span>   <span><a href='https://github.com/FARAZLOTFI/underwater-object-tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Faraz Lotfi, Khalil Virji, Nicholas Dudek, Gregory Dudek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16618">A comparison of RL-based and PID controllers for 6-DOF swimming robots: hybrid underwater object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present an exploration and assessment of employing a centralized deep Q-network (DQN) controller as a substitute for the prevalent use of PID controllers in the context of 6DOF swimming robots. Our primary focus centers on illustrating this transition with the specific case of underwater object tracking. DQN offers advantages such as data efficiency and off-policy learning, while remaining simpler to implement than other reinforcement learning methods. Given the absence of a dynamic model for our robot, we propose an RL agent to control this multi-input-multi-output (MIMO) system, where a centralized controller may offer more robust control than distinct PIDs. Our approach involves initially using classical controllers for safe exploration, then gradually shifting to DQN to take full control of the robot.
  We divide the underwater tracking task into vision and control modules. We use established methods for vision-based tracking and introduce a centralized DQN controller. By transmitting bounding box data from the vision module to the control module, we enable adaptation to various objects and effortless vision system replacement. Furthermore, dealing with low-dimensional data facilitates cost-effective online learning for the controller. Our experiments, conducted within a Unity-based simulator, validate the effectiveness of a centralized RL agent over separated PID controllers, showcasing the applicability of our framework for training the underwater RL agent and improved performance compared to traditional control methods. The code for both real and simulation implementations is at https://github.com/FARAZLOTFI/underwater-object-tracking.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2401.11228.pdf' target='_blank'>https://arxiv.org/pdf/2401.11228.pdf</a></span>   <span><a href='https://github.com/OpenSpaceAI/UVLTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Jinpeng Zhang, Mengxue Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11228">Unifying Visual and Vision-Language Tracking via Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking aims to locate the target object in a video sequence according to the state specified by different modal references, including the initial bounding box (BBOX), natural language (NL), or both (NL+BBOX). Due to the gap between different modalities, most existing trackers are designed for single or partial of these reference settings and overspecialize on the specific modality. Differently, we present a unified tracker called UVLTrack, which can simultaneously handle all three reference settings (BBOX, NL, NL+BBOX) with the same parameters. The proposed UVLTrack enjoys several merits. First, we design a modality-unified feature extractor for joint visual and language feature learning and propose a multi-modal contrastive loss to align the visual and language features into a unified semantic space. Second, a modality-adaptive box head is proposed, which makes full use of the target reference to mine ever-changing scenario features dynamically from video contexts and distinguish the target in a contrastive way, enabling robust performance in different reference settings. Extensive experimental results demonstrate that UVLTrack achieves promising performance on seven visual tracking datasets, three vision-language tracking datasets, and three visual grounding datasets. Codes and models will be open-sourced at https://github.com/OpenSpaceAI/UVLTrack.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2401.06614.pdf' target='_blank'>https://arxiv.org/pdf/2401.06614.pdf</a></span>   <span><a href='https://vveicao.github.io/projects/Motion2VecSets/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Cao, Chang Luo, Biao Zhang, Matthias NieÃner, Jiapeng Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06614">Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based priors enable more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent sets instead of using global latent codes. This novel 4D representation allows us to learn local shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporally-coherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid computational overhead, we designed an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations. More detailed information can be found at https://vveicao.github.io/projects/Motion2VecSets/.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2401.04650.pdf' target='_blank'>https://arxiv.org/pdf/2401.04650.pdf</a></span>   <span><a href='https://parses-lab.github.io/kresling_control' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Immanuel Ampomah Mensah, Jessica Healey, Celina Wu, Andrea Lacunza, Nathaniel Hanson, Kristen L. Dorsey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04650">Hold 'em and Fold 'em: Towards Human-scale, Feedback-Controlled Soft Origami Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An underdeveloped capability in soft robotics is proprioceptive feedback control, where soft actuators can be sensed and controlled using only sensors on the robot's body. Additionally, soft actuators are often unable to support human-scale loads due to the extremely compliant materials in use. Developing both feedback control and the ability to actuate under large loads (e.g. 500 N) are key capacities required to move soft robotics into everyday applications. In this work, we independently demonstrate these key factors towards controlling and actuating human-scale loads: proprioceptive (embodied) feedback control of a soft, pneumatically-actuated origami robot; and actuation of these origami origami robots under a person's weight in an open-loop configuration. In both demonstrations, the actuators are controlled by internal fluidic pressure. Capacitive sensors patterned onto the robot provide position estimation and serve as input to a feedback controller. We demonstrate position control of a single actuator during stepped setpoints and sinusoidal trajectory following, with root mean square error (RMSE) below 4 mm. We also showcase the actuator's potential towards human-scale robotics as an "origami balance board" by joining three actuators into an open-loop controlled system with a platform that varies its height, roll, and pitch. This work contributes to the field of soft robotics by demonstrating closed-loop feedback position control without visual tracking as an input and lightweight, soft actuators that can support a person's weight. The project repository, including videos, CAD files, and ROS code, is available at https://parses-lab.github.io/kresling_control.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2401.03142.pdf' target='_blank'>https://arxiv.org/pdf/2401.03142.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/EVPTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangtao Shi, Bineng Zhong, Qihua Liang, Ning Li, Shengping Zhang, Xianxian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03142">Explicit Visual Prompts for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How to effectively exploit spatio-temporal information is crucial to capture target appearance changes in visual tracking. However, most deep learning-based trackers mainly focus on designing a complicated appearance model or template updating strategy, while lacking the exploitation of context between consecutive frames and thus entailing the \textit{when-and-how-to-update} dilemma. To address these issues, we propose a novel explicit visual prompts framework for visual tracking, dubbed \textbf{EVPTrack}. Specifically, we utilize spatio-temporal tokens to propagate information between consecutive frames without focusing on updating templates. As a result, we cannot only alleviate the challenge of \textit{when-to-update}, but also avoid the hyper-parameters associated with updating strategies. Then, we utilize the spatio-temporal tokens to generate explicit visual prompts that facilitate inference in the current frame. The prompts are fed into a transformer encoder together with the image tokens without additional processing. Consequently, the efficiency of our model is improved by avoiding \textit{how-to-update}. In addition, we consider multi-scale information as explicit visual prompts, providing multiscale template features to enhance the EVPTrack's ability to handle target scale changes. Extensive experimental results on six benchmarks (i.e., LaSOT, LaSOT\rm $_{ext}$, GOT-10k, UAV123, TrackingNet, and TNL2K.) validate that our EVPTrack can achieve competitive performance at a real-time speed by effectively exploiting both spatio-temporal and multi-scale information. Code and models are available at https://github.com/GXNU-ZhongLab/EVPTrack.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2401.02826.pdf' target='_blank'>https://arxiv.org/pdf/2401.02826.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Cross_Resolution_SOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yabin Zhu, Xiao Wang, Chenglong Li, Bo Jiang, Lin Zhu, Zhixiang Huang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02826">CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing datasets for RGB-DVS tracking are collected with DVS346 camera and their resolution ($346 \times 260$) is low for practical applications. Actually, only visible cameras are deployed in many practical systems, and the newly designed neuromorphic cameras may have different resolutions. The latest neuromorphic sensors can output high-definition event streams, but it is very difficult to achieve strict alignment between events and frames on both spatial and temporal views. Therefore, how to achieve accurate tracking with unaligned neuromorphic and visible sensors is a valuable but unresearched problem. In this work, we formally propose the task of object tracking using unaligned neuromorphic and visible cameras. We build the first unaligned frame-event dataset CRSOT collected with a specially built data acquisition system, which contains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In addition, we propose a novel unaligned object tracking framework that can realize robust tracking even using the loosely aligned RGB-Event data. Specifically, we extract the template and search regions of RGB and Event data and feed them into a unified ViT backbone for feature embedding. Then, we propose uncertainty perception modules to encode the RGB and Event features, respectively, then, we propose a modality uncertainty fusion module to aggregate the two modalities. These three branches are jointly optimized in the training phase. Extensive experiments demonstrate that our tracker can collaborate the dual modalities for high-performance tracking even without strictly temporal and spatial alignment. The source code, dataset, and pre-trained models will be released at https://github.com/Event-AHU/Cross_Resolution_SOT.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2401.01686.pdf' target='_blank'>https://arxiv.org/pdf/2401.01686.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/ODTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo, Shengping Zhang, Xianxian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01686">ODTrack: Online Dense Temporal Token Learning for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online contextual reasoning and association across consecutive video frames are critical to perceive instances in visual tracking. However, most current top-performing trackers persistently lean on sparse temporal relationships between reference and search frames via an offline mode. Consequently, they can only interact independently within each image-pair and establish limited temporal correlations. To alleviate the above problem, we propose a simple, flexible and effective video-level tracking pipeline, named \textbf{ODTrack}, which densely associates the contextual relationships of video frames in an online token propagation manner. ODTrack receives video frames of arbitrary length to capture the spatio-temporal trajectory relationships of an instance, and compresses the discrimination features (localization information) of a target into a token sequence to achieve frame-to-frame association. This new solution brings the following benefits: 1) the purified token sequences can serve as prompts for the inference in the next video frame, whereby past information is leveraged to guide future inference; 2) the complex online update strategies are effectively avoided by the iterative propagation of token sequences, and thus we can achieve more efficient model representation and computation. ODTrack achieves a new \textit{SOTA} performance on seven benchmarks, while running at real-time speed. Code and models are available at \url{https://github.com/GXNU-ZhongLab/ODTrack}.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2401.00605.pdf' target='_blank'>https://arxiv.org/pdf/2401.00605.pdf</a></span>   <span><a href='https://github.com/AdelaideAuto-IDLab/Distributed-limitedFoV-CDPMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Chen, Hoa Van Nguyen, Alex S. Leong, Sabita Panicker, Robin Baker, Damith C. Ranasinghe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00605">Distributed Multi-Object Tracking Under Limited Field of View Heterogeneous Sensors with Density Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of tracking multiple, unknown, and time-varying numbers of objects using a distributed network of heterogeneous sensors. In an effort to derive a formulation for practical settings, we consider limited and unknown sensor field-of-views (FoVs), sensors with limited local computational resources and communication channel capacity. The resulting distributed multi-object tracking algorithm involves solving an NP-hard multidimensional assignment problem either optimally for small-size problems or sub-optimally for general practical problems. For general problems, we propose an efficient distributed multi-object tracking algorithm that performs track-to-track fusion using a clustering-based analysis of the state space transformed into a density space to mitigate the complexity of the assignment problem. The proposed algorithm can more efficiently group local track estimates for fusion than existing approaches. To ensure we achieve globally consistent identities for tracks across a network of nodes as objects move between FoVs, we develop a graph-based algorithm to achieve label consensus and minimise track segmentation. Numerical experiments with synthetic and real-world trajectory datasets demonstrate that our proposed method is significantly more computationally efficient than state-of-the-art solutions, achieving similar tracking accuracy and bandwidth requirements but with improved label consistency.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2410.01678.pdf' target='_blank'>https://arxiv.org/pdf/2410.01678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayesha Ishaq, Mohamed El Amine Boudjoghra, Jean Lahoud, Fahad Shahbaz Khan, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01678">Open3DTrack: Towards Open-Vocabulary 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking plays a critical role in autonomous driving by enabling the real-time monitoring and prediction of multiple objects' movements. Traditional 3D tracking systems are typically constrained by predefined object categories, limiting their adaptability to novel, unseen objects in dynamic environments. To address this limitation, we introduce open-vocabulary 3D tracking, which extends the scope of 3D tracking to include objects beyond predefined categories. We formulate the problem of open-vocabulary 3D tracking and introduce dataset splits designed to represent various open-vocabulary scenarios. We propose a novel approach that integrates open-vocabulary capabilities into a 3D tracking framework, allowing for generalization to unseen object classes. Our method effectively reduces the performance gap between tracking known and novel objects through strategic adaptation. Experimental results demonstrate the robustness and adaptability of our method in diverse outdoor driving scenarios. To the best of our knowledge, this work is the first to address open-vocabulary 3D tracking, presenting a significant advancement for autonomous systems in real-world settings. Code, trained models, and dataset splits are available publicly.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2511.20886.pdf' target='_blank'>https://arxiv.org/pdf/2511.20886.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiancheng Pan, Runze Wang, Tianwen Qian, Mohammad Mahdi, Yanwei Fu, Xiangyang Xue, Xiaomeng Huang, Luc Van Gool, Danda Pani Paudel, Yuqian Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20886">V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2505.12340.pdf' target='_blank'>https://arxiv.org/pdf/2505.12340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jirong Zha, Yuxuan Fan, Kai Li, Han Li, Chen Gao, Xinlei Chen, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12340">DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State estimation is challenging for 3D object tracking with high maneuverability, as the target's state transition function changes rapidly, irregularly, and is unknown to the estimator. Existing work based on interacting multiple model (IMM) achieves more accurate estimation than single-filter approaches through model combination, aligning appropriate models for different motion modes of the target object over time. However, two limitations of conventional IMM remain unsolved. First, the solution space of the model combination is constrained as the target's diverse kinematic properties in different directions are ignored. Second, the model combination weights calculated by the observation likelihood are not accurate enough due to the measurement uncertainty. In this paper, we propose a novel framework, DIMM, to effectively combine estimates from different motion models in each direction, thus increasing the 3D object tracking accuracy. First, DIMM extends the model combination solution space of conventional IMM from a hyperplane to a hypercube by designing a 3D-decoupled multi-hierarchy filter bank, which describes the target's motion with various-order linear models. Second, DIMM generates more reliable combination weight matrices through a differentiable adaptive fusion network for importance allocation rather than solely relying on the observation likelihood; it contains an attention-based twin delayed deep deterministic policy gradient (TD3) method with a hierarchical reward. Experiments demonstrate that DIMM significantly improves the tracking accuracy of existing state estimation methods by 31.61%~99.23%.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2408.00606.pdf' target='_blank'>https://arxiv.org/pdf/2408.00606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Feilin Han, Leping Zhang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00606">U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs Autonomous Flight</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern perception systems for autonomous flight are sensitive to occlusion and have limited long-range capability, which is a key bottleneck in improving low-altitude economic task performance. Recent research has shown that the UAV-to-UAV (U2U) cooperative perception system has great potential to revolutionize the autonomous flight industry. However, the lack of a large-scale dataset is hindering progress in this area. This paper presents U2UData, the first large-scale cooperative perception dataset for swarm UAVs autonomous flight. The dataset was collected by three UAVs flying autonomously in the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames, 945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes. It also includes brightness, temperature, humidity, smoke, and airflow values covering all flight routes. U2USim is the first real-world mapping swarm UAVs simulation environment. It takes Yunnan Province as the prototype and includes 4 terrains, 7 weather conditions, and 8 sensor types. U2UData introduces two perception tasks: cooperative 3D object detection and cooperative 3D object tracking. This paper provides comprehensive benchmarks of recent cooperative perception algorithms on these tasks.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2504.06958.pdf' target='_blank'>https://arxiv.org/pdf/2504.06958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06958">VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2406.11303.pdf' target='_blank'>https://arxiv.org/pdf/2406.11303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11303">VideoVista: A Versatile Benchmark for Video Understanding and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant breakthroughs in video analysis driven by the rapid development of large multimodal models (LMMs), there remains a lack of a versatile evaluation benchmark to comprehensively assess these models' performance in video understanding and reasoning. To address this, we present VideoVista, a video QA benchmark that integrates challenges across diverse content categories, durations, and abilities. Specifically, VideoVista comprises 25,000 questions derived from 3,400 videos spanning 14 categories (e.g., Howto, Film, and Entertainment) with durations ranging from a few seconds to over 10 minutes. Besides, it encompasses 19 types of understanding tasks (e.g., anomaly detection, interaction understanding) and 8 reasoning tasks (e.g., logical reasoning, causal reasoning). To achieve this, we present an automatic data construction framework, leveraging powerful GPT-4o alongside advanced analysis tools (e.g., video splitting, object segmenting, and tracking). We also utilize this framework to construct training data to enhance the capabilities of video-related LMMs (Video-LMMs). Through a comprehensive and quantitative evaluation of cutting-edge models, we reveal that: 1) Video-LMMs face difficulties in fine-grained video tasks involving temporal location, object tracking, and anomaly detection; 2) Video-LMMs present inferior logical and relation reasoning abilities; 3) Open-source Video-LMMs' performance is significantly lower than GPT-4o and Gemini-1.5, lagging by 20 points. This highlights the crucial role VideoVista will play in advancing LMMs that can accurately understand videos and perform precise reasoning.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2507.05718.pdf' target='_blank'>https://arxiv.org/pdf/2507.05718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Que, Jie Yang, Tao Du, Shuqiang Xia, Chao-Kai Wen, Shi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05718">Cooperative Mapping, Localization, and Beam Management via Multi-Modal SLAM in ISAC Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous localization and mapping (SLAM) plays a critical role in integrated sensing and communication (ISAC) systems for sixth-generation (6G) millimeter-wave (mmWave) networks, enabling environmental awareness and precise user equipment (UE) positioning. While cooperative multi-user SLAM has demonstrated potential in leveraging distributed sensing, its application within multi-modal ISAC systems remains limited, particularly in terms of theoretical modeling and communication-layer integration. This paper proposes a novel multi-modal SLAM framework that addresses these limitations through three key contributions. First, a Bayesian estimation framework is developed for cooperative multi-user SLAM, along with a two-stage algorithm for robust radio map construction under dynamic and heterogeneous sensing conditions. Second, a multi-modal localization strategy is introduced, fusing SLAM results with camera-based multi-object tracking and inertial measurement unit (IMU) data via an error-aware model, significantly improving UE localization in multi-user scenarios. Third, a sensing-aided beam management scheme is proposed, utilizing global radio maps and localization data to generate UE-specific prior information for beam selection, thereby reducing inter-user interference and enhancing downlink spectral efficiency. Simulation results demonstrate that the proposed system improves radio map accuracy by up to 60%, enhances localization accuracy by 37.5%, and significantly outperforms traditional methods in both indoor and outdoor environments.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2504.12709.pdf' target='_blank'>https://arxiv.org/pdf/2504.12709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shumin Wang, Zhuoran Yang, Lidian Wang, Zhipeng Tang, Heng Li, Lehan Pan, Sha Zhang, Jie Peng, Jianmin Ji, Yanyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12709">Self-Supervised Pre-training with Combined Datasets for 3D Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The significant achievements of pre-trained models leveraging large volumes of data in the field of NLP and 2D vision inspire us to explore the potential of extensive data pre-training for 3D perception in autonomous driving. Toward this goal, this paper proposes to utilize massive unlabeled data from heterogeneous datasets to pre-train 3D perception models. We introduce a self-supervised pre-training framework that learns effective 3D representations from scratch on unlabeled data, combined with a prompt adapter based domain adaptation strategy to reduce dataset bias. The approach significantly improves model performance on downstream tasks such as 3D object detection, BEV segmentation, 3D object tracking, and occupancy prediction, and shows steady performance increase as the training data volume scales up, demonstrating the potential of continually benefit 3D perception models for autonomous driving. We will release the source code to inspire further investigations in the community.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2510.19560.pdf' target='_blank'>https://arxiv.org/pdf/2510.19560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Deng, Xian Zhong, Wenxuan Liu, Zhaofei Yu, Jingling Yuan, Tiejun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19560">HAD: Hierarchical Asymmetric Distillation to Bridge Spatio-Temporal Gaps in Event-Based Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB cameras excel at capturing rich texture details with high spatial resolution, whereas event cameras offer exceptional temporal resolution and a high dynamic range (HDR). Leveraging their complementary strengths can substantially enhance object tracking under challenging conditions, such as high-speed motion, HDR environments, and dynamic background interference. However, a significant spatio-temporal asymmetry exists between these two modalities due to their fundamentally different imaging mechanisms, hindering effective multi-modal integration. To address this issue, we propose {Hierarchical Asymmetric Distillation} (HAD), a multi-modal knowledge distillation framework that explicitly models and mitigates spatio-temporal asymmetries. Specifically, HAD proposes a hierarchical alignment strategy that minimizes information loss while maintaining the student network's computational efficiency and parameter compactness. Extensive experiments demonstrate that HAD consistently outperforms state-of-the-art methods, and comprehensive ablation studies further validate the effectiveness and necessity of each designed component. The code will be released soon.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2511.19475.pdf' target='_blank'>https://arxiv.org/pdf/2511.19475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianlu Zhang, Qiang Zhang, Guiguang Ding, Jungong Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19475">Tracking and Segmenting Anything in Any Modality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking and segmentation play essential roles in video understanding, providing basic positional information and temporal association of objects within video sequences. Despite their shared objective, existing approaches often tackle these tasks using specialized architectures or modality-specific parameters, limiting their generalization and scalability. Recent efforts have attempted to unify multiple tracking and segmentation subtasks from the perspectives of any modality input or multi-task inference. However, these approaches tend to overlook two critical challenges: the distributional gap across different modalities and the feature representation gap across tasks. These issues hinder effective cross-task and cross-modal knowledge sharing, ultimately constraining the development of a true generalist model. To address these limitations, we propose a universal tracking and segmentation framework named SATA, which unifies a broad spectrum of tracking and segmentation subtasks with any modality input. Specifically, a Decoupled Mixture-of-Expert (DeMoE) mechanism is presented to decouple the unified representation learning task into the modeling process of cross-modal shared knowledge and specific information, thus enabling the model to maintain flexibility while enhancing generalization. Additionally, we introduce a Task-aware Multi-object Tracking (TaMOT) pipeline to unify all the task outputs as a unified set of instances with calibrated ID information, thereby alleviating the degradation of task-specific knowledge during multi-task training. SATA demonstrates superior performance on 18 challenging tracking and segmentation benchmarks, offering a novel perspective for more generalizable video understanding.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2411.00553.pdf' target='_blank'>https://arxiv.org/pdf/2411.00553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Mancusi, Mattia Bernardi, Aniello Panariello, Angelo Porrello, Rita Cucchiara, Simone Calderara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00553">Is Multiple Object Tracking a Matter of Specialization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end transformer-based trackers have achieved remarkable performance on most human-related datasets. However, training these trackers in heterogeneous scenarios poses significant challenges, including negative interference - where the model learns conflicting scene-specific parameters - and limited domain generalization, which often necessitates expensive fine-tuning to adapt the models to new domains. In response to these challenges, we introduce Parameter-efficient Scenario-specific Tracking Architecture (PASTA), a novel framework that combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL). Specifically, we define key scenario attributes (e.g, camera-viewpoint, lighting condition) and train specialized PEFT modules for each attribute. These expert modules are combined in parameter space, enabling systematic generalization to new domains without increasing inference time. Extensive experiments on MOTSynth, along with zero-shot evaluations on MOT17 and PersonPath22 demonstrate that a neural tracker built from carefully selected modules surpasses its monolithic counterpart. We release models and code.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2503.07516.pdf' target='_blank'>https://arxiv.org/pdf/2503.07516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weize Li, Yunhao Du, Qixiang Yin, Zhicheng Zhao, Fei Su, Daqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07516">Just Functioning as a Hook for Two-Stage Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) aims to localize target trajectories in videos specified by natural language expressions. Despite recent progress, the intrinsic relationship between the two subtasks of tracking and referring in RMOT has not been fully studied. In this paper, we present a systematic analysis of their interdependence, revealing that current two-stage Referring-by-Tracking (RBT) frameworks remain fundamentally limited by insufficient modeling of subtask interactions and inflexible reliance on semantic alignment modules like CLIP. To this end, we propose JustHook, a novel two-stage RBT framework where a Hook module is firstly designed to redefine the linkage between subtasks. The Hook is built centered on grid sampling at the feature-level and is used for context-aware target feature extraction. Moreover, we propose a Parallel Combined Decoder (PCD) that learns in a unified joint feature space rather than relying on pre-defined cross-modal embeddings. Our design not only enhances the interpretability and modularity but also significantly improves the generalization. Extensive experiments on Refer-KITTI, Refer-KITTI-V2, and Refer-Dance demonstrate that JustHook achieves state-of-the-art performance, improving the HOTA by +6.9\% on Refer-KITTI-V2 with superior efficiency. Code will be available soon.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2404.12031.pdf' target='_blank'>https://arxiv.org/pdf/2404.12031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeliang Ma, Song Yang, Zhe Cui, Zhicheng Zhao, Fei Su, Delong Liu, Jingyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12031">MLS-Track: Multilevel Semantic Interaction in RMOT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The new trend in multi-object tracking task is to track objects of interest using natural language. However, the scarcity of paired prompt-instance data hinders its progress. To address this challenge, we propose a high-quality yet low-cost data generation method base on Unreal Engine 5 and construct a brand-new benchmark dataset, named Refer-UE-City, which primarily includes scenes from intersection surveillance videos, detailing the appearance and actions of people and vehicles. Specifically, it provides 14 videos with a total of 714 expressions, and is comparable in scale to the Refer-KITTI dataset. Additionally, we propose a multi-level semantic-guided multi-object framework called MLS-Track, where the interaction between the model and text is enhanced layer by layer through the introduction of Semantic Guidance Module (SGM) and Semantic Correlation Branch (SCB). Extensive experiments on Refer-UE-City and Refer-KITTI datasets demonstrate the effectiveness of our proposed framework and it achieves state-of-the-art performance. Code and datatsets will be available.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2503.11218.pdf' target='_blank'>https://arxiv.org/pdf/2503.11218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andong Lu, Mai Wen, Jinhu Wang, Yuanzhi Guo, Chenglong Li, Jin Tang, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11218">Towards General Multimodal Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multimodal tracking studies focus on bi-modal scenarios such as RGB-Thermal, RGB-Event, and RGB-Language. Although promising tracking performance is achieved through leveraging complementary cues from different sources, it remains challenging in complex scenes due to the limitations of bi-modal scenarios. In this work, we introduce a general multimodal visual tracking task that fully exploits the advantages of four modalities, including RGB, thermal infrared, event, and language, for robust tracking under challenging conditions. To provide a comprehensive evaluation platform for general multimodal visual tracking, we construct QuadTrack600, a large-scale, high-quality benchmark comprising 600 video sequences (totaling 384.7K high-resolution (640x480) frame groups). In each frame group, all four modalities are spatially aligned and meticulously annotated with bounding boxes, while 21 sequence-level challenge attributes are provided for detailed performance analysis. Despite quad-modal data provides richer information, the differences in information quantity among modalities and the computational burden from four modalities are two challenging issues in fusing four modalities. To handle these issues, we propose a novel approach called QuadFusion, which incorporates an efficient Multiscale Fusion Mamba with four different scanning scales to achieve sufficient interactions of the four modalities while overcoming the exponential computational burden, for general multimodal visual tracking. Extensive experiments on the QuadTrack600 dataset and three bi-modal tracking datasets, including LasHeR, VisEvent, and TNL2K, validate the effectiveness of our QuadFusion.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2505.04917.pdf' target='_blank'>https://arxiv.org/pdf/2505.04917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Peng, Chenxu Wang, Minrui Zou, Danyang Li, Zhengpeng Yang, Yimian Dai, Ming-Ming Cheng, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04917">A Simple Detector with Frame Dynamics is a Strong Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2411.13317.pdf' target='_blank'>https://arxiv.org/pdf/2411.13317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sivan Doveh, Nimrod Shabtay, Wei Lin, Eli Schwartz, Hilde Kuehne, Raja Giryes, Rogerio Feris, Leonid Karlinsky, James Glass, Assaf Arbelle, Shimon Ullman, M. Jehanzeb Mirza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13317">Teaching VLMs to Localize Specific Objects from In-context Examples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have shown remarkable capabilities across diverse visual tasks, including image recognition, video understanding, and Visual Question Answering (VQA) when explicitly trained for these tasks. Despite these advances, we find that present-day VLMs (including the proprietary GPT-4o) lack a fundamental cognitive ability: learning to localize specific objects in a scene by taking into account the context. In this work, we focus on the task of few-shot personalized localization, where a model is given a small set of annotated images (in-context examples) -- each with a category label and bounding box -- and is tasked with localizing the same object type in a query image. Personalized localization can be particularly important in cases of ambiguity of several related objects that can respond to a text or an object that is hard to describe with words. To provoke personalized localization abilities in models, we present a data-centric solution that fine-tunes them using carefully curated data from video object tracking datasets. By leveraging sequences of frames tracking the same object across multiple shots, we simulate instruction-tuning dialogues that promote context awareness. To reinforce this, we introduce a novel regularization technique that replaces object labels with pseudo-names, ensuring the model relies on visual context rather than prior knowledge. Our method significantly enhances the few-shot localization performance of recent VLMs ranging from 7B to 72B in size, without sacrificing generalization, as demonstrated on several benchmarks tailored towards evaluating personalized localization abilities. This work is the first to explore and benchmark personalized few-shot localization for VLMs -- exposing critical weaknesses in present-day VLMs, and laying a foundation for future research in context-driven vision-language applications.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2406.05810.pdf' target='_blank'>https://arxiv.org/pdf/2406.05810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Ma, Ningfei Wang, Zhengyu Zhao, Qian Wang, Qi Alfred Chen, Chao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05810">ControlLoc: Physical-World Hijacking Attack on Visual Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research in adversarial machine learning has focused on visual perception in Autonomous Driving (AD) and has shown that printed adversarial patches can attack object detectors. However, it is important to note that AD visual perception encompasses more than just object detection; it also includes Multiple Object Tracking (MOT). MOT enhances the robustness by compensating for object detection errors and requiring consistent object detection results across multiple frames before influencing tracking results and driving decisions. Thus, MOT makes attacks on object detection alone less effective. To attack such robust AD visual perception, a digital hijacking attack has been proposed to cause dangerous driving scenarios. However, this attack has limited effectiveness.
  In this paper, we introduce a novel physical-world adversarial patch attack, ControlLoc, designed to exploit hijacking vulnerabilities in entire AD visual perception. ControlLoc utilizes a two-stage process: initially identifying the optimal location for the adversarial patch, and subsequently generating the patch that can modify the perceived location and shape of objects with the optimal location. Extensive evaluations demonstrate the superior performance of ControlLoc, achieving an impressive average attack success rate of around 98.1% across various AD visual perceptions and datasets, which is four times greater effectiveness than the existing hijacking attack. The effectiveness of ControlLoc is further validated in physical-world conditions, including real vehicle tests under different conditions such as outdoor light conditions with an average attack success rate of 77.5%. AD system-level impact assessments are also included, such as vehicle collision, using industry-grade AD systems and production-grade AD simulators with an average vehicle collision rate and unnecessary emergency stop rate of 81.3%.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2406.05800.pdf' target='_blank'>https://arxiv.org/pdf/2406.05800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Ma, Ningfei Wang, Zhengyu Zhao, Qi Alfred Chen, Chao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05800">SlowPerception: Physical-World Latency Attack against Visual Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous Driving (AD) systems critically depend on visual perception for real-time object detection and multiple object tracking (MOT) to ensure safe driving. However, high latency in these visual perception components can lead to significant safety risks, such as vehicle collisions. While previous research has extensively explored latency attacks within the digital realm, translating these methods effectively to the physical world presents challenges. For instance, existing attacks rely on perturbations that are unrealistic or impractical for AD, such as adversarial perturbations affecting areas like the sky, or requiring large patches that obscure most of a camera's view, thus making them impossible to be conducted effectively in the real world.
  In this paper, we introduce SlowPerception, the first physical-world latency attack against AD perception, via generating projector-based universal perturbations. SlowPerception strategically creates numerous phantom objects on various surfaces in the environment, significantly increasing the computational load of Non-Maximum Suppression (NMS) and MOT, thereby inducing substantial latency. Our SlowPerception achieves second-level latency in physical-world settings, with an average latency of 2.5 seconds across different AD perception systems, scenarios, and hardware configurations. This performance significantly outperforms existing state-of-the-art latency attacks. Additionally, we conduct AD system-level impact assessments, such as vehicle collisions, using industry-grade AD systems with production-grade AD simulators with a 97% average rate. We hope that our analyses can inspire further research in this critical domain, enhancing the robustness of AD systems against emerging vulnerabilities.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2507.03441.pdf' target='_blank'>https://arxiv.org/pdf/2507.03441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias Zeller, Daniel Casado Herraez, Jens Behley, Michael Heidingsfeld, Cyrill Stachniss
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03441">Radar Tracker: Moving Instance Tracking in Sparse and Noisy Radar Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots and autonomous vehicles should be aware of what happens in their surroundings. The segmentation and tracking of moving objects are essential for reliable path planning, including collision avoidance. We investigate this estimation task for vehicles using radar sensing. We address moving instance tracking in sparse radar point clouds to enhance scene interpretation. We propose a learning-based radar tracker incorporating temporal offset predictions to enable direct center-based association and enhance segmentation performance by including additional motion cues. We implement attention-based tracking for sparse radar scans to include appearance features and enhance performance. The final association combines geometric and appearance features to overcome the limitations of center-based tracking to associate instances reliably. Our approach shows an improved performance on the moving instance tracking benchmark of the RadarScenes dataset compared to the current state of the art.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2502.06583.pdf' target='_blank'>https://arxiv.org/pdf/2502.06583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiantao Hu, Bineng Zhong, Qihua Liang, Zhiyi Mo, Liangtao Shi, Ying Tai, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06583">Adaptive Perception for Unified Visual Multi-modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, many multi-modal trackers prioritize RGB as the dominant modality, treating other modalities as auxiliary, and fine-tuning separately various multi-modal tasks. This imbalance in modality dependence limits the ability of methods to dynamically utilize complementary information from each modality in complex scenarios, making it challenging to fully perceive the advantages of multi-modal. As a result, a unified parameter model often underperforms in various multi-modal tracking tasks. To address this issue, we propose APTrack, a novel unified tracker designed for multi-modal adaptive perception. Unlike previous methods, APTrack explores a unified representation through an equal modeling strategy. This strategy allows the model to dynamically adapt to various modalities and tasks without requiring additional fine-tuning between different tasks. Moreover, our tracker integrates an adaptive modality interaction (AMI) module that efficiently bridges cross-modality interactions by generating learnable tokens. Experiments conducted on five diverse multi-modal datasets (RGBT234, LasHeR, VisEvent, DepthTrack, and VOT-RGBD2022) demonstrate that APTrack not only surpasses existing state-of-the-art unified multi-modal trackers but also outperforms trackers designed for specific multi-modal tasks.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2411.16934.pdf' target='_blank'>https://arxiv.org/pdf/2411.16934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaira Manigrasso, Matteo Dunnhofer, Antonino Furnari, Moritz Nottebaum, Antonio Finocchiaro, Davide Marana, Rosario Forte, Giovanni Maria Farinella, Christian Micheloni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16934">Online Episodic Memory Visual Query Localization with Egocentric Streaming Object Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Episodic memory retrieval enables wearable cameras to recall objects or events previously observed in video. However, existing formulations assume an "offline" setting with full video access at query time, limiting their applicability in real-world scenarios with power and storage-constrained wearable devices. Towards more application-ready episodic memory systems, we introduce Online Visual Query 2D (OVQ2D), a task where models process video streams online, observing each frame only once, and retrieve object localizations using a compact memory instead of full video history. We address OVQ2D with ESOM (Egocentric Streaming Object Memory), a novel framework integrating an object discovery module, an object tracking module, and a memory module that find, track, and store spatio-temporal object information for efficient querying. Experiments on Ego4D demonstrate ESOM's superiority over other online approaches, though OVQ2D remains challenging, with top performance at only ~4% success. ESOM's accuracy increases markedly with perfect object tracking (31.91%), discovery (40.55%), or both (81.92%), underscoring the need of applied research on these components.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2505.03184.pdf' target='_blank'>https://arxiv.org/pdf/2505.03184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Xu, Ruotong Li, Mengjun Yi, Baile XU, Furao Shen, Jian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03184">Interactive Instance Annotation with Siamese Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating instance masks is time-consuming and labor-intensive. A promising solution is to predict contours using a deep learning model and then allow users to refine them. However, most existing methods focus on in-domain scenarios, limiting their effectiveness for cross-domain annotation tasks. In this paper, we propose SiamAnno, a framework inspired by the use of Siamese networks in object tracking. SiamAnno leverages one-shot learning to annotate previously unseen objects by taking a bounding box as input and predicting object boundaries, which can then be adjusted by annotators. Trained on one dataset and tested on another without fine-tuning, SiamAnno achieves state-of-the-art (SOTA) performance across multiple datasets, demonstrating its ability to handle domain and environment shifts in cross-domain tasks. We also provide more comprehensive results compared to previous work, establishing a strong baseline for future research. To our knowledge, SiamAnno is the first model to explore Siamese architecture for instance annotation.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2511.18264.pdf' target='_blank'>https://arxiv.org/pdf/2511.18264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruijie Fan, Junyan Ye, Huan Chen, Zilong Huang, Xiaolei Wang, Weijia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18264">SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing satellite video tracking methods often struggle with generalization, requiring scenario-specific training to achieve satisfactory performance, and are prone to track loss in the presence of occlusion. To address these challenges, we propose SatSAM2, a zero-shot satellite video tracker built on SAM2, designed to adapt foundation models to the remote sensing domain. SatSAM2 introduces two core modules: a Kalman Filter-based Constrained Motion Module (KFCMM) to exploit temporal motion cues and suppress drift, and a Motion-Constrained State Machine (MCSM) to regulate tracking states based on motion dynamics and reliability. To support large-scale evaluation, we propose MatrixCity Video Object Tracking (MVOT), a synthetic benchmark containing 1,500+ sequences and 157K annotated frames with diverse viewpoints, illumination, and occlusion conditions. Extensive experiments on two satellite tracking benchmarks and MVOT show that SatSAM2 outperforms both traditional and foundation model-based trackers, including SAM2 and its variants. Notably, on the OOTB dataset, SatSAM2 achieves a 5.84% AUC improvement over state-of-the-art methods. Our code and dataset will be publicly released to encourage further research.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2502.18748.pdf' target='_blank'>https://arxiv.org/pdf/2502.18748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaheer Mohamed, Tharindu Fernando, Sridha Sridharan, Peyman Moghadam, Clinton Fookes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18748">Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking using snapshot mosaic cameras is emerging as it provides enhanced spectral information alongside spatial data, contributing to a more comprehensive understanding of material properties. Using transformers, which have consistently outperformed convolutional neural networks (CNNs) in learning better feature representations, would be expected to be effective for Hyperspectral object tracking. However, training large transformers necessitates extensive datasets and prolonged training periods. This is particularly critical for complex tasks like object tracking, and the scarcity of large datasets in the hyperspectral domain acts as a bottleneck in achieving the full potential of powerful transformer models. This paper proposes an effective methodology that adapts large pretrained transformer-based foundation models for hyperspectral object tracking. We propose an adaptive, learnable spatial-spectral token fusion module that can be extended to any transformer-based backbone for learning inherent spatial-spectral features in hyperspectral data. Furthermore, our model incorporates a cross-modality training pipeline that facilitates effective learning across hyperspectral datasets collected with different sensor modalities. This enables the extraction of complementary knowledge from additional modalities, whether or not they are present during testing. Our proposed model also achieves superior performance with minimal training iterations.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2512.21641.pdf' target='_blank'>https://arxiv.org/pdf/2512.21641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.21641">TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding natural-language references to objects in dynamic 3D driving scenes is essential for interactive autonomous systems. In practice, many referring expressions describe targets through recent motion or short-term interactions, which cannot be resolved from static appearance or geometry alone. We study temporal language-based 3D grounding, where the objective is to identify the referred object in the current frame by leveraging multi-frame observations. We propose TrackTeller, a temporal multimodal grounding framework that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning in a unified architecture. TrackTeller constructs a shared UniScene representation aligned with textual semantics, generates language-aware 3D proposals, and refines grounding decisions using motion history and short-term dynamics. Experiments on the NuPrompt benchmark demonstrate that TrackTeller consistently improves language-grounded tracking performance, outperforming strong baselines with a 70% relative improvement in Average Multi-Object Tracking Accuracy and a 3.15-3.4 times reduction in False Alarm Frequency.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2509.24741.pdf' target='_blank'>https://arxiv.org/pdf/2509.24741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue-Feng Zhu, Tianyang Xu, Yifan Pan, Jinjie Gu, Xi Li, Jiwen Lu, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24741">Collaborating Vision, Depth, and Thermal Signals for Multi-Modal Tracking: Dataset and Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multi-modal object tracking approaches primarily focus on dual-modal paradigms, such as RGB-Depth or RGB-Thermal, yet remain challenged in complex scenarios due to limited input modalities. To address this gap, this work introduces a novel multi-modal tracking task that leverages three complementary modalities, including visible RGB, Depth (D), and Thermal Infrared (TIR), aiming to enhance robustness in complex scenarios. To support this task, we construct a new multi-modal tracking dataset, coined RGBDT500, which consists of 500 videos with synchronised frames across the three modalities. Each frame provides spatially aligned RGB, depth, and thermal infrared images with precise object bounding box annotations. Furthermore, we propose a novel multi-modal tracker, dubbed RDTTrack. RDTTrack integrates tri-modal information for robust tracking by leveraging a pretrained RGB-only tracking model and prompt learning techniques. In specific, RDTTrack fuses thermal infrared and depth modalities under a proposed orthogonal projection constraint, then integrates them with RGB signals as prompts for the pre-trained foundation tracking model, effectively harmonising tri-modal complementary cues. The experimental results demonstrate the effectiveness and advantages of the proposed method, showing significant improvements over existing dual-modal approaches in terms of tracking accuracy and robustness in complex scenarios.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2508.13000.pdf' target='_blank'>https://arxiv.org/pdf/2508.13000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Hui Li, Shaochuan Zhao, Tao Zhou, Chunyang Cheng, Xiaojun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13000">Omni Survey for Multimodality Analysis in Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of smart cities has led to the generation of massive amounts of multi-modal data in the context of a range of tasks that enable a comprehensive monitoring of the smart city infrastructure and services. This paper surveys one of the most critical tasks, multi-modal visual object tracking (MMVOT), from the perspective of multimodality analysis. Generally, MMVOT differs from single-modal tracking in four key aspects, data collection, modality alignment and annotation, model designing, and evaluation. Accordingly, we begin with an introduction to the relevant data modalities, laying the groundwork for their integration. This naturally leads to a discussion of challenges of multi-modal data collection, alignment, and annotation. Subsequently, existing MMVOT methods are categorised, based on different ways to deal with visible (RGB) and X modalities: programming the auxiliary X branch with replicated or non-replicated experimental configurations from the RGB branch. Here X can be thermal infrared (T), depth (D), event (E), near infrared (NIR), language (L), or sonar (S). The final part of the paper addresses evaluation and benchmarking. In summary, we undertake an omni survey of all aspects of multi-modal visual object tracking (VOT), covering six MMVOT tasks and featuring 338 references in total. In addition, we discuss the fundamental rhetorical question: Is multi-modal tracking always guaranteed to provide a superior solution to unimodal tracking with the help of information fusion, and if not, in what circumstances its application is beneficial. Furthermore, for the first time in this field, we analyse the distributions of the object categories in the existing MMVOT datasets, revealing their pronounced long-tail nature and a noticeable lack of animal categories when compared with RGB datasets.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2407.10485.pdf' target='_blank'>https://arxiv.org/pdf/2407.10485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufeng Yao, Jinlong Peng, Qingdong He, Bo Peng, Hao Chen, Mingmin Chi, Chao Liu, Jon Atli Benediktsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10485">MM-Tracker: Motion Mamba with Margin Loss for UAV-platform Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) platforms requires efficient motion modeling. This is because UAV-MOT faces both local object motion and global camera motion. Motion blur also increases the difficulty of detecting large moving objects. Previous UAV motion modeling approaches either focus only on local motion or ignore motion blurring effects, thus limiting their tracking performance and speed. To address these issues, we propose the Motion Mamba Module, which explores both local and global motion features through cross-correlation and bi-directional Mamba Modules for better motion modeling. To address the detection difficulties caused by motion blur, we also design motion margin loss to effectively improve the detection accuracy of motion blurred objects. Based on the Motion Mamba module and motion margin loss, our proposed MM-Tracker surpasses the state-of-the-art in two widely open-source UAV-MOT datasets. Code will be available.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2403.09634.pdf' target='_blank'>https://arxiv.org/pdf/2403.09634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Chen, Jinglun Li, Zhaoyu Chen, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09634">OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker. OneTracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker. This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object. Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker. Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter-efficient finetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our OneTracker outperforms other models and achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2502.17822.pdf' target='_blank'>https://arxiv.org/pdf/2502.17822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Zhang, Xin Li, Xin Lin, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17822">Easy-Poly: A Easy Polyhedral Framework For 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D multi-object tracking (3D MOT) have predominantly relied on tracking-by-detection pipelines. However, these approaches often neglect potential enhancements in 3D detection processes, leading to high false positives (FP), missed detections (FN), and identity switches (IDS), particularly in challenging scenarios such as crowded scenes, small-object configurations, and adverse weather conditions. Furthermore, limitations in data preprocessing, association mechanisms, motion modeling, and life-cycle management hinder overall tracking robustness. To address these issues, we present Easy-Poly, a real-time, filter-based 3D MOT framework for multiple object categories. Our contributions include: (1) An Augmented Proposal Generator utilizing multi-modal data augmentation and refined SpConv operations, significantly improving mAP and NDS on nuScenes; (2) A Dynamic Track-Oriented (DTO) data association algorithm that effectively manages uncertainties and occlusions through optimal assignment and multiple hypothesis handling; (3) A Dynamic Motion Modeling (DMM) incorporating a confidence-weighted Kalman filter and adaptive noise covariances, enhancing MOTA and AMOTA in challenging conditions; and (4) An extended life-cycle management system with adjustive thresholds to reduce ID switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 64.96% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 74.5%), while also running in real-time. These findings highlight Easy-Poly's adaptability and robustness in diverse scenarios, making it a compelling choice for autonomous driving and related 3D MOT applications. The source code of this paper will be published upon acceptance.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2410.23907.pdf' target='_blank'>https://arxiv.org/pdf/2410.23907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Run Luo, Zikai Song, Longze Chen, Yunshui Li, Min Yang, Wei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23907">IP-MOT: Instance Prompt Learning for Cross-Domain Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) aims to associate multiple objects across video frames and is a challenging vision task due to inherent complexities in the tracking environment. Most existing approaches train and track within a single domain, resulting in a lack of cross-domain generalizability to data from other domains. While several works have introduced natural language representation to bridge the domain gap in visual tracking, these textual descriptions often provide too high-level a view and fail to distinguish various instances within the same class. In this paper, we address this limitation by developing IP-MOT, an end-to-end transformer model for MOT that operates without concrete textual descriptions. Our approach is underpinned by two key innovations: Firstly, leveraging a pre-trained vision-language model, we obtain instance-level pseudo textual descriptions via prompt-tuning, which are invariant across different tracking scenes; Secondly, we introduce a query-balanced strategy, augmented by knowledge distillation, to further boost the generalization capabilities of our model. Extensive experiments conducted on three widely used MOT benchmarks, including MOT17, MOT20, and DanceTrack, demonstrate that our approach not only achieves competitive performance on same-domain data compared to state-of-the-art models but also significantly improves the performance of query-based trackers by large margins for cross-domain inputs.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2404.05351.pdf' target='_blank'>https://arxiv.org/pdf/2404.05351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Umberto Albertin, Alessandro Navone, Mauro Martini, Marcello Chiaberge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05351">Semi-Supervised Novelty Detection for Precise Ultra-Wideband Error Signal Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultra-Wideband (UWB) technology is an emerging low-cost solution for localization in a generic environment. However, UWB signal can be affected by signal reflections and non-line-of-sight (NLoS) conditions between anchors; hence, in a broader sense, the specific geometry of the environment and the disposition of obstructing elements in the map may drastically hinder the reliability of UWB for precise robot localization. This work aims to mitigate this problem by learning a map-specific characterization of the UWB quality signal with a fingerprint semi-supervised novelty detection methodology. An unsupervised autoencoder neural network is trained on nominal UWB map conditions, and then it is used to predict errors derived from the introduction of perturbing novelties in the environment. This work poses a step change in the understanding of UWB localization and its reliability in evolving environmental conditions. The resulting performance of the proposed method is proved by fine-grained experiments obtained with a visual tracking ground truth.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2510.19078.pdf' target='_blank'>https://arxiv.org/pdf/2510.19078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyu Jiang, Wenhao Chai, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19078">UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a growing interest in developing effective alignment pipelines to generate unified representations from different modalities for multi-modal fusion and generation. As an important component of Human-Centric applications, Human Pose representations are critical in many downstream tasks, such as Human Pose Estimation, Action Recognition, Human-Computer Interaction, Object tracking, etc. Human Pose representations or embeddings can be extracted from images, 2D keypoints, 3D skeletons, mesh models, and lots of other modalities. Yet, there are limited instances where the correlation among all of those representations has been clearly researched using a contrastive paradigm. In this paper, we propose UniHPR, a unified Human Pose Representation learning pipeline, which aligns Human Pose embeddings from images, 2D and 3D human poses. To align more than two data representations at the same time, we propose a novel singular value-based contrastive learning loss, which better aligns different modalities and further boosts performance. To evaluate the effectiveness of the aligned representation, we choose 2D and 3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, with a simple 3D human pose decoder, UniHPR achieves remarkable performance metrics: MPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW dataset with cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D pose retrieval with our unified human pose representations in Human3.6M dataset, where the retrieval error is 9.24mm in MPJPE.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2503.08145.pdf' target='_blank'>https://arxiv.org/pdf/2503.08145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Yifan Jiao, Dan Meng, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08145">Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to track objects without being limited to a predefined set of categories. Current OV-MOT methods typically rely primarily on instance-level detection and association, often overlooking trajectory information that is unique and essential for object tracking tasks. Utilizing trajectory information can enhance association stability and classification accuracy, especially in cases of occlusion and category ambiguity, thereby improving adaptability to novel classes. Thus motivated, in this paper we propose \textbf{TRACT}, an open-vocabulary tracker that leverages trajectory information to improve both object association and classification in OV-MOT. Specifically, we introduce a \textit{Trajectory Consistency Reinforcement} (\textbf{TCR}) strategy, that benefits tracking performance by improving target identity and category consistency. In addition, we present \textbf{TraCLIP}, a plug-and-play trajectory classification module. It integrates \textit{Trajectory Feature Aggregation} (\textbf{TFA}) and \textit{Trajectory Semantic Enrichment} (\textbf{TSE}) strategies to fully leverage trajectory information from visual and language perspectives for enhancing the classification results. Extensive experiments on OV-TAO show that our TRACT significantly improves tracking performance, highlighting trajectory information as a valuable asset for OV-MOT. Code will be released.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2506.20381.pdf' target='_blank'>https://arxiv.org/pdf/2506.20381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Kang, Xin Chen, Jie Zhao, Chunjuan Bo, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20381">Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based visual trackers have demonstrated significant advancements due to their powerful modeling capabilities. However, their practicality is limited on resource-constrained devices because of their slow processing speeds. To address this challenge, we present HiT, a novel family of efficient tracking models that achieve high performance while maintaining fast operation across various devices. The core innovation of HiT lies in its Bridge Module, which connects lightweight transformers to the tracking framework, enhancing feature representation quality. Additionally, we introduce a dual-image position encoding approach to effectively encode spatial information. HiT achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark, outperforming all previous efficient trackers.Building on HiT, we propose DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by selecting routes with varying computational requirements. DyHiT uses search area features extracted by the backbone network and inputs them into an efficient dynamic router to classify tracking scenarios. Based on the classification, DyHiT applies a divide-and-conquer strategy, selecting appropriate routes to achieve a superior trade-off between accuracy and speed. The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free acceleration method based on the dynamic routing architecture of DyHiT. This method significantly improves the execution speed of various high-performance trackers without sacrificing accuracy. For instance, our acceleration method enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of 69.9% on the LaSOT.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2503.22199.pdf' target='_blank'>https://arxiv.org/pdf/2503.22199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Gao, Yunhe Zhang, Langkun Chen, Yan Jiang, Weiying Xie, Yunsong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22199">Hyperspectral Adapter for Object Tracking based on Hyperspectral Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking based on hyperspectral video attracts increasing attention to the rich material and motion information in the hyperspectral videos. The prevailing hyperspectral methods adapt pretrained RGB-based object tracking networks for hyperspectral tasks by fine-tuning the entire network on hyperspectral datasets, which achieves impressive results in challenging scenarios. However, the performance of hyperspectral trackers is limited by the loss of spectral information during the transformation, and fine-tuning the entire pretrained network is inefficient for practical applications. To address the issues, a new hyperspectral object tracking method, hyperspectral adapter for tracking (HyA-T), is proposed in this work. The hyperspectral adapter for the self-attention (HAS) and the hyperspectral adapter for the multilayer perceptron (HAM) are proposed to generate the adaption information and to transfer the multi-head self-attention (MSA) module and the multilayer perceptron (MLP) in pretrained network for the hyperspectral object tracking task by augmenting the adaption information into the calculation of the MSA and MLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed to augment the original spectral information into the input of the tracking network. The proposed methods extract spectral information directly from the hyperspectral images, which prevent the loss of the spectral information. Moreover, only the parameters in the proposed methods are fine-tuned, which is more efficient than the existing methods. Extensive experiments were conducted on four datasets with various spectral bands, verifing the effectiveness of the proposed methods. The HyA-T achieves state-of-the-art performance on all the datasets.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2412.19138.pdf' target='_blank'>https://arxiv.org/pdf/2412.19138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19138">SUTrack: Towards Simple and Unified Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a simple yet unified single object tracking (SOT) framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based, RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model trained in a single session. Due to the distinct nature of the data, current methods typically design individual architectures and train separate models for each task. This fragmentation results in redundant training processes, repetitive technological innovations, and limited cross-modal knowledge sharing. In contrast, SUTrack demonstrates that a single model with a unified input representation can effectively handle various common SOT tasks, eliminating the need for task-specific designs and separate training sessions. Additionally, we introduce a task-recognition auxiliary training strategy and a soft token type embedding to further enhance SUTrack's performance with minimal overhead. Experiments show that SUTrack outperforms previous task-specific counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a range of models catering edge devices as well as high-performance GPUs, striking a good trade-off between speed and accuracy. We hope SUTrack could serve as a strong foundation for further compelling research into unified tracking models. Code and models are available at github.com/chenxin-dlut/SUTrack.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2403.17651.pdf' target='_blank'>https://arxiv.org/pdf/2403.17651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Zhu, Xin Chen, Haiwen Diao, Shuai Li, Jun-Yan He, Chenyang Li, Bin Luo, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17651">Exploring Dynamic Transformer for Efficient Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The speed-precision trade-off is a critical problem for visual object tracking which usually requires low latency and deployment on constrained resources. Existing solutions for efficient tracking mainly focus on adopting light-weight backbones or modules, which nevertheless come at the cost of a sacrifice in precision. In this paper, inspired by dynamic network routing, we propose DyTrack, a dynamic transformer framework for efficient tracking. Real-world tracking scenarios exhibit diverse levels of complexity. We argue that a simple network is sufficient for easy frames in video sequences, while more computation could be assigned to difficult ones. DyTrack automatically learns to configure proper reasoning routes for various inputs, gaining better utilization of the available computational budget. Thus, it can achieve higher performance with the same running speed. We formulate instance-specific tracking as a sequential decision problem and attach terminating branches to intermediate layers of the entire model. Especially, to fully utilize the computations, we introduce the feature recycling mechanism to reuse the outputs of predecessors. Furthermore, a target-aware self-distillation strategy is designed to enhance the discriminating capabilities of early predictions by effectively mimicking the representation pattern of the deep model. Extensive experiments on multiple benchmarks demonstrate that DyTrack achieves promising speed-precision trade-offs with only a single model. For instance, DyTrack obtains 64.9% AUC on LaSOT with a speed of 256 fps.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2402.12763.pdf' target='_blank'>https://arxiv.org/pdf/2402.12763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Jinlin Wu, Jian Chen, Lujie Li, Hongbin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12763">BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localizing the bronchoscope in real time is essential for ensuring intervention quality. However, most existing methods struggle to balance between speed and generalization. To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway association.To achieve real-time performance, we employ a benchmark lightweight detector for efficient lumen detection. We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures. To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway graph that encodes the hierarchy of bronchial tree structures.Experiments on nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64 \%, while accessing up to the 4th generation of airways.Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.Experimental evaluation underscores BronchoTrack's real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2511.12026.pdf' target='_blank'>https://arxiv.org/pdf/2511.12026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rulin Zhou, Wenlong He, An Wang, Jianhang Zhang, Xuanhui Zeng, Xi Zhang, Chaowei Zhu, Haijun Hu, Hongliang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12026">Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2510.00181.pdf' target='_blank'>https://arxiv.org/pdf/2510.00181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis Burbano, Diego Ortiz, Qi Sun, Siwei Yang, Haoqin Tu, Cihang Xie, Yinzhi Cao, Alvaro A Cardenas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00181">CHAI: Command Hijacking against embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms state-of-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2411.06702.pdf' target='_blank'>https://arxiv.org/pdf/2411.06702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Syuen Lim, Yadan Luo, Zhi Chen, Tianqi Wei, Scott Chapman, Zi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06702">Track Any Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Detection and Multi-Object Tracking of Sweet Peppers Challenge, we present Track Any Peppers (TAP) - a weakly supervised ensemble technique for sweet peppers tracking. TAP leverages the zero-shot detection capabilities of vision-language foundation models like Grounding DINO to automatically generate pseudo-labels for sweet peppers in video sequences with minimal human intervention. These pseudo-labels, refined when necessary, are used to train a YOLOv8 segmentation network. To enhance detection accuracy under challenging conditions, we incorporate pre-processing techniques such as relighting adjustments and apply depth-based filtering during post-inference. For object tracking, we integrate the Matching by Segment Anything (MASA) adapter with the BoT-SORT algorithm. Our approach achieves a HOTA score of 80.4%, MOTA of 66.1%, Recall of 74.0%, and Precision of 90.7%, demonstrating effective tracking of sweet peppers without extensive manual effort. This work highlights the potential of foundation models for efficient and accurate object detection and tracking in agricultural settings.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2409.04979.pdf' target='_blank'>https://arxiv.org/pdf/2409.04979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Lin, Zhe Liu, Yongtao Wang, Le Zhang, Ce Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04979">RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2509.19851.pdf' target='_blank'>https://arxiv.org/pdf/2509.19851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Bogenberger, Oliver Harrison, Orrin Dahanaggamaarachchi, Lukas Brunke, Jingxing Qian, Siqi Zhou, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19851">Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots deployed in real-world environments, such as homes, must not only navigate safely but also understand their surroundings and adapt to environment changes. To perform tasks efficiently, they must build and maintain a semantic map that accurately reflects the current state of the environment. Existing research on semantic exploration largely focuses on static scenes without persistent object-level instance tracking. A consistent map is, however, crucial for real-world robotic applications where objects in the environment can be removed, reintroduced, or shifted over time. In this work, to close this gap, we propose an open-vocabulary, semantic exploration system for semi-static environments. Our system maintains a consistent map by building a probabilistic model of object instance stationarity, systematically tracking semi-static changes, and actively exploring areas that have not been visited for a prolonged period of time. In addition to active map maintenance, our approach leverages the map's semantic richness with LLM-based reasoning for open-vocabulary object-goal navigation. This enables the robot to search more efficiently by prioritizing contextually relevant areas. We evaluate our approach across multiple real-world semi-static environments. Our system detects 95% of map changes on average, improving efficiency by more than 29% as compared to random and patrol baselines. Overall, our approach achieves a mapping precision within 2% of a fully rebuilt map while requiring substantially less exploration and further completes object goal navigation tasks about 14% faster than the next-best tested strategy (coverage patrolling). A video of our work can be found at http://tiny.cc/sem-explor-semi-static .
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2412.12561.pdf' target='_blank'>https://arxiv.org/pdf/2412.12561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjun Huang, Yang Ni, Hanning Chen, Yirui He, Ian Bryant, Yezi Liu, Mohsen Imani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12561">Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to localize an arbitrary number of targets based on a language expression and continuously track them in a video. This intricate task involves reasoning on multi-modal data and precise target localization with temporal association. However, prior studies overlook the imbalanced data distribution between newborn targets and existing targets due to the nature of the task. In addition, they only indirectly fuse multi-modal features, struggling to deliver clear guidance on newborn target detection. To solve the above issues, we conduct a collaborative matching strategy to alleviate the impact of the imbalance, boosting the ability to detect newborn targets while maintaining tracking performance. In the encoder, we integrate and enhance the cross-modal and multi-scale fusion, overcoming the bottlenecks in previous work, where limited multi-modal information is shared and interacted between feature maps. In the decoder, we also develop a referring-infused adaptation that provides explicit referring guidance through the query tokens. The experiments showcase the superior performance of our model (+3.42%) compared to prior works, demonstrating the effectiveness of our designs.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2508.01599.pdf' target='_blank'>https://arxiv.org/pdf/2508.01599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minhaj Uddin Ahmad, Sagar Dasgupta, Mizanur Rahman, Sakib Khan, Md Wasiul Haque, Suhala Rabab Saba, David Bodoh, Nathan Huynh, Li Zhao, Eren Erman Ozguven
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01599">Lessons Learned from the Real-World Deployment of Multi-Sensor Fusion for Proactive Work Zone Safety Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proactive safety systems that anticipate and mitigate traffic risks before incidents occur are increasingly recognized as essential for improving work zone safety. Unlike traditional reactive methods, these systems rely on real-time sensing, trajectory prediction, and intelligent infrastructure to detect potential hazards. Existing simulation-based studies often overlook, and real-world deployment studies rarely discuss the practical challenges associated with deploying such systems in operational settings, particularly those involving roadside infrastructure and multi-sensor integration and fusion. This study addresses that gap by presenting deployment insights and technical lessons learned from a real-world implementation of a multi-sensor safety system at an active bridge repair work zone along the N-2/US-75 corridor in Lincoln, Nebraska. The deployed system combines LiDAR, radar, and camera sensors with an edge computing platform to support multi-modal object tracking, trajectory fusion, and real-time analytics. Specifically, this study presents key lessons learned across three critical stages of deployment: (1) sensor selection and placement, (2) sensor calibration, system integration, and validation, and (3) sensor fusion. Additionally, we propose a predictive digital twin framework that leverages fused trajectory data for early conflict detection and real-time warning generation, enabling proactive safety interventions.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2406.01559.pdf' target='_blank'>https://arxiv.org/pdf/2406.01559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Han, Yawen Lu, Guohao Sun, James C. Liang, Zhiwen Cao, Qifan Wang, Qiang Guan, Sohail A. Dianat, Raghuveer M. Rao, Tong Geng, Zhiqiang Tao, Dongfang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01559">Prototypical Transformer as Unified Motion Learners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce the Prototypical Transformer (ProtoFormer), a general and unified framework that approaches various motion tasks from a prototype perspective. ProtoFormer seamlessly integrates prototype learning with Transformer by thoughtfully considering motion dynamics, introducing two innovative designs. First, Cross-Attention Prototyping discovers prototypes based on signature motion patterns, providing transparency in understanding motion scenes. Second, Latent Synchronization guides feature representation learning via prototypes, effectively mitigating the problem of motion uncertainty. Empirical results demonstrate that our approach achieves competitive performance on popular motion tasks such as optical flow and scene depth. Furthermore, it exhibits generality across various downstream tasks, including object tracking and video stabilization.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2403.10830.pdf' target='_blank'>https://arxiv.org/pdf/2403.10830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deyi Ji, Siqi Gao, Lanyun Zhu, Qi Zhu, Yiru Zhao, Peng Xu, Hongtao Lu, Feng Zhao, Jieping Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10830">View-Centric Multi-Object Tracking with Homographic Matching in Moving UAV</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the challenge of multi-object tracking (MOT) in moving Unmanned Aerial Vehicle (UAV) scenarios, where irregular flight trajectories, such as hovering, turning left/right, and moving up/down, lead to significantly greater complexity compared to fixed-camera MOT. Specifically, changes in the scene background not only render traditional frame-to-frame object IOU association methods ineffective but also introduce significant view shifts in the objects, which complicates tracking. To overcome these issues, we propose a novel universal HomView-MOT framework, which for the first time, harnesses the view Homography inherent in changing scenes to solve MOT challenges in moving environments, incorporating Homographic Matching and View-Centric concepts. We introduce a Fast Homography Estimation (FHE) algorithm for rapid computation of Homography matrices between video frames, enabling object View-Centric ID Learning (VCIL) and leveraging multi-view Homography to learn cross-view ID features. Concurrently, our Homographic Matching Filter (HMF) maps object bounding boxes from different frames onto a common view plane for a more realistic physical IOU association. Extensive experiments have proven that these innovations allow HomView-MOT to achieve state-of-the-art performance on prominent UAV MOT datasets VisDrone and UAVDT.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2503.11496.pdf' target='_blank'>https://arxiv.org/pdf/2503.11496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofeng Liang, Runwei Guan, Wangwang Lian, Daizong Liu, Xiaolou Sun, Dongming Wu, Yutao Yue, Weiping Ding, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11496">Cognitive Disentanglement for Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a significant application of multi-source information fusion in intelligent transportation perception systems, Referring Multi-Object Tracking (RMOT) involves localizing and tracking specific objects in video sequences based on language references. However, existing RMOT approaches often treat language descriptions as holistic embeddings and struggle to effectively integrate the rich semantic information contained in language expressions with visual features. This limitation is especially apparent in complex scenes requiring comprehensive understanding of both static object attributes and spatial motion information. In this paper, we propose a Cognitive Disentanglement for Referring Multi-Object Tracking (CDRMT) framework that addresses these challenges. It adapts the "what" and "where" pathways from the human visual processing system to RMOT tasks. Specifically, our framework first establishes cross-modal connections while preserving modality-specific characteristics. It then disentangles language descriptions and hierarchically injects them into object queries, refining object understanding from coarse to fine-grained semantic levels. Finally, we reconstruct language representations based on visual features, ensuring that tracked objects faithfully reflect the referring expression. Extensive experiments on different benchmark datasets demonstrate that CDRMT achieves substantial improvements over state-of-the-art methods, with average gains of 6.0% in HOTA score on Refer-KITTI and 3.2% on Refer-KITTI-V2. Our approach advances the state-of-the-art in RMOT while simultaneously providing new insights into multi-source information fusion.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2501.02467.pdf' target='_blank'>https://arxiv.org/pdf/2501.02467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhou, Jinglun Li, Lingyi Hong, Kaixun Jiang, Pinxue Guo, Weifeng Ge, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02467">DeTrack: In-model Latent Denoising Learning for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the model's robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we utilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2503.22943.pdf' target='_blank'>https://arxiv.org/pdf/2503.22943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Wang, Ruishan Guo, Pengtao Ma, Ciyu Ruan, Xinyu Luo, Wenhua Ding, Tianyang Zhong, Jingao Xu, Yunhao Liu, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22943">Event Camera Meets Resource-Aware Mobile Computing: Abstraction, Algorithm, Acceleration, Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in achieving high-accuracy and low-latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution and low latency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, lack of stable, persistent semantic information, and large data volume pose challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature from 2014 to 2025 and presents a comprehensive overview of event-based mobile sensing, encompassing its fundamental principles, event \textit{abstraction} methods, \textit{algorithm} advancements, and both hardware and software \textit{acceleration} strategies. We discuss key \textit{applications} of event cameras in mobile sensing, including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving the event camera with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms. To support ongoing research, we provide an open-source \textit{Online Sheet} with recent developments. We hope this survey serves as a reference, facilitating the adoption of event-based vision across diverse applications.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2412.09991.pdf' target='_blank'>https://arxiv.org/pdf/2412.09991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengmeng Wang, Teli Ma, Shuo Xin, Xiaojun Hou, Jiazheng Xing, Guang Dai, Jingdong Wang, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09991">Visual Object Tracking across Diverse Data Modalities: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is an attractive and significant research area in computer vision, which aims to recognize and track specific targets in video sequences where the target objects are arbitrary and class-agnostic. The VOT technology could be applied in various scenarios, processing data of diverse modalities such as RGB, thermal infrared and point cloud. Besides, since no one sensor could handle all the dynamic and varying environments, multi-modal VOT is also investigated. This paper presents a comprehensive survey of the recent progress of both single-modal and multi-modal VOT, especially the deep learning methods. Specifically, we first review three types of mainstream single-modal VOT, including RGB, thermal infrared and point cloud tracking. In particular, we conclude four widely-used single-modal frameworks, abstracting their schemas and categorizing the existing inheritors. Then we summarize four kinds of multi-modal VOT, including RGB-Depth, RGB-Thermal, RGB-LiDAR and RGB-Language. Moreover, the comparison results in plenty of VOT benchmarks of the discussed modalities are presented. Finally, we provide recommendations and insightful observations, inspiring the future development of this fast-growing literature.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2406.02147.pdf' target='_blank'>https://arxiv.org/pdf/2406.02147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Tang, Lijun Zhou, Pengkun Hao, Zihang He, Kalok Ho, Shuo Gu, Zhihui Hao, Haiyang Sun, Kun Zhan, Peng Jia, XianPeng Lang, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02147">S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multiple object tracking (MOT) plays a crucial role in autonomous driving perception. Recent end-to-end query-based trackers simultaneously detect and track objects, which have shown promising potential for the 3D MOT task. However, existing methods are still in the early stages of development and lack systematic improvements, failing to track objects in certain complex scenarios, like occlusions and the small size of target object's situations. In this paper, we first summarize the current end-to-end 3D MOT framework by decomposing it into three constituent parts: query initialization, query propagation, and query matching. Then we propose corresponding improvements, which lead to a strong yet simple tracker: S2-Track. Specifically, for query initialization, we present 2D-Prompted Query Initialization, which leverages predicted 2D object and depth information to prompt an initial estimate of the object's 3D location. For query propagation, we introduce an Uncertainty-aware Probabilistic Decoder to capture the uncertainty of complex environment in object prediction with probabilistic attention. For query matching, we propose a Hierarchical Query Denoising strategy to enhance training robustness and convergence. As a result, our S2-Track achieves state-of-the-art performance on nuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous best end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st place on the nuScenes tracking task leaderboard.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2511.11478.pdf' target='_blank'>https://arxiv.org/pdf/2511.11478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat Chung, Taisei Hanyu, Toan Nguyen, Huy Le, Frederick Bumgarner, Duy Minh Ho Nguyen, Khoa Vo, Kashu Yamazaki, Chase Rainwater, Tung Kieu, Anh Nguyen, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11478">Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2505.19990.pdf' target='_blank'>https://arxiv.org/pdf/2505.19990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Hong, Shilin Yan, Zehao Xiao, Jiayin Cai, Xiaolong Jiang, Yao Hu, Henghui Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19990">Progressive Scaling Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a progressive scaling training strategy for visual object tracking, systematically analyzing the influence of training data volume, model size, and input resolution on tracking performance. Our empirical study reveals that while scaling each factor leads to significant improvements in tracking accuracy, naive training suffers from suboptimal optimization and limited iterative refinement. To address this issue, we introduce DT-Training, a progressive scaling framework that integrates small teacher transfer and dual-branch alignment to maximize model potential. The resulting scaled tracker consistently outperforms state-of-the-art methods across multiple benchmarks, demonstrating strong generalization and transferability of the proposed method. Furthermore, we validate the broader applicability of our approach to additional tasks, underscoring its versatility beyond tracking.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2505.18111.pdf' target='_blank'>https://arxiv.org/pdf/2505.18111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Yen Yang, Hsiang-Wei Huang, Pyong-Kun Kim, Chien-Kai Kuo, Jui-Wei Chang, Kwang-Ju Kim, Chung-I Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18111">Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an effective approach for adapting the Segment Anything Model 2 (SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the powerful pre-trained capabilities of SAM2 and incorporates several key techniques to enhance its performance in VOT applications. By combining SAM2 with our proposed optimizations, we achieved a first place AUC score of 89.4 on the 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the effectiveness of our approach. This paper details our methodology, the specific enhancements made to SAM2, and a comprehensive analysis of our results in the context of VOT solutions along with the multi-modality aspect of the dataset.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2503.01907.pdf' target='_blank'>https://arxiv.org/pdf/2503.01907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunjun Li, Cheng-Yen Yang, Hsiang-Wei Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01907">Technical Report for ReID-SAM on SkiTB Visual Tracking Challenge 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report introduces ReID-SAM, a novel model developed for the SkiTB Challenge that addresses the complexities of tracking skier appearance. Our approach integrates the SAMURAI tracker with a person re-identification (Re-ID) module and advanced post-processing techniques to enhance accuracy in challenging skiing scenarios. We employ an OSNet-based Re-ID model to minimize identity switches and utilize YOLOv11 with Kalman filtering or STARK-based object detection for precise equipment tracking. When evaluated on the SkiTB dataset, ReID-SAM achieved a state-of-the-art F1-score of 0.870, surpassing existing methods across alpine, ski jumping, and freestyle skiing disciplines. These results demonstrate significant advancements in skier tracking accuracy and provide valuable insights for computer vision applications in winter sports.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2502.01896.pdf' target='_blank'>https://arxiv.org/pdf/2502.01896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nastaran Darabi, Divake Kumar, Sina Tayebati, Amit Ranjan Trivedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01896">INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present INTACT, a novel two-phase framework designed to enhance the robustness of deep neural networks (DNNs) against noisy LiDAR data in safety-critical perception tasks. INTACT combines meta-learning with adversarial curriculum training (ACT) to systematically address challenges posed by data corruption and sparsity in 3D point clouds. The meta-learning phase equips a teacher network with task-agnostic priors, enabling it to generate robust saliency maps that identify critical data regions. The ACT phase leverages these saliency maps to progressively expose a student network to increasingly complex noise patterns, ensuring targeted perturbation and improved noise resilience. INTACT's effectiveness is demonstrated through comprehensive evaluations on object detection, tracking, and classification benchmarks using diverse datasets, including KITTI, Argoverse, and ModelNet40. Results indicate that INTACT improves model robustness by up to 20% across all tasks, outperforming standard adversarial and curriculum training methods. This framework not only addresses the limitations of conventional training strategies but also offers a scalable and efficient solution for real-world deployment in resource-constrained safety-critical systems. INTACT's principled integration of meta-learning and adversarial training establishes a new paradigm for noise-tolerant 3D perception in safety-critical applications. INTACT improved KITTI Multiple Object Tracking Accuracy (MOTA) by 9.6% (64.1% -> 75.1%) and by 12.4% under Gaussian noise (52.5% -> 73.7%). Similarly, KITTI mean Average Precision (mAP) rose from 59.8% to 69.8% (50% point drop) and 49.3% to 70.9% (Gaussian noise), highlighting the framework's ability to enhance deep learning model resilience in safety-critical object tracking scenarios.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2408.09191.pdf' target='_blank'>https://arxiv.org/pdf/2408.09191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Wang, Yongcai Wang, Zhimin Xu, Yongyu Guo, Wanting Li, Zhe Huang, Xuewei Bai, Deying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09191">GSLAMOT: A Tracklet and Query Graph-based Simultaneous Locating, Mapping, and Multiple Object Tracking System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For interacting with mobile objects in unfamiliar environments, simultaneously locating, mapping, and tracking the 3D poses of multiple objects are crucially required. This paper proposes a Tracklet Graph and Query Graph-based framework, i.e., GSLAMOT, to address this challenge. GSLAMOT utilizes camera and LiDAR multimodal information as inputs and divides the representation of the dynamic scene into a semantic map for representing the static environment, a trajectory of the ego-agent, and an online maintained Tracklet Graph (TG) for tracking and predicting the 3D poses of the detected mobile objects. A Query Graph (QG) is constructed in each frame by object detection to query and update TG. For accurate object association, a Multi-criteria Star Graph Association (MSGA) method is proposed to find matched objects between the detections in QG and the predicted tracklets in TG. Then, an Object-centric Graph Optimization (OGO) method is proposed to simultaneously optimize the TG, the semantic map, and the agent trajectory. It triangulates the detected objects into the map to enrich the map's semantic information. We address the efficiency issues to handle the three tightly coupled tasks in parallel. Experiments are conducted on KITTI, Waymo, and an emulated Traffic Congestion dataset that highlights challenging scenarios. Experiments show that GSLAMOT enables accurate crowded object tracking while conducting SLAM accurately in challenging scenarios, demonstrating more excellent performances than the state-of-the-art methods. The code and dataset are at https://gslamot.github.io.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2407.13937.pdf' target='_blank'>https://arxiv.org/pdf/2407.13937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng-Yao Kuan, Jen-Hao Cheng, Hsiang-Wei Huang, Wenhao Chai, Cheng-Yen Yang, Hugo Latapie, Gaowen Liu, Bing-Fei Wu, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13937">Boosting Online 3D Multi-Object Tracking through Camera-Radar Cross Check</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of autonomous driving, the integration of multi-modal perception techniques based on data from diverse sensors has demonstrated substantial progress. Effectively surpassing the capabilities of state-of-the-art single-modality detectors through sensor fusion remains an active challenge. This work leverages the respective advantages of cameras in perspective view and radars in Bird's Eye View (BEV) to greatly enhance overall detection and tracking performance. Our approach, Camera-Radar Associated Fusion Tracking Booster (CRAFTBooster), represents a pioneering effort to enhance radar-camera fusion in the tracking stage, contributing to improved 3D MOT accuracy. The superior experimental results on the K-Radaar dataset, which exhibit 5-6% on IDF1 tracking performance gain, validate the potential of effective sensor fusion in advancing autonomous driving.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2407.09051.pdf' target='_blank'>https://arxiv.org/pdf/2407.09051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wang, Yongcai Wang, Deying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09051">DroneMOT: Drone-based Multi-Object Tracking Considering Detection Difficulties and Simultaneous Moving of Drones and Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) on static platforms, such as by surveillance cameras, has achieved significant progress, with various paradigms providing attractive performances. However, the effectiveness of traditional MOT methods is significantly reduced when it comes to dynamic platforms like drones. This decrease is attributed to the distinctive challenges in the MOT-on-drone scenario: (1) objects are generally small in the image plane, blurred, and frequently occluded, making them challenging to detect and recognize; (2) drones move and see objects from different angles, causing the unreliability of the predicted positions and feature embeddings of the objects. This paper proposes DroneMOT, which firstly proposes a Dual-domain Integrated Attention (DIA) module that considers the fast movements of drones to enhance the drone-based object detection and feature embedding for small-sized, blurred, and occluded objects. Then, an innovative Motion-Driven Association (MDA) scheme is introduced, considering the concurrent movements of both the drone and the objects. Within MDA, an Adaptive Feature Synchronization (AFS) technique is presented to update the object features seen from different angles. Additionally, a Dual Motion-based Prediction (DMP) method is employed to forecast the object positions. Finally, both the refined feature embeddings and the predicted positions are integrated to enhance the object association. Comprehensive evaluations on VisDrone2019-MOT and UAVDT datasets show that DroneMOT provides substantial performance improvements over the state-of-the-art in the domain of MOT on drones.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2403.10826.pdf' target='_blank'>https://arxiv.org/pdf/2403.10826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiang-Wei Huang, Cheng-Yen Yang, Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10826">MambaMOT: State-Space Model as Motion Predictor for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of multi-object tracking (MOT), traditional methods often rely on the Kalman filter for motion prediction, leveraging its strengths in linear motion scenarios. However, the inherent limitations of these methods become evident when confronted with complex, nonlinear motions and occlusions prevalent in dynamic environments like sports and dance. This paper explores the possibilities of replacing the Kalman filter with a learning-based motion model that effectively enhances tracking accuracy and adaptability beyond the constraints of Kalman filter-based tracker. In this paper, our proposed method MambaMOT and MambaMOT+, demonstrate advanced performance on challenging MOT datasets such as DanceTrack and SportsMOT, showcasing their ability to handle intricate, non-linear motion patterns and frequent occlusions more effectively than traditional methods.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2507.09095.pdf' target='_blank'>https://arxiv.org/pdf/2507.09095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Hasan Shahriar, Md Mohaimin Al Barat, Harshavardhan Sundar, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09095">On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal fusion (MMF) plays a critical role in the perception of autonomous driving, which primarily fuses camera and LiDAR streams for a comprehensive and efficient scene understanding. However, its strict reliance on precise temporal synchronization exposes it to new vulnerabilities. In this paper, we introduce DejaVu, a novel attack that exploits network-induced delays to create subtle temporal misalignments across sensor streams, severely degrading downstream MMF-based perception tasks. Our comprehensive attack analysis across different models and datasets reveals these sensors' task-specific imbalanced sensitivities: object detection is overly dependent on LiDAR inputs while object tracking is highly reliant on the camera inputs. Consequently, with a single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to 88.5%, while with a three-frame camera delay, multiple object tracking accuracy (MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense patch that can work alongside the existing perception model to monitor temporal alignment through cross-modal temporal consistency. AION leverages multimodal shared representation learning and dynamic time warping to determine the path of temporal alignment and calculate anomaly scores based on the alignment. Our thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with low false positives across datasets and model architectures, demonstrating it as a robust and generalized defense against the temporal misalignment attacks.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2507.05663.pdf' target='_blank'>https://arxiv.org/pdf/2507.05663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neelay Joglekar, Fei Liu, Florian Richter, Michael C. Yip
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05663">Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote Center of Motion (RCM) robotic manipulators have revolutionized Minimally Invasive Surgery, enabling precise, dexterous surgical manipulation within the patient's body cavity without disturbing the insertion point on the patient. Accurate RCM tool control is vital for incorporating autonomous subtasks like suturing, blood suction, and tumor resection into robotic surgical procedures, reducing surgeon fatigue and improving patient outcomes. However, these cable-driven systems are subject to significant joint reading errors, corrupting the kinematics computation necessary to perform control. Although visual tracking with endoscopic cameras can correct errors on in-view joints, errors in the kinematic chain prior to the insertion point are irreparable because they remain out of view. No prior work has characterized the stability of control under these conditions. We fill this gap by designing a provably stable tracking-in-the-loop controller for the out-of-view portion of the RCM manipulator kinematic chain. We additionally incorporate this controller into a bilevel control scheme for the full kinematic chain. We rigorously benchmark our method in simulated and real world settings to verify our theoretical findings. Our work provides key insights into the next steps required for the transition from teleoperated to autonomous surgery.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2405.04390.pdf' target='_blank'>https://arxiv.org/pdf/2405.04390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Min, Dawei Zhao, Liang Xiao, Jian Zhao, Xinli Xu, Zheng Zhu, Lei Jin, Jianshu Li, Yulan Guo, Junliang Xing, Liping Jing, Yiming Nie, Bin Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04390">DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-centric autonomous driving has recently raised wide attention due to its lower cost. Pre-training is essential for extracting a universal representation. However, current vision-centric pre-training typically relies on either 2D or 3D pre-text tasks, overlooking the temporal characteristics of autonomous driving as a 4D scene understanding task. In this paper, we address this challenge by introducing a world model-based autonomous driving 4D representation learning framework, dubbed \emph{DriveWorld}, which is capable of pre-training from multi-camera driving videos in a spatio-temporal fashion. Specifically, we propose a Memory State-Space Model for spatio-temporal modelling, which consists of a Dynamic Memory Bank module for learning temporal-aware latent dynamics to predict future changes and a Static Scene Propagation module for learning spatial-aware latent statics to offer comprehensive scene contexts. We additionally introduce a Task Prompt to decouple task-aware features for various downstream tasks. The experiments demonstrate that DriveWorld delivers promising results on various autonomous driving tasks. When pre-trained with the OpenScene dataset, DriveWorld achieves a 7.5% increase in mAP for 3D object detection, a 3.0% increase in IoU for online mapping, a 5.0% increase in AMOTA for multi-object tracking, a 0.1m decrease in minADE for motion forecasting, a 3.0% increase in IoU for occupancy prediction, and a 0.34m reduction in average L2 error for planning.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2403.16410.pdf' target='_blank'>https://arxiv.org/pdf/2403.16410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijia Guo, Yuanxi Bai, Liwen Hu, Mianzhi Liu, Ziyi Guo, Lei Ma, Tiejun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16410">Spike-NeRF: Neural Radiance Field Based On Spike Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a neuromorphic sensor with high temporal resolution, spike cameras offer notable advantages over traditional cameras in high-speed vision applications such as high-speed optical estimation, depth estimation, and object tracking. Inspired by the success of the spike camera, we proposed Spike-NeRF, the first Neural Radiance Field derived from spike data, to achieve 3D reconstruction and novel viewpoint synthesis of high-speed scenes. Instead of the multi-view images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike streams captured by a moving spike camera in a very short time. To reconstruct a correct and stable 3D scene from high-frequency but unstable spike data, we devised spike masks along with a distinctive loss function. We evaluate our method qualitatively and numerically on several challenging synthetic scenes generated by blender with the spike camera simulator. Our results demonstrate that Spike-NeRF produces more visually appealing results than the existing methods and the baseline we proposed in high-speed scenes. Our code and data will be released soon.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2402.14392.pdf' target='_blank'>https://arxiv.org/pdf/2402.14392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhou, Pinxue Guo, Lingyi Hong, Jinglun Li, Wei Zhang, Weifeng Ge, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14392">Reading Relevant Feature from Global Representation Memory for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reference features from a template or historical frames are crucial for visual object tracking. Prior works utilize all features from a fixed template or memory for visual object tracking. However, due to the dynamic nature of videos, the required reference historical information for different search regions at different time steps is also inconsistent. Therefore, using all features in the template and memory can lead to redundancy and impair tracking performance. To alleviate this issue, we propose a novel tracking paradigm, consisting of a relevance attention mechanism and a global representation memory, which can adaptively assist the search region in selecting the most relevant historical information from reference features. Specifically, the proposed relevance attention mechanism in this work differs from previous approaches in that it can dynamically choose and build the optimal global representation memory for the current frame by accessing cross-frame information globally. Moreover, it can flexibly read the relevant historical information from the constructed memory to reduce redundancy and counteract the negative effects of harmful information. Extensive experiments validate the effectiveness of the proposed method, achieving competitive performance on five challenging datasets with 71 FPS.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2512.06251.pdf' target='_blank'>https://arxiv.org/pdf/2512.06251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangzhou Lin, Yuping Wang, Yuliang Guo, Zixun Huang, Xinyu Huang, Haichong Zhang, Kazunori Yamada, Zhengzhong Tu, Liu Ren, Ziming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06251">NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2504.09361.pdf' target='_blank'>https://arxiv.org/pdf/2504.09361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahuan Long, Tingsong Jiang, Wen Yao, Shuai Jia, Weijia Zhang, Weien Zhou, Chao Ma, Xiaoqian Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09361">PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple objects in a continuous video stream is crucial for many computer vision tasks. It involves detecting and associating objects with their respective identities across successive frames. Despite significant progress made in multiple object tracking (MOT), recent studies have revealed the vulnerability of existing MOT methods to adversarial attacks. Nevertheless, all of these attacks belong to digital attacks that inject pixel-level noise into input images, and are therefore ineffective in physical scenarios. To fill this gap, we propose PapMOT, which can generate physical adversarial patches against MOT for both digital and physical scenarios. Besides attacking the detection mechanism, PapMOT also optimizes a printable patch that can be detected as new targets to mislead the identity association process. Moreover, we introduce a patch enhancement strategy to further degrade the temporal consistency of tracking results across video frames, resulting in more aggressive attacks. We further develop new evaluation metrics to assess the robustness of MOT against such attacks. Extensive evaluations on multiple datasets demonstrate that our PapMOT can successfully attack various architectures of MOT trackers in digital scenarios. We also validate the effectiveness of PapMOT for physical attacks by deploying printed adversarial patches in the real world.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2407.08394.pdf' target='_blank'>https://arxiv.org/pdf/2407.08394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengbo Zhang, Li Xu, Duo Peng, Hossein Rahmani, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08394">Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model. Our main idea is to leverage the rich knowledge encapsulated within the pre-trained diffusion model, such as the understanding of image semantics and structural information, to address unsupervised visual tracking. To this end, we design an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target. Furthermore, to facilitate dynamic adaptation of the prompt to the target's movements, we propose an online prompt updater. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method, which also achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2511.01427.pdf' target='_blank'>https://arxiv.org/pdf/2511.01427.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Xu Zhou, Feng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.01427">UniSOT: A Unified Framework for Multi-Modality Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking aims to localize target object with specific reference modalities (bounding box, natural language or both) in a sequence of specific video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different reference modalities enable various human-machine interactions, and different video modalities are demanded in complex scenarios to enhance tracking robustness. Existing trackers are designed for single or several video modalities with single or several reference modalities, which leads to separate model designs and limits practical applications. Practically, a unified tracker is needed to handle various requirements. To the best of our knowledge, there is still no tracker that can perform tracking with these above reference modalities across these video modalities simultaneously. Thus, in this paper, we present a unified tracker, UniSOT, for different combinations of three reference modalities and four video modalities with uniform parameters. Extensive experimental results on 18 visual tracking, vision-language tracking and RGB+X tracking benchmarks demonstrate that UniSOT shows superior performance against modality-specific counterparts. Notably, UniSOT outperforms previous counterparts by over 3.0\% AUC on TNL2K across all three reference modalities and outperforms Un-Track by over 2.0\% main metric across all three RGB+X video modalities.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2508.00358.pdf' target='_blank'>https://arxiv.org/pdf/2508.00358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Gong, Mengjun Chen, Hao Liu, Gao Yongsheng, Lei Yang, Naibang Wang, Ziying Song, Haoqun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00358">Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) enables autonomous vehicles to continuously perceive dynamic objects, supplying essential temporal cues for prediction, behavior understanding, and safe planning. However, conventional tracking-by-detection methods typically rely on static coordinate transformations based on ego-vehicle poses, disregarding ego-vehicle speed-induced variations in observation noise and reference frame changes, which degrades tracking stability and accuracy in dynamic, high-speed scenarios. In this paper, we investigate the critical role of ego-vehicle speed in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that dynamically adapts uncertainty modeling to ego-vehicle speed, significantly improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that adaptively predicts key parameters of SG-LKF. To enhance inter-frame association and trajectory continuity, we introduce a self-supervised trajectory consistency loss jointly optimized with semantic and positional constraints. Extensive experiments show that SG-LKF ranks first among all vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2412.04915.pdf' target='_blank'>https://arxiv.org/pdf/2412.04915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khurram Azeem Hashmi, Talha Uddin Sheikh, Didier Stricker, Muhammad Zeshan Afzal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04915">Beyond Boxes: Mask-Guided Spatio-Temporal Feature Aggregation for Video Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The primary challenge in Video Object Detection (VOD) is effectively exploiting temporal information to enhance object representations. Traditional strategies, such as aggregating region proposals, often suffer from feature variance due to the inclusion of background information. We introduce a novel instance mask-based feature aggregation approach, significantly refining this process and deepening the understanding of object dynamics across video frames. We present FAIM, a new VOD method that enhances temporal Feature Aggregation by leveraging Instance Mask features. In particular, we propose the lightweight Instance Feature Extraction Module (IFEM) to learn instance mask features and the Temporal Instance Classification Aggregation Module (TICAM) to aggregate instance mask and classification features across video frames. Using YOLOX as a base detector, FAIM achieves 87.9% mAP on the ImageNet VID dataset at 33 FPS on a single 2080Ti GPU, setting a new benchmark for the speed-accuracy trade-off. Additional experiments on multiple datasets validate that our approach is robust, method-agnostic, and effective in multi-object tracking, demonstrating its broader applicability to video understanding tasks.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2409.17025.pdf' target='_blank'>https://arxiv.org/pdf/2409.17025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrito Das, Bilal Sidiqi, Laurent Mennillo, Zhehua Mao, Mikael Brudfors, Miguel Xochicale, Danyal Z. Khan, Nicola Newall, John G. Hanrahan, Matthew J. Clarkson, Danail Stoyanov, Hani J. Marcus, Sophia Bano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17025">Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improved surgical skill is generally associated with improved patient outcomes, although assessment is subjective; labour-intensive; and requires domain specific expertise. Automated data driven metrics can alleviate these difficulties, as demonstrated by existing machine learning instrument tracking models in minimally invasive surgery. However, these models have been tested on limited datasets of laparoscopic surgery, with a focus on isolated tasks and robotic surgery. In this paper, a new public dataset is introduced, focusing on simulated surgery, using the nasal phase of endoscopic pituitary surgery as an exemplar. Simulated surgery allows for a realistic yet repeatable environment, meaning the insights gained from automated assessment can be used by novice surgeons to hone their skills on the simulator before moving to real surgery. PRINTNet (Pituitary Real-time INstrument Tracking Network) has been created as a baseline model for this automated assessment. Consisting of DeepLabV3 for classification and segmentation; StrongSORT for tracking; and the NVIDIA Holoscan SDK for real-time performance, PRINTNet achieved 71.9% Multiple Object Tracking Precision running at 22 Frames Per Second. Using this tracking output, a Multilayer Perceptron achieved 87% accuracy in predicting surgical skill level (novice or expert), with the "ratio of total procedure time to instrument visible time" correlated with higher surgical skill. This therefore demonstrates the feasibility of automated surgical skill assessment in simulated endoscopic pituitary surgery. The new publicly available dataset can be found here: https://doi.org/10.5522/04/26511049.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2408.16445.pdf' target='_blank'>https://arxiv.org/pdf/2408.16445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sierra Bonilla, Chiara Di Vece, Rema Daher, Xinwei Ju, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16445">Mismatched: Evaluating the Limits of Image Matching Approaches and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three-dimensional (3D) reconstruction from two-dimensional images is an active research field in computer vision, with applications ranging from navigation and object tracking to segmentation and three-dimensional modeling. Traditionally, parametric techniques have been employed for this task. However, recent advancements have seen a shift towards learning-based methods. Given the rapid pace of research and the frequent introduction of new image matching methods, it is essential to evaluate them. In this paper, we present a comprehensive evaluation of various image matching methods using a structure-from-motion pipeline. We assess the performance of these methods on both in-domain and out-of-domain datasets, identifying key limitations in both the methods and benchmarks. We also investigate the impact of edge detection as a pre-processing step. Our analysis reveals that image matching for 3D reconstruction remains an open challenge, necessitating careful selection and tuning of models for specific scenarios, while also highlighting mismatches in how metrics currently represent method performance.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2512.10945.pdf' target='_blank'>https://arxiv.org/pdf/2512.10945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Henghui Ding, Chang Liu, Shuting He, Kaining Ying, Xudong Jiang, Chen Change Loy, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10945">MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2505.20718.pdf' target='_blank'>https://arxiv.org/pdf/2505.20718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Wu, Shuhang Xu, Hao Chen, Churan Wang, Zhoujun Li, Yizhou Wang, Fangwei Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20718">VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel self-improving framework that enhances Embodied Visual Tracking (EVT) with Vision-Language Models (VLMs) to address the limitations of current active visual tracking systems in recovering from tracking failure. Our approach combines the off-the-shelf active tracking methods with VLMs' reasoning capabilities, deploying a fast visual policy for normal tracking and activating VLM reasoning only upon failure detection. The framework features a memory-augmented self-reflection mechanism that enables the VLM to progressively improve by learning from past experiences, effectively addressing VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate significant performance improvements, with our framework boosting success rates by $72\%$ with state-of-the-art RL-based approaches and $220\%$ with PID-based methods in challenging environments. This work represents the first integration of VLM-based reasoning to assist EVT agents in proactive failure recovery, offering substantial advances for real-world robotic applications that require continuous target monitoring in dynamic, unstructured environments. Project website: https://sites.google.com/view/evt-recovery-assistant.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2505.20710.pdf' target='_blank'>https://arxiv.org/pdf/2505.20710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Wu, Hao Chen, Churan Wang, Fakhri Karray, Zhoujun Li, Yizhou Wang, Fangwei Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20710">Hierarchical Instruction-aware Embodied Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for reinforcement learning-based models due to the substantial gap between high-level user instructions and low-level agent actions. While recent advancements in language models (e.g., LLMs, VLMs, VLAs) have improved instruction comprehension, these models face critical limitations in either inference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To address these challenges, we propose \textbf{Hierarchical Instruction-aware Embodied Visual Tracking (HIEVT)} agent, which bridges instruction comprehension and action generation using \textit{spatial goals} as intermediaries. HIEVT first introduces \textit{LLM-based Semantic-Spatial Goal Aligner} to translate diverse human instructions into spatial goals that directly annotate the desired spatial position. Then the \textit{RL-based Adaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to position the target as specified by the spatial goal. To benchmark UC-EVT tasks, we collect over ten million trajectories for training and evaluate across one seen environment and nine unseen challenging environments. Extensive experiments and real-world deployments demonstrate the robustness and generalizability of HIEVT across diverse environments, varying target dynamics, and complex instruction combinations. The complete project is available at https://sites.google.com/view/hievt.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2412.10749.pdf' target='_blank'>https://arxiv.org/pdf/2412.10749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangbin Li, Jinxing Zhou, Jing Zhang, Shengeng Tang, Kun Li, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10749">Patch-level Sounding Object Tracking for Audio-Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Answering questions related to audio-visual scenes, i.e., the AVQA task, is becoming increasingly popular. A critical challenge is accurately identifying and tracking sounding objects related to the question along the timeline. In this paper, we present a new Patch-level Sounding Object Tracking (PSOT) method. It begins with a Motion-driven Key Patch Tracking (M-KPT) module, which relies on visual motion information to identify salient visual patches with significant movements that are more likely to relate to sounding objects and questions. We measure the patch-wise motion intensity map between neighboring video frames and utilize it to construct and guide a motion-driven graph network. Meanwhile, we design a Sound-driven KPT (S-KPT) module to explicitly track sounding patches. This module also involves a graph network, with the adjacency matrix regularized by the audio-visual correspondence map. The M-KPT and S-KPT modules are performed in parallel for each temporal segment, allowing balanced tracking of salient and sounding objects. Based on the tracked patches, we further propose a Question-driven KPT (Q-KPT) module to retain patches highly relevant to the question, ensuring the model focuses on the most informative clues. The audio-visual-question features are updated during the processing of these modules, which are then aggregated for final answer prediction. Extensive experiments on standard datasets demonstrate the effectiveness of our method, achieving competitive performance even compared to recent large-scale pretraining-based approaches.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2410.08781.pdf' target='_blank'>https://arxiv.org/pdf/2410.08781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinxue Guo, Zixu Zhao, Jianxiong Gao, Chongruo Wu, Tong He, Zheng Zhang, Tianjun Xiao, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08781">VideoSAM: Open-World Video Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video segmentation is essential for advancing robotics and autonomous driving, particularly in open-world settings where continuous perception and object association across video frames are critical. While the Segment Anything Model (SAM) has excelled in static image segmentation, extending its capabilities to video segmentation poses significant challenges. We tackle two major hurdles: a) SAM's embedding limitations in associating objects across frames, and b) granularity inconsistencies in object segmentation. To this end, we introduce VideoSAM, an end-to-end framework designed to address these challenges by improving object tracking and segmentation consistency in dynamic environments. VideoSAM integrates an agglomerated backbone, RADIO, enabling object association through similarity metrics and introduces Cycle-ack-Pairs Propagation with a memory mechanism for stable object tracking. Additionally, we incorporate an autoregressive object-token mechanism within the SAM decoder to maintain consistent granularity across frames. Our method is extensively evaluated on the UVO and BURST benchmarks, and robotic videos from RoboTAP, demonstrating its effectiveness and robustness in real-world scenarios. All codes will be available.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2406.09982.pdf' target='_blank'>https://arxiv.org/pdf/2406.09982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacinto Colan, Ana Davila, Yasuhisa Hasegawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09982">Constrained Motion Planning for a Robotic Endoscope Holder based on Hierarchical Quadratic Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Minimally Invasive Surgeries (MIS) are challenging for surgeons due to the limited field of view and constrained range of motion imposed by narrow access ports. These challenges can be addressed by robot-assisted endoscope systems which provide precise and stabilized positioning, as well as constrained and smooth motion control of the endoscope. In this work, we propose an online hierarchical optimization framework for visual servoing control of the endoscope in MIS. The framework prioritizes maintaining a remote-center-of-motion (RCM) constraint to prevent tissue damage, while a visual tracking task is defined as a secondary task to enable autonomous tracking of visual features of interest. We validated our approach using a 6-DOF Denso VS050 manipulator and achieved optimization solving times under 0.4 ms and maximum RCM deviation of approximately 0.4 mm. Our results demonstrate the effectiveness of the proposed approach in addressing the constrained motion planning challenges of MIS, enabling precise and autonomous endoscope positioning and visual tracking.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2405.02425.pdf' target='_blank'>https://arxiv.org/pdf/2405.02425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02425">Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We apply multi-agent deep reinforcement learning (RL) to train end-to-end robot soccer policies with fully onboard computation and sensing via egocentric RGB vision. This setting reflects many challenges of real-world robotics, including active perception, agile full-body control, and long-horizon planning in a dynamic, partially-observable, multi-agent domain. We rely on large-scale, simulation-based data generation to obtain complex behaviors from egocentric vision which can be successfully transferred to physical robots using low-cost sensors. To achieve adequate visual realism, our simulation combines rigid-body physics with learned, realistic rendering via multiple Neural Radiance Fields (NeRFs). We combine teacher-based multi-agent RL and cross-experiment data reuse to enable the discovery of sophisticated soccer strategies. We analyze active-perception behaviors including object tracking and ball seeking that emerge when simply optimizing perception-agnostic soccer play. The agents display equivalent levels of performance and agility as policies with access to privileged, ground-truth state. To our knowledge, this paper constitutes a first demonstration of end-to-end training for multi-agent robot soccer, mapping raw pixel observations to joint-level actions, that can be deployed in the real world. Videos of the game-play and analyses can be seen on our website https://sites.google.com/view/vision-soccer .
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2403.01781.pdf' target='_blank'>https://arxiv.org/pdf/2403.01781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tung Le, Khai Nguyen, Shanlin Sun, Nhat Ho, Xiaohui Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01781">Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis. Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT). Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost. Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an unsupervised shape matching framework. This unsupervised framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability measures. We also introduce an adaptive refinement process utilizing entropy regularized OT, further refining feature alignments for accurate point-to-point correspondences. Our method demonstrates superior performance in non-rigid shape matching, including near-isometric and non-isometric scenarios, and excels in downstream tasks like segmentation transfer. The empirical results on diverse datasets highlight our framework's effectiveness and generalization capabilities, setting new standards in non-rigid shape matching with efficient OT metrics and an adaptive refinement module.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2405.17903.pdf' target='_blank'>https://arxiv.org/pdf/2405.17903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongze Sun, Rui Liu, Wuque Cai, Jun Wang, Yue Wang, Huajin Tang, Yan Cui, Dezhong Yao, Daqing Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17903">Reliable Object Tracking by Multimodal Hybrid Feature Extraction and Transformer-Based Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking, which is primarily based on visible light image sequences, encounters numerous challenges in complicated scenarios, such as low light conditions, high dynamic ranges, and background clutter. To address these challenges, incorporating the advantages of multiple visual modalities is a promising solution for achieving reliable object tracking. However, the existing approaches usually integrate multimodal inputs through adaptive local feature interactions, which cannot leverage the full potential of visual cues, thus resulting in insufficient feature modeling. In this study, we propose a novel multimodal hybrid tracker (MMHT) that utilizes frame-event-based data for reliable single object tracking. The MMHT model employs a hybrid backbone consisting of an artificial neural network (ANN) and a spiking neural network (SNN) to extract dominant features from different visual modalities and then uses a unified encoder to align the features across different domains. Moreover, we propose an enhanced transformer-based module to fuse multimodal features using attention mechanisms. With these methods, the MMHT model can effectively construct a multiscale and multidimensional visual feature space and achieve discriminative feature modeling. Extensive experiments demonstrate that the MMHT model exhibits competitive performance in comparison with that of other state-of-the-art methods. Overall, our results highlight the effectiveness of the MMHT model in terms of addressing the challenges faced in visual object tracking tasks.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2404.06247.pdf' target='_blank'>https://arxiv.org/pdf/2404.06247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianlang Chen, Xuhong Ren, Qing Guo, Felix Juefei-Xu, Di Lin, Wei Feng, Lei Ma, Jianjun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06247">LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames. This can lead to significant robustness and security issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest. This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2412.09097.pdf' target='_blank'>https://arxiv.org/pdf/2412.09097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengcai Zhou, Halvin Yang, Luping Xiang, Kun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09097">Temporal-Assisted Beamforming and Trajectory Prediction in Sensing-Enabled UAV Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the evolving landscape of high-speed communication, the shift from traditional pilot-based methods to a Sensing-Oriented Approach (SOA) is anticipated to gain momentum. This paper delves into the development of an innovative Integrated Sensing and Communication (ISAC) framework, specifically tailored for beamforming and trajectory prediction processes. Central to this research is the exploration of an Unmanned Aerial Vehicle (UAV)-enabled communication system, which seamlessly integrates ISAC technology. This integration underscores the synergistic interplay between sensing and communication capabilities. The proposed system initially deploys omnidirectional beams for the sensing-focused phase, subsequently transitioning to directional beams for precise object tracking. This process incorporates an Extended Kalman Filtering (EKF) methodology for the accurate estimation and prediction of object states. A novel frame structure is introduced, employing historical sensing data to optimize beamforming in real-time for subsequent time slots, a strategy we refer to as 'temporal-assisted' beamforming. To refine the temporal-assisted beamforming technique, we employ Successive Convex Approximation (SCA) in tandem with Iterative Rank Minimization (IRM), yielding high-quality suboptimal solutions. Comparative analysis with conventional pilot-based systems reveals that our approach yields a substantial improvement of 156\% in multi-object scenarios and 136\% in single-object scenarios.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2411.15600.pdf' target='_blank'>https://arxiv.org/pdf/2411.15600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15600">How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language tracking (VLT) extends traditional single object tracking by incorporating textual information, providing semantic guidance to enhance tracking performance under challenging conditions like fast motion and deformations. However, current VLT trackers often underperform compared to single-modality methods on multiple benchmarks, with semantic information sometimes becoming a "distraction." To address this, we propose VLTVerse, the first fine-grained evaluation framework for VLT trackers that comprehensively considers multiple challenge factors and diverse semantic information, hoping to reveal the role of language in VLT. Our contributions include: (1) VLTVerse introduces 10 sequence-level challenge labels and 6 types of multi-granularity semantic information, creating a flexible and multi-dimensional evaluation space for VLT; (2) leveraging 60 subspaces formed by combinations of challenge factors and semantic types, we conduct systematic fine-grained evaluations of three mainstream SOTA VLT trackers, uncovering their performance bottlenecks across complex scenarios and offering a novel perspective on VLT evaluation; (3) through decoupled analysis of experimental results, we examine the impact of various semantic types on specific challenge factors in relation to different algorithms, providing essential guidance for enhancing VLT across data, evaluation, and algorithmic dimensions. The VLTVerse, toolkit, and results will be available at \url{http://metaverse.aitestunion.com}.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2411.01816.pdf' target='_blank'>https://arxiv.org/pdf/2411.01816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh Nguyen Canh, Huy-Hoang Ngo, Xiem HoangVan, Nak Young Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01816">Toward Integrating Semantic-aware Path Planning and Reliable Localization for UAV Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localization is one of the most crucial tasks for Unmanned Aerial Vehicle systems (UAVs) directly impacting overall performance, which can be achieved with various sensors and applied to numerous tasks related to search and rescue operations, object tracking, construction, etc. However, due to the negative effects of challenging environments, UAVs may lose signals for localization. In this paper, we present an effective path-planning system leveraging semantic segmentation information to navigate around texture-less and problematic areas like lakes, oceans, and high-rise buildings using a monocular camera. We introduce a real-time semantic segmentation architecture and a novel keyframe decision pipeline to optimize image inputs based on pixel distribution, reducing processing time. A hierarchical planner based on the Dynamic Window Approach (DWA) algorithm, integrated with a cost map, is designed to facilitate efficient path planning. The system is implemented in a photo-realistic simulation environment using Unity, aligning with segmentation model parameters. Comprehensive qualitative and quantitative evaluations validate the effectiveness of our approach, showing significant improvements in the reliability and efficiency of UAV localization in challenging environments.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2410.02492.pdf' target='_blank'>https://arxiv.org/pdf/2410.02492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02492">DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual language tracking (VLT) has emerged as a cutting-edge research area, harnessing linguistic data to enhance algorithms with multi-modal inputs and broadening the scope of traditional single object tracking (SOT) to encompass video understanding applications. Despite this, most VLT benchmarks still depend on succinct, human-annotated text descriptions for each video. These descriptions often fall short in capturing the nuances of video content dynamics and lack stylistic variety in language, constrained by their uniform level of detail and a fixed annotation frequency. As a result, algorithms tend to default to a "memorize the answer" strategy, diverging from the core objective of achieving a deeper understanding of video content. Fortunately, the emergence of large language models (LLMs) has enabled the generation of diverse text. This work utilizes LLMs to generate varied semantic annotations (in terms of text lengths and granularities) for representative SOT benchmarks, thereby establishing a novel multi-modal benchmark. Specifically, we (1) propose a new visual language tracking benchmark with diverse texts, named DTVLT, based on five prominent VLT and SOT benchmarks, including three sub-tasks: short-term tracking, long-term tracking, and global instance tracking. (2) We offer four granularity texts in our benchmark, considering the extent and density of semantic information. We expect this multi-granular generation strategy to foster a favorable environment for VLT and video understanding research. (3) We conduct comprehensive experimental analyses on DTVLT, evaluating the impact of diverse text on tracking performance and hope the identified performance bottlenecks of existing algorithms can support further research in VLT and video understanding. The proposed benchmark, experimental results and toolkit will be released gradually on http://videocube.aitestunion.com/.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2405.12139.pdf' target='_blank'>https://arxiv.org/pdf/2405.12139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuchen Li, Xiaokun Feng, Shiyu Hu, Meiqi Wu, Dailing Zhang, Jing Zhang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12139">DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Tracking (VLT) enhances single object tracking (SOT) by integrating natural language descriptions from a video, for the precise tracking of a specified object. By leveraging high-level semantic information, VLT guides object tracking, alleviating the constraints associated with relying on a visual modality. Nevertheless, most VLT benchmarks are annotated in a single granularity and lack a coherent semantic framework to provide scientific guidance. Moreover, coordinating human annotators for high-quality annotations is laborious and time-consuming. To address these challenges, we introduce DTLLM-VLT, which automatically generates extensive and multi-granularity text to enhance environmental diversity. (1) DTLLM-VLT generates scientific and multi-granularity text descriptions using a cohesive prompt framework. Its succinct and highly adaptable design allows seamless integration into various visual tracking benchmarks. (2) We select three prominent benchmarks to deploy our approach: short-term tracking, long-term tracking, and global instance tracking. We offer four granularity combinations for these benchmarks, considering the extent and density of semantic information, thereby showcasing the practicality and versatility of DTLLM-VLT. (3) We conduct comparative experiments on VLT benchmarks with different text granularities, evaluating and analyzing the impact of diverse text on tracking performance. Conclusionally, this work leverages LLM to provide multi-granularity semantic information for VLT task from efficient and diverse perspectives, enabling fine-grained evaluation of multi-modal trackers. In the future, we believe this work can be extended to more datasets to support vision datasets understanding.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2411.18850.pdf' target='_blank'>https://arxiv.org/pdf/2411.18850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lipeng Gu, Xuefeng Yan, Weiming Wang, Honghua Chen, Dingkun Zhu, Liangliang Nan, Mingqiang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18850">CrossTracker: Robust Multi-modal 3D Multi-Object Tracking via Cross Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fusion of camera- and LiDAR-based detections offers a promising solution to mitigate tracking failures in 3D multi-object tracking (MOT). However, existing methods predominantly exploit camera detections to correct tracking failures caused by potential LiDAR detection problems, neglecting the reciprocal benefit of refining camera detections using LiDAR data. This limitation is rooted in their single-stage architecture, akin to single-stage object detectors, lacking a dedicated trajectory refinement module to fully exploit the complementary multi-modal information. To this end, we introduce CrossTracker, a novel two-stage paradigm for online multi-modal 3D MOT. CrossTracker operates in a coarse-to-fine manner, initially generating coarse trajectories and subsequently refining them through an independent refinement process. Specifically, CrossTracker incorporates three essential modules: i) a multi-modal modeling (M^3) module that, by fusing multi-modal information (images, point clouds, and even plane geometry extracted from images), provides a robust metric for subsequent trajectory generation. ii) a coarse trajectory generation (C-TG) module that generates initial coarse dual-stream trajectories, and iii) a trajectory refinement (TR) module that refines coarse trajectories through cross correction between camera and LiDAR streams. Comprehensive experiments demonstrate the superior performance of our CrossTracker over its eighteen competitors, underscoring its effectiveness in harnessing the synergistic benefits of camera and LiDAR sensors for robust multi-modal 3D MOT.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2410.16695.pdf' target='_blank'>https://arxiv.org/pdf/2410.16695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Yu, Yuezun Li, Xin Sun, Junyu Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16695">MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Phytoplankton are a crucial component of aquatic ecosystems, and effective monitoring of them can provide valuable insights into ocean environments and ecosystem changes. Traditional phytoplankton monitoring methods are often complex and lack timely analysis. Therefore, deep learning algorithms offer a promising approach for automated phytoplankton monitoring. However, the lack of large-scale, high-quality training samples has become a major bottleneck in advancing phytoplankton tracking. In this paper, we propose a challenging benchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse background information and variations in motion during observation. The dataset includes 27 species of phytoplankton and zooplankton, 14 different backgrounds to simulate diverse and complex underwater environments, and a total of 140 videos. To enable accurate real-time observation of phytoplankton, we introduce a multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion Tracker(DSFT), which addresses issues such as focus shifts during tracking and the loss of small target information when computing frame-to-frame similarity. Specifically, we introduce an additional feature extractor to predict the residuals of the standard feature extractor's output, and compute multi-scale frame-to-frame similarity based on features from different layers of the extractor. Extensive experiments on the MPT have demonstrated the validity of the dataset and the superiority of DSFT in tracking phytoplankton, providing an effective solution for phytoplankton monitoring.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2403.00976.pdf' target='_blank'>https://arxiv.org/pdf/2403.00976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlin Song, Antoine Richard, Miguel Olivares-Mendez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00976">Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotics, motion capture systems have been widely used to measure the accuracy of localization algorithms. Moreover, this infrastructure can also be used for other computer vision tasks, such as the evaluation of Visual (-Inertial) SLAM dynamic initialization, multi-object tracking, or automatic annotation. Yet, to work optimally, these functionalities require having accurate and reliable spatial-temporal calibration parameters between the camera and the global pose sensor. In this study, we provide two novel solutions to estimate these calibration parameters. Firstly, we design an offline target-based method with high accuracy and consistency. Spatial-temporal parameters, camera intrinsic, and trajectory are optimized simultaneously. Then, we propose an online target-less method, eliminating the need for a calibration target and enabling the estimation of time-varying spatial-temporal parameters. Additionally, we perform detailed observability analysis for the target-less method. Our theoretical findings regarding observability are validated by simulation experiments and provide explainable guidelines for calibration. Finally, the accuracy and consistency of two proposed methods are evaluated with hand-held real-world datasets where traditional hand-eye calibration method do not work.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2510.05070.pdf' target='_blank'>https://arxiv.org/pdf/2510.05070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05070">ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body loco-manipulation promises transformative capabilities for daily service and warehouse tasks. While recent advances in general motion tracking (GMT) have enabled humanoids to reproduce diverse human motions, these policies lack the precision and object awareness required for loco-manipulation. To this end, we introduce ResMimic, a two-stage residual learning framework for precise and expressive humanoid control from human motion data. First, a GMT policy, trained on large-scale human-only motion, serves as a task-agnostic base for generating human-like whole-body movements. An efficient but precise residual policy is then learned to refine the GMT outputs to improve locomotion and incorporate object interaction. To further facilitate efficient training, we design (i) a point-cloud-based object tracking reward for smoother optimization, (ii) a contact reward that encourages accurate humanoid body-object interactions, and (iii) a curriculum-based virtual object controller to stabilize early training. We evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results show substantial gains in task success, training efficiency, and robustness over strong baselines. Videos are available at https://resmimic.github.io/ .
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2508.08219.pdf' target='_blank'>https://arxiv.org/pdf/2508.08219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Sun, Quanyun Wu, Hanqing Xu, Kyle Gao, Zhengsen Xu, Yiping Chen, Dedong Zhang, Lingfei Ma, John S. Zelek, Jonathan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08219">SAGOnline: Segment Any Gaussians Online</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Current methods suffer from prohibitive computational costs, limited 3D spatial reasoning, and an inability to track multiple objects simultaneously. We present Segment Any Gaussians Online (SAGOnline), a lightweight and zero-shot framework for real-time 3D segmentation in Gaussian scenes that addresses these limitations through two key innovations: (1) a decoupled strategy that integrates video foundation models (e.g., SAM2) for view-consistent 2D mask propagation across synthesized views; and (2) a GPU-accelerated 3D mask generation and Gaussian-level instance labeling algorithm that assigns unique identifiers to 3D primitives, enabling lossless multi-object tracking and segmentation across views. SAGOnline achieves state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU) benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times in inference speed (27 ms/frame). Qualitative results demonstrate robust multi-object segmentation and tracking in complex scenes. Our contributions include: (i) a lightweight and zero-shot framework for 3D segmentation in Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling simultaneous segmentation and tracking, and (iii) the effective adaptation of 2D video foundation models to the 3D domain. This work allows real-time rendering and 3D scene understanding, paving the way for practical AR/VR and robotic applications.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2412.07392.pdf' target='_blank'>https://arxiv.org/pdf/2412.07392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhayy Ud Din, Ahsan B. Bakht, Waseem Akram, Yihao Dong, Lakmal Seneviratne, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07392">Benchmarking Vision-Based Object Tracking for USVs in Complex Maritime Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based target tracking is crucial for unmanned surface vehicles (USVs) to perform tasks such as inspection, monitoring, and surveillance. However, real-time tracking in complex maritime environments is challenging due to dynamic camera movement, low visibility, and scale variation. Typically, object detection methods combined with filtering techniques are commonly used for tracking, but they often lack robustness, particularly in the presence of camera motion and missed detections. Although advanced tracking methods have been proposed recently, their application in maritime scenarios is limited. To address this gap, this study proposes a vision-guided object-tracking framework for USVs, integrating state-of-the-art tracking algorithms with low-level control systems to enable precise tracking in dynamic maritime environments. We benchmarked the performance of seven distinct trackers, developed using advanced deep learning techniques such as Siamese Networks and Transformers, by evaluating them on both simulated and real-world maritime datasets. In addition, we evaluated the robustness of various control algorithms in conjunction with these tracking systems. The proposed framework was validated through simulations and real-world sea experiments, demonstrating its effectiveness in handling dynamic maritime conditions. The results show that SeqTrack, a Transformer-based tracker, performed best in adverse conditions, such as dust storms. Among the control algorithms evaluated, the linear quadratic regulator controller (LQR) demonstrated the most robust and smooth control, allowing for stable tracking of the USV.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2412.02734.pdf' target='_blank'>https://arxiv.org/pdf/2412.02734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaofeng Hu, Sifan Zhou, Zhihang Yuan, Dawei Yang, Shibo Zhao, Ci-Jyun Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02734">MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking is essential in autonomous driving and robotics. Existing methods often struggle with sparse and incomplete point cloud scenarios. To address these limitations, we propose a Multimodal-guided Virtual Cues Projection (MVCP) scheme that generates virtual cues to enrich sparse point clouds. Additionally, we introduce an enhanced tracker MVCTrack based on the generated virtual cues. Specifically, the MVCP scheme seamlessly integrates RGB sensors into LiDAR-based systems, leveraging a set of 2D detections to create dense 3D virtual cues that significantly improve the sparsity of point clouds. These virtual cues can naturally integrate with existing LiDAR-based 3D trackers, yielding substantial performance gains. Extensive experiments demonstrate that our method achieves competitive performance on the NuScenes dataset.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2411.01756.pdf' target='_blank'>https://arxiv.org/pdf/2411.01756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Sun, Fan Yu, Shaoxiang Chen, Yu Zhang, Junwei Huang, Chenhui Li, Yang Li, Changbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01756">ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking aims to locate a targeted object in a video sequence based on an initial bounding box. Recently, Vision-Language~(VL) trackers have proposed to utilize additional natural language descriptions to enhance versatility in various applications. However, VL trackers are still inferior to State-of-The-Art (SoTA) visual trackers in terms of tracking performance. We found that this inferiority primarily results from their heavy reliance on manual textual annotations, which include the frequent provision of ambiguous language descriptions. In this paper, we propose ChatTracker to leverage the wealth of world knowledge in the Multimodal Large Language Model (MLLM) to generate high-quality language descriptions and enhance tracking performance. To this end, we propose a novel reflection-based prompt optimization module to iteratively refine the ambiguous and inaccurate descriptions of the target with tracking feedback. To further utilize semantic information produced by MLLM, a simple yet effective VL tracking framework is proposed and can be easily integrated as a plug-and-play module to boost the performance of both VL and visual trackers. Experimental results show that our proposed ChatTracker achieves a performance comparable to existing methods.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2509.02182.pdf' target='_blank'>https://arxiv.org/pdf/2509.02182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shyma Alhuwaider, Motasem Alfarra, Juan C. Perez, Merey Ramazanova, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02182">ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel tracklet-based dataset for benchmarking test-time adaptation (TTA) methods. The aim of this dataset is to mimic the intricate challenges encountered in real-world environments such as images captured by hand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus on how models face distribution shifts, when deployed, and on violations to the customary independent-and-identically-distributed (i.i.d.) assumption in machine learning. Yet, these benchmarks fail to faithfully represent realistic scenarios that naturally display temporal dependencies, such as how consecutive frames from a video stream likely show the same object across time. We address this shortcoming of current datasets by proposing a novel TTA benchmark we call the "Inherent Temporal Dependencies" (ITD) dataset. We ensure the instances in ITD naturally embody temporal dependencies by collecting them from tracklets-sequences of object-centric images we compile from the bounding boxes of an object-tracking dataset. We use ITD to conduct a thorough experimental analysis of current TTA methods, and shed light on the limitations of these methods when faced with the challenges of temporal dependencies. Moreover, we build upon these insights and propose a novel adversarial memory initialization strategy to improve memory-based TTA methods. We find this strategy substantially boosts the performance of various methods on our challenging benchmark.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2509.02028.pdf' target='_blank'>https://arxiv.org/pdf/2509.02028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Halima Bouzidi, Haoyu Liu, Mohammad Abdullah Al Faruque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02028">See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-vision understanding has driven the development of advanced perception systems, most notably the emerging paradigm of Referring Multi-Object Tracking (RMOT). By leveraging natural-language queries, RMOT systems can selectively track objects that satisfy a given semantic description, guided through Transformer-based spatial-temporal reasoning modules. End-to-End (E2E) RMOT models further unify feature extraction, temporal memory, and spatial reasoning within a Transformer backbone, enabling long-range spatial-temporal modeling over fused textual-visual representations. Despite these advances, the reliability and robustness of RMOT remain underexplored. In this paper, we examine the security implications of RMOT systems from a design-logic perspective, identifying adversarial vulnerabilities that compromise both the linguistic-visual referring and track-object matching components. Additionally, we uncover a novel vulnerability in advanced RMOT models employing FIFO-based memory, whereby targeted and consistent attacks on their spatial-temporal reasoning introduce errors that persist within the history buffer over multiple subsequent frames. We present VEIL, a novel adversarial framework designed to disrupt the unified referring-matching mechanisms of RMOT models. We show that carefully crafted digital and physical perturbations can corrupt the tracking logic reliability, inducing track ID switches and terminations. We conduct comprehensive evaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL and demonstrate the urgent need for security-aware RMOT designs for critical large-scale applications.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2507.09880.pdf' target='_blank'>https://arxiv.org/pdf/2507.09880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keito Suzuki, Bang Du, Runfa Blark Li, Kunyao Chen, Lei Wang, Peng Liu, Ning Bi, Truong Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09880">OpenHuman4D: Open-Vocabulary 4D Human Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding dynamic 3D human representation has become increasingly critical in virtual and extended reality applications. However, existing human part segmentation methods are constrained by reliance on closed-set datasets and prolonged inference times, which significantly restrict their applicability. In this paper, we introduce the first 4D human parsing framework that simultaneously addresses these challenges by reducing the inference time and introducing open-vocabulary capabilities. Building upon state-of-the-art open-vocabulary 3D human parsing techniques, our approach extends the support to 4D human-centric video with three key innovations: 1) We adopt mask-based video object tracking to efficiently establish spatial and temporal correspondences, avoiding the necessity of segmenting all frames. 2) A novel Mask Validation module is designed to manage new target identification and mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating memory-conditioned attention and logits equalization for robust embedding fusion. Extensive experiments demonstrate the effectiveness and flexibility of the proposed method on 4D human-centric parsing tasks, achieving up to 93.3% acceleration compared to the previous state-of-the-art method, which was limited to parsing fixed classes.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2503.09191.pdf' target='_blank'>https://arxiv.org/pdf/2503.09191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juana Valeria Hurtado, Sajad Marvi, Rohit Mohan, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09191">Learning Appearance and Motion Cues for Panoptic Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoptic tracking enables pixel-level scene interpretation of videos by integrating instance tracking in panoptic segmentation. This provides robots with a spatio-temporal understanding of the environment, an essential attribute for their operation in dynamic environments. In this paper, we propose a novel approach for panoptic tracking that simultaneously captures general semantic information and instance-specific appearance and motion features. Unlike existing methods that overlook dynamic scene attributes, our approach leverages both appearance and motion cues through dedicated network heads. These interconnected heads employ multi-scale deformable convolutions that reason about scene motion offsets with semantic context and motion-enhanced appearance features to learn tracking embeddings. Furthermore, we introduce a novel two-step fusion module that integrates the outputs from both heads by first matching instances from the current time step with propagated instances from previous time steps and subsequently refines associations using motion-enhanced appearance embeddings, improving robustness in challenging scenarios. Extensive evaluations of our proposed \netname model on two benchmark datasets demonstrate that it achieves state-of-the-art performance in panoptic tracking accuracy, surpassing prior methods in maintaining object identities over time. To facilitate future research, we make the code available at http://panoptictracking.cs.uni-freiburg.de
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2412.06258.pdf' target='_blank'>https://arxiv.org/pdf/2412.06258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yin, Calvin Yeung, Qingrui Hu, Jun Ichikawa, Hirotsugu Azechi, Susumu Takahashi, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06258">Enhanced Multi-Object Tracking Using Pose-based Virtual Markers in 3x3 Basketball</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is crucial for various multi-agent analyses such as evaluating team sports tactics and player movements and performance. While pedestrian tracking has advanced with Tracking-by-Detection MOT, team sports like basketball pose unique challenges. These challenges include players' unpredictable movements, frequent close interactions, and visual similarities that complicate pose labeling and lead to significant occlusions, frequent ID switches, and high manual annotation costs. To address these challenges, we propose a novel pose-based virtual marker (VM) MOT method for team sports, named Sports-vmTracking. This method builds on the vmTracking approach developed for multi-animal tracking with active learning. First, we constructed a 3x3 basketball pose dataset for VMs and applied active learning to enhance model performance in generating VMs. Then, we overlaid the VMs on video to identify players, extract their poses with unique IDs, and convert these into bounding boxes for comparison with automated MOT methods. Using our 3x3 basketball dataset, we demonstrated that our VM configuration has been highly effective, and reduced the need for manual corrections and labeling during pose model training while maintaining high accuracy. Our approach achieved an average HOTA score of 72.3%, over 10 points higher than other state-of-the-art methods without VM, and resulted in 0 ID switches. Beyond improving performance in handling occlusions and minimizing ID switches, our framework could substantially increase the time and cost efficiency compared to traditional manual annotation.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2406.19655.pdf' target='_blank'>https://arxiv.org/pdf/2406.19655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingrui Hu, Atom Scott, Calvin Yeung, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19655">Basketball-SORT: An Association Method for Complex Multi-object Occlusion Problems in Basketball Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent deep learning-based object detection approaches have led to significant progress in multi-object tracking (MOT) algorithms. The current MOT methods mainly focus on pedestrian or vehicle scenes, but basketball sports scenes are usually accompanied by three or more object occlusion problems with similar appearances and high-intensity complex motions, which we call complex multi-object occlusion (CMOO). Here, we propose an online and robust MOT approach, named Basketball-SORT, which focuses on the CMOO problems in basketball videos. To overcome the CMOO problem, instead of using the intersection-over-union-based (IoU-based) approach, we use the trajectories of neighboring frames based on the projected positions of the players. Our method designs the basketball game restriction (BGR) and reacquiring Long-Lost IDs (RLLI) based on the characteristics of basketball scenes, and we also solve the occlusion problem based on the player trajectories and appearance features. Experimental results show that our method achieves a Higher Order Tracking Accuracy (HOTA) score of 63.48$\%$ on the basketball fixed video dataset and outperforms other recent popular approaches. Overall, our approach solved the CMOO problem more effectively than recent MOT algorithms.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2405.17323.pdf' target='_blank'>https://arxiv.org/pdf/2405.17323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingwei Liu, Yasutomo Kawanishi, Takahiro Komamizu, Ichiro Ide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17323">Tracking Small Birds by Detection Candidate Region Filtering and Detection History-aware Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on tracking birds that appear small in a panoramic video. When the size of the tracked object is small in the image (small object tracking) and move quickly, object detection and association suffers. To address these problems, we propose Adaptive Slicing Aided Hyper Inference (Adaptive SAHI), which reduces the candidate regions to apply detection, and Detection History-aware Similarity Criterion (DHSC), which accurately associates objects in consecutive frames based on the detection history. Experiments on the NUBird2022 dataset verifies the effectiveness of the proposed method by showing improvements in both accuracy and speed.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2404.10534.pdf' target='_blank'>https://arxiv.org/pdf/2404.10534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nadezda Kirillova, M. Jehanzeb Mirza, Horst Bischof, Horst Possegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10534">Into the Fog: Evaluating Robustness of Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art Multiple Object Tracking (MOT) approaches have shown remarkable performance when trained and evaluated on current benchmarks. However, these benchmarks primarily consist of clear weather scenarios, overlooking adverse atmospheric conditions such as fog, haze, smoke and dust. As a result, the robustness of trackers against these challenging conditions remains underexplored. To address this gap, we introduce physics-based volumetric fog simulation method for arbitrary MOT datasets, utilizing frame-by-frame monocular depth estimation and a fog formation optical model. We enhance our simulation by rendering both homogeneous and heterogeneous fog and propose to use the dark channel prior method to estimate atmospheric light, showing promising results even in night and indoor scenes. We present the leading benchmark MOTChallenge (third release) augmented with fog (smoke for indoor scenes) of various intensities and conduct a comprehensive evaluation of MOT methods, revealing their limitations under fog and fog-like challenges.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2512.01366.pdf' target='_blank'>https://arxiv.org/pdf/2512.01366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Li, Jiajun Yan, Yuzhou Wei, Kechen Liu, Yize Zhao, Chong Zhang, Hongzi Zhu, Li Lu, Shan Chang, Minyi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01366">BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists. In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user. The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud. To minimize the power consumption of the earbud and the phone while guaranteeing the best tracking accuracy, a novel 3D object tracking algorithm is devised, integrating both a Kalman filter based trajectory estimation scheme and an optimal image sampling strategy based on reinforcement learning. Moreover, the impact of constant user head movements on the tracking accuracy is significantly eliminated by leveraging the estimated pitch and yaw angles to correct the object depth estimation and align the camera coordinate system to the user's body coordinate system, respectively. We implement a prototype BlinkBud system and conduct extensive real-world experiments. Results show that BlinkBud is lightweight with ultra-low mean power consumptions of 29.8 mW and 702.6 mW on the earbud and smartphone, respectively, and can accurately detect hazards with a low average false positive ratio (FPR) and false negative ratio (FNR) of 4.90% and 1.47%, respectively.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2511.21053.pdf' target='_blank'>https://arxiv.org/pdf/2511.21053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenglizhao Chen, Shaofeng Liang, Runwei Guan, Xiaolou Sun, Haocheng Zhao, Haiyun Jiang, Tao Huang, Henghui Ding, Qing-Long Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21053">AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2510.14992.pdf' target='_blank'>https://arxiv.org/pdf/2510.14992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leela Krishna, Mengyang Zhao, Saicharithreddy Pasula, Harshit Rajgarhia, Abhishek Mukherji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14992">GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training robust world models requires large-scale, precisely labeled multimodal datasets, a process historically bottlenecked by slow and expensive manual annotation. We present a production-tested GAZE pipeline that automates the conversion of raw, long-form video into rich, task-ready supervision for world-model training. Our system (i) normalizes proprietary 360-degree formats into standard views and shards them for parallel processing; (ii) applies a suite of AI models (scene understanding, object tracking, audio transcription, PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii) consolidates signals into a structured output specification for rapid human validation. The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per review hour) and reduces human review volume by >80% through conservative auto-skipping of low-salience segments. By increasing label density and consistency while integrating privacy safeguards and chain-of-custody metadata, our method generates high-fidelity, privacy-aware datasets directly consumable for learning cross-modal dynamics and action-conditioned prediction. We detail our orchestration, model choices, and data dictionary to provide a scalable blueprint for generating high-quality world model training data without sacrificing throughput or governance.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2505.20834.pdf' target='_blank'>https://arxiv.org/pdf/2505.20834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20834">Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency. The code will be released.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2410.01806.pdf' target='_blank'>https://arxiv.org/pdf/2410.01806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Segu, Luigi Piccinelli, Siyuan Li, Yung-Hsu Yang, Bernt Schiele, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01806">Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their trajectories. However, it remains a key open research question on how to model long-range dependencies within tracklets, interdependencies among tracklets, and the associated temporal occlusions. To this end, we introduce Samba, a novel linear-time set-of-sequences model designed to jointly process multiple tracklets by synchronizing the multiple selective state-spaces used to model each tracklet. Samba autoregressively predicts the future track query for each sequence while maintaining synchronized long-term memory representations across tracklets. By integrating Samba into a tracking-by-propagation framework, we propose SambaMOTR, the first tracker effectively addressing the aforementioned issues, including long-range dependencies, tracklet interdependencies, and temporal occlusions. Additionally, we introduce an effective technique for dealing with uncertain observations (MaskObs) and an efficient training recipe to scale SambaMOTR to longer sequences. By modeling long-range dependencies and interactions among tracked objects, SambaMOTR implicitly learns to track objects accurately through occlusions without any hand-crafted heuristics. Our approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT, and SportsMOT datasets.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2409.17221.pdf' target='_blank'>https://arxiv.org/pdf/2409.17221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Segu, Luigi Piccinelli, Siyuan Li, Luc Van Gool, Fisher Yu, Bernt Schiele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17221">Walker: Self-supervised Multiple Object Tracking by Walking on Temporal Appearance Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The supervision of state-of-the-art multiple object tracking (MOT) methods requires enormous annotation efforts to provide bounding boxes for all frames of all videos, and instance IDs to associate them through time. To this end, we introduce Walker, the first self-supervised tracker that learns from videos with sparse bounding box annotations, and no tracking labels. First, we design a quasi-dense temporal object appearance graph, and propose a novel multi-positive contrastive objective to optimize random walks on the graph and learn instance similarities. Then, we introduce an algorithm to enforce mutually-exclusive connective properties across instances in the graph, optimizing the learned topology for MOT. At inference time, we propose to associate detected instances to tracklets based on the max-likelihood transition state under motion-constrained bi-directional walks. Walker is the first self-supervised tracker to achieve competitive performance on MOT17, DanceTrack, and BDD100K. Remarkably, our proposal outperforms the previous self-supervised trackers even when drastically reducing the annotation requirements by up to 400x.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2405.17698.pdf' target='_blank'>https://arxiv.org/pdf/2405.17698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isla Duporge, Maksim Kholiavchenko, Roi Harel, Scott Wolf, Dan Rubenstein, Meg Crofoot, Tanya Berger-Wolf, Stephen Lee, Julie Barreau, Jenna Kline, Michelle Ramirez, Charles Stewart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17698">BaboonLand Dataset: Tracking Primates in the Wild and Automating Behaviour Recognition from Drone Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using drones to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding group primate behavior. Previous studies have demonstrated that it is possible to automate the classification of primate behavior from video data, but these studies have been carried out in captivity or from ground-based cameras. To understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made. This study presents a novel dataset from drone videos for baboon detection, tracking, and behavior recognition. The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes. A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection. The tracking dataset is derived from the detection dataset, where all bounding boxes are assigned the same ID throughout the video. This process resulted in half an hour of very dense tracking data. The behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal; each mini-scene was manually annotated with 12 distinct behavior types, resulting in over 20 hours of data. Benchmark results show mean average precision (mAP) of 92.62\% for the YOLOv8-X detection model, multiple object tracking precision (MOTA) of 63.81\% for the BotSort tracking algorithm, and micro top-1 accuracy of 63.97\% for the X3D behavior recognition model. Using deep learning to classify wildlife behavior from drone footage facilitates non-invasive insight into the collective behavior of an entire group.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2403.18238.pdf' target='_blank'>https://arxiv.org/pdf/2403.18238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangyu Xu, Wanxuan Lu, Hongfeng Yu, Yongqiang Mao, Hanbo Bi, Chenglong Liu, Xian Sun, Kun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18238">TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint Prediction in Aerial Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As drone technology advances, using unmanned aerial vehicles for aerial surveys has become the dominant trend in modern low-altitude remote sensing. The surge in aerial video data necessitates accurate prediction for future scenarios and motion states of the interested target, particularly in applications like traffic management and disaster response. Existing video prediction methods focus solely on predicting future scenes (video frames), suffering from the neglect of explicitly modeling target's motion states, which is crucial for aerial video interpretation. To address this issue, we introduce a novel task called Target-Aware Aerial Video Prediction, aiming to simultaneously predict future scenes and motion states of the target. Further, we design a model specifically for this task, named TAFormer, which provides a unified modeling approach for both video and target motion states. Specifically, we introduce Spatiotemporal Attention (STA), which decouples the learning of video dynamics into spatial static attention and temporal dynamic attention, effectively modeling the scene appearance and motion. Additionally, we design an Information Sharing Mechanism (ISM), which elegantly unifies the modeling of video and target motion by facilitating information interaction through two sets of messenger tokens. Moreover, to alleviate the difficulty of distinguishing targets in blurry predictions, we introduce Target-Sensitive Gaussian Loss (TSGL), enhancing the model's sensitivity to both target's position and content. Extensive experiments on UAV123VP and VisDroneVP (derived from single-object tracking datasets) demonstrate the exceptional performance of TAFormer in target-aware video prediction, showcasing its adaptability to the additional requirements of aerial video interpretation for target awareness.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2402.12644.pdf' target='_blank'>https://arxiv.org/pdf/2402.12644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Lin, Xiang Zhang, Lei Yang, Lei Yu, Bin Zhou, Xiaowei Luo, Wenping Wang, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12644">Neuromorphic Synergy for Video Binarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bimodal objects, such as the checkerboard pattern used in camera calibration, markers for object tracking, and text on road signs, to name a few, are prevalent in our daily lives and serve as a visual form to embed information that can be easily recognized by vision systems. While binarization from intensity images is crucial for extracting the embedded information in the bimodal objects, few previous works consider the task of binarization of blurry images due to the relative motion between the vision sensor and the environment. The blurry images can result in a loss in the binarization quality and thus degrade the downstream applications where the vision system is in motion. Recently, neuromorphic cameras offer new capabilities for alleviating motion blur, but it is non-trivial to first deblur and then binarize the images in a real-time manner. In this work, we propose an event-based binary reconstruction method that leverages the prior knowledge of the bimodal target's properties to perform inference independently in both event space and image space and merge the results from both domains to generate a sharp binary image. We also develop an efficient integration method to propagate this binary image to high frame rate binary video. Finally, we develop a novel method to naturally fuse events and images for unsupervised threshold identification. The proposed method is evaluated in publicly available and our collected data sequence, and shows the proposed method can outperform the SOTA methods to generate high frame rate binary video in real-time on CPU-only devices.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2511.20716.pdf' target='_blank'>https://arxiv.org/pdf/2511.20716.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Guo, Yun Shen, Xijun Wang, Chaoqun You, Yun Rui, Tony Q. S. Quek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20716">Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2511.19057.pdf' target='_blank'>https://arxiv.org/pdf/2511.19057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Wu, Shuai Tang, Jiale Wang, Longkun Zou, Mingyue Guo, Rongqin Liang, Ke Chen, Yaowei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19057">LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2510.25233.pdf' target='_blank'>https://arxiv.org/pdf/2510.25233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jee Won Lee, Hansol Lim, Sooyeun Yang, Jongseong Brad Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25233">Hybrid Vision Servoing with Depp Alignment and GRU-Based Occlusion Recovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based control systems, such as image-based visual servoing (IBVS), have been extensively explored for precise robot manipulation. A persistent challenge, however, is maintaining robust target tracking under partial or full occlusions. Classical methods like Lucas-Kanade (LK) offer lightweight tracking but are fragile to occlusion and drift, while deep learning-based approaches often require continuous visibility and intensive computation. To address these gaps, we propose a hybrid visual tracking framework that bridges advanced perception with real-time servo control. First, a fast global template matcher constrains the pose search region; next, a deep-feature Lucas-Kanade module operating on early VGG layers refines alignment to sub-pixel accuracy (<2px); then, a lightweight residual regressor corrects local misalignments caused by texture degradation or partial occlusion. When visual confidence falls below a threshold, a GRU-based predictor seamlessly extrapolates pose updates from recent motion history. Crucially, the pipeline's final outputs-translation, rotation, and scale deltas-are packaged as direct control signals for 30Hz image-based servo loops. Evaluated on handheld video sequences with up to 90% occlusion, our system sustains under 2px tracking error, demonstrating the robustness and low-latency precision essential for reliable real-world robot vision applications.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2509.14147.pdf' target='_blank'>https://arxiv.org/pdf/2509.14147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanxing Li, Shengyang Wang, Fangyu Sun, Shuyu Wu, Dexin Zuo, Wenxian Yu, Danping Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14147">StableTracker: Learning to Stably Track Target via Differentiable Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>FPV object tracking methods heavily rely on handcraft modular designs, resulting in hardware overload and cumulative error, which seriously degrades the tracking performance, especially for rapidly accelerating or decelerating targets. To address these challenges, we present \textbf{StableTracker}, a learning-based control policy that enables quadrotors to robustly follow the moving target from arbitrary perspectives. The policy is trained using backpropagation-through-time via differentiable simulation, allowing the quadrotor to maintain the target at the center of the visual field in both horizontal and vertical directions, while keeping a fixed relative distance, thereby functioning as an autonomous aerial camera. We compare StableTracker against both state-of-the-art traditional algorithms and learning baselines. Simulation experiments demonstrate that our policy achieves superior accuracy, stability and generalization across varying safe distances, trajectories, and target velocities. Furthermore, a real-world experiment on a quadrotor with an onboard computer validated practicality of the proposed approach.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2507.07483.pdf' target='_blank'>https://arxiv.org/pdf/2507.07483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiangqiang Wu, Yi Yu, Chenqi Kong, Ziquan Liu, Jia Wan, Haoliang Li, Alex C. Kot, Antoni B. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07483">Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of social media, vast amounts of user-uploaded videos (e.g., YouTube) are utilized as training data for Visual Object Tracking (VOT). However, the VOT community has largely overlooked video data-privacy issues, as many private videos have been collected and used for training commercial models without authorization. To alleviate these issues, this paper presents the first investigation on preventing personal video data from unauthorized exploitation by deep trackers. Existing methods for preventing unauthorized data use primarily focus on image-based tasks (e.g., image classification), directly applying them to videos reveals several limitations, including inefficiency, limited effectiveness, and poor generalizability. To address these issues, we propose a novel generative framework for generating Temporal Unlearnable Examples (TUEs), and whose efficient computation makes it scalable for usage on large-scale video datasets. The trackers trained w/ TUEs heavily rely on unlearnable noises for temporal matching, ignoring the original data structure and thus ensuring training video data-privacy. To enhance the effectiveness of TUEs, we introduce a temporal contrastive loss, which further corrupts the learning of existing trackers when using our TUEs for training. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in video data-privacy protection, with strong transferability across VOT models, datasets, and temporal matching tasks.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2506.18737.pdf' target='_blank'>https://arxiv.org/pdf/2506.18737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanliang Yao, Runwei Guan, Yi Ni, Sen Xu, Yong Yue, Xiaohui Zhu, Ryan Wen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18737">USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking in inland waterways plays a crucial role in safe and cost-effective applications, including waterborne transportation, sightseeing tours, environmental monitoring and surface rescue. Our Unmanned Surface Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU, delivers robust tracking capabilities in complex waterborne environments. By leveraging these sensors, our USV collected comprehensive object tracking data, which we present as USVTrack, the first 4D radar-camera tracking dataset tailored for autonomous driving in new generation waterborne transportation systems. Our USVTrack dataset presents rich scenarios, featuring diverse various waterways, varying times of day, and multiple weather and lighting conditions. Moreover, we present a simple but effective radar-camera matching method, termed RCM, which can be plugged into popular two-stage association trackers. Experimental results utilizing RCM demonstrate the effectiveness of the radar-camera matching in improving object tracking accuracy and reliability for autonomous driving in waterborne environments. The USVTrack dataset is public on https://usvtrack.github.io.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2501.00758.pdf' target='_blank'>https://arxiv.org/pdf/2501.00758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenlong Xu, Bineng Zhong, Qihua Liang, Yaozong Zheng, Guorong Li, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00758">Less is More: Token Context-aware Learning for Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, several studies have shown that utilizing contextual information to perceive target states is crucial for object tracking. They typically capture context by incorporating multiple video frames. However, these naive frame-context methods fail to consider the importance of each patch within a reference frame, making them susceptible to noise and redundant tokens, which deteriorates tracking performance. To address this challenge, we propose a new token context-aware tracking pipeline named LMTrack, designed to automatically learn high-quality reference tokens for efficient visual tracking. Embracing the principle of Less is More, the core idea of LMTrack is to analyze the importance distribution of all reference tokens, where important tokens are collected, continually attended to, and updated. Specifically, a novel Token Context Memory module is designed to dynamically collect high-quality spatio-temporal information of a target in an autoregressive manner, eliminating redundant background tokens from the reference frames. Furthermore, an effective Unidirectional Token Attention mechanism is designed to establish dependencies between reference tokens and search frame, enabling robust cross-frame association and target localization. Extensive experiments demonstrate the superiority of our tracker, achieving state-of-the-art results on tracking benchmarks such as GOT-10K, TrackingNet, and LaSOT.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2412.13615.pdf' target='_blank'>https://arxiv.org/pdf/2412.13615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohai Li, Bineng Zhong, Qihua Liang, Guorong Li, Zhiyi Mo, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13615">MambaLCT: Boosting Tracking via Long-term Context State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effectively constructing context information with long-term dependencies from video sequences is crucial for object tracking. However, the context length constructed by existing work is limited, only considering object information from adjacent frames or video clips, leading to insufficient utilization of contextual information. To address this issue, we propose MambaLCT, which constructs and utilizes target variation cues from the first frame to the current frame for robust tracking. First, a novel unidirectional Context Mamba module is designed to scan frame features along the temporal dimension, gathering target change cues throughout the entire sequence. Specifically, target-related information in frame features is compressed into a hidden state space through selective scanning mechanism. The target information across the entire video is continuously aggregated into target variation cues. Next, we inject the target change cues into the attention mechanism, providing temporal information for modeling the relationship between the template and search frames. The advantage of MambaLCT is its ability to continuously extend the length of the context, capturing complete target change cues, which enhances the stability and robustness of the tracker. Extensive experiments show that long-term context information enhances the model's ability to perceive targets in complex scenarios. MambaLCT achieves new SOTA performance on six benchmarks while maintaining real-time running speeds.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2409.14543.pdf' target='_blank'>https://arxiv.org/pdf/2409.14543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Raj, Lei Wang, Tom Gedeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14543">TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately detecting and tracking high-speed, small objects, such as balls in sports videos, is challenging due to factors like motion blur and occlusion. Although recent deep learning frameworks like TrackNetV1, V2, and V3 have advanced tennis ball and shuttlecock tracking, they often struggle in scenarios with partial occlusion or low visibility. This is primarily because these models rely heavily on visual features without explicitly incorporating motion information, which is crucial for precise tracking and trajectory prediction. In this paper, we introduce an enhancement to the TrackNet family by fusing high-level visual features with learnable motion attention maps through a motion-aware fusion mechanism, effectively emphasizing the moving ball's location and improving tracking performance. Our approach leverages frame differencing maps, modulated by a motion prompt layer, to highlight key motion regions over time. Experimental results on the tennis ball and shuttlecock datasets show that our method enhances the tracking performance of both TrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built on top of the existing TrackNet, as TrackNetV4.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2510.15347.pdf' target='_blank'>https://arxiv.org/pdf/2510.15347.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Sun, Yao Zhao, Meiqin Liu, Chao Yao, Jian Jin, Weisi Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15347">Symmetric Entropy-Constrained Video Coding for Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As video transmission increasingly serves machine vision systems (MVS) instead of human vision systems (HVS), video coding for machines (VCM) has become a critical research topic. Existing VCM methods often bind codecs to specific downstream models, requiring retraining or supervised data and thus limiting generalization in multi-task scenarios. Recently, unified VCM frameworks have employed visual backbones (VB) and visual foundation models (VFM) to support multiple video understanding tasks with a single codec. They mainly utilize VB/VFM to maintain semantic consistency or suppress non-semantic information, but seldom explore how to directly link video coding with understanding under VB/VFM guidance. Hence, we propose a Symmetric Entropy-Constrained Video Coding framework for Machines (SEC-VCM). It establishes a symmetric alignment between the video codec and VB, allowing the codec to leverage VB's representation capabilities to preserve semantics and discard MVS-irrelevant information. Specifically, a bi-directional entropy-constraint (BiEC) mechanism ensures symmetry between the process of video decoding and VB encoding by suppressing conditional entropy. This helps the codec to explicitly handle semantic information beneficial for MVS while squeezing useless information. Furthermore, a semantic-pixel dual-path fusion (SPDF) module injects pixel-level priors into the final reconstruction. Through semantic-pixel fusion, it suppresses artifacts harmful to MVS and improves machine-oriented reconstruction quality. Experimental results show our framework achieves state-of-the-art (SOTA) in rate-task performance, with significant bitrate savings over VTM on video instance segmentation (37.41%), video object segmentation (29.83%), object detection (46.22%), and multiple object tracking (44.94%). We will release our code.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2507.04762.pdf' target='_blank'>https://arxiv.org/pdf/2507.04762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Damanaki, Ioulia Kapsali, Nikos Piperigkos, Alexandros Gkillas, Aris S. Lalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04762">Robustifying 3D Perception via Least-Squares Graphs for Multi-Agent Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The critical perception capabilities of EdgeAI systems, such as autonomous vehicles, are required to be resilient against adversarial threats, by enabling accurate identification and localization of multiple objects in the scene over time, mitigating their impact. Single-agent tracking offers resilience to adversarial attacks but lacks situational awareness, underscoring the need for multi-agent cooperation to enhance context understanding and robustness. This paper proposes a novel mitigation framework on 3D LiDAR scene against adversarial noise by tracking objects based on least-squares graph on multi-agent adversarial bounding boxes. Specifically, we employ the least-squares graph tool to reduce the induced positional error of each detection's centroid utilizing overlapped bounding boxes on a fully connected graph via differential coordinates and anchor points. Hence, the multi-vehicle detections are fused and refined mitigating the adversarial impact, and associated with existing tracks in two stages performing tracking to further suppress the adversarial threat. An extensive evaluation study on the real-world V2V4Real dataset demonstrates that the proposed method significantly outperforms both state-of-the-art single and multi-agent tracking frameworks by up to 23.3% under challenging adversarial conditions, operating as a resilient approach without relying on additional defense mechanisms.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2506.09469.pdf' target='_blank'>https://arxiv.org/pdf/2506.09469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Damanaki, Nikos Piperigkos, Alexandros Gkillas, Aris S. Lalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09469">Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) plays a crucial role in autonomous driving systems, as it lays the foundations for advanced perception and precise path planning modules. Nonetheless, single agent based MOT lacks in sensing surroundings due to occlusions, sensors failures, etc. Hence, the integration of multiagent information is essential for comprehensive understanding of the environment. This paper proposes a novel Cooperative MOT framework for tracking objects in 3D LiDAR scene by formulating and solving a graph topology-aware optimization problem so as to fuse information coming from multiple vehicles. By exploiting a fully connected graph topology defined by the detected bounding boxes, we employ the Graph Laplacian processing optimization technique to smooth the position error of bounding boxes and effectively combine them. In that manner, we reveal and leverage inherent coherences of diverse multi-agent detections, and associate the refined bounding boxes to tracked objects at two stages, optimizing localization and tracking accuracies. An extensive evaluation study has been conducted, using the real-world V2V4Real dataset, where the proposed method significantly outperforms the baseline frameworks, including the state-of-the-art deep-learning DMSTrack and V2V4Real, in various testing sequences.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2505.11905.pdf' target='_blank'>https://arxiv.org/pdf/2505.11905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takuya Ikeda, Sergey Zakharov, Muhammad Zubair Irshad, Istvan Balazs Opra, Shun Iwase, Dian Chen, Mark Tjersland, Robert Lee, Alexandre Dilly, Rares Ambrus, Koichi Nishiwaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11905">GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for 6-DoF object tracking and high-quality 3D reconstruction from monocular RGBD video. Existing methods, while achieving impressive results, often struggle with complex objects, particularly those exhibiting symmetry, intricate geometry or complex appearance. To bridge these gaps, we introduce an adaptive method that combines 3D Gaussian Splatting, hybrid geometry/appearance tracking, and key frame selection to achieve robust tracking and accurate reconstructions across a diverse range of objects. Additionally, we present a benchmark covering these challenging object classes, providing high-quality annotations for evaluating both tracking and reconstruction performance. Our approach demonstrates strong capabilities in recovering high-fidelity object meshes, setting a new standard for single-sensor 3D reconstruction in open-world environments.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2503.12968.pdf' target='_blank'>https://arxiv.org/pdf/2503.12968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12968">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2412.01543.pdf' target='_blank'>https://arxiv.org/pdf/2412.01543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufeng Jin, Vignesh Prasad, Snehal Jauhri, Mathias Franzius, Georgia Chalvatzaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01543">6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and accurate object pose estimation is an essential component for modern vision systems in many applications such as Augmented Reality, autonomous driving, and robotics. While research in model-based 6D object pose estimation has delivered promising results, model-free methods are hindered by the high computational load in rendering and inferring consistent poses of arbitrary objects in a live RGB-D video stream. To address this issue, we present 6DOPE-GS, a novel method for online 6D object pose estimation \& tracking with a single RGB-D camera by effectively leveraging advances in Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses and 3D object reconstruction. To achieve the necessary efficiency and accuracy for live tracking, our method uses incremental 2D Gaussian Splatting with an intelligent dynamic keyframe selection procedure to achieve high spatial object coverage and prevent erroneous pose updates. We also propose an opacity statistic-based pruning mechanism for adaptive Gaussian density control, to ensure training stability and efficiency. We evaluate our method on the HO3D and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of state-of-the-art baselines for model-free simultaneous 6D pose tracking and reconstruction while providing a 5$\times$ speedup. We also demonstrate the method's suitability for live, dynamic object tracking and reconstruction in a real-world setting.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2409.00618.pdf' target='_blank'>https://arxiv.org/pdf/2409.00618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lipeng Gu, Mingqiang Wei, Xuefeng Yan, Dingkun Zhu, Wei Zhao, Haoran Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00618">YOLOO: You Only Learn from Others Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal 3D multi-object tracking (MOT) typically necessitates extensive computational costs of deep neural networks (DNNs) to extract multi-modal representations. In this paper, we propose an intriguing question: May we learn from multiple modalities only during training to avoid multi-modal input in the inference phase? To answer it, we propose \textbf{YOLOO}, a novel multi-modal 3D MOT paradigm: You Only Learn from Others Once. YOLOO empowers the point cloud encoder to learn a unified tri-modal representation (UTR) from point clouds and other modalities, such as images and textual cues, all at once. Leveraging this UTR, YOLOO achieves efficient tracking solely using the point cloud encoder without compromising its performance, fundamentally obviating the need for computationally intensive DNNs. Specifically, YOLOO includes two core components: a unified tri-modal encoder (UTEnc) and a flexible geometric constraint (F-GC) module. UTEnc integrates a point cloud encoder with image and text encoders adapted from pre-trained CLIP. It seamlessly fuses point cloud information with rich visual-textual knowledge from CLIP into the point cloud encoder, yielding highly discriminative UTRs that facilitate the association between trajectories and detections. Additionally, F-GC filters out mismatched associations with similar representations but significant positional discrepancies. It further enhances the robustness of UTRs without requiring any scene-specific tuning, addressing a key limitation of customized geometric constraints (e.g., 3D IoU). Lastly, high-quality 3D trajectories are generated by a traditional data association component. By integrating these advancements into a multi-modal 3D MOT scheme, our YOLOO achieves substantial gains in both robustness and efficiency.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2407.05017.pdf' target='_blank'>https://arxiv.org/pdf/2407.05017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Jiang, Fangyuan Wang, Rongzhang Zheng, Han Liu, Yixiong Huo, Jinzhang Peng, Lu Tian, Emad Barsoum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05017">VIPS-Odom: Visual-Inertial Odometry Tightly-coupled with Parking Slots for Autonomous Parking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise localization is of great importance for autonomous parking task since it provides service for the downstream planning and control modules, which significantly affects the system performance. For parking scenarios, dynamic lighting, sparse textures, and the instability of global positioning system (GPS) signals pose challenges for most traditional localization methods. To address these difficulties, we propose VIPS-Odom, a novel semantic visual-inertial odometry framework for underground autonomous parking, which adopts tightly-coupled optimization to fuse measurements from multi-modal sensors and solves odometry. Our VIPS-Odom integrates parking slots detected from the synthesized bird-eye-view (BEV) image with traditional feature points in the frontend, and conducts tightly-coupled optimization with joint constraints introduced by measurements from the inertial measurement unit, wheel speed sensor and parking slots in the backend. We develop a multi-object tracking framework to robustly track parking slots' states. To prove the superiority of our method, we equip an electronic vehicle with related sensors and build an experimental platform based on ROS2 system. Extensive experiments demonstrate the efficacy and advantages of our method compared with other baselines for parking scenarios.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2512.14595.pdf' target='_blank'>https://arxiv.org/pdf/2512.14595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyu Li, Xingcheng Zhou, Guang Chen, Alois Knoll, Hu Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14595">TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2511.17053.pdf' target='_blank'>https://arxiv.org/pdf/2511.17053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Fu, Mengyang Zhao, Ke Niu, Kaixin Peng, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17053">OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2509.14060.pdf' target='_blank'>https://arxiv.org/pdf/2509.14060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Du, Weiwei Xing, Ming Li, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14060">VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multi-object tracking (MOT) algorithms typically overlook issues inherent in low-quality videos, leading to significant degradation in tracking performance when confronted with real-world image deterioration. Therefore, advancing the application of MOT algorithms in real-world low-quality video scenarios represents a critical and meaningful endeavor. To address the challenges posed by low-quality scenarios, inspired by vision-language models, this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking framework (VSE-MOT). Specifically, we first design a tri-branch architecture that leverages a vision-language model to extract global visual semantic information from images and fuse it with query vectors. Subsequently, to further enhance the utilization of visual semantic information, we introduce the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic information to suit multi-object tracking tasks, while the VSFM improves the efficacy of feature fusion. Through extensive experiments, we validate the effectiveness and superiority of the proposed method in real-world low-quality video scenarios. Its tracking performance metrics outperform those of existing methods by approximately 8% to 20%, while maintaining robust performance in conventional scenarios.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2507.21460.pdf' target='_blank'>https://arxiv.org/pdf/2507.21460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mianzhao Wang, Fan Shi, Xu Cheng, Feifei Zhang, Shengyong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21460">An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality 4D light field representation with efficient angular feature modeling is crucial for scene perception, as it can provide discriminative spatial-angular cues to identify moving targets. However, recent developments still struggle to deliver reliable angular modeling in the temporal domain, particularly in complex low-light scenes. In this paper, we propose a novel light field epipolar-plane structure image (ESI) representation that explicitly defines the geometric structure within the light field. By capitalizing on the abrupt changes in the angles of light rays within the epipolar plane, this representation can enhance visual expression in low-light scenes and reduce redundancy in high-dimensional light fields. We further propose an angular-temporal interaction network (ATINet) for light field object tracking that learns angular-aware representations from the geometric structural cues and angular-temporal interaction cues of light fields. Furthermore, ATINet can also be optimized in a self-supervised manner to enhance the geometric feature interaction across the temporal domain. Finally, we introduce a large-scale light field low-light dataset for object tracking. Extensive experimentation demonstrates that ATINet achieves state-of-the-art performance in single object tracking. Furthermore, we extend the proposed method to multiple object tracking, which also shows the effectiveness of high-quality light field angular-temporal modeling.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2507.19908.pdf' target='_blank'>https://arxiv.org/pdf/2507.19908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengmeng Wang, Haonan Wang, Yulong Li, Xiangjie Kong, Jiaxin Du, Guojiang Shen, Feng Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19908">TrackAny3D: Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D LiDAR-based single object tracking (SOT) relies on sparse and irregular point clouds, posing challenges from geometric variations in scale, motion patterns, and structural complexity across object categories. Current category-specific approaches achieve good accuracy but are impractical for real-world use, requiring separate models for each category and showing limited generalization. To tackle these issues, we propose TrackAny3D, the first framework to transfer large-scale pretrained 3D models for category-agnostic 3D SOT. We first integrate parameter-efficient adapters to bridge the gap between pretraining and tracking tasks while preserving geometric priors. Then, we introduce a Mixture-of-Geometry-Experts (MoGE) architecture that adaptively activates specialized subnetworks based on distinct geometric characteristics. Additionally, we design a temporal context optimization strategy that incorporates learnable temporal tokens and a dynamic mask weighting module to propagate historical information and mitigate temporal drift. Experiments on three commonly-used benchmarks show that TrackAny3D establishes new state-of-the-art performance on category-agnostic 3D SOT, demonstrating strong generalization and competitiveness. We hope this work will enlighten the community on the importance of unified models and further expand the use of large-scale pretrained models in this field.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2502.01357.pdf' target='_blank'>https://arxiv.org/pdf/2502.01357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01357">Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D multi-object tracking (MOT) is vital for autonomous vehicles, yet LiDAR and camera-based methods degrade in adverse weather. Meanwhile, Radar-based solutions remain robust but often suffer from limited vertical resolution and simplistic motion models. Existing Kalman filter-based approaches also rely on fixed noise covariance, hampering adaptability when objects make sudden maneuvers. We propose Bayes-4DRTrack, a 4D Radar-based MOT framework that adopts a transformer-based motion prediction network to capture nonlinear motion dynamics and employs Bayesian approximation in both detection and prediction steps. Moreover, our two-stage data association leverages Doppler measurements to better distinguish closely spaced targets. Evaluated on the K-Radar dataset (including adverse weather scenarios), Bayes-4DRTrack demonstrates a 5.7% gain in Average Multi-Object Tracking Accuracy (AMOTA) over methods with traditional motion models and fixed noise covariance. These results showcase enhanced robustness and accuracy in demanding, real-world conditions.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2410.10053.pdf' target='_blank'>https://arxiv.org/pdf/2410.10053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pha Nguyen, Ngan Le, Jackson Cothren, Alper Yilmaz, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10053">DINTR: Tracking via Diffusion-based Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a fundamental task in computer vision, requiring the localization of objects of interest across video frames. Diffusion models have shown remarkable capabilities in visual generation, making them well-suited for addressing several requirements of the tracking problem. This work proposes a novel diffusion-based methodology to formulate the tracking task. Firstly, their conditional process allows for injecting indications of the target object into the generation process. Secondly, diffusion mechanics can be developed to inherently model temporal correspondences, enabling the reconstruction of actual frames in video. However, existing diffusion models rely on extensive and unnecessary mapping to a Gaussian noise domain, which can be replaced by a more efficient and stable interpolation process. Our proposed interpolation mechanism draws inspiration from classic image-processing techniques, offering a more interpretable, stable, and faster approach tailored specifically for the object tracking task. By leveraging the strengths of diffusion models while circumventing their limitations, our Diffusion-based INterpolation TrackeR (DINTR) presents a promising new paradigm and achieves a superior multiplicity on seven benchmarks across five indicator representations.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2407.08222.pdf' target='_blank'>https://arxiv.org/pdf/2407.08222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing Wang, Joel Janek Dabrowski, Josh Pinskier, Lois Liow, Vinoth Viswanathan, Richard Scalzo, David Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08222">PINN-Ray: A Physics-Informed Neural Network to Model Soft Robotic Fin Ray Fingers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modelling complex deformation for soft robotics provides a guideline to understand their behaviour, leading to safe interaction with the environment. However, building a surrogate model with high accuracy and fast inference speed can be challenging for soft robotics due to the nonlinearity from complex geometry, large deformation, material nonlinearity etc. The reality gap from surrogate models also prevents their further deployment in the soft robotics domain. In this study, we proposed a physics-informed Neural Networks (PINNs) named PINN-Ray to model complex deformation for a Fin Ray soft robotic gripper, which embeds the minimum potential energy principle from elastic mechanics and additional high-fidelity experimental data into the loss function of neural network for training. This method is significant in terms of its generalisation to complex geometry and robust to data scarcity as compared to other data-driven neural networks. Furthermore, it has been extensively evaluated to model the deformation of the Fin Ray finger under external actuation. PINN-Ray demonstrates improved accuracy as compared with Finite element modelling (FEM) after applying the data assimilation scheme to treat the sim-to-real gap. Additionally, we introduced our automated framework to design, fabricate soft robotic fingers, and characterise their deformation by visual tracking, which provides a guideline for the fast prototype of soft robotics.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2407.01959.pdf' target='_blank'>https://arxiv.org/pdf/2407.01959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Li, Yubo Cui, Zhiheng Li, Zheng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01959">FlowTrack: Point-level Flow Network for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) is a crucial task in fields of mobile robotics and autonomous driving. Traditional motion-based approaches achieve target tracking by estimating the relative movement of target between two consecutive frames. However, they usually overlook local motion information of the target and fail to exploit historical frame information effectively. To overcome the above limitations, we propose a point-level flow method with multi-frame information for 3D SOT task, called FlowTrack. Specifically, by estimating the flow for each point in the target, our method could capture the local motion details of target, thereby improving the tracking performance. At the same time, to handle scenes with sparse points, we present a learnable target feature as the bridge to efficiently integrate target information from past frames. Moreover, we design a novel Instance Flow Head to transform dense point-level flow into instance-level motion, effectively aggregating local motion information to obtain global target motion. Finally, our method achieves competitive performance with improvements of 5.9% on the KITTI dataset and 2.9% on NuScenes. The code will be made publicly available soon.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2404.12359.pdf' target='_blank'>https://arxiv.org/pdf/2404.12359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Ost, Tanushree Banerjee, Mario Bijelic, Felix Heide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12359">Inverse Neural Rendering for Explainable Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today, most methods for image understanding tasks rely on feed-forward neural networks. While this approach has allowed for empirical accuracy, efficiency, and task adaptation via fine-tuning, it also comes with fundamental disadvantages. Existing networks often struggle to generalize across different datasets, even on the same task. By design, these networks ultimately reason about high-dimensional scene features, which are challenging to analyze. This is true especially when attempting to predict 3D information based on 2D images. We propose to recast 3D multi-object tracking from RGB cameras as an \emph{Inverse Rendering (IR)} problem, by optimizing via a differentiable rendering pipeline over the latent space of pre-trained 3D object representations and retrieve the latents that best represent object instances in a given input image. To this end, we optimize an image loss over generative latent spaces that inherently disentangle shape and appearance properties. We investigate not only an alternate take on tracking but our method also enables examining the generated objects, reasoning about failure situations, and resolving ambiguous cases. We validate the generalization and scaling capabilities of our method by learning the generative prior exclusively from synthetic data and assessing camera-based 3D tracking on the nuScenes and Waymo datasets. Both these datasets are completely unseen to our method and do not require fine-tuning. Videos and code are available at https://light.princeton.edu/inverse-rendering-tracking/.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2404.02562.pdf' target='_blank'>https://arxiv.org/pdf/2404.02562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonglin Liu, Shujie Chen, Jianfeng Dong, Xun Wang, Di Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02562">Representation Alignment Contrastive Regularization for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving high-performance in multi-object tracking algorithms heavily relies on modeling spatio-temporal relationships during the data association stage. Mainstream approaches encompass rule-based and deep learning-based methods for spatio-temporal relationship modeling. While the former relies on physical motion laws, offering wider applicability but yielding suboptimal results for complex object movements, the latter, though achieving high-performance, lacks interpretability and involves complex module designs. This work aims to simplify deep learning-based spatio-temporal relationship models and introduce interpretability into features for data association. Specifically, a lightweight single-layer transformer encoder is utilized to model spatio-temporal relationships. To make features more interpretative, two contrastive regularization losses based on representation alignment are proposed, derived from spatio-temporal consistency rules. By applying weighted summation to affinity matrices, the aligned features can seamlessly integrate into the data association stage of the original tracking workflow. Experimental results showcase that our model enhances the majority of existing tracking networks' performance without excessive complexity, with minimal increase in training overhead and nearly negligible computational and storage costs.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2511.03332.pdf' target='_blank'>https://arxiv.org/pdf/2511.03332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Yang, Yiming Xu, Timo Kaiser, Hao Cheng, Bodo Rosenhahn, Michael Ying Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03332">Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2510.13016.pdf' target='_blank'>https://arxiv.org/pdf/2510.13016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanveer Hannan, Shuaicong Wu, Mark Weber, Suprosanna Shit, Jindong Gu, Rajat Koner, Aljoša Ošep, Laura Leal-Taixé, Thomas Seidl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13016">SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding fine-grained actions and accurately localizing their corresponding actors in space and time are fundamental capabilities for advancing next-generation AI systems, including embodied agents, autonomous platforms, and human-AI interaction frameworks. Despite recent progress in video understanding, existing methods predominantly address either coarse-grained action recognition or generic object tracking, thereby overlooking the challenge of jointly detecting and tracking multiple objects according to their actions while grounding them temporally. To address this gap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task that requires models to simultaneously detect, track, and temporally localize all referent objects in videos based on natural language descriptions of their actions. To support this task, we construct SVAG-Bench, a large-scale benchmark comprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering a diverse range of objects, actions, and real-world scenes. We further propose SVAGFormer, a baseline framework that adapts state of the art vision language models for joint spatial and temporal grounding, and introduce SVAGEval, a standardized evaluation toolkit for fair and reproducible benchmarking. Empirical results show that existing models perform poorly on SVAG, particularly in dense or complex scenes, underscoring the need for more advanced reasoning over fine-grained object-action interactions in long videos.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2411.13183.pdf' target='_blank'>https://arxiv.org/pdf/2411.13183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuiran Wang, Xuehui Yu, Wenwen Yu, Guorong Li, Xiangyuan Lan, Qixiang Ye, Jianbin Jiao, Zhenjun Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13183">ClickTrack: Towards Real-time Interactive Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking(SOT) relies on precise object bounding box initialization. In this paper, we reconsidered the deficiencies in the current approaches to initializing single object trackers and propose a new paradigm for single object tracking algorithms, ClickTrack, a new paradigm using clicking interaction for real-time scenarios. Moreover, click as an input type inherently lack hierarchical information. To address ambiguity in certain special scenarios, we designed the Guided Click Refiner(GCR), which accepts point and optional textual information as inputs, transforming the point into the bounding box expected by the operator. The bounding box will be used as input of single object trackers. Experiments on LaSOT and GOT-10k benchmarks show that tracker combined with GCR achieves stable performance in real-time interactive scenarios. Furthermore, we explored the integration of GCR into the Segment Anything model(SAM), significantly reducing ambiguity issues when SAM receives point inputs.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2406.20083.pdf' target='_blank'>https://arxiv.org/pdf/2406.20083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, Luca Weihs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.20083">PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PoliFormer (Policy Transformer), an RGB-only indoor navigation agent trained end-to-end with reinforcement learning at scale that generalizes to the real-world without adaptation despite being trained purely in simulation. PoliFormer uses a foundational vision transformer encoder with a causal transformer decoder enabling long-term memory and reasoning. It is trained for hundreds of millions of interactions across diverse environments, leveraging parallelized, multi-machine rollouts for efficient training with high throughput. PoliFormer is a masterful navigator, producing state-of-the-art results across two distinct embodiments, the LoCoBot and Stretch RE-1 robots, and four navigation benchmarks. It breaks through the plateaus of previous work, achieving an unprecedented 85.5% success rate in object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement. PoliFormer can also be trivially extended to a variety of downstream applications such as object tracking, multi-object navigation, and open-vocabulary navigation with no finetuning.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2406.01380.pdf' target='_blank'>https://arxiv.org/pdf/2406.01380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Liu, Wenhan Cao, Chang Liu, Tianyi Zhang, Shengbo Eben Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01380">Convolutional Unscented Kalman Filter for Multi-Object Tracking with Outliers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is an essential technique for navigation in autonomous driving. In tracking-by-detection systems, biases, false positives, and misses, which are referred to as outliers, are inevitable due to complex traffic scenarios. Recent tracking methods are based on filtering algorithms that overlook these outliers, leading to reduced tracking accuracy or even loss of the objects trajectory. To handle this challenge, we adopt a probabilistic perspective, regarding the generation of outliers as misspecification between the actual distribution of measurement data and the nominal measurement model used for filtering. We further demonstrate that, by designing a convolutional operation, we can mitigate this misspecification. Incorporating this operation into the widely used unscented Kalman filter (UKF) in commonly adopted tracking algorithms, we derive a variant of the UKF that is robust to outliers, called the convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian conjugate property, thus allowing for real-time tracking. We also prove that ConvUKF has a bounded tracking error in the presence of outliers, which implies robust stability. The experimental results on the KITTI and nuScenes datasets show improved accuracy compared to representative baseline algorithms for MOT tasks.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2404.12963.pdf' target='_blank'>https://arxiv.org/pdf/2404.12963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Rapado-Rincon, Akshay K. Burusa, Eldert J. van Henten, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12963">A comparison between single-stage and two-stage 3D tracking algorithms for greenhouse robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the current demand for automation in the agro-food industry, accurately detecting and localizing relevant objects in 3D is essential for successful robotic operations. However, this is a challenge due the presence of occlusions. Multi-view perception approaches allow robots to overcome occlusions, but a tracking component is needed to associate the objects detected by the robot over multiple viewpoints. Multi-object tracking (MOT) algorithms can be categorized between two-stage and single-stage methods. Two-stage methods tend to be simpler to adapt and implement to custom applications, while single-stage methods present a more complex end-to-end tracking method that can yield better results in occluded situations at the cost of more training data. The potential advantages of single-stage methods over two-stage methods depends on the complexity of the sequence of viewpoints that a robot needs to process. In this work, we compare a 3D two-stage MOT algorithm, 3D-SORT, against a 3D single-stage MOT algorithm, MOT-DETR, in three different types of sequences with varying levels of complexity. The sequences represent simpler and more complex motions that a robot arm can perform in a tomato greenhouse. Our experiments in a tomato greenhouse show that the single-stage algorithm consistently yields better tracking accuracy, especially in the more challenging sequences where objects are fully occluded or non-visible during several viewpoints.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2402.15895.pdf' target='_blank'>https://arxiv.org/pdf/2402.15895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinkun Cao, Jiangmiao Pang, Kris Kitani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15895">Multi-Object Tracking by Hierarchical Visual Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new visual hierarchical representation paradigm for multi-object tracking. It is more effective to discriminate between objects by attending to objects' compositional visual regions and contrasting with the background contextual information instead of sticking to only the semantic visual cue such as bounding boxes. This compositional-semantic-contextual hierarchy is flexible to be integrated in different appearance-based multi-object tracking methods. We also propose an attention-based visual feature module to fuse the hierarchical visual representations. The proposed method achieves state-of-the-art accuracy and time efficiency among query-based methods on multiple multi-object tracking benchmarks.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2402.07677.pdf' target='_blank'>https://arxiv.org/pdf/2402.07677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Li, Hannah Schieber, Niklas Corell, Bernhard Egger, Julian Kreimeier, Daniel Roth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07677">GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user's hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly graph. We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this graph enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2401.09942.pdf' target='_blank'>https://arxiv.org/pdf/2401.09942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir M. Mansourian, Vladimir Somers, Christophe De Vleeschouwer, Shohreh Kasaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09942">Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective tracking and re-identification of players is essential for analyzing soccer videos. But, it is a challenging task due to the non-linear motion of players, the similarity in appearance of players from the same team, and frequent occlusions. Therefore, the ability to extract meaningful embeddings to represent players is crucial in developing an effective tracking and re-identification system. In this paper, a multi-purpose part-based person representation method, called PRTreID, is proposed that performs three tasks of role classification, team affiliation, and re-identification, simultaneously. In contrast to available literature, a single network is trained with multi-task supervision to solve all three tasks, jointly. The proposed joint method is computationally efficient due to the shared backbone. Also, the multi-task learning leads to richer and more discriminative representations, as demonstrated by both quantitative and qualitative results. To demonstrate the effectiveness of PRTreID, it is integrated with a state-of-the-art tracking method, using a part-based post-processing module to handle long-term tracking. The proposed tracking method outperforms all existing tracking methods on the challenging SoccerNet tracking dataset.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2512.13684.pdf' target='_blank'>https://arxiv.org/pdf/2512.13684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13684">Recurrent Video Masked Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2508.14776.pdf' target='_blank'>https://arxiv.org/pdf/2508.14776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Li, Arren Glover, Chiara Bartolozzi, Lorenzo Natale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14776">6-DoF Object Tracking with Event-based Optical Flow and Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking the position and orientation of objects in space (i.e., in 6-DoF) in real time is a fundamental problem in robotics for environment interaction. It becomes more challenging when objects move at high-speed due to frame rate limitations in conventional cameras and motion blur. Event cameras are characterized by high temporal resolution, low latency and high dynamic range, that can potentially overcome the impacts of motion blur. Traditional RGB cameras provide rich visual information that is more suitable for the challenging task of single-shot object pose estimation. In this work, we propose using event-based optical flow combined with an RGB based global object pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the core advantages of both types of vision sensors. Specifically, we propose an event-based optical flow algorithm for object motion measurement to implement an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF velocity with low frequency estimated pose from the global pose estimator, the method can track pose when objects move at high-speed. The proposed algorithm is tested and validated on both synthetic and real world data, demonstrating its effectiveness, especially in high-speed motion scenarios.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2508.01730.pdf' target='_blank'>https://arxiv.org/pdf/2508.01730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Ma, Hui Luo, Qi Chen, Yuankai Qi, Yumei Sun, Amin Beheshti, Jianlin Zhang, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01730">Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2506.19621.pdf' target='_blank'>https://arxiv.org/pdf/2506.19621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noel JosÃ© Rodrigues Vicente, Enrique Lehner, Angel Villar-Corrales, Jan Nogga, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19621">VideoPCDNet: Video Parsing and Prediction with Phase Correlation Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting video content is essential for planning and reasoning in dynamic environments. Despite advancements, unsupervised learning of object representations and dynamics remains challenging. We present VideoPCDNet, an unsupervised framework for object-centric video decomposition and prediction. Our model uses frequency-domain phase correlation techniques to recursively parse videos into object components, which are represented as transformed versions of learned object prototypes, enabling accurate and interpretable tracking. By explicitly modeling object motion through a combination of frequency domain operations and lightweight learned modules, VideoPCDNet enables accurate unsupervised object tracking and prediction of future video frames. In our experiments, we demonstrate that VideoPCDNet outperforms multiple object-centric baseline models for unsupervised tracking and prediction on several synthetic datasets, while learning interpretable object and motion representations.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2505.20455.pdf' target='_blank'>https://arxiv.org/pdf/2505.20455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Hong, Anthony Liang, Kevin Kim, Harshitha Rajaprakash, Jesse Thomason, Erdem BÄ±yÄ±k, Jesse Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20455">HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We hand the community HAND, a simple and time-efficient method for teaching robots new manipulation tasks through human hand demonstrations. Instead of relying on task-specific robot demonstrations collected via teleoperation, HAND uses easy-to-provide hand demonstrations to retrieve relevant behaviors from task-agnostic robot play data. Using a visual tracking pipeline, HAND extracts the motion of the human hand from the hand demonstration and retrieves robot sub-trajectories in two stages: first filtering by visual similarity, then retrieving trajectories with similar behaviors to the hand. Fine-tuning a policy on the retrieved data enables real-time learning of tasks in under four minutes, without requiring calibrated cameras or detailed hand pose estimation. Experiments also show that HAND outperforms retrieval baselines by over 2x in average task success rates on real robots. Videos can be found at our project website: https://liralab.usc.edu/handretrieval/.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2505.12606.pdf' target='_blank'>https://arxiv.org/pdf/2505.12606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Xuan, Zechao Li, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12606">Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal object tracking integrates auxiliary modalities such as depth, thermal infrared, event flow, and language to provide additional information beyond RGB images, showing great potential in improving tracking stabilization in complex scenarios. Existing methods typically start from an RGB-based tracker and learn to understand auxiliary modalities only from training data. Constrained by the limited multi-modal training data, the performance of these methods is unsatisfactory. To alleviate this limitation, this work proposes a unified multi-modal tracker Diff-MM by exploiting the multi-modal understanding capability of the pre-trained text-to-image generation model. Diff-MM leverages the UNet of pre-trained Stable Diffusion as a tracking feature extractor through the proposed parallel feature extraction pipeline, which enables pairwise image inputs for object tracking. We further introduce a multi-modal sub-module tuning method that learns to gain complementary information between different modalities. By harnessing the extensive prior knowledge in the generation model, we achieve a unified tracker with uniform parameters for RGB-N/D/T/E tracking. Experimental results demonstrate the promising performance of our method compared with recently proposed trackers, e.g., its AUC outperforms OneTracker by 8.3% on TNL2K.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2505.07254.pdf' target='_blank'>https://arxiv.org/pdf/2505.07254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Nagy, Naoufel Werghi, Bilal Hassan, Jorge Dias, Majid Khonji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07254">Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\% and 0.81\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\% in higher order tracking accuracy (HOTA) and 1.55\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2502.05938.pdf' target='_blank'>https://arxiv.org/pdf/2502.05938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sourav Sanyal, Amogh Joshi, Manish Nagaraj, Rohan Kumar Manna, Kaushik Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05938">Energy-Efficient Autonomous Aerial Navigation with Dynamic Vision Sensors: A Physics-Guided Neuromorphic Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based object tracking is a critical component for achieving autonomous aerial navigation, particularly for obstacle avoidance. Neuromorphic Dynamic Vision Sensors (DVS) or event cameras, inspired by biological vision, offer a promising alternative to conventional frame-based cameras. These cameras can detect changes in intensity asynchronously, even in challenging lighting conditions, with a high dynamic range and resistance to motion blur. Spiking neural networks (SNNs) are increasingly used to process these event-based signals efficiently and asynchronously. Meanwhile, physics-based artificial intelligence (AI) provides a means to incorporate system-level knowledge into neural networks via physical modeling. This enhances robustness, energy efficiency, and provides symbolic explainability. In this work, we present a neuromorphic navigation framework for autonomous drone navigation. The focus is on detecting and navigating through moving gates while avoiding collisions. We use event cameras for detecting moving objects through a shallow SNN architecture in an unsupervised manner. This is combined with a lightweight energy-aware physics-guided neural network (PgNN) trained with depth inputs to predict optimal flight times, generating near-minimum energy paths. The system is implemented in the Gazebo simulator and integrates a sensor-fused vision-to-planning neuro-symbolic framework built with the Robot Operating System (ROS) middleware. This work highlights the future potential of integrating event-based vision with physics-guided planning for energy-efficient autonomous navigation, particularly for low-latency decision-making.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2501.16753.pdf' target='_blank'>https://arxiv.org/pdf/2501.16753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hy Nguyen, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16753">Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into $N$ chunks, where $N$ is the number of heads. Each segment captures only a fraction of the original embeddings information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings -- this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2501.13994.pdf' target='_blank'>https://arxiv.org/pdf/2501.13994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hy Nguyen, Bao Pham, Hung Du, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13994">CSAOT: Cooperative Multi-Agent System for Active Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object Tracking is essential for many computer vision applications, such as autonomous navigation, surveillance, and robotics. Unlike Passive Object Tracking (POT), which relies on static camera viewpoints to detect and track objects across consecutive frames, Active Object Tracking (AOT) requires a controller agent to actively adjust its viewpoint to maintain visual contact with a moving target in complex environments. Existing AOT solutions are predominantly single-agent-based, which struggle in dynamic and complex scenarios due to limited information gathering and processing capabilities, often resulting in suboptimal decision-making. Alleviating these limitations necessitates the development of a multi-agent system where different agents perform distinct roles and collaborate to enhance learning and robustness in dynamic and complex environments. Although some multi-agent approaches exist for AOT, they typically rely on external auxiliary agents, which require additional devices, making them costly. In contrast, we introduce the Collaborative System for Active Object Tracking (CSAOT), a method that leverages multi-agent deep reinforcement learning (MADRL) and a Mixture of Experts (MoE) framework to enable multiple agents to operate on a single device, thereby improving tracking performance and reducing costs. Our approach enhances robustness against occlusions and rapid motion while optimizing camera movements to extend tracking duration. We validated the effectiveness of CSAOT on various interactive maps with dynamic and stationary obstacles.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2411.19941.pdf' target='_blank'>https://arxiv.org/pdf/2411.19941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Heyward, JoÃ£o Carreira, Dima Damen, Andrew Zisserman, Viorica PÄtrÄucean
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19941">Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Following the successful 2023 edition, we organised the Second Perception Test challenge as a half-day workshop alongside the IEEE/CVF European Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking state-of-the-art video models and measuring the progress since last year using the Perception Test benchmark. This year, the challenge had seven tracks (up from six last year) and covered low-level and high-level tasks, with language and non-language interfaces, across video, audio, and text modalities; the additional track covered hour-long video understanding and introduced a novel video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks were: object tracking, point tracking, temporal action localisation, temporal sound localisation, multiple-choice video question-answering, grounded video question-answering, and hour-long video question-answering. We summarise in this report the challenge tasks and results, and introduce in detail the novel hour-long video QA benchmark 1h-walk VQA.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2410.02638.pdf' target='_blank'>https://arxiv.org/pdf/2410.02638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Herzog, Johannes Gilg, Philipp Wolters, Torben Teepe, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02638">Spatial-Temporal Multi-Cuts for Online Multiple-Camera Vehicle Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate online multiple-camera vehicle tracking is essential for intelligent transportation systems, autonomous driving, and smart city applications. Like single-camera multiple-object tracking, it is commonly formulated as a graph problem of tracking-by-detection. Within this framework, existing online methods usually consist of two-stage procedures that cluster temporally first, then spatially, or vice versa. This is computationally expensive and prone to error accumulation. We introduce a graph representation that allows spatial-temporal clustering in a single, combined step: New detections are spatially and temporally connected with existing clusters. By keeping sparse appearance and positional cues of all detections in a cluster, our method can compare clusters based on the strongest available evidence. The final tracks are obtained online using a simple multicut assignment procedure. Our method does not require any training on the target scene, pre-extraction of single-camera tracks, or additional annotations. Notably, we outperform the online state-of-the-art on the CityFlow dataset in terms of IDF1 by more than 14%, and on the Synthehicle dataset by more than 25%, respectively. The code is publicly available.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2408.12232.pdf' target='_blank'>https://arxiv.org/pdf/2408.12232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12232">BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2408.07157.pdf' target='_blank'>https://arxiv.org/pdf/2408.07157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Alotaibi, Brian L. Mark, Mohammad Reza Fasihi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07157">Object Tracking Incorporating Transfer Learning into Unscented and Cubature Kalman Filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel filtering algorithm that employs Bayesian transfer learning to address the challenges posed by mismatched intensity of the noise in a pair of sensors, each of which tracks an object using a nonlinear dynamic system model. In this setting, the primary sensor experiences a higher noise intensity in tracking the object than the source sensor. To improve the estimation accuracy of the primary sensor, we propose a framework that integrates Bayesian transfer learning into an Unscented Kalman Filter (UKF) and a Cubature Kalman Filter (CKF). In this approach, the parameters of the predicted observations in the source sensor are transferred to the primary sensor and used as an additional prior in the filtering process. Our simulation results show that the transfer learning approach significantly outperforms the conventional isolated UKF and CKF. Comparisons to a form of measurement vector fusion are also presented.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2407.08049.pdf' target='_blank'>https://arxiv.org/pdf/2407.08049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Cheng, Arindam Sengupta, Siyang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08049">Deep Learning-Based Robust Multi-Object Tracking via Fusion of mmWave Radar and Camera Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving holds great promise in addressing traffic safety concerns by leveraging artificial intelligence and sensor technology. Multi-Object Tracking plays a critical role in ensuring safer and more efficient navigation through complex traffic scenarios. This paper presents a novel deep learning-based method that integrates radar and camera data to enhance the accuracy and robustness of Multi-Object Tracking in autonomous driving systems. The proposed method leverages a Bi-directional Long Short-Term Memory network to incorporate long-term temporal information and improve motion prediction. An appearance feature model inspired by FaceNet is used to establish associations between objects across different frames, ensuring consistent tracking. A tri-output mechanism is employed, consisting of individual outputs for radar and camera sensors and a fusion output, to provide robustness against sensor failures and produce accurate tracking results. Through extensive evaluations of real-world datasets, our approach demonstrates remarkable improvements in tracking accuracy, ensuring reliable performance even in low-visibility scenarios.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2405.11536.pdf' target='_blank'>https://arxiv.org/pdf/2405.11536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Nagy, Naoufel Werghi, Bilal Hassan, Jorge Dias, Majid Khonji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11536">RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State Estimation Drift Mitigation on LiDAR PointCloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses limitations in 3D tracking-by-detection methods, particularly in identifying legitimate trajectories and reducing state estimation drift in Kalman filters. Existing methods often use threshold-based filtering for detection scores, which can fail for distant and occluded objects, leading to false positives. To tackle this, we propose a novel track validity mechanism and multi-stage observational gating process, significantly reducing ghost tracks and enhancing tracking performance. Our method achieves a $29.47\%$ improvement in Multi-Object Tracking Accuracy (MOTA) on the KITTI validation dataset with the Second detector. Additionally, a refined Kalman filter term reduces localization noise, improving higher-order tracking accuracy (HOTA) by $4.8\%$. The online framework, RobMOT, outperforms state-of-the-art methods across multiple detectors, with HOTA improvements of up to $3.92\%$ on the KITTI testing dataset and $8.7\%$ on the validation dataset, while achieving low identity switch scores. RobMOT excels in challenging scenarios, tracking distant objects and prolonged occlusions, with a $1.77\%$ MOTA improvement on the Waymo Open dataset, and operates at a remarkable 3221 FPS on a single CPU, proving its efficiency for real-time multi-object tracking.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2404.07495.pdf' target='_blank'>https://arxiv.org/pdf/2404.07495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weisheng Xu, Sifan Zhou, Jiaqi Xiong, Ziyu Zhao, Zhihang Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07495">PillarTrack:Boosting Pillar Representation for Transformer-based 3D Single Object Tracking on Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D single object tracking (3D SOT) is a critical issue in robotics and autonomous driving. Existing 3D SOT methods typically adhere to a point-based processing pipeline, wherein the re-sampling operation invariably leads to either redundant or missing information, thereby impacting performance. To address these issues, we propose PillarTrack, a novel pillar-based 3D SOT framework. First, we transform sparse point clouds into dense pillars to preserve the local and global geometrics. Second, we propose a Pyramid-Encoded Pillar Feature Encoder (PE-PFE) design to enhance the robustness of pillar feature for translation/rotation/scale. Third, we present an efficient Transformer-based backbone from the perspective of modality differences. Finally, we construct our PillarTrack based on above designs. Extensive experiments show that our method achieves comparable performance on the KITTI and NuScenes datasets, significantly enhancing the performance of the baseline.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2403.05852.pdf' target='_blank'>https://arxiv.org/pdf/2403.05852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du, Jing Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05852">SSF-Net: Spatial-Spectral Fusion Network with Spectral Angle Awareness for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral video (HSV) offers valuable spatial, spectral, and temporal information simultaneously, making it highly suitable for handling challenges such as background clutter and visual similarity in object tracking. However, existing methods primarily focus on band regrouping and rely on RGB trackers for feature extraction, resulting in limited exploration of spectral information and difficulties in achieving complementary representations of object features. In this paper, a spatial-spectral fusion network with spectral angle awareness (SST-Net) is proposed for hyperspectral (HS) object tracking. Firstly, to address the issue of insufficient spectral feature extraction in existing networks, a spatial-spectral feature backbone ($S^2$FB) is designed. With the spatial and spectral extraction branch, a joint representation of texture and spectrum is obtained. Secondly, a spectral attention fusion module (SAFM) is presented to capture the intra- and inter-modality correlation to obtain the fused features from the HS and RGB modalities. It can incorporate the visual information into the HS spectral context to form a robust representation. Thirdly, to ensure a more accurate response of the tracker to the object position, a spectral angle awareness module (SAAM) investigates the region-level spectral similarity between the template and search images during the prediction stage. Furthermore, we develop a novel spectral angle awareness loss (SAAL) to offer guidance for the SAAM based on similar regions. Finally, to obtain the robust tracking results, a weighted prediction method is considered to combine the HS and RGB predicted motions of objects to leverage the strengths of each modality. Extensive experiments on the HOTC dataset demonstrate the effectiveness of the proposed SSF-Net, compared with state-of-the-art trackers.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2511.20239.pdf' target='_blank'>https://arxiv.org/pdf/2511.20239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Krejčí, Oliver Kost, Yuxuan Xia, Lennart Svensson, Ondřej Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20239">Occlusion-Aware Multi-Object Tracking via Expected Probability of Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses multi-object systems, where objects may occlude one another relative to the sensor. The standard point-object model for detection-based sensors is enhanced so that the probability of detection considers the presence of all objects. A principled tracking method is derived, assigning each object an expected probability of detection, where the expectation is taken over the reduced Palm density, which means conditionally on the object's existence. The assigned probability thus considers the object's visibility relative to the sensor, under the presence of other objects. Unlike existing methods, the proposed method systematically accounts for uncertainties related to all objects in a clear and manageable way. The method is demonstrated through a visual tracking application using the multi-Bernoulli mixture (MBM) filter with marks.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2511.15580.pdf' target='_blank'>https://arxiv.org/pdf/2511.15580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sifan Zhou, Yichao Cao, Jiahao Nie, Yuqian Fu, Ziyu Zhao, Xiaobo Lu, Shuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15580">CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2509.13396.pdf' target='_blank'>https://arxiv.org/pdf/2509.13396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinan Wang, Di Shi, Fengyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13396">Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel three-stage framework for real-time foreign object intrusion (FOI) detection and tracking in power transmission systems. The framework integrates: (1) a YOLOv7 segmentation model for fast and robust object localization, (2) a ConvNeXt-based feature extractor trained with triplet loss to generate discriminative embeddings, and (3) a feature-assisted IoU tracker that ensures resilient multi-object tracking under occlusion and motion. To enable scalable field deployment, the pipeline is optimized for deployment on low-cost edge hardware using mixed-precision inference. The system supports incremental updates by adding embeddings from previously unseen objects into a reference database without requiring model retraining. Extensive experiments on real-world surveillance and drone video datasets demonstrate the framework's high accuracy and robustness across diverse FOI scenarios. In addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's practicality and scalability for real-world edge applications.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2509.02111.pdf' target='_blank'>https://arxiv.org/pdf/2509.02111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Missaoui, Orcun Cetintas, Guillem BrasÃ³, Tim Meinhardt, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02111">NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The long-standing division between \textit{online} and \textit{offline} Multi-Object Tracking (MOT) has led to fragmented solutions that fail to address the flexible temporal requirements of real-world deployment scenarios. Current \textit{online} trackers rely on frame-by-frame hand-crafted association strategies and struggle with long-term occlusions, whereas \textit{offline} approaches can cover larger time gaps, but still rely on heuristic stitching for arbitrarily long sequences. In this paper, we introduce NOOUGAT, the first tracker designed to operate with arbitrary temporal horizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework that processes non-overlapping subclips, and fuses them through a novel Autoregressive Long-term Tracking (ALT) layer. The subclip size controls the trade-off between latency and temporal context, enabling a wide range of deployment scenarios, from frame-by-frame to batch processing. NOOUGAT achieves state-of-the-art performance across both tracking regimes, improving \textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 on MOT20, with even greater gains in \textit{offline} mode.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2508.14607.pdf' target='_blank'>https://arxiv.org/pdf/2508.14607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengzhi Zhong, Xinzhe Wang, Dan Zeng, Qihua Zhou, Feixiang He, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14607">SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential for low-power computation, yet their application in visual tasks remains largely confined to image classification, object detection, and event-based tracking. In contrast, real-world vision systems still widely use conventional RGB video streams, where the potential of directly-trained SNNs for complex temporal tasks such as multi-object tracking (MOT) remains underexplored. To address this challenge, we propose SMTrack-the first directly trained deep SNN framework for end-to-end multi-object tracking on standard RGB videos. SMTrack introduces an adaptive and scale-aware Normalized Wasserstein Distance loss (Asa-NWDLoss) to improve detection and localization performance under varying object scales and densities. Specifically, the method computes the average object size within each training batch and dynamically adjusts the normalization factor, thereby enhancing sensitivity to small objects. For the association stage, we incorporate the TrackTrack identity module to maintain robust and consistent object trajectories. Extensive evaluations on BEE24, MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking in complex scenarios.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2508.13647.pdf' target='_blank'>https://arxiv.org/pdf/2508.13647.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrejÄÃ­, Oliver Kost, Yuxuan Xia, Lennart Svensson, OndÅej Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13647">Model-based Multi-object Visual Tracking: Identification and Standard Model Limitations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper uses multi-object tracking methods known from the radar tracking community to address the problem of pedestrian tracking using 2D bounding box detections. The standard point-object (SPO) model is adopted, and the posterior density is computed using the Poisson multi-Bernoulli mixture (PMBM) filter. The selection of the model parameters rooted in continuous time is discussed, including the birth and survival probabilities. Some parameters are selected from the first principles, while others are identified from the data, which is, in this case, the publicly available MOT-17 dataset. Although the resulting PMBM algorithm yields promising results, a mismatch between the SPO model and the data is revealed. The model-based approach assumes that modifying the problematic components causing the SPO model-data mismatch will lead to better model-based algorithms in future developments.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2508.09524.pdf' target='_blank'>https://arxiv.org/pdf/2508.09524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yipei Wang, Shiyu Hu, Shukun Jia, Panxi Xu, Hongfei Ma, Yiping Ma, Jing Zhang, Xiaobo Lu, Xin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09524">SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the first systematic investigation and quantification of Similar Object Interference (SOI), a long-overlooked yet critical bottleneck in Single Object Tracking (SOT). Through controlled Online Interference Masking (OIM) experiments, we quantitatively demonstrate that eliminating interference sources leads to substantial performance improvements (AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a primary constraint for robust tracking and highlighting the feasibility of external cognitive guidance. Building upon these insights, we adopt natural language as a practical form of external guidance, and construct SOIBench-the first semantic cognitive guidance benchmark specifically targeting SOI challenges. It automatically mines SOI frames through multi-tracker collective judgment and introduces a multi-level annotation protocol to generate precise semantic guidance texts. Systematic evaluation on SOIBench reveals a striking finding: existing vision-language tracking (VLT) methods fail to effectively exploit semantic cognitive guidance, achieving only marginal improvements or even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we propose a novel paradigm employing large-scale vision-language models (VLM) as external cognitive engines that can be seamlessly integrated into arbitrary RGB trackers. This approach demonstrates substantial improvements under semantic cognitive guidance (AUC gains up to 0.93), representing a significant advancement over existing VLT methods. We hope SOIBench will serve as a standardized evaluation platform to advance semantic cognitive tracking research and contribute new insights to the tracking research community.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2507.13706.pdf' target='_blank'>https://arxiv.org/pdf/2507.13706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ãngel F. GarcÃ­a-FernÃ¡ndez, Jinhao Gu, Lennart Svensson, Yuxuan Xia, Jan KrejÄÃ­, Oliver Kost, OndÅej Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13706">GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces two quasi-metrics for performance assessment of multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an extension of the generalised optimal subpattern assignment (GOSPA) metric and measures the discrepancy between sets of objects. The other quasi-metric is an extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy between sets of trajectories. Similar to the GOSPA-based metrics, these quasi-metrics include costs for localisation error for properly detected objects, the number of false objects and the number of missed objects. The T-GOSPA quasi-metric also includes a track switching cost. Differently from the GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of penalising missed and false objects with different costs, and the localisation costs are not required to be symmetric. These properties can be useful in MOT evaluation in certain applications. The performance of several Bayesian MOT algorithms is assessed with the T-GOSPA quasi-metric via simulations.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2506.15148.pdf' target='_blank'>https://arxiv.org/pdf/2506.15148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xia, Ãngel F. GarcÃ­a-FernÃ¡ndez, Johan Karlsson, Yu Ge, Lennart Svensson, Ting Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15148">Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a generalization of the trajectory general optimal sub-pattern assignment (GOSPA) metric for evaluating multi-object tracking algorithms that provide trajectory estimates with track-level uncertainties. This metric builds on the recently introduced probabilistic GOSPA metric to account for both the existence and state estimation uncertainties of individual object states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a multidimensional assignment problem, and its linear programming relaxation--also a valid metric--is computable in polynomial time. Additionally, this metric retains the interpretability of TGOSPA, and we show that its decomposition yields intuitive costs terms associated to expected localization error and existence probability mismatch error for properly detected objects, expected missed and false detection error, and track switch error. The effectiveness of the proposed metric is demonstrated through a simulation study.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2506.03335.pdf' target='_blank'>https://arxiv.org/pdf/2506.03335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dheeraj Khanna, Jerrin Bright, Yuhao Chen, John S. Zelek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03335">SportMamba: Adaptive Non-Linear Multi-Object Tracking with State Space Models for Team Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in team sports is particularly challenging due to the fast-paced motion and frequent occlusions resulting in motion blur and identity switches, respectively. Predicting player positions in such scenarios is particularly difficult due to the observed highly non-linear motion patterns. Current methods are heavily reliant on object detection and appearance-based tracking, which struggle to perform in complex team sports scenarios, where appearance cues are ambiguous and motion patterns do not necessarily follow a linear pattern. To address these challenges, we introduce SportMamba, an adaptive hybrid MOT technique specifically designed for tracking in dynamic team sports. The technical contribution of SportMamba is twofold. First, we introduce a mamba-attention mechanism that models non-linear motion by implicitly focusing on relevant embedding dependencies. Second, we propose a height-adaptive spatial association metric to reduce ID switches caused by partial occlusions by accounting for scale variations due to depth changes. Additionally, we extend the detection search space with adaptive buffers to improve associations in fast-motion scenarios. Our proposed technique, SportMamba, demonstrates state-of-the-art performance on various metrics in the SportsMOT dataset, which is characterized by complex motion and severe occlusion. Furthermore, we demonstrate its generalization capability through zero-shot transfer to VIP-HTD, an ice hockey dataset.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2503.01547.pdf' target='_blank'>https://arxiv.org/pdf/2503.01547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arash Nasr Esfahani, Hamed Hosseini, Mehdi Tale Masouleh, Ahmad Kalhor, Hedieh Sajedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01547">AI-Driven Relocation Tracking in Dynamic Kitchen Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As smart homes become more prevalent in daily life, the ability to understand dynamic environments is essential which is increasingly dependent on AI systems. This study focuses on developing an intelligent algorithm which can navigate a robot through a kitchen, recognizing objects, and tracking their relocation. The kitchen was chosen as the testing ground due to its dynamic nature as objects are frequently moved, rearranged and replaced. Various techniques, such as SLAM feature-based tracking and deep learning-based object detection (e.g., Faster R-CNN), are commonly used for object tracking. Additionally, methods such as optical flow analysis and 3D reconstruction have also been used to track the relocation of objects. These approaches often face challenges when it comes to problems such as lighting variations and partial occlusions, where parts of the object are hidden in some frames but visible in others. The proposed method in this study leverages the YOLOv5 architecture, initialized with pre-trained weights and subsequently fine-tuned on a custom dataset. A novel method was developed, introducing a frame-scoring algorithm which calculates a score for each object based on its location and features within all frames. This scoring approach helps to identify changes by determining the best-associated frame for each object and comparing the results in each scene, overcoming limitations seen in other methods while maintaining simplicity in design. The experimental results demonstrate an accuracy of 97.72%, a precision of 95.83% and a recall of 96.84% for this algorithm, which highlights the efficacy of the model in detecting spatial changes.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2412.08321.pdf' target='_blank'>https://arxiv.org/pdf/2412.08321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrejÄÃ­, Oliver Kost, OndÅej Straka, Yuxuan Xia, Lennart Svensson, Ãngel F. GarcÃ­a-FernÃ¡ndez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08321">TGOSPA Metric Parameters Selection and Evaluation for Visual Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking algorithms are deployed in various applications, each with different performance requirements. For example, track switches pose significant challenges for offline scene understanding, as they hinder the accuracy of data interpretation. Conversely, in online surveillance applications, their impact is often minimal. This disparity underscores the need for application-specific performance evaluations that are both simple and mathematically sound. The trajectory generalized optimal sub-pattern assignment (TGOSPA) metric offers a principled approach to evaluate multi-object tracking performance. It accounts for localization errors, the number of missed and false objects, and the number of track switches, providing a comprehensive assessment framework. This paper illustrates the effective use of the TGOSPA metric in computer vision tasks, addressing challenges posed by the need for application-specific scoring methodologies. By exploring the TGOSPA parameter selection, we enable users to compare, comprehend, and optimize the performance of algorithms tailored for specific tasks, such as target tracking and training of detector or re-ID modules.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2412.00692.pdf' target='_blank'>https://arxiv.org/pdf/2412.00692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Tim Meinhardt, Orcun Cetintas, Cheng-Yen Yang, Sameer Satish Pusegaonkar, Benjamin Missaoui, Sujit Biswas, Zheng Tang, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00692">MCBLT: Multi-Camera Multi-Object 3D Tracking in Long Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object perception from multi-view cameras is crucial for intelligent systems, particularly in indoor environments, e.g., warehouses, retail stores, and hospitals. Most traditional multi-target multi-camera (MTMC) detection and tracking methods rely on 2D object detection, single-view multi-object tracking (MOT), and cross-view re-identification (ReID) techniques, without properly handling important 3D information by multi-view image aggregation. In this paper, we propose a 3D object detection and tracking framework, named MCBLT, which first aggregates multi-view images with necessary camera calibration parameters to obtain 3D object detections in bird's-eye view (BEV). Then, we introduce hierarchical graph neural networks (GNNs) to track these 3D detections in BEV for MTMC tracking results. Unlike existing methods, MCBLT has impressive generalizability across different scenes and diverse camera settings, with exceptional capability for long-term association handling. As a result, our proposed MCBLT establishes a new state-of-the-art on the AICity'24 dataset with $81.22$ HOTA, and on the WildTrack dataset with $95.6$ IDF1.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2405.15137.pdf' target='_blank'>https://arxiv.org/pdf/2405.15137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratyusha Musunuru, Yuchao Li, Jamison Weber, Dimitri Bertsekas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15137">An Approximate Dynamic Programming Framework for Occlusion-Robust Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we consider data association problems involving multi-object tracking (MOT). In particular, we address the challenges arising from object occlusions. We propose a framework called approximate dynamic programming track (ADPTrack), which applies dynamic programming principles to improve an existing method called the base heuristic. Given a set of tracks and the next target frame, the base heuristic extends the tracks by matching them to the objects of this target frame directly. In contrast, ADPTrack first processes a few subsequent frames and applies the base heuristic starting from the next target frame to obtain tentative tracks. It then leverages the tentative tracks to match the objects of the target frame. This tends to reduce the occlusion-based errors and leads to an improvement over the base heuristic. When tested on the MOT17 video dataset, the proposed method demonstrates a 0.7% improvement in the association accuracy (IDF1 metric) over a state-of-the-art method that is used as the base heuristic. It also obtains improvements with respect to all the other standard metrics. Empirically, we found that the improvements are particularly pronounced in scenarios where the video data is obtained by fixed-position cameras.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2404.09857.pdf' target='_blank'>https://arxiv.org/pdf/2404.09857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangwei Zhong, Kui Wu, Hai Ci, Churan Wang, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09857">Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied visual tracking is to follow a target object in dynamic 3D environments using an agent's egocentric vision. This is a vital and challenging skill for embodied agents. However, existing methods suffer from inefficient training and poor generalization. In this paper, we propose a novel framework that combines visual foundation models(VFM) and offline reinforcement learning(offline RL) to empower embodied visual tracking. We use a pre-trained VFM, such as "Tracking Anything", to extract semantic segmentation masks with text prompts. We then train a recurrent policy network with offline RL, e.g., Conservative Q-Learning, to learn from the collected demonstrations without online interactions. To further improve the robustness and generalization of the policy network, we also introduce a mask re-targeting mechanism and a multi-level data collection strategy. In this way, we can train a robust policy within an hour on a consumer-level GPU, e.g., Nvidia RTX 3090. We evaluate our agent on several high-fidelity environments with challenging situations, such as distraction and occlusion. The results show that our agent outperforms state-of-the-art methods in terms of sample efficiency, robustness to distractors, and generalization to unseen scenarios and targets. We also demonstrate the transferability of the learned agent from virtual environments to a real-world robot.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2402.17098.pdf' target='_blank'>https://arxiv.org/pdf/2402.17098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Gao, Shi-Min Li, Feng Gao, Fei Wang, Ru-Yue Yuan, Hamido Fujita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17098">In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based methods monopolize the latest research in the field of thermal infrared (TIR) object tracking. However, relying solely on deep learning models to obtain better tracking results requires carefully selecting feature information that is beneficial to representing the target object and designing a reasonable template update strategy, which undoubtedly increases the difficulty of model design. Thus, recent TIR tracking methods face many challenges in complex scenarios. This paper introduces a novel Deep Bayesian Filtering (DBF) method to enhance TIR tracking in these challenging situations. DBF is distinctive in its dual-model structure: the system and observation models. The system model leverages motion data to estimate the potential positions of the target object based on two-dimensional Brownian motion, thus generating a prior probability. Following this, the observation model comes into play upon capturing the TIR image. It serves as a classifier and employs infrared information to ascertain the likelihood of these estimated positions, creating a likelihood probability. According to the guidance of the two models, the position of the target object can be determined, and the template can be dynamically updated. Experimental analysis across several benchmark datasets reveals that DBF achieves competitive performance, surpassing most existing TIR tracking methods in complex scenarios.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2511.17609.pdf' target='_blank'>https://arxiv.org/pdf/2511.17609.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Van Ma, Unse Fatima, Tepy Sokun Chriv, Haroon Imran, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17609">3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2510.20807.pdf' target='_blank'>https://arxiv.org/pdf/2510.20807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dean L Slack, G Thomas Hudson, Thomas Winterbottom, Noura Al Moubayed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20807">Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2510.17860.pdf' target='_blank'>https://arxiv.org/pdf/2510.17860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zenghuang Fu, Xiaofeng Han, Mingda Jia, Jin ming Yang, Qi Zeng, Muyang Zahng, Changwei Wang, Weiliang Meng, Xiaopeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17860">DMTrack: Deformable State-Space Modeling for UAV Multi-Object Tracking with Kalman Fusion and Uncertainty-Aware Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) from unmanned aerial vehicles (UAVs) presents unique challenges due to unpredictable object motion, frequent occlusions, and limited appearance cues inherent to aerial viewpoints. These issues are further exacerbated by abrupt UAV movements, leading to unreliable trajectory estimation and identity switches. Conventional motion models, such as Kalman filters or static sequence encoders, often fall short in capturing both linear and non-linear dynamics under such conditions. To tackle these limitations, we propose DMTrack, a deformable motion tracking framework tailored for UAV-based MOT. Our DMTrack introduces three key components: DeformMamba, a deformable state-space predictor that dynamically aggregates historical motion states for adaptive trajectory modeling; MotionGate, a lightweight gating module that fuses Kalman and Mamba predictions based on motion context and uncertainty; and an uncertainty-aware association strategy that enhances identity preservation by aligning motion trends with prediction confidence. Extensive experiments on the VisDrone-MOT and UAVDT benchmarks demonstrate that our DMTrack achieves state-of-the-art performance in identity consistency and tracking accuracy, particularly under high-speed and non-linear motion. Importantly, our method operates without appearance models and maintains competitive efficiency, highlighting its practicality for robust UAV-based tracking.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2508.11323.pdf' target='_blank'>https://arxiv.org/pdf/2508.11323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Zhang, Xinyao Wang, Boxi Wu, Tu Zheng, Wang Yunhua, Zheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11323">Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking is a critical and challenging task in the field of autonomous driving. A common paradigm relies on modeling individual object motion, e.g., Kalman filters, to predict trajectories. While effective in simple scenarios, this approach often struggles in crowded environments or with inaccurate detections, as it overlooks the rich geometric relationships between objects. This highlights the need to leverage spatial cues. However, existing geometry-aware methods can be susceptible to interference from irrelevant objects, leading to ambiguous features and incorrect associations. To address this, we propose focusing on cue-consistency: identifying and matching stable spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency Tracker (DSC-Track) to implement this principle. Firstly, we design a unified spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative trajectory embeddings while suppressing interference. Secondly, our cue-consistency transformer module explicitly aligns consistent feature representations between historical tracks and current detections. Finally, a dynamic update mechanism preserves salient spatiotemporal information for stable online tracking. Extensive experiments on the nuScenes and Waymo Open Datasets validate the effectiveness and robustness of our approach. On the nuScenes benchmark, for instance, our method achieves state-of-the-art performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets, respectively.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2507.23473.pdf' target='_blank'>https://arxiv.org/pdf/2507.23473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Xie, Congxuan Zhang, Fagan Wang, Peng Liu, Feng Lu, Zhen Chen, Weiming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23473">CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread application of Unmanned Aerial Vehicles (UAVs) has raised serious public safety and privacy concerns, making UAV perception crucial for anti-UAV tasks. However, existing UAV tracking datasets predominantly feature conspicuous objects and lack diversity in scene complexity and attribute representation, limiting their applicability to real-world scenarios. To overcome these limitations, we present the CST Anti-UAV, a new thermal infrared dataset specifically designed for Single Object Tracking (SOT) in Complex Scenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k high-quality bounding box annotations, highlighting two key properties: a significant number of tiny-sized UAV targets and the diverse and complex scenes. To the best of our knowledge, CST Anti-UAV is the first dataset to incorporate complete manual frame-level attribute annotations, enabling precise evaluations under varied challenges. To conduct an in-depth performance analysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed dataset. Experimental results demonstrate that tracking tiny UAVs in complex environments remains a challenge, as the state-of-the-art method achieves only 35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410 dataset. These findings underscore the limitations of existing benchmarks and the need for further advancements in UAV tracking research. The CST Anti-UAV benchmark is about to be publicly released, which not only fosters the development of more robust SOT methods but also drives innovation in anti-UAV systems.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2507.01535.pdf' target='_blank'>https://arxiv.org/pdf/2507.01535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingxi Liu, Calvin Chen, Junhao Li, Guyang Yu, Haoqian Song, Xuchen Liu, Jinqiang Cui, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01535">TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Vision Transformer (ViT) model has long struggled with the challenge of quadratic complexity, a limitation that becomes especially critical in unmanned aerial vehicle (UAV) tracking systems, where data must be processed in real time. In this study, we explore the recently proposed State-Space Model, Mamba, leveraging its computational efficiency and capability for long-sequence modeling to effectively process dense image sequences in tracking tasks. First, we highlight the issue of temporal inconsistency in existing Mamba-based methods, specifically the failure to account for temporal continuity in the Mamba scanning mechanism. Secondly, building upon this insight,we propose TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model for handling image sequence of tracking problem. In our framework, the mamba scan is performed in a nested way while independently process temporal and spatial coherent patch tokens. While the template frame is encoded as query token and utilized for tracking in every scan. Extensive experiments conducted on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves state-of-the-art precision while offering noticeable higher speed in UAV tracking.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2505.20381.pdf' target='_blank'>https://arxiv.org/pdf/2505.20381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Chen, Yanqiu Yu, En Yu, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20381">ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-object tracking (RMOT) is an important research field in computer vision. Its task form is to guide the models to track the objects that conform to the language instruction. However, the RMOT task commonly requires clear language instructions, such methods often fail to work when complex language instructions with reasoning characteristics appear. In this work, we propose a new task, called Reasoning-based Multi-Object Tracking (ReaMOT). ReaMOT is a more challenging task that requires accurate reasoning about objects that match the language instruction with reasoning characteristic and tracking the objects' trajectories. To advance the ReaMOT task and evaluate the reasoning capabilities of tracking models, we construct ReaMOT Challenge, a reasoning-based multi-object tracking benchmark built upon 12 datasets. Specifically, it comprises 1,156 language instructions with reasoning characteristic, 423,359 image-language pairs, and 869 diverse scenes, which is divided into three levels of reasoning difficulty. In addition, we propose a set of evaluation metrics tailored for the ReaMOT task. Furthermore, we propose ReaTrack, a training-free framework for reasoning-based multi-object tracking based on large vision-language models (LVLM) and SAM2, as a baseline for the ReaMOT task. Extensive experiments on the ReaMOT Challenge benchmark demonstrate the effectiveness of our ReaTrack framework.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2411.15459.pdf' target='_blank'>https://arxiv.org/pdf/2411.15459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinqi Liu, Li Zhou, Zikun Zhou, Jianqiu Chen, Zhenyu He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15459">MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision-language tracking task aims to perform object tracking based on various modality references. Existing Transformer-based vision-language tracking methods have made remarkable progress by leveraging the global modeling ability of self-attention. However, current approaches still face challenges in effectively exploiting the temporal information and dynamically updating reference features during tracking. Recently, the State Space Model (SSM), known as Mamba, has shown astonishing ability in efficient long-sequence modeling. Particularly, its state space evolving process demonstrates promising capabilities in memorizing multimodal temporal information with linear complexity. Witnessing its success, we propose a Mamba-based vision-language tracking model to exploit its state space evolving ability in temporal space for robust multimodal tracking, dubbed MambaVLT. In particular, our approach mainly integrates a time-evolving hybrid state space block and a selective locality enhancement block, to capture contextual information for multimodal modeling and adaptive reference feature update. Besides, we introduce a modality-selection module that dynamically adjusts the weighting between visual and language references, mitigating potential ambiguities from either reference type. Extensive experimental results show that our method performs favorably against state-of-the-art trackers across diverse benchmarks.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2410.08529.pdf' target='_blank'>https://arxiv.org/pdf/2410.08529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Qian, Ruize Han, Junhui Hou, Linqi Song, Wei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08529">VOVTrack: Exploring the Potentiality in Videos for Open-Vocabulary Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary multi-object tracking (OVMOT) represents a critical new challenge involving the detection and tracking of diverse object categories in videos, encompassing both seen categories (base classes) and unseen categories (novel classes). This issue amalgamates the complexities of open-vocabulary object detection (OVD) and multi-object tracking (MOT). Existing approaches to OVMOT often merge OVD and MOT methodologies as separate modules, predominantly focusing on the problem through an image-centric lens. In this paper, we propose VOVTrack, a novel method that integrates object states relevant to MOT and video-centric training to address this challenge from a video object tracking standpoint. First, we consider the tracking-related state of the objects during tracking and propose a new prompt-guided attention mechanism for more accurate localization and classification (detection) of the time-varying objects. Subsequently, we leverage raw video data without annotations for training by formulating a self-supervised object similarity learning technique to facilitate temporal object association (tracking). Experimental results underscore that VOVTrack outperforms existing methods, establishing itself as a state-of-the-art solution for open-vocabulary tracking task.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2408.02049.pdf' target='_blank'>https://arxiv.org/pdf/2408.02049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Wu, Kun Sun, Pei An, Mathieu Salzmann, Yanning Zhang, Jiaqi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02049">3D Single-object Tracking in Point Clouds with High Temporal Variation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in the KITTI dataset. On the KITTI-HV with 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker CXTracker by 11.3%/15.7% in Success/Precision.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2407.14047.pdf' target='_blank'>https://arxiv.org/pdf/2407.14047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Qian, Ruize Han, Wei Feng, Junhui Hou, Linqi Song, Song Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14047">OCTrack: Benchmarking the Open-Corpus Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a novel yet practical problem of open-corpus multi-object tracking (OCMOT), which extends the MOT into localizing, associating, and recognizing generic-category objects of both seen (base) and unseen (novel) classes, but without the category text list as prompt. To study this problem, the top priority is to build a benchmark. In this work, we build OCTrackB, a large-scale and comprehensive benchmark, to provide a standard evaluation platform for the OCMOT problem. Compared to previous datasets, OCTrackB has more abundant and balanced base/novel classes and the corresponding samples for evaluation with less bias. We also propose a new multi-granularity recognition metric to better evaluate the generative object recognition in OCMOT. By conducting the extensive benchmark evaluation, we report and analyze the results of various state-of-the-art methods, which demonstrate the rationale of OCMOT, as well as the usefulness and advantages of OCTrackB.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2405.06765.pdf' target='_blank'>https://arxiv.org/pdf/2405.06765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasios Arsenos, Vasileios Karampinis, Evangelos Petrongonas, Christos Skliros, Dimitrios Kollias, Stefanos Kollias, Athanasios Voulodimos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06765">Common Corruptions for Enhancing and Evaluating Robustness in Air-to-Air Visual Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The main barrier to achieving fully autonomous flights lies in autonomous aircraft navigation. Managing non-cooperative traffic presents the most important challenge in this problem. The most efficient strategy for handling non-cooperative traffic is based on monocular video processing through deep learning models. This study contributes to the vision-based deep learning aircraft detection and tracking literature by investigating the impact of data corruption arising from environmental and hardware conditions on the effectiveness of these methods. More specifically, we designed $7$ types of common corruptions for camera inputs taking into account real-world flight conditions. By applying these corruptions to the Airborne Object Tracking (AOT) dataset we constructed the first robustness benchmark dataset named AOT-C for air-to-air aerial object detection. The corruptions included in this dataset cover a wide range of challenging conditions such as adverse weather and sensor noise. The second main contribution of this letter is to present an extensive experimental evaluation involving $8$ diverse object detectors to explore the degradation in the performance under escalating levels of corruptions (domain shifts). Based on the evaluation results, the key observations that emerge are the following: 1) One-stage detectors of the YOLO family demonstrate better robustness, 2) Transformer-based and multi-stage detectors like Faster R-CNN are extremely vulnerable to corruptions, 3) Robustness against corruptions is related to the generalization ability of models. The third main contribution is to present that finetuning on our augmented synthetic data results in improvements in the generalisation ability of the object detector in real-world flight experiments.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2405.06749.pdf' target='_blank'>https://arxiv.org/pdf/2405.06749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Karampinis, Anastasios Arsenos, Orfeas Filippopoulos, Evangelos Petrongonas, Christos Skliros, Dimitrios Kollias, Stefanos Kollias, Athanasios Voulodimos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06749">Ensuring UAV Safety: A Vision-only and Real-time Framework for Collision Avoidance Through Object Detection, Tracking, and Distance Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last twenty years, unmanned aerial vehicles (UAVs) have garnered growing interest due to their expanding applications in both military and civilian domains. Detecting non-cooperative aerial vehicles with efficiency and estimating collisions accurately are pivotal for achieving fully autonomous aircraft and facilitating Advanced Air Mobility (AAM). This paper presents a deep-learning framework that utilizes optical sensors for the detection, tracking, and distance estimation of non-cooperative aerial vehicles. In implementing this comprehensive sensing framework, the availability of depth information is essential for enabling autonomous aerial vehicles to perceive and navigate around obstacles. In this work, we propose a method for estimating the distance information of a detected aerial object in real time using only the input of a monocular camera. In order to train our deep learning components for the object detection, tracking and depth estimation tasks we utilize the Amazon Airborne Object Tracking (AOT) Dataset. In contrast to previous approaches that integrate the depth estimation module into the object detector, our method formulates the problem as image-to-image translation. We employ a separate lightweight encoder-decoder network for efficient and robust depth estimation. In a nutshell, the object detection module identifies and localizes obstacles, conveying this information to both the tracking module for monitoring obstacle movement and the depth estimation module for calculating distances. Our approach is evaluated on the Airborne Object Tracking (AOT) dataset which is the largest (to the best of our knowledge) air-to-air airborne object dataset.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2404.09504.pdf' target='_blank'>https://arxiv.org/pdf/2404.09504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiangqiang Wu, Antoni B. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09504">Learning Tracking Representations from Single Point Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing deep trackers are typically trained with largescale video frames with annotated bounding boxes. However, these bounding boxes are expensive and time-consuming to annotate, in particular for large scale datasets. In this paper, we propose to learn tracking representations from single point annotations (i.e., 4.5x faster to annotate than the traditional bounding box) in a weakly supervised manner. Specifically, we propose a soft contrastive learning (SoCL) framework that incorporates target objectness prior into end-to-end contrastive learning. Our SoCL consists of adaptive positive and negative sample generation, which is memory-efficient and effective for learning tracking representations. We apply the learned representation of SoCL to visual tracking and show that our method can 1) achieve better performance than the fully supervised baseline trained with box annotations under the same annotation time cost; 2) achieve comparable performance of the fully supervised baseline by using the same number of training frames and meanwhile reducing annotation time cost by 78% and total fees by 85%; 3) be robust to annotation noise.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2403.20225.pdf' target='_blank'>https://arxiv.org/pdf/2403.20225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Woo, Kwanyong Park, Inkyu Shin, Myungchul Kim, In So Kweon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20225">MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-target multi-camera tracking is a crucial task that involves identifying and tracking individuals over time using video streams from multiple cameras. This task has practical applications in various fields, such as visual surveillance, crowd behavior analysis, and anomaly detection. However, due to the difficulty and cost of collecting and labeling data, existing datasets for this task are either synthetically generated or artificially constructed within a controlled camera network setting, which limits their ability to model real-world dynamics and generalize to diverse camera configurations. To address this issue, we present MTMMC, a real-world, large-scale dataset that includes long video sequences captured by 16 multi-modal cameras in two different environments - campus and factory - across various time, weather, and season conditions. This dataset provides a challenging test-bed for studying multi-camera tracking under diverse real-world complexities and includes an additional input modality of spatially aligned and temporally synchronized RGB and thermal cameras, which enhances the accuracy of multi-camera tracking. MTMMC is a super-set of existing datasets, benefiting independent fields such as person detection, re-identification, and multiple object tracking. We provide baselines and new learning setups on this dataset and set the reference scores for future studies. The datasets, models, and test server will be made publicly available.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2512.19583.pdf' target='_blank'>https://arxiv.org/pdf/2512.19583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinhuai Wang, Runyi Yu, Hok Wai Tsui, Xiaoyi Lin, Hui Zhang, Qihan Zhao, Ke Fan, Miao Li, Jie Song, Jingbo Wang, Qifeng Chen, Ping Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19583">Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2512.10102.pdf' target='_blank'>https://arxiv.org/pdf/2512.10102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neelima Prasad, Jarek Reynolds, Neel Karsanbhai, Tanusree Sharma, Lotus Zhang, Abigale Stangl, Yang Wang, Leah Findlater, Danna Gurari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10102">Hierarchical Instance Tracking to Balance Privacy Preservation with Accessible Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel task, hierarchical instance tracking, which entails tracking all instances of predefined categories of objects and parts, while maintaining their hierarchical relationships. We introduce the first benchmark dataset supporting this task, consisting of 2,765 unique entities that are tracked in 552 videos and belong to 40 categories (across objects and parts). Evaluation of seven variants of four models tailored to our novel task reveals the new dataset is challenging. Our dataset is available at https://vizwiz.org/tasks-and-datasets/hierarchical-instance-tracking/
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2511.16494.pdf' target='_blank'>https://arxiv.org/pdf/2511.16494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongcai Tan, Lan Wei, Dandan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.16494">Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2509.21715.pdf' target='_blank'>https://arxiv.org/pdf/2509.21715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Yang, Gady Agam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21715">Motion-Aware Transformer for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in videos remains challenging due to complex object motions and crowded scenes. Recent DETR-based frameworks offer end-to-end solutions but typically process detection and tracking queries jointly within a single Transformer Decoder layer, leading to conflicts and degraded association accuracy. We introduce the Motion-Aware Transformer (MATR), which explicitly predicts object movements across frames to update track queries in advance. By reducing query collisions, MATR enables more consistent training and improves both detection and association. Extensive experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers significant gains across standard metrics. On DanceTrack, MATR improves HOTA by more than 9 points over MOTR without additional data and reaches a new state-of-the-art score of 71.3 with supplementary data. MATR also achieves state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6 mHOTA) without relying on external datasets. These results demonstrate that explicitly modeling motion within end-to-end Transformers offers a simple yet highly effective approach to advancing multi-object tracking.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2509.11873.pdf' target='_blank'>https://arxiv.org/pdf/2509.11873.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anne Marthe Sophie Ngo Bibinbe, Patrick Gagnon, Jamie Ahloy-Dallaire, Eric R. Paquet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11873">Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision livestock farming requires advanced monitoring tools to meet the increasing management needs of the industry. Computer vision systems capable of long-term multi-animal tracking (MAT) are essential for continuous behavioral monitoring in livestock production. MAT, a specialized subset of multi-object tracking (MOT), shares many challenges with MOT, but also faces domain-specific issues including frequent animal occlusion, highly similar appearances among animals, erratic motion patterns, and a wide range of behavior types. While some existing MAT tools are user-friendly and widely adopted, they often underperform compared to state-of-the-art MOT methods, which can result in inaccurate downstream tasks such as behavior analysis, health state estimation, and related applications. In this study, we benchmarked both MAT and MOT approaches for long-term tracking of pigs. We compared tools such as DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT, cross-input consistency, and newer approaches like Track-Anything and PromptTrack. All methods were evaluated on a 10-minute pig tracking dataset. Our results demonstrate that, overall, MOT approaches outperform traditional MAT tools, even for long-term tracking scenarios. These findings highlight the potential of recent MOT techniques to enhance the accuracy and reliability of automated livestock tracking.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2508.14370.pdf' target='_blank'>https://arxiv.org/pdf/2508.14370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Hashempoor, Yu Dong Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14370">FastTracker: Real-Time and Accurate Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional multi-object tracking (MOT) systems are predominantly designed for pedestrian tracking and often exhibit limited generalization to other object categories. This paper presents a generalized tracking framework capable of handling multiple object types, with a particular emphasis on vehicle tracking in complex traffic scenes. The proposed method incorporates two key components: (1) an occlusion-aware re-identification mechanism that enhances identity preservation for heavily occluded objects, and (2) a road-structure-aware tracklet refinement strategy that utilizes semantic scene priors such as lane directions, crosswalks, and road boundaries to improve trajectory continuity and accuracy. In addition, we introduce a new benchmark dataset comprising diverse vehicle classes with frame-level tracking annotations, specifically curated to support evaluation of vehicle-focused tracking methods. Extensive experimental results demonstrate that the proposed approach achieves robust performance on both the newly introduced dataset and several public benchmarks, highlighting its effectiveness in general-purpose object tracking. While our framework is designed for generalized multi-class tracking, it also achieves strong performance on conventional benchmarks, with HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark are available: github.com/Hamidreza-Hashempoor/FastTracker, huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2507.21411.pdf' target='_blank'>https://arxiv.org/pdf/2507.21411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kentaro Takahira, Yue Yu, Takanori Fujiwara, Ryo Suzuki, Huamin Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21411">InSituTale: Enhancing Augmented Data Storytelling with Physical Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented data storytelling enhances narrative delivery by integrating visualizations with physical environments and presenter actions. Existing systems predominantly rely on body gestures or speech to control visualizations, leaving interactions with physical objects largely underexplored. We introduce augmented physical data storytelling, an approach enabling presenters to manipulate visualizations through physical object interactions. To inform this approach, we first conducted a survey of data-driven presentations to identify common visualization commands. We then conducted workshops with nine HCI/VIS researchers to collect mappings between physical manipulations and these commands. Guided by these insights, we developed InSituTale, a prototype that combines object tracking via a depth camera with Vision-LLM for detecting real-world events. Through physical manipulations, presenters can dynamically execute various visualization commands, delivering cohesive data storytelling experiences that blend physical and digital elements. A user study with 12 participants demonstrated that InSituTale enables intuitive interactions, offers high utility, and facilitates an engaging presentation experience.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2507.18594.pdf' target='_blank'>https://arxiv.org/pdf/2507.18594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuecheng Bai, Yuxiang Wang, Boyu Hu, Qinyuan Jie, Chuanzhi Xu, Hongru Xiao, Kechen Li, Vera Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18594">DRWKV: Focusing on Object Edges for Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2506.05543.pdf' target='_blank'>https://arxiv.org/pdf/2506.05543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sethuraman TV, Savya Khosla, Vignesh Srinivasakumar, Jiahui Huang, Seoung Wug Oh, Simon Jenni, Derek Hoiem, Joon-Young Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05543">FRAME: Pre-Training Video Feature Representations via Anticipation and Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense video prediction tasks, such as object tracking and semantic segmentation, require video encoders that generate temporally consistent, spatially dense features for every frame. However, existing approaches fall short: image encoders like DINO or CLIP lack temporal awareness, while video models such as VideoMAE underperform compared to image encoders on dense prediction tasks. We address this gap with FRAME, a self-supervised video frame encoder tailored for dense video understanding. FRAME learns to predict current and future DINO patch features from past and present RGB frames, leading to spatially precise and temporally coherent representations. To our knowledge, FRAME is the first video encoder to leverage image-based models for dense prediction while outperforming them on tasks requiring fine-grained visual correspondence. As an auxiliary capability, FRAME aligns its class token with CLIP's semantic space, supporting language-driven tasks such as video classification. We evaluate FRAME across six dense prediction tasks on seven datasets, where it consistently outperforms image encoders and existing self-supervised video models. Despite its versatility, FRAME maintains a compact architecture suitable for a range of downstream applications.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2505.00739.pdf' target='_blank'>https://arxiv.org/pdf/2505.00739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiushi Yang, Yuan Yao, Miaomiao Cui, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00739">MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional capabilities in interactive object segmentation for both images and videos. However, as a foundational model on interactive segmentation, SAM2 performs segmentation directly based on mask memory from the past six frames, leading to two significant challenges. Firstly, during inference in videos, objects may disappear since SAM2 relies solely on memory without accounting for object motion information, which limits its long-range object tracking capabilities. Secondly, its memory is constructed from fixed past frames, making it susceptible to challenges associated with object disappearance or occlusion, due to potentially inaccurate segmentation results in memory. To address these problems, we present MoSAM, incorporating two key strategies to integrate object motion cues into the model and establish more reliable feature memory. Firstly, we propose Motion-Guided Prompting (MGP), which represents the object motion in both sparse and dense manners, then injects them into SAM2 through a set of motion-guided prompts. MGP enables the model to adjust its focus towards the direction of motion, thereby enhancing the object tracking capabilities. Furthermore, acknowledging that past segmentation results may be inaccurate, we devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically identifies frames likely to contain accurate segmentation in both pixel- and frame-level. By eliminating potentially inaccurate mask predictions from memory, we can leverage more reliable memory features to exploit similar regions for improving segmentation results. Extensive experiments on various benchmarks of video object segmentation and video instance segmentation demonstrate that our MoSAM achieves state-of-the-art results compared to other competitors.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2503.13023.pdf' target='_blank'>https://arxiv.org/pdf/2503.13023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michal Danilowicz, Tomasz Kryjak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13023">Real-Time Multi-Object Tracking using YOLOv8 and SORT on a SoC FPGA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is one of the most important problems in computer vision and a key component of any vision-based perception system used in advanced autonomous mobile robotics. Therefore, its implementation on low-power and real-time embedded platforms is highly desirable. Modern MOT algorithms should be able to track objects of a given class (e.g. people or vehicles). In addition, the number of objects to be tracked is not known in advance, and they may appear and disappear at any time, as well as be obscured. For these reasons, the most popular and successful approaches have recently been based on the tracking paradigm. Therefore, the presence of a high quality object detector is essential, which in practice accounts for the vast majority of the computational and memory complexity of the whole MOT system. In this paper, we propose an FPGA (Field-Programmable Gate Array) implementation of an embedded MOT system based on a quantized YOLOv8 detector and the SORT (Simple Online Realtime Tracker) tracker. We use a modified version of the FINN framework to utilize external memory for model parameters and to support operations necessary required by YOLOv8. We discuss the evaluation of detection and tracking performance using the COCO and MOT15 datasets, where we achieve 0.21 mAP and 38.9 MOTA respectively. As the computational platform, we use an MPSoC system (Zynq UltraScale+ device from AMD/Xilinx) where the detector is deployed in reprogrammable logic and the tracking algorithm is implemented in the processor system.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2502.17434.pdf' target='_blank'>https://arxiv.org/pdf/2502.17434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Li, Mingxi Jia, Tuluhan Akbulut, Yu Xiang, George Konidaris, Srinath Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17434">V-HOP: Visuo-Haptic 6D Object Pose Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Project website: https://ivl.cs.brown.edu/research/v-hop
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2501.15953.pdf' target='_blank'>https://arxiv.org/pdf/2501.15953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Chu, Yicong Li, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15953">Understanding Long Videos via LLM-Powered Entity Relation Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The analysis of extended video content poses unique challenges in artificial intelligence, particularly when dealing with the complexity of tracking and understanding visual elements across time. Current methodologies that process video frames sequentially struggle to maintain coherent tracking of objects, especially when these objects temporarily vanish and later reappear in the footage. A critical limitation of these approaches is their inability to effectively identify crucial moments in the video, largely due to their limited grasp of temporal relationships. To overcome these obstacles, we present GraphVideoAgent, a cutting-edge system that leverages the power of graph-based object tracking in conjunction with large language model capabilities. At its core, our framework employs a dynamic graph structure that maps and monitors the evolving relationships between visual entities throughout the video sequence. This innovative approach enables more nuanced understanding of how objects interact and transform over time, facilitating improved frame selection through comprehensive contextual awareness. Our approach demonstrates remarkable effectiveness when tested against industry benchmarks. In evaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2 improvement over existing methods while requiring analysis of only 8.2 frames on average. Similarly, testing on the NExT-QA benchmark yielded a 2.0 performance increase with an average frame requirement of 8.1. These results underscore the efficiency of our graph-guided methodology in enhancing both accuracy and computational performance in long-form video understanding tasks.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2501.10129.pdf' target='_blank'>https://arxiv.org/pdf/2501.10129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Futian Wang, Fengxiang Liu, Xiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10129">Spatio-temporal Graph Learning on Adaptive Mined Key Frames for High-performance Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of multi-object tracking, the challenge of accurately capturing the spatial and temporal relationships between objects in video sequences remains a significant hurdle. This is further complicated by frequent occurrences of mutual occlusions among objects, which can lead to tracking errors and reduced performance in existing methods. Motivated by these challenges, we propose a novel adaptive key frame mining strategy that addresses the limitations of current tracking approaches. Specifically, we introduce a Key Frame Extraction (KFE) module that leverages reinforcement learning to adaptively segment videos, thereby guiding the tracker to exploit the intrinsic logic of the video content. This approach allows us to capture structured spatial relationships between different objects as well as the temporal relationships of objects across frames. To tackle the issue of object occlusions, we have developed an Intra-Frame Feature Fusion (IFF) module. Unlike traditional graph-based methods that primarily focus on inter-frame feature fusion, our IFF module uses a Graph Convolutional Network (GCN) to facilitate information exchange between the target and surrounding objects within a frame. This innovation significantly enhances target distinguishability and mitigates tracking loss and appearance similarity due to occlusions. By combining the strengths of both long and short trajectories and considering the spatial relationships between objects, our proposed tracker achieves impressive results on the MOT17 dataset, i.e., 68.6 HOTA, 81.0 IDF1, 66.6 AssA, and 893 IDS, proving its effectiveness and accuracy.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2411.18476.pdf' target='_blank'>https://arxiv.org/pdf/2411.18476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangtao Shuai, Martin Baerveldt, Manh Nguyen-Duc, Anh Le-Tuan, Manfred Hauswirth, Danh Le-Phuoc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18476">A comparison of extended object tracking with multi-modal sensors in indoor environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a preliminary study of an efficient object tracking approach, comparing the performance of two different 3D point cloud sensory sources: LiDAR and stereo cameras, which have significant price differences. In this preliminary work, we focus on single object tracking. We first developed a fast heuristic object detector that utilizes prior information about the environment and target. The resulting target points are subsequently fed into an extended object tracking framework, where the target shape is parameterized using a star-convex hypersurface model. Experimental results show that our object tracking method using a stereo camera achieves performance similar to that of a LiDAR sensor, with a cost difference of more than tenfold.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2411.08433.pdf' target='_blank'>https://arxiv.org/pdf/2411.08433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiang Wang, Jiaxin Liu, Miaojie Feng, Zhaoxing Zhang, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08433">3D Multi-Object Tracking with Semi-Supervised GRU-Kalman Filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT), a fundamental component of environmental perception, is essential for intelligent systems like autonomous driving and robotic sensing. Although Tracking-by-Detection frameworks have demonstrated excellent performance in recent years, their application in real-world scenarios faces significant challenges. Object movement in complex environments is often highly nonlinear, while existing methods typically rely on linear approximations of motion. Furthermore, system noise is frequently modeled as a Gaussian distribution, which fails to capture the true complexity of the noise dynamics. These oversimplified modeling assumptions can lead to significant reductions in tracking precision. To address this, we propose a GRU-based MOT method, which introduces a learnable Kalman filter into the motion module. This approach is able to learn object motion characteristics through data-driven learning, thereby avoiding the need for manual model design and model error. At the same time, to avoid abnormal supervision caused by the wrong association between annotations and trajectories, we design a semi-supervised learning strategy to accelerate the convergence speed and improve the robustness of the model. Evaluation experiment on the nuScenes and Argoverse2 datasets demonstrates that our system exhibits superior performance and significant potential compared to traditional TBD methods.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2410.02094.pdf' target='_blank'>https://arxiv.org/pdf/2410.02094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sabine Muzellec, Drew Linsley, Alekh K. Ashok, Ennio Mingolla, Girik Malik, Rufin VanRullen, Thomas Serre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02094">Tracking objects that change in appearance with phase synchrony</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objects we encounter often change appearance as we interact with them. Changes in illumination (shadows), object pose, or the movement of non-rigid objects can drastically alter available image features. How do biological visual systems track objects as they change? One plausible mechanism involves attentional mechanisms for reasoning about the locations of objects independently of their appearances -- a capability that prominent neuroscience theories have associated with computing through neural synchrony. Here, we describe a novel deep learning circuit that can learn to precisely control attention to features separately from their location in the world through neural synchrony: the complex-valued recurrent neural network (CV-RNN). Next, we compare object tracking in humans, the CV-RNN, and other deep neural networks (DNNs), using FeatureTracker: a large-scale challenge that asks observers to track objects as their locations and appearances change in precisely controlled ways. While humans effortlessly solved FeatureTracker, state-of-the-art DNNs did not. In contrast, our CV-RNN behaved similarly to humans on the challenge, providing a computational proof-of-concept for the role of phase synchronization as a neural substrate for tracking appearance-morphing objects as they move about.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2407.04249.pdf' target='_blank'>https://arxiv.org/pdf/2407.04249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Hashempoor, Rosemary Koikara, Yu Dong Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04249">FeatureSORT: Essential Features for Effective Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FeatureSORT, a simple yet effective online multiple object tracker that reinforces the DeepSORT baseline with a redesigned detector and additional feature cues. In contrast to conventional detectors that only provide bounding boxes, our modified YOLOX architecture is extended to output multiple appearance attributes, including clothing color, clothing style, and motion direction, alongside the bounding boxes. These feature cues, together with a ReID network, form complementary embeddings that substantially improve association accuracy. Furthermore, we incorporate stronger post-processing strategies, such as global linking and Gaussian Smoothing Process interpolation, to handle missing associations and detections. During online tracking, we define a measurement-to-track distance function that jointly considers IoU, direction, color, style, and ReID similarity. This design enables FeatureSORT to maintain consistent identities through longer occlusions while reducing identity switches. Extensive experiments on standard MOT benchmarks demonstrate that FeatureSORT achieves state-of-the-art online performance, with MOTA scores of 79.7 on MOT16, 80.6 on MOT17, 77.9 on MOT20, and 92.2 on DanceTrack, underscoring the effectiveness of feature-enriched detection and modular post processing in advancing multi-object tracking.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2404.13953.pdf' target='_blank'>https://arxiv.org/pdf/2404.13953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinzhe Xu, Huajian Huang, Yingshu Chen, Sai-Kit Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13953">360VOTS: Visual Object Tracking and Segmentation in Omnidirectional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking and segmentation in omnidirectional videos are challenging due to the wide field-of-view and large spherical distortion brought by 360Â° images. To alleviate these problems, we introduce a novel representation, extended bounding field-of-view (eBFoV), for target localization and use it as the foundation of a general 360 tracking framework which is applicable for both omnidirectional visual object tracking and segmentation tasks. Building upon our previous work on omnidirectional visual object tracking (360VOT), we propose a comprehensive dataset and benchmark that incorporates a new component called omnidirectional video object segmentation (360VOS). The 360VOS dataset includes 290 sequences accompanied by dense pixel-wise masks and covers a broader range of target categories. To support both the development and evaluation of algorithms in this domain, we divide the dataset into a training subset with 170 sequences and a testing subset with 120 sequences. Furthermore, we tailor evaluation metrics for both omnidirectional tracking and segmentation to ensure rigorous assessment. Through extensive experiments, we benchmark state-of-the-art approaches and demonstrate the effectiveness of our proposed 360 tracking framework and training dataset. Homepage: https://360vots.hkustvgd.com/
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2403.10574.pdf' target='_blank'>https://arxiv.org/pdf/2403.10574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang, Liangtao Shi, Shuxiang Song, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10574">Autoregressive Queries for Adaptive Tracking with Spatio-TemporalTransformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rich spatio-temporal information is crucial to capture the complicated target appearance variations in visual tracking. However, most top-performing tracking algorithms rely on many hand-crafted components for spatio-temporal information aggregation. Consequently, the spatio-temporal information is far away from being fully explored. To alleviate this issue, we propose an adaptive tracker with spatio-temporal transformers (named AQATrack), which adopts simple autoregressive queries to effectively learn spatio-temporal information without many hand-designed components. Firstly, we introduce a set of learnable and autoregressive queries to capture the instantaneous target appearance changes in a sliding window fashion. Then, we design a novel attention mechanism for the interaction of existing queries to generate a new query in current frame. Finally, based on the initial target template and learnt autoregressive queries, a spatio-temporal information fusion module (STM) is designed for spatiotemporal formation aggregation to locate a target object. Benefiting from the STM, we can effectively combine the static appearance and instantaneous changes to guide robust tracking. Extensive experiments show that our method significantly improves the tracker's performance on six popular tracking benchmarks: LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2402.04519.pdf' target='_blank'>https://arxiv.org/pdf/2402.04519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhao, Shiyu Hu, Yipei Wang, Jing Zhang, Yimin Hu, Rongshuai Liu, Haibin Ling, Yin Li, Renshu Li, Kun Liu, Jiadong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04519">BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking (SOT) is a fundamental problem in computer vision, with a wide range of applications, including autonomous driving, augmented reality, and robot navigation. The robustness of SOT faces two main challenges: tiny target and fast motion. These challenges are especially manifested in videos captured by unmanned aerial vehicles (UAV), where the target is usually far away from the camera and often with significant motion relative to the camera. To evaluate the robustness of SOT methods, we propose BioDrone -- the first bionic drone-based visual benchmark for SOT. Unlike existing UAV datasets, BioDrone features videos captured from a flapping-wing UAV system with a major camera shake due to its aerodynamics. BioDrone hence highlights the tracking of tiny targets with drastic changes between consecutive frames, providing a new robust vision benchmark for SOT. To date, BioDrone offers the largest UAV-based SOT benchmark with high-quality fine-grained manual annotations and automatically generates frame-level labels, designed for robust vision analyses. Leveraging our proposed BioDrone, we conduct a systematic evaluation of existing SOT methods, comparing the performance of 20 representative models and studying novel means of optimizing a SOTA method (KeepTrack KeepTrack) for robust SOT. Our evaluation leads to new baselines and insights for robust SOT. Moving forward, we hope that BioDrone will not only serve as a high-quality benchmark for robust SOT, but also invite future research into robust computer vision. The database, toolkits, evaluation server, and baseline results are available at http://biodrone.aitestunion.com.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2401.12743.pdf' target='_blank'>https://arxiv.org/pdf/2401.12743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Xie, Wankou Yang, Chunyu Wang, Lei Chu, Yue Cao, Chao Ma, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12743">Correlation-Embedded Transformer Tracking: A Single-Branch Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing robust and discriminative appearance models has been a long-standing research challenge in visual object tracking. In the prevalent Siamese-based paradigm, the features extracted by the Siamese-like networks are often insufficient to model the tracked targets and distractor objects, thereby hindering them from being robust and discriminative simultaneously. While most Siamese trackers focus on designing robust correlation operations, we propose a novel single-branch tracking framework inspired by the transformer. Unlike the Siamese-like feature extraction, our tracker deeply embeds cross-image feature correlation in multiple layers of the feature network. By extensively matching the features of the two images through multiple layers, it can suppress non-target features, resulting in target-aware feature extraction. The output features can be directly used for predicting target locations without additional correlation steps. Thus, we reformulate the two-branch Siamese tracking as a conceptually simple, fully transformer-based Single-Branch Tracking pipeline, dubbed SBT. After conducting an in-depth analysis of the SBT baseline, we summarize many effective design principles and propose an improved tracker dubbed SuperSBT. SuperSBT adopts a hierarchical architecture with a local modeling layer to enhance shallow-level features. A unified relation modeling is proposed to remove complex handcrafted layer pattern designs. SuperSBT is further improved by masked image modeling pre-training, integrating temporal modeling, and equipping with dedicated prediction heads. Thus, SuperSBT outperforms the SBT baseline by 4.7%,3.0%, and 4.5% AUC scores in LaSOT, TrackingNet, and GOT-10K. Notably, SuperSBT greatly raises the speed of SBT from 37 FPS to 81 FPS. Extensive experiments show that our method achieves superior results on eight VOT benchmarks.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2401.03872.pdf' target='_blank'>https://arxiv.org/pdf/2401.03872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alan Lukezic, Ziga Trojer, Jiri Matas, Matej Kristan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03872">A New Dataset and a Distractor-Aware Architecture for Transparent Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performance of modern trackers degrades substantially on transparent objects compared to opaque objects. This is largely due to two distinct reasons. Transparent objects are unique in that their appearance is directly affected by the background. Furthermore, transparent object scenes often contain many visually similar objects (distractors), which often lead to tracking failure. However, development of modern tracking architectures requires large training sets, which do not exist in transparent object tracking. We present two contributions addressing the aforementioned issues. We propose the first transparent object tracking training dataset Trans2k that consists of over 2k sequences with 104,343 images overall, annotated by bounding boxes and segmentation masks. Standard trackers trained on this dataset consistently improve by up to 16%. Our second contribution is a new distractor-aware transparent object tracker (DiTra) that treats localization accuracy and target identification as separate tasks and implements them by a novel architecture. DiTra sets a new state-of-the-art in transparent object tracking and generalizes well to opaque objects.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2512.22244.pdf' target='_blank'>https://arxiv.org/pdf/2512.22244.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniyal Ganiuly, Nurzhau Bolatbek, Assel Smaiyl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22244">Failure Analysis of Safety Controllers in Autonomous Vehicles Under Object-Based LiDAR Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous vehicles rely on LiDAR based perception to support safety critical control functions such as adaptive cruise control and automatic emergency braking. While previous research has shown that LiDAR perception can be manipulated through object based spoofing and injection attacks, the impact of such attacks on vehicle safety controllers is still not well understood. This paper presents a systematic failure analysis of longitudinal safety controllers under object based LiDAR attacks in highway driving scenarios. The study focuses on realistic cut in and car following situations in which adversarial objects introduce persistent perception errors without directly modifying vehicle control software. A high fidelity simulation framework integrating LiDAR perception, object tracking, and closed loop vehicle control is used to evaluate how false and displaced object detections propagate through the perception planning and control pipeline. The results demonstrate that even short duration LiDAR induced object hallucinations can trigger unsafe braking, delayed responses to real hazards, and unstable control behavior. In cut in scenarios, a clear increase in unsafe deceleration events and time to collision violations is observed when compared to benign conditions, despite identical controller parameters. The analysis further shows that controller failures are more strongly influenced by the temporal consistency of spoofed objects than by spatial inaccuracies alone. These findings reveal a critical gap between perception robustness and control level safety guarantees in autonomous driving systems. By explicitly characterizing safety controller failure modes under adversarial perception, this work provides practical insights for the design of attack aware safety mechanisms and more resilient control strategies for LiDAR dependent autonomous vehicles.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2511.15077.pdf' target='_blank'>https://arxiv.org/pdf/2511.15077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengjing Tian, Yinan Han, Xiantong Zhao, Xuehu Liu, Qi Lang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15077">MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2510.19981.pdf' target='_blank'>https://arxiv.org/pdf/2510.19981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martha Teiko Teye, Ori Maoz, Matthias Rottmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.19981">FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework that builds on existing 3D detectors by introducing a transformer-based smoother and a fusion-driven tracker. Inspired by query-based tracking frameworks, FutrTrack employs a multimodal two-stage transformer refinement and tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without the need for an explicit motion model. The tracker assigns and propagates identities across frames, leveraging both geometric and semantic cues for robust re-identification under occlusion and viewpoint changes. Prior to tracking, we refine sequences of bounding boxes with a temporal smoother over a moving window to refine trajectories, reduce jitter, and improve spatial consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that query-based transformer tracking methods benefit significantly from multimodal sensor features compared with previous single-sensor approaches. With an aMOTA of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D MOT benchmarks, reducing identity switches while maintaining competitive accuracy. Our approach provides an efficient framework for improving transformer-based trackers to compete with other neural-network-based methods even with limited data and without pretraining.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2507.23251.pdf' target='_blank'>https://arxiv.org/pdf/2507.23251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fereshteh Aghaee Meibodi, Shadi Alijani, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23251">A Deep Dive into Generic Object Tracking: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generic object tracking remains an important yet challenging task in computer vision due to complex spatio-temporal dynamics, especially in the presence of occlusions, similar distractors, and appearance variations. Over the past two decades, a wide range of tracking paradigms, including Siamese-based trackers, discriminative trackers, and, more recently, prominent transformer-based approaches, have been introduced to address these challenges. While a few existing survey papers in this field have either concentrated on a single category or widely covered multiple ones to capture progress, our paper presents a comprehensive review of all three categories, with particular emphasis on the rapidly evolving transformer-based methods. We analyze the core design principles, innovations, and limitations of each approach through both qualitative and quantitative comparisons. Our study introduces a novel categorization and offers a unified visual and tabular comparison of representative methods. Additionally, we organize existing trackers from multiple perspectives and summarize the major evaluation benchmarks, highlighting the fast-paced advancements in transformer-based tracking driven by their robust spatio-temporal modeling capabilities.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2507.02393.pdf' target='_blank'>https://arxiv.org/pdf/2507.02393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokyeong Lee, Sithu Aung, Junyong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02393">PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection (M3OD) has long faced challenges due to data scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity. Although various weakly supervised methods and pseudo-labeling methods have been proposed to address these issues, they are mostly limited by domain-specific learning or rely solely on shape information from a single observation. In this paper, we propose a novel pseudo-labeling framework that uses only video data and is more robust to occlusion, without requiring a multi-view setup, additional sensors, camera poses, or domain-specific training. Specifically, we explore a technique for aggregating the pseudo-LiDARs of both static and dynamic objects across temporally adjacent frames using object point tracking, enabling 3D attribute extraction in scenarios where 3D data acquisition is infeasible. Extensive experiments demonstrate that our method ensures reliable accuracy and strong scalability, making it a practical and effective solution for M3OD.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2505.12753.pdf' target='_blank'>https://arxiv.org/pdf/2505.12753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martha Teiko Teye, Ori Maoz, Matthias Rottmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12753">LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking from LiDAR point clouds presents unique challenges due to the sparse and irregular nature of the data, compounded by the need for temporal coherence across frames. Traditional tracking systems often rely on hand-crafted features and motion models, which can struggle to maintain consistent object identities in crowded or fast-moving scenes. We present a lidar-based two-staged DETR inspired transformer; a smoother and tracker. The smoother stage refines lidar object detections, from any off-the-shelf detector, across a moving temporal window. The tracker stage uses a DETR-based attention block to maintain tracks across time by associating tracked objects with the refined detections using the point cloud as context. The model is trained on the datasets nuScenes and KITTI in both online and offline (forward peeking) modes demonstrating strong performance across metrics such as ID-switch and multiple object tracking accuracy (MOTA). The numerical results indicate that the online mode outperforms the lidar-only baseline and SOTA models on the nuScenes dataset, with an aMOTA of 0.724 and an aMOTP of 0.475, while the offline mode provides an additional 3 pp aMOTP.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2505.04088.pdf' target='_blank'>https://arxiv.org/pdf/2505.04088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shang Zhang, Huanbin Zhang, Dali Feng, Yujie Cui, Ruoyan Xiong, Cen He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04088">SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thermal infrared (TIR) object tracking often suffers from challenges such as target occlusion, motion blur, and background clutter, which significantly degrade the performance of trackers. To address these issues, this paper pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a bidirectional state-space model and a self-attention mechanism. Specifically, we introduce the Motion Mamba module into the Siamese architecture to ex-tract motion features and recover overlooked edge details using bidirectional modeling and self-attention. We propose a Siamese parameter-sharing strate-gy that allows certain convolutional layers to share weights. This approach reduces computational redundancy while preserving strong feature represen-tation. In addition, we design a motion edge-aware regression loss to improve tracking accuracy, especially for motion-blurred targets. Extensive experi-ments are conducted on four TIR tracking benchmarks, including LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT achieves superior performance in TIR target tracking.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2504.20391.pdf' target='_blank'>https://arxiv.org/pdf/2504.20391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tran Thien Dat Nguyen, Ba Tuong Vo, Ba-Ngu Vo, Hoa Van Nguyen, Changbeom Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20391">The Mean of Multi-Object Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the concept of a mean for trajectories and multi-object trajectories (defined as sets or multi-sets of trajectories) along with algorithms for computing them. Specifically, we use the FrÃ©chet mean, and metrics based on the optimal sub-pattern assignment (OSPA) construct, to extend the notion of average from vectors to trajectories and multi-object trajectories. Further, we develop efficient algorithms to compute these means using greedy search and Gibbs sampling. Using distributed multi-object tracking as an application, we demonstrate that the FrÃ©chet mean approach to multi-object trajectory consensus significantly outperforms state-of-the-art distributed multi-object tracking methods.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2504.03047.pdf' target='_blank'>https://arxiv.org/pdf/2504.03047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reef Alturki, Adrian Hilton, Jean-Yves Guillemaut
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03047">Attention-Aware Multi-View Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In spite of the recent advancements in multi-object tracking, occlusion poses a significant challenge. Multi-camera setups have been used to address this challenge by providing a comprehensive coverage of the scene. Recent multi-view pedestrian detection models have highlighted the potential of an early-fusion strategy, projecting feature maps of all views to a common ground plane or the Bird's Eye View (BEV), and then performing detection. This strategy has been shown to improve both detection and tracking performance. However, the perspective transformation results in significant distortion on the ground plane, affecting the robustness of the appearance features of the pedestrians. To tackle this limitation, we propose a novel model that incorporates attention mechanisms in a multi-view pedestrian tracking scenario. Our model utilizes an early-fusion strategy for detection, and a cross-attention mechanism to establish robust associations between pedestrians in different frames, while efficiently propagating pedestrian features across frames, resulting in a more robust feature representation for each pedestrian. Extensive experiments demonstrate that our model outperforms state-of-the-art models, with an IDF1 score of $96.1\%$ on Wildtrack dataset, and $85.7\%$ on MultiviewX dataset.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2502.16569.pdf' target='_blank'>https://arxiv.org/pdf/2502.16569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Safa, Waqas Aman, Ali Al-Zawqari, Saif Al-Kuwari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16569">Benchmarking Online Object Trackers for Underwater Robot Position Locking Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomously controlling the position of Remotely Operated underwater Vehicles (ROVs) is of crucial importance for a wide range of underwater engineering applications, such as in the inspection and maintenance of underwater industrial structures. Consequently, studying vision-based underwater robot navigation and control has recently gained increasing attention to counter the numerous challenges faced in underwater conditions, such as lighting variability, turbidity, camera image distortions (due to bubbles), and ROV positional disturbances (due to underwater currents). In this paper, we propose (to the best of our knowledge) a first rigorous unified benchmarking of more than seven Machine Learning (ML)-based one-shot object tracking algorithms for vision-based position locking of ROV platforms. We propose a position-locking system that processes images of an object of interest in front of which the ROV must be kept stable. Then, our proposed system uses the output result of different object tracking algorithms to automatically correct the position of the ROV against external disturbances. We conducted numerous real-world experiments using a BlueROV2 platform within an indoor pool and provided clear demonstrations of the strengths and weaknesses of each tracking approach. Finally, to help alleviate the scarcity of underwater ROV data, we release our acquired data base as open-source with the hope of benefiting future research.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2502.01207.pdf' target='_blank'>https://arxiv.org/pdf/2502.01207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannes Homburger, Stefan Wirtensohn, Patrick Hoher, Tim Baur, Dennis Griesser, Moritz Diehl, Johannes Reuter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01207">Solgenia -- A Test Vessel Toward Energy-Efficient Autonomous Water Taxi Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous surface vessels are a promising building block of the future's transport sector and are investigated by research groups worldwide. This paper presents a comprehensive and systematic overview of the autonomous research vessel Solgenia including the latest investigations and recently presented methods that contributed to the fields of autonomous systems, applied numerical optimization, nonlinear model predictive control, multi-extended-object-tracking, computer vision, and collision avoidance. These are considered to be the main components of autonomous water taxi applications. Autonomous water taxis have the potential to transform the traffic in cities close to the water into a more efficient, sustainable, and flexible future state. Regarding this transformation, the test platform Solgenia offers an opportunity to gain new insights by investigating novel methods in real-world experiments. An established test platform will strongly reduce the effort required for real-world experiments in the future.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2501.07133.pdf' target='_blank'>https://arxiv.org/pdf/2501.07133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiantong Zhao, Xiuping Liu, Shengjing Tian, Yinan Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07133">Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (3DSOT) in LiDAR point clouds is a critical task for outdoor perception, enabling real-time perception of object location, orientation, and motion. Despite the impressive performance of current 3DSOT methods, evaluating them on clean datasets inadequately reflects their comprehensive performance, as the adverse weather conditions in real-world surroundings has not been considered. One of the main obstacles is the lack of adverse weather benchmarks for the evaluation of 3DSOT. To this end, this work proposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather, which comprises two synthetic datasets (KITTI-A and nuScenes-A) and one real-world dataset (CADC-SOT) spanning three weather types: rain, fog, and snow. Based on this benchmark, five representative 3D trackers from different tracking frameworks conducted robustness evaluation, resulting in significant performance degradations. This prompts the question: What are the factors that cause current advanced methods to fail on such adverse weather samples? Consequently, we explore the impacts of adverse weather and answer the above question from three perspectives: 1) target distance; 2) template shape corruption; and 3) target shape corruption. Finally, based on domain randomization and contrastive learning, we designed a dual-branch tracking framework for adverse weather, named DRCT, achieving excellent performance in benchmarks.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2501.04336.pdf' target='_blank'>https://arxiv.org/pdf/2501.04336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Huang, Yuyang Ji, Xiaofang Wang, Nikhil Mehta, Tong Xiao, Donghyun Lee, Sigmund Vanvalkenburgh, Shengxin Zha, Bolin Lai, Licheng Yu, Ning Zhang, Yong Jae Lee, Miao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04336">Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-form video understanding with Large Vision Language Models is challenged by the need to analyze temporally dispersed yet spatially concentrated key moments within limited context windows. In this work, we introduce VideoMindPalace, a new framework inspired by the "Mind Palace", which organizes critical video moments into a topologically structured semantic graph. VideoMindPalace organizes key information through (i) hand-object tracking and interaction, (ii) clustered activity zones representing specific areas of recurring activities, and (iii) environment layout mapping, allowing natural language parsing by LLMs to provide grounded insights on spatio-temporal and 3D context. In addition, we propose the Video MindPalace Benchmark (VMB), to assess human-like reasoning, including spatial localization, temporal reasoning, and layout-aware sequential understanding. Evaluated on VMB and established video QA datasets, including EgoSchema, NExT-QA, IntentQA, and the Active Memories Benchmark, VideoMindPalace demonstrates notable gains in spatio-temporal coherence and human-aligned reasoning, advancing long-form video analysis capabilities in VLMs.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2411.08144.pdf' target='_blank'>https://arxiv.org/pdf/2411.08144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangge Li, Benjamin C Yang, Sayan Mitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08144">Visual Tracking with Intermittent Visibility: Switched Control Design and Implementation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of visual target tracking in scenarios where a pursuer may experience intermittent loss of visibility of the target. The design of a Switched Visual Tracker (SVT) is presented which aims to meet the competing requirements of maintaining both proximity and visibility. SVT alternates between a visual tracking mode for following the target, and a recovery mode for regaining visual contact when the target falls out of sight. We establish the stability of SVT by extending the average dwell time theorem from switched systems theory, which may be of independent interest. Our implementation of SVT on an Agilicious drone [1] illustrates its effectiveness on tracking various target trajectories: it reduces the average tracking error by up to 45% and significantly improves visibility duration compared to a baseline algorithm. The results show that our approach effectively handles intermittent vision loss, offering enhanced robustness and adaptability for real-world autonomous missions. Additionally, we demonstrate how the stability analysis provides valuable guidance for selecting parameters, such as tracking speed and recovery distance, to optimize the SVT's performance.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2410.20893.pdf' target='_blank'>https://arxiv.org/pdf/2410.20893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengjing Tian, Yinan Han, Xiantong Zhao, Bin Liu, Xiuping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20893">Evaluating the Robustness of LiDAR Point Cloud Tracking Against Adversarial Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we delve into the robustness of neural network-based LiDAR point cloud tracking models under adversarial attacks, a critical aspect often overlooked in favor of performance enhancement. These models, despite incorporating advanced architectures like Transformer or Bird's Eye View (BEV), tend to neglect robustness in the face of challenges such as adversarial attacks, domain shifts, or data corruption. We instead focus on the robustness of the tracking models under the threat of adversarial attacks. We begin by establishing a unified framework for conducting adversarial attacks within the context of 3D object tracking, which allows us to thoroughly investigate both white-box and black-box attack strategies. For white-box attacks, we tailor specific loss functions to accommodate various tracking paradigms and extend existing methods such as FGSM, C\&W, and PGD to the point cloud domain. In addressing black-box attack scenarios, we introduce a novel transfer-based approach, the Target-aware Perturbation Generation (TAPG) algorithm, with the dual objectives of achieving high attack performance and maintaining low perceptibility. This method employs a heuristic strategy to enforce sparse attack constraints and utilizes random sub-vector factorization to bolster transferability. Our experimental findings reveal a significant vulnerability in advanced tracking methods when subjected to both black-box and white-box attacks, underscoring the necessity for incorporating robustness against adversarial attacks into the design of LiDAR point cloud tracking models. Notably, compared to existing methods, the TAPG also strikes an optimal balance between the effectiveness of the attack and the concealment of the perturbations.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2410.14093.pdf' target='_blank'>https://arxiv.org/pdf/2410.14093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kosuke Tatsumura, Yohei Hamakawa, Masaya Yamasaki, Koji Oya, Hiroshi Fujimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14093">Enhancing In-vehicle Multiple Object Tracking Systems with Embeddable Ising Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A cognitive function of tracking multiple objects, needed in autonomous mobile vehicles, comprises object detection and their temporal association. While great progress owing to machine learning has been recently seen for elaborating the similarity matrix between the objects that have been recognized and the objects detected in a current video frame, less for the assignment problem that finally determines the temporal association, which is a combinatorial optimization problem. Here we show an in-vehicle multiple object tracking system with a flexible assignment function for tracking through multiple long-term occlusion events. To solve the flexible assignment problem formulated as a nondeterministic polynomial time-hard problem, the system relies on an embeddable Ising machine based on a quantum-inspired algorithm called simulated bifurcation. Using a vehicle-mountable computing platform, we demonstrate a realtime system-wide throughput (23 frames per second on average) with the enhanced functionality.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2409.16111.pdf' target='_blank'>https://arxiv.org/pdf/2409.16111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yannik Blei, Michael Krawez, Nisarga Nilavadi, Tanja Katharina Kaiser, Wolfram Burgard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16111">CloudTrack: Scalable UAV Tracking with Cloud Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and rescue scenarios to gather information in the search area. The automatic identification of the person searched for in aerial footage could increase the autonomy of such systems, reduce the search time, and thus increase the missed person's chances of survival. In this paper, we present a novel approach to perform semantically conditioned open vocabulary object tracking that is specifically designed to cope with the limitations of UAV hardware. Our approach has several advantages. It can run with verbal descriptions of the missing person, e.g., the color of the shirt, it does not require dedicated training to execute the mission and can efficiently track a potentially moving person. Our experimental results demonstrate the versatility and efficacy of our approach.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2408.12727.pdf' target='_blank'>https://arxiv.org/pdf/2408.12727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojin Shin, Donghwa Kang, Daejin Choi, Brent Kang, Jinkyu Lee, Hyeongboo Baek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12727">BankTweak: Adversarial Attack against Multi-Object Trackers by Manipulating Feature Banks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to construct moving trajectories for objects, and modern multi-object trackers mainly utilize the tracking-by-detection methodology. Initial approaches to MOT attacks primarily aimed to degrade the detection quality of the frames under attack, thereby reducing accuracy only in those specific frames, highlighting a lack of \textit{efficiency}. To improve efficiency, recent advancements manipulate object positions to cause persistent identity (ID) switches during the association phase, even after the attack ends within a few frames. However, these position-manipulating attacks have inherent limitations, as they can be easily counteracted by adjusting distance-related parameters in the association phase, revealing a lack of \textit{robustness}. In this paper, we present \textsf{BankTweak}, a novel adversarial attack designed for MOT trackers, which features efficiency and robustness. \textsf{BankTweak} focuses on the feature extractor in the association phase and reveals vulnerability in the Hungarian matching method used by feature-based MOT systems. Exploiting the vulnerability, \textsf{BankTweak} induces persistent ID switches (addressing \textit{efficiency}) even after the attack ends by strategically injecting altered features into the feature banks without modifying object positions (addressing \textit{robustness}). To demonstrate the applicability, we apply \textsf{BankTweak} to three multi-object trackers (DeepSORT, StrongSORT, and MOTDT) with one-stage, two-stage, anchor-free, and transformer detectors. Extensive experiments on the MOT17 and MOT20 datasets show that our method substantially surpasses existing attacks, exposing the vulnerability of the tracking-by-detection framework to \textsf{BankTweak}.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2406.00589.pdf' target='_blank'>https://arxiv.org/pdf/2406.00589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Qi, Junlin Zhang, Xin Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00589">Robust Visual Tracking via Iterative Gradient Descent and Threshold Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual tracking fundamentally involves regressing the state of the target in each frame of a video. Despite significant progress, existing regression-based trackers still tend to experience failures and inaccuracies. To enhance the precision of target estimation, this paper proposes a tracking technique based on robust regression. Firstly, we introduce a novel robust linear regression estimator, which achieves favorable performance when the error vector follows i.i.d Gaussian-Laplacian distribution. Secondly, we design an iterative process to quickly solve the problem of outliers. In fact, the coefficients are obtained by Iterative Gradient Descent and Threshold Selection algorithm (IGDTS). In addition, we expend IGDTS to a generative tracker, and apply IGDTS-distance to measure the deviation between the sample and the model. Finally, we propose an update scheme to capture the appearance changes of the tracked object and ensure that the model is updated correctly. Experimental results on several challenging image sequences show that the proposed tracker outperformance existing trackers.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2403.04112.pdf' target='_blank'>https://arxiv.org/pdf/2403.04112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Pieroni, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04112">Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel multi-modal Multi-Object Tracking (MOT) algorithm for self-driving cars that combines camera and LiDAR data. Camera frames are processed with a state-of-the-art 3D object detector, whereas classical clustering techniques are used to process LiDAR observations. The proposed MOT algorithm comprises a three-step association process, an Extended Kalman filter for estimating the motion of each detected dynamic obstacle, and a track management phase. The EKF motion model requires the current measured relative position and orientation of the observed object and the longitudinal and angular velocities of the ego vehicle as inputs. Unlike most state-of-the-art multi-modal MOT approaches, the proposed algorithm does not rely on maps or knowledge of the ego global pose. Moreover, it uses a 3D detector exclusively for cameras and is agnostic to the type of LiDAR sensor used. The algorithm is validated both in simulation and with real-world data, with satisfactory results.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2401.13285.pdf' target='_blank'>https://arxiv.org/pdf/2401.13285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengjing Tian, Yinan Han, Xiuping Liu, Xiantong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13285">Small Object Tracking in LiDAR Point Cloud: Learning the Target-awareness Prototype and Fine-grained Search Region</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Object Tracking in LiDAR point cloud is one of the most essential parts of environmental perception, in which small objects are inevitable in real-world scenarios and will bring a significant barrier to the accurate location. However, the existing methods concentrate more on exploring universal architectures for common categories and overlook the challenges that small objects have long been thorny due to the relative deficiency of foreground points and a low tolerance for disturbances. To this end, we propose a Siamese network-based method for small object tracking in the LiDAR point cloud, which is composed of the target-awareness prototype mining (TAPM) module and the regional grid subdivision (RGS) module. The TAPM module adopts the reconstruction mechanism of the masked decoder to learn the prototype in the feature space, aiming to highlight the presence of foreground points that will facilitate the subsequent location of small objects. Through the above prototype is capable of accentuating the small object of interest, the positioning deviation in feature maps still leads to high tracking errors. To alleviate this issue, the RGS module is proposed to recover the fine-grained features of the search region based on ViT and pixel shuffle layers. In addition, apart from the normal settings, we elaborately design a scaling experiment to evaluate the robustness of the different trackers on small objects. Extensive experiments on KITTI and nuScenes demonstrate that our method can effectively improve the tracking performance of small targets without affecting normal-sized objects.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2512.09633.pdf' target='_blank'>https://arxiv.org/pdf/2512.09633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09633">Benchmarking SAM2-based Trackers on FMOX</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2512.09235.pdf' target='_blank'>https://arxiv.org/pdf/2512.09235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Eimran Hossain Eimon, Hyomin Choi, Fabien Racapé, Mateen Ulhaq, Velibor Adzic, Hari Kalva, Borko Furht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.09235">Efficient Feature Compression for Machines with Global Statistics Preservation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2511.21139.pdf' target='_blank'>https://arxiv.org/pdf/2511.21139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baoli Sun, Xinzhu Ma, Ning Wang, Zhihui Wang, Zhiyong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21139">Referring Video Object Segmentation with Cross-Modality Proxy Queries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring video object segmentation (RVOS) is an emerging cross-modality task that aims to generate pixel-level maps of the target objects referred by given textual expressions. The main concept involves learning an accurate alignment of visual elements and language expressions within a semantic space. Recent approaches address cross-modality alignment through conditional queries, tracking the target object using a query-response based mechanism built upon transformer structure. However, they exhibit two limitations: (1) these conditional queries lack inter-frame dependency and variation modeling, making accurate target tracking challenging amid significant frame-to-frame variations; and (2) they integrate textual constraints belatedly, which may cause the video features potentially focus on the non-referred objects. Therefore, we propose a novel RVOS architecture called ProxyFormer, which introduces a set of proxy queries to integrate visual and text semantics and facilitate the flow of semantics between them. By progressively updating and propagating proxy queries across multiple stages of video feature encoder, ProxyFormer ensures that the video features are focused on the object of interest. This dynamic evolution also enables the establishment of inter-frame dependencies, enhancing the accuracy and coherence of object tracking. To mitigate high computational costs, we decouple cross-modality interactions into temporal and spatial dimensions. Additionally, we design a Joint Semantic Consistency (JSC) training strategy to align semantic consensus between the proxy queries and the combined video-text pairs. Comprehensive experiments on four widely used RVOS benchmarks demonstrate the superiority of our ProxyFormer to the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2509.19096.pdf' target='_blank'>https://arxiv.org/pdf/2509.19096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilhan Skender, Kailin Tong, Selim Solmaz, Daniel Watzenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19096">Investigating Traffic Accident Detection Using Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic safety remains a critical global concern, with timely and accurate accident detection essential for hazard reduction and rapid emergency response. Infrastructure-based vision sensors offer scalable and efficient solutions for continuous real-time monitoring, facilitating automated detection of accidents directly from captured images. This research investigates the zero-shot capabilities of multimodal large language models (MLLMs) for detecting and describing traffic accidents using images from infrastructure cameras, thus minimizing reliance on extensive labeled datasets. Main contributions include: (1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA, explicitly addressing the scarcity of diverse, realistic, infrastructure-based accident data through controlled simulations; (2) Comparative performance analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in accident identification and descriptive capabilities without prior fine-tuning; and (3) Integration of advanced visual analytics, specifically YOLO for object detection, Deep SORT for multi-object tracking, and Segment Anything (SAM) for instance segmentation, into enhanced prompts to improve model accuracy and explainability. Key numerical results show Pixtral as the top performer with an F1-score of 0.71 and 83% recall, while Gemini models gained precision with enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and recall losses. Gemma 3 offered the most balanced performance with minimal metric fluctuation. These findings demonstrate the substantial potential of integrating MLLMs with advanced visual analytics techniques, enhancing their applicability in real-world automated traffic monitoring systems.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2509.18451.pdf' target='_blank'>https://arxiv.org/pdf/2509.18451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithvi Raj Singh, Raju Gottumukkala, Anthony Maida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18451">An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unpredictable movement patterns and small visual mark make precise tracking of fast-moving tiny objects like a racquetball one of the challenging problems in computer vision. This challenge is particularly relevant for sport robotics applications, where lightweight and accurate tracking systems can improve robot perception and planning capabilities. While Kalman filter-based tracking methods have shown success in general object tracking scenarios, their performance degrades substantially when dealing with rapidly moving objects that exhibit irregular bouncing behavior. In this study, we evaluate the performance of five state-of-the-art Kalman filter-based tracking methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom dataset containing 10,000 annotated racquetball frames captured at 720p-1280p resolution. We focus our analysis on two critical performance factors: inference speed and update frequency per image, examining how these parameters affect tracking accuracy and reliability for fast-moving tiny objects. Our experimental evaluation across four distinct scenarios reveals that DeepOCSORT achieves the lowest tracking error with an average ADE of 31.15 pixels compared to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms. However, our results show that all Kalman filter-based trackers exhibit significant tracking drift with spatial errors ranging from 3-11cm (ADE values: 31-114 pixels), indicating fundamental limitations in handling the unpredictable motion patterns of fast-moving tiny objects like racquetballs. Our analysis demonstrates that current tracking approaches require substantial improvements, with error rates 3-4x higher than standard object tracking benchmarks, highlighting the need for specialized methodologies for fast-moving tiny object tracking applications.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2509.18272.pdf' target='_blank'>https://arxiv.org/pdf/2509.18272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tornike Karchkhadze, Kuan-Lin Chen, Mojtaba Heydari, Robert Henzel, Alessandro Toso, Mehrez Souden, Joshua Atkins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18272">StereoFoley: Object-Aware Stereo Audio Generation from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present StereoFoley, a video-to-audio generation framework that produces semantically aligned, temporally synchronized, and spatially accurate stereo sound at 48 kHz. While recent generative video-to-audio models achieve strong semantic and temporal fidelity, they largely remain limited to mono or fail to deliver object-aware stereo imaging, constrained by the lack of professionally mixed, spatially accurate video-to-audio datasets. First, we develop and train a base model that generates stereo audio from video, achieving state-of-the-art in both semantic accuracy and synchronization. Next, to overcome dataset limitations, we introduce a synthetic data generation pipeline that combines video analysis, object tracking, and audio synthesis with dynamic panning and distance-based loudness controls, enabling spatially accurate object-aware sound. Finally, we fine-tune the base model on this synthetic dataset, yielding clear object-audio correspondence. Since no established metrics exist, we introduce stereo object-awareness measures and validate it through a human listening study, showing strong correlation with perception. This work establishes the first end-to-end framework for stereo object-aware video-to-audio generation, addressing a critical gap and setting a new benchmark in the field.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2509.12924.pdf' target='_blank'>https://arxiv.org/pdf/2509.12924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik ForssÃ©n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12924">MATTER: Multiscale Attention for Registration Error Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking. This makes detecting and quantifying registration misalignment, i.e., PCR quality validation, an important task. All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes. In this work, we instead use regression for PCR validation, allowing for a more fine-grained quantification of the registration quality. We also extend previously used misalignment-related features by using multiscale extraction and attention-based aggregation. This leads to accurate and robust registration error estimation on diverse datasets, especially for point clouds with heterogeneous spatial densities. Furthermore, when used to guide a mapping downstream task, our method significantly improves the mapping quality for a given amount of re-registered frames, compared to the state-of-the-art classification-based method.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2509.11453.pdf' target='_blank'>https://arxiv.org/pdf/2509.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11453">Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics and autonomous systems. Existing methods typically follow frame-wise motion estimation or a sequence-based paradigm. However, the two-frame methods are efficient but lack long-term temporal context, making them vulnerable in sparse or occluded scenes, while sequence-based methods that process multiple point clouds gain robustness at a significant computational cost. To resolve this dilemma, we propose a novel trajectory-based paradigm and its instantiation, TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame tracker by implicitly learning motion continuity from historical bounding box trajectories alone-without requiring additional, costly point cloud inputs. It first generates a fast, explicit motion proposal and then uses an implicit motion modeling module to predict the future trajectory, which in turn refines and corrects the initial proposal. Extensive experiments on the large-scale NuScenes benchmark show that TrajTrack achieves new state-of-the-art performance, dramatically improving tracking precision by 4.48% over a strong baseline while running at 56 FPS. Besides, we also demonstrate the strong generalizability of TrajTrack across different base trackers. Video is available at https://www.bilibili.com/video/BV1ahYgzmEWP.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2509.09349.pdf' target='_blank'>https://arxiv.org/pdf/2509.09349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Nell, Shane Gilroy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09349">Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behavior classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviors such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioral analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2509.08421.pdf' target='_blank'>https://arxiv.org/pdf/2509.08421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Toida, Taigo Sakai, Naoki Kato, Kazutoyo Yokota, Takeshi Nakamura, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08421">Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-View Multi-Object Tracking (MVMOT) is essential for applications such as surveillance, autonomous driving, and sports analytics. However, maintaining consistent object identities across multiple cameras remains challenging due to viewpoint changes, lighting variations, and occlusions, which often lead to tracking errors.Recent methods project features from multiple cameras into a unified Bird's-Eye-View (BEV) space to improve robustness against occlusion. However, this projection introduces feature distortion and non-uniform density caused by variations in object scale with distance. These issues degrade the quality of the fused representation and reduce detection and tracking accuracy.To address these problems, we propose SCFusion, a framework that combines three techniques to improve multi-view feature integration. First, it applies a sparse transformation to avoid unnatural interpolation during projection. Next, it performs density-aware weighting to adaptively fuse features based on spatial confidence and camera distance. Finally, it introduces a multi-view consistency loss that encourages each camera to learn discriminative features independently before fusion.Experiments show that SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline method TrackTacular. These results demonstrate that SCFusion effectively mitigates the limitations of conventional BEV projection and provides a robust and accurate solution for multi-view object detection and tracking.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2509.06536.pdf' target='_blank'>https://arxiv.org/pdf/2509.06536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06536">Benchmarking EfficientTAM on FMO datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast and tiny object tracking remains a challenge in computer vision and in this paper we first introduce a JSON metadata file associated with four open source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we extend the description of the FMOs datasets with additional ground truth information in JSON format (called FMOX) with object size information. Finally we use our FMOX file to test a recently proposed foundational model for tracking (called EfficientTAM) showing that its performance compares well with the pipelines originally taylored for these FMO datasets. Our comparison of these state-of-the-art techniques on FMOX is provided with Trajectory Intersection of Union (TIoU) scores. The code and JSON is shared open source allowing FMOX to be accessible and usable for other machine learning pipelines aiming to process FMO datasets.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2509.03499.pdf' target='_blank'>https://arxiv.org/pdf/2509.03499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Barnard, Elaine Liu, Kristine Walz, Brian Schlining, Nancy Jacobsen Stout, Lonny Lundsten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03499">DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarking multi-object tracking and object detection model performance is an essential step in machine learning model development, as it allows researchers to evaluate model detection and tracker performance on human-generated 'test' data, facilitating consistent comparisons between models and trackers and aiding performance optimization. In this study, a novel benchmark video dataset was developed and used to assess the performance of several Monterey Bay Aquarium Research Institute object detection models and a FathomNet single-class object detection model together with several trackers. The dataset consists of four video sequences representing midwater and benthic deep-sea habitats. Performance was evaluated using Higher Order Tracking Accuracy, a metric that balances detection, localization, and association accuracy. To the best of our knowledge, this is the first publicly available benchmark for multi-object tracking in deep-sea video footage. We provide the benchmark data, a clearly documented workflow for generating additional benchmark videos, as well as example Python notebooks for computing metrics.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2507.16015.pdf' target='_blank'>https://arxiv.org/pdf/2507.16015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Dunnhofer, Zaira Manigrasso, Christian Micheloni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16015">Is Tracking really more challenging in First Person Egocentric Vision?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking and segmentation are becoming fundamental tasks for understanding human activities in egocentric vision. Recent research has benchmarked state-of-the-art methods and concluded that first person egocentric vision presents challenges compared to previously studied domains. However, these claims are based on evaluations conducted across significantly different scenarios. Many of the challenging characteristics attributed to egocentric vision are also present in third person videos of human-object activities. This raises a critical question: how much of the observed performance drop stems from the unique first person viewpoint inherent to egocentric vision versus the domain of human-object activities? To address this question, we introduce a new benchmark study designed to disentangle such factors. Our evaluation strategy enables a more precise separation of challenges related to the first person perspective from those linked to the broader domain of human-object activity understanding. By doing so, we provide deeper insights into the true sources of difficulty in egocentric tracking and segmentation, facilitating more targeted advancements on this task.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2507.12832.pdf' target='_blank'>https://arxiv.org/pdf/2507.12832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Kondo, Norimichi Ukita, Riku Kanayama, Yuki Yoshida, Takayuki Yamaguchi, Xiang Yu, Guang Liang, Xinyao Liu, Guan-Zhang Wang, Wei-Ta Chu, Bing-Cheng Chuang, Jia-Hua Lee, Pin-Tseng Kuo, I-Hsuan Chu, Yi-Shein Hsiao, Cheng-Han Wu, Po-Yi Wu, Jui-Chien Tsou, Hsuan-Chi Liu, Chun-Yi Lee, Yuan-Fu Yang, Kosuke Shigematsu, Asuka Shin, Ba Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12832">MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2504.04097.pdf' target='_blank'>https://arxiv.org/pdf/2504.04097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohang Han, Matti Vahs, Jana Tumova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04097">Risk-Aware Robot Control in Dynamic Environments Using Belief Control Barrier Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety for autonomous robots operating in dynamic environments can be challenging due to factors such as unmodeled dynamics, noisy sensor measurements, and partial observability. To account for these limitations, it is common to maintain a belief distribution over the true state. This belief could be a non-parametric, sample-based representation to capture uncertainty more flexibly. In this paper, we propose a novel form of Belief Control Barrier Functions (BCBFs) specifically designed to ensure safety in dynamic environments under stochastic dynamics and a sample-based belief about the environment state. Our approach incorporates provable concentration bounds on tail risk measures into BCBFs, effectively addressing possible multimodal and skewed belief distributions represented by samples. Moreover, the proposed method demonstrates robustness against distributional shifts up to a predefined bound. We validate the effectiveness and real-time performance (approximately 1kHz) of the proposed method through two simulated underwater robotic applications: object tracking and dynamic collision avoidance.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2502.17399.pdf' target='_blank'>https://arxiv.org/pdf/2502.17399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuchuan Yu, Ching-I Huang, Hsueh-Cheng Wang, Lap-Fai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17399">Enriching Physical-Virtual Interaction in AR Gaming by Tracking Identical Real Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented reality (AR) games, particularly those designed for headsets, have become increasingly prevalent with advancements in both hardware and software. However, the majority of AR games still rely on pre-scanned or static scenes, and interaction mechanisms are often limited to controllers or hand-tracking. Additionally, the presence of identical objects in AR games poses challenges for conventional object tracking techniques, which often struggle to differentiate between identical objects or necessitate the installation of fixed cameras for global object movement tracking. In response to these limitations, we present a novel approach to address the tracking of identical objects in an AR scene to enrich physical-virtual interaction. Our method leverages partial scene observations captured by an AR headset, utilizing the perspective and spatial data provided by this technology. Object identities within the scene are determined through the solution of a label assignment problem using integer programming. To enhance computational efficiency, we incorporate a Voronoi diagram-based pruning method into our approach. Our implementation of this approach in a farm-to-table AR game demonstrates its satisfactory performance and robustness. Furthermore, we showcase the versatility and practicality of our method through applications in AR storytelling and a simulated gaming robot. Our video demo is available at: https://youtu.be/rPGkLYuKvCQ.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2411.19167.pdf' target='_blank'>https://arxiv.org/pdf/2411.19167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Shangchen Han, Fan Zhang, Linguang Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19167">HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2411.11514.pdf' target='_blank'>https://arxiv.org/pdf/2411.11514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Li, Michael Burke, Subramanian Ramamoorthy, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11514">Learning a Neural Association Network for Self-supervised Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework to learn data association for multi-object tracking in a self-supervised manner. Fully-supervised learning methods are known to achieve excellent tracking performances, but acquiring identity-level annotations is tedious and time-consuming. Motivated by the fact that in real-world scenarios object motion can be usually represented by a Markov process, we present a novel expectation maximization (EM) algorithm that trains a neural network to associate detections for tracking, without requiring prior knowledge of their temporal correspondences. At the core of our method lies a neural Kalman filter, with an observation model conditioned on associations of detections parameterized by a neural network. Given a batch of frames as input, data associations between detections from adjacent frames are predicted by a neural network followed by a Sinkhorn normalization that determines the assignment probabilities of detections to states. Kalman smoothing is then used to obtain the marginal probability of observations given the inferred states, producing a training objective to maximize this marginal probability using gradient descent. The proposed framework is fully differentiable, allowing the underlying neural model to be trained end-to-end. We evaluate our approach on the challenging MOT17, MOT20, and BDD100K datasets and achieve state-of-the-art results in comparison to self-supervised trackers using public detections.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2411.02220.pdf' target='_blank'>https://arxiv.org/pdf/2411.02220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryoma Yataka, Pu Perry Wang, Petros Boufounos, Ryuhei Takahashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02220">SIRA: Scalable Inter-frame Relation and Association for Radar Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional radar feature extraction faces limitations due to low spatial resolution, noise, multipath reflection, the presence of ghost targets, and motion blur. Such limitations can be exacerbated by nonlinear object motion, particularly from an ego-centric viewpoint. It becomes evident that to address these challenges, the key lies in exploiting temporal feature relation over an extended horizon and enforcing spatial motion consistency for effective association. To this end, this paper proposes SIRA (Scalable Inter-frame Relation and Association) with two designs. First, inspired by Swin Transformer, we introduce extended temporal relation, generalizing the existing temporal relation layer from two consecutive frames to multiple inter-frames with temporally regrouped window attention for scalability. Second, we propose motion consistency track with the concept of a pseudo-tracklet generated from observational data for better trajectory prediction and subsequent object association. Our approach achieves 58.11 mAP@0.5 for oriented object detection and 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing previous state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA, respectively.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2411.00608.pdf' target='_blank'>https://arxiv.org/pdf/2411.00608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Cheng Chen, Yuan-yao Lou, Mustafa Abdallah, Kwang Taik Kim, Saurabh Bagchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00608">HopTrack: A Real-time Multi-Object Tracking System for Embedded Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) poses significant challenges in computer vision. Despite its wide application in robotics, autonomous driving, and smart manufacturing, there is limited literature addressing the specific challenges of running MOT on embedded devices. State-of-the-art MOT trackers designed for high-end GPUs often experience low processing rates (<11fps) when deployed on embedded devices. Existing MOT frameworks for embedded devices proposed strategies such as fusing the detector model with the feature embedding model to reduce inference latency or combining different trackers to improve tracking accuracy, but tend to compromise one for the other. This paper introduces HopTrack, a real-time multi-object tracking system tailored for embedded devices. Our system employs a novel discretized static and dynamic matching approach along with an innovative content-aware dynamic sampling technique to enhance tracking accuracy while meeting the real-time requirement. Compared with the best high-end GPU modified baseline Byte (Embed) and the best existing baseline on embedded devices MobileNet-JDE, HopTrack achieves a processing speed of up to 39.29 fps on NVIDIA AGX Xavier with a multi-object tracking accuracy (MOTA) of up to 63.12% on the MOT16 benchmark, outperforming both counterparts by 2.15% and 4.82%, respectively. Additionally, the accuracy improvement is coupled with the reduction in energy consumption (20.8%), power (5%), and memory usage (8%), which are crucial resources on embedded devices. HopTrack is also detector agnostic allowing the flexibility of plug-and-play.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2410.24183.pdf' target='_blank'>https://arxiv.org/pdf/2410.24183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Tesori, Giorgio Battistelli, Luigi Chisci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24183">Extended Object Tracking and Classification based on Linear Splines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a framework based on linear splines for 2-dimensional extended object tracking and classification. Unlike state of the art models, linear splines allow to represent extended objects whose contour is an arbitrarily complex curve. An exact likelihood is derived for the case in which noisy measurements can be scattered from any point on the contour of the extended object, while an approximate Monte Carlo likelihood is provided for the case wherein scattering points can be anywhere, i.e. inside or on the contour, on the object surface. Exploiting such likelihood to measure how well the observed data fit a given shape, a suitable estimator is developed. The proposed estimator models the extended object in terms of a kinematic state, providing object position and orientation, along with a shape vector, characterizing object contour and surface. The kinematic state is estimated via a nonlinear Kalman filter, while the shape vector is estimated via a Bayesian classifier so that classification is implicitly solved during shape estimation. Numerical experiments are provided to assess, compared to state of the art extended object estimators, the effectiveness of the proposed one.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2410.16329.pdf' target='_blank'>https://arxiv.org/pdf/2410.16329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Zhong, Yang Yang, Fengqiang Wan, Henglu Wei, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16329">The Solution for Single Object Tracking Task of Perception Test Challenge 2024</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents our method for Single Object Tracking (SOT), which aims to track a specified object throughout a video sequence. We employ the LoRAT method. The essence of the work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. We train our model using the extensive LaSOT and GOT-10k datasets, which provide a solid foundation for robust performance. Additionally, we implement the alpha-refine technique for post-processing the bounding box outputs. Although the alpha-refine method does not yield the anticipated results, our overall approach achieves a score of 0.813, securing first place in the competition.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2410.15518.pdf' target='_blank'>https://arxiv.org/pdf/2410.15518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thinh Phan, Isaac Phillips, Andrew Lockett, Michael T. Kidd, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15518">TrackMe:A Simple and Effective Multiple Object Tracking Annotation Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking, especially animal tracking, is one of the key topics that attract a lot of attention due to its benefits of animal behavior understanding and monitoring. Recent state-of-the-art tracking methods are founded on deep learning architectures for object detection, appearance feature extraction and track association. Despite the good tracking performance, these methods are trained and evaluated on common objects such as human and cars. To perform on the animal, there is a need to create large datasets of different types in multiple conditions. The dataset construction comprises of data collection and data annotation. In this work, we put more focus on the latter task. Particularly, we renovate the well-known tool, LabelMe, so as to assist common user with or without in-depth knowledge about computer science to annotate the data with less effort. The new tool named as TrackMe inherits the simplicity, high compatibility with varied systems, minimal hardware requirement and convenient feature utilization from the predecessor. TrackMe is an upgraded version with essential features for multiple object tracking annotation.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2410.10527.pdf' target='_blank'>https://arxiv.org/pdf/2410.10527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Guo, Canlun Zheng, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10527">Motion-guided small MAV detection in complex and non-planar scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a growing interest in the visual detection of micro aerial vehicles (MAVs) due to its importance in numerous applications. However, the existing methods based on either appearance or motion features encounter difficulties when the background is complex or the MAV is too small. In this paper, we propose a novel motion-guided MAV detector that can accurately identify small MAVs in complex and non-planar scenes. This detector first exploits a motion feature enhancement module to capture the motion features of small MAVs. Then it uses multi-object tracking and trajectory filtering to eliminate false positives caused by motion parallax. Finally, an appearance-based classifier and an appearance-based detector that operates on the cropped regions are used to achieve precise detection results. Our proposed method can effectively and efficiently detect extremely small MAVs from dynamic and complex backgrounds because it aggregates pixel-level motion features and eliminates false positives based on the motion and appearance features of MAVs. Experiments on the ARD-MAV dataset demonstrate that the proposed method could achieve high performance in small MAV detection under challenging conditions and outperform other state-of-the-art methods across various metrics
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2408.17098.pdf' target='_blank'>https://arxiv.org/pdf/2408.17098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgardo Solano-Carrillo, Felix Sattler, Antje Alex, Alexander Klein, Bruno Pereira Costa, Angel Bueno Rodriguez, Jannis Stoppe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17098">UTrack: Multi-Object Tracking with Uncertain Detections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The tracking-by-detection paradigm is the mainstream in multi-object tracking, associating tracks to the predictions of an object detector. Although exhibiting uncertainty through a confidence score, these predictions do not capture the entire variability of the inference process. For safety and security critical applications like autonomous driving, surveillance, etc., knowing this predictive uncertainty is essential though. Therefore, we introduce, for the first time, a fast way to obtain the empirical predictive distribution during object detection and incorporate that knowledge in multi-object tracking. Our mechanism can easily be integrated into state-of-the-art trackers, enabling them to fully exploit the uncertainty in the detections. Additionally, novel association methods are introduced that leverage the proposed mechanism. We demonstrate the effectiveness of our contribution on a variety of benchmarks, such as MOT17, MOT20, DanceTrack, and KITTI.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2407.04170.pdf' target='_blank'>https://arxiv.org/pdf/2407.04170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus Krimmel, Jan Achterhold, Joerg Stueckler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04170">Attention Normalization Impacts Cardinality Generalization in Slot Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object-centric scene decompositions are important representations for downstream tasks in fields such as computer vision and robotics. The recently proposed Slot Attention module, already leveraged by several derivative works for image segmentation and object tracking in videos, is a deep learning component which performs unsupervised object-centric scene decomposition on input images. It is based on an attention architecture, in which latent slot vectors, which hold compressed information on objects, attend to localized perceptual features from the input image. In this paper, we demonstrate that design decisions on normalizing the aggregated values in the attention architecture have considerable impact on the capabilities of Slot Attention to generalize to a higher number of slots and objects as seen during training. We propose and investigate alternatives to the original normalization scheme which increase the generalization capabilities of Slot Attention to varying slot and object counts, resulting in performance gains on the task of unsupervised image segmentation. The newly proposed normalizations represent minimal and easy to implement modifications of the usual Slot Attention module, changing the value aggregation mechanism from a weighted mean operation to a scaled weighted sum operation.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2406.18414.pdf' target='_blank'>https://arxiv.org/pdf/2406.18414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kemiao Huang, Yinqi Chen, Meiying Zhang, Qi Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18414">BiTrack: Bidirectional Offline 3D Multi-Object Tracking Using Camera-LiDAR Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compared with real-time multi-object tracking (MOT), offline multi-object tracking (OMOT) has the advantages to perform 2D-3D detection fusion, erroneous link correction, and full track optimization but has to deal with the challenges from bounding box misalignment and track evaluation, editing, and refinement. This paper proposes "BiTrack", a 3D OMOT framework that includes modules of 2D-3D detection fusion, initial trajectory generation, and bidirectional trajectory re-optimization to achieve optimal tracking results from camera-LiDAR data. The novelty of this paper includes threefold: (1) development of a point-level object registration technique that employs a density-based similarity metric to achieve accurate fusion of 2D-3D detection results; (2) development of a set of data association and track management skills that utilizes a vertex-based similarity metric as well as false alarm rejection and track recovery mechanisms to generate reliable bidirectional object trajectories; (3) development of a trajectory re-optimization scheme that re-organizes track fragments of different fidelities in a greedy fashion, as well as refines each trajectory with completion and smoothing techniques. The experiment results on the KITTI dataset demonstrate that BiTrack achieves the state-of-the-art performance for 3D OMOT tasks in terms of accuracy and efficiency.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2405.05646.pdf' target='_blank'>https://arxiv.org/pdf/2405.05646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerardo Duran-Martin, Matias Altamirano, Alexander Y. Shestopaloff, Leandro SÃ¡nchez-Betancourt, Jeremias Knoblauch, Matt Jones, FranÃ§ois-Xavier Briol, Kevin Murphy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05646">Outlier-robust Kalman Filtering through Generalised Bayes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We derive a novel, provably robust, and closed-form Bayesian update rule for online filtering in state-space models in the presence of outliers and misspecified measurement models. Our method combines generalised Bayesian inference with filtering methods such as the extended and ensemble Kalman filter. We use the former to show robustness and the latter to ensure computational efficiency in the case of nonlinear models. Our method matches or outperforms other robust filtering methods (such as those based on variational Bayes) at a much lower computational cost. We show this empirically on a range of filtering problems with outlier measurements, such as object tracking, state estimation in high-dimensional chaotic systems, and online learning of neural networks.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2404.05136.pdf' target='_blank'>https://arxiv.org/pdf/2404.05136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijia Lu, Bing Shuai, Yanbei Chen, Zhenlin Xu, Davide Modolo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05136">Self-Supervised Multi-Object Tracking with Path Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel concept of path consistency to learn robust object matching without using manual object identity supervision. Our key idea is that, to track a object through frames, we can obtain multiple different association results from a model by varying the frames it can observe, i.e., skipping frames in observation. As the differences in observations do not alter the identities of objects, the obtained association results should be consistent. Based on this rationale, we generate multiple observation paths, each specifying a different set of frames to be skipped, and formulate the Path Consistency Loss that enforces the association results are consistent across different observation paths. We use the proposed loss to train our object matching model with only self-supervision. By extensive experiments on three tracking datasets (MOT17, PersonPath22, KITTI), we demonstrate that our method outperforms existing unsupervised methods with consistent margins on various evaluation metrics, and even achieves performance close to supervised methods.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2402.08774.pdf' target='_blank'>https://arxiv.org/pdf/2402.08774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angus Fung, Beno Benhabib, Goldie Nejat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08774">LDTrack: Dynamic People Tracking by Service Robots using Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking of dynamic people in cluttered and crowded human-centered environments is a challenging robotics problem due to the presence of intraclass variations including occlusions, pose deformations, and lighting variations. This paper introduces a novel deep learning architecture, using conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for tracking multiple dynamic people under intraclass variations. By uniquely utilizing conditional latent diffusion models to capture temporal person embeddings, our architecture can adapt to appearance changes of people over time. We incorporated a latent feature encoder network which enables the diffusion process to operate within a high-dimensional latent space to allow for the extraction and spatial-temporal refinement of such rich features as person appearance, motion, location, identity, and contextual information. Extensive experiments demonstrate the effectiveness of LDTrack over other state-of-the-art tracking methods in cluttered and crowded human-centered environments under intraclass variations. Namely, the results show our method outperforms existing deep learning robotic people tracking methods in both tracking accuracy and tracking precision with statistical significance. Additionally, a comprehensive multi-object tracking comparison study was performed against the state-of-the-art methods in urban environments, demonstrating the generalizability of LDTrack. An ablation study was performed to validate the design choices of LDTrack.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2401.13950.pdf' target='_blank'>https://arxiv.org/pdf/2401.13950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vitaliy Kim, Gunho Jung, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13950">AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many multi-object tracking (MOT) approaches, which employ the Kalman Filter as a motion predictor, assume constant velocity and Gaussian-distributed filtering noises. These assumptions render the Kalman Filter-based trackers effective in linear motion scenarios. However, these linear assumptions serve as a key limitation when estimating future object locations within scenarios involving non-linear motion and occlusions. To address this issue, we propose a motion-based MOT approach with an adaptable motion predictor, called AM-SORT, which adapts to estimate non-linear uncertainties. AM-SORT is a novel extension of the SORT-series trackers that supersedes the Kalman Filter with the transformer architecture as a motion predictor. We introduce a historical trajectory embedding that empowers the transformer to extract spatio-temporal features from a sequence of bounding boxes. AM-SORT achieves competitive performance compared to state-of-the-art trackers on DanceTrack, with 56.3 IDF1 and 55.6 HOTA. We conduct extensive experiments to demonstrate the effectiveness of our method in predicting non-linear movement under occlusions.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2401.10805.pdf' target='_blank'>https://arxiv.org/pdf/2401.10805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paritosh Parmar, Eric Peh, Basura Fernando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10805">Learning to Visually Connect Actions and their Effects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We identify and explore two different aspects of the concept of CATE: Action Selection (AS) and Effect-Affinity Assessment (EAA), where video understanding models connect actions and effects at semantic and fine-grained levels, respectively. We design various baseline models for AS and EAA. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. Our experiments show that in solving AS and EAA, models learn intuitive properties like object tracking and pose encoding without explicit supervision. We demonstrate that CATE can be an effective self-supervised task for learning video representations from unlabeled videos. The study aims to showcase the fundamental nature and versatility of CATE, with the hope of inspiring advanced formulations and models.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2512.24838.pdf' target='_blank'>https://arxiv.org/pdf/2512.24838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Ahmed Al Muzaddid, Jordan A. James, William J. Beksi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.24838">CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple-object tracking (MOT) in agricultural environments presents major challenges due to repetitive patterns, similar object appearances, sudden illumination changes, and frequent occlusions. Contemporary trackers in this domain rely on the motion of objects rather than appearance for association. Nevertheless, they struggle to maintain object identities when targets undergo frequent and strong occlusions. The high similarity of object appearances makes integrating appearance-based association nontrivial for agricultural scenarios. To solve this problem we propose CropTrack, a novel MOT framework based on the combination of appearance and motion information. CropTrack integrates a reranking-enhanced appearance association, a one-to-many association with appearance-based conflict resolution strategy, and an exponential moving average prototype feature bank to improve appearance-based association. Evaluated on publicly available agricultural MOT datasets, CropTrack demonstrates consistent identity preservation, outperforming traditional motion-based tracking methods. Compared to the state of the art, CropTrack achieves significant gains in identification F1 and association accuracy scores with a lower number of identity switches.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2512.14426.pdf' target='_blank'>https://arxiv.org/pdf/2512.14426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Steuernagel, Marcus Baum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14426">Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended object tracking involves estimating both the physical extent and kinematic parameters of a target object, where typically multiple measurements are observed per time step. In this article, we propose a deterministic closed-form elliptical extended object tracker, based on decoupling of the kinematics, orientation, and axis lengths. By disregarding potential correlations between these state components, fewer approximations are required for the individual estimators than for an overall joint solution. The resulting algorithm outperforms existing algorithms, reaching the accuracy of sampling-based procedures. Additionally, a batch-based variant is introduced, yielding highly efficient computation while outperforming all comparable state-of-the-art algorithms. This is validated both by a simulation study using common models from literature, as well as an extensive quantitative evaluation on real automotive radar data.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2511.18694.pdf' target='_blank'>https://arxiv.org/pdf/2511.18694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Wen, Edwin Meriaux, Mariana Sosa Guzmán, Zhizun Wang, Junming Shi, Gregory Dudek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.18694">Stable Multi-Drone GNSS Tracking System for Marine Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2511.11824.pdf' target='_blank'>https://arxiv.org/pdf/2511.11824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongping Dong, Pengyang Yu, Shuangjian Li, Liming Chen, Mohand Tahar Kechadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11824">SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2511.08865.pdf' target='_blank'>https://arxiv.org/pdf/2511.08865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Tai, Hansheng Wu, Haixu Long, Zhengbin Long, Zhaoyu Zheng, Haodong Xiang, Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08865">MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2510.26369.pdf' target='_blank'>https://arxiv.org/pdf/2510.26369.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuma Kano, Yuki Mori, Shin Katayama, Kenta Urano, Takuro Yonezawa, Nobuo Kawaguchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26369">CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Worker location data is key to higher productivity in industrial sites. Cameras are a promising tool for localization in logistics warehouses since they also offer valuable environmental contexts such as package status. However, identifying individuals with only visual data is often impractical. Accordingly, several prior studies identified people in videos by comparing their trajectories and wearable sensor measurements. While this approach has advantages such as independence from appearance, the existing methods may break down under real-world conditions. To overcome this challenge, we propose CorVS, a novel data-driven person identification method based on correspondence between visual tracking trajectories and sensor measurements. Firstly, our deep learning model predicts correspondence probabilities and reliabilities for every pair of a trajectory and sensor measurements. Secondly, our algorithm matches the trajectories and sensor measurements over time using the predicted probabilities and reliabilities. We developed a dataset with actual warehouse operations and demonstrated the method's effectiveness for real-world applications.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2510.17409.pdf' target='_blank'>https://arxiv.org/pdf/2510.17409.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dmitrii Galimzianov, Viacheslav Vyshegorodtsev, Ivan Nezhivykh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17409">Monitoring Horses in Stalls: From Object to Event Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring the behavior of stalled horses is essential for early detection of health and welfare issues but remains labor-intensive and time-consuming. In this study, we present a prototype vision-based monitoring system that automates the detection and tracking of horses and people inside stables using object detection and multi-object tracking techniques. The system leverages YOLOv11 and BoT-SORT for detection and tracking, while event states are inferred based on object trajectories and spatial relations within the stall. To support development, we constructed a custom dataset annotated with assistance from foundation models CLIP and GroundingDINO. The system distinguishes between five event types and accounts for the camera's blind spots. Qualitative evaluation demonstrated reliable performance for horse-related events, while highlighting limitations in detecting people due to data scarcity. This work provides a foundation for real-time behavioral monitoring in equine facilities, with implications for animal welfare and stable management.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2510.09878.pdf' target='_blank'>https://arxiv.org/pdf/2510.09878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Khanchi, Maria Amer, Charalambos Poullis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09878">Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) methods often rely on Intersection-over-Union (IoU) for association. However, this becomes unreliable when objects are similar or occluded. Also, computing IoU for segmentation masks is computationally expensive. In this work, we use segmentation masks to capture object shapes, but we do not compute segmentation IoU. Instead, we fuse depth and mask features and pass them through a compact encoder trained self-supervised. This encoder produces stable object representations, which we use as an additional similarity cue alongside bounding box IoU and re-identification features for matching. We obtain depth maps from a zero-shot depth estimator and object masks from a promptable visual segmentation model to obtain fine-grained spatial cues. Our MOT method is the first to use the self-supervised encoder to refine segmentation masks without computing masks IoU. MOT can be divided into joint detection-ReID (JDR) and tracking-by-detection (TBD) models. The latter are computationally more efficient. Experiments of our TBD method on challenging benchmarks with non-linear motion, occlusion, and crowded scenes, such as SportsMOT and DanceTrack, show that our method outperforms the TBD state-of-the-art on most metrics, while achieving competitive performance on simpler benchmarks with linear motion, such as MOT17.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2510.09092.pdf' target='_blank'>https://arxiv.org/pdf/2510.09092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juanqin Liu, Leonardo Plotegher, Eloy Roura, Shaoming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09092">GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2509.22910.pdf' target='_blank'>https://arxiv.org/pdf/2509.22910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwei Du, Jing-Chen Peng, Patricio A. Vela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22910">Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given that Visual SLAM relies on appearance cues for localization and scene understanding, texture-less or visually degraded environments (e.g., plain walls or low lighting) lead to poor pose estimation and track loss. However, robots are typically equipped with sensors that provide some form of dead reckoning odometry with reasonable short-time performance but unreliable long-time performance. The Good Weights (GW) algorithm described here provides a framework to adaptively integrate dead reckoning (DR) with passive visual SLAM for continuous and accurate frame-level pose estimation. Importantly, it describes how all modules in a comprehensive SLAM system must be modified to incorporate DR into its design. Adaptive weighting increases DR influence when visual tracking is unreliable and reduces when visual feature information is strong, maintaining pose track without overreliance on DR. Good Weights yields a practical solution for mobile navigation that improves visual SLAM performance and robustness. Experiments on collected datasets and in real-world deployment demonstrate the benefits of Good Weights.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2508.12982.pdf' target='_blank'>https://arxiv.org/pdf/2508.12982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrejÄÃ­, OndÅej Straka, Petr Girg, JiÅÃ­ Benedikt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12982">Revisiting Functional Derivatives in Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probability generating functionals (PGFLs) are efficient and powerful tools for tracking independent objects in clutter. It was shown that PGFLs could be used for the elegant derivation of practical multi-object tracking algorithms, e.g., the probability hypothesis density (PHD) filter. However, derivations using PGFLs use the so-called functional derivatives whose definitions usually appear too complicated or heuristic, involving Dirac delta ``functions''. This paper begins by comparing different definitions of functional derivatives and exploring their relationships and implications for practical applications. It then proposes a rigorous definition of the functional derivative, utilizing straightforward yet precise mathematics for clarity. Key properties of the functional derivative are revealed and discussed.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2508.08117.pdf' target='_blank'>https://arxiv.org/pdf/2508.08117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Han, Pengcheng Fang, Yueying Tian, Jianhui Yu, Xiaohao Cai, Daniel Roggen, Philip Birch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08117">GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in monocular videos is fundamentally challenged by occlusions and depth ambiguity, issues that conventional tracking-by-detection (TBD) methods struggle to resolve owing to a lack of geometric awareness. To address these limitations, we introduce GRASPTrack, a novel depth-aware MOT framework that integrates monocular depth estimation and instance segmentation into a standard TBD pipeline to generate high-fidelity 3D point clouds from 2D detections, thereby enabling explicit 3D geometric reasoning. These 3D point clouds are then voxelized to enable a precise and robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To further enhance tracking robustness, our approach incorporates Depth-aware Adaptive Noise Compensation, which dynamically adjusts the Kalman filter process noise based on occlusion severity for more reliable state estimation. Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which extends the motion direction consistency from the image plane into 3D space to improve motion-based association cues, particularly for objects with complex trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack benchmarks demonstrate that our method achieves competitive performance, significantly improving tracking robustness in complex scenes with frequent occlusions and intricate motion patterns.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2506.04122.pdf' target='_blank'>https://arxiv.org/pdf/2506.04122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharang Kaul, Mario Berk, Thiemo Gerbich, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04122">Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding reliable matches is essential in multi-object tracking to ensure the accuracy and reliability of perception systems in safety-critical applications such as autonomous vehicles. Effective matching mitigates perception errors, enhancing object identification and tracking for improved performance and safety. However, traditional metrics such as Intersection over Union (IoU) and Center Point Distances (CPDs), which are effective in 2D image planes, often fail to find critical matches in complex 3D scenes. To address this limitation, we introduce Contour Errors (CEs), an ego or object-centric metric for identifying matches of interest in tracking scenarios from a functional perspective. By comparing bounding boxes in the ego vehicle's frame, contour errors provide a more functionally relevant assessment of object matches. Extensive experiments on the nuScenes dataset demonstrate that contour errors improve the reliability of matches over the state-of-the-art 2D IoU and CPD metrics in tracking-by-detection methods. In 3D car tracking, our results show that Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges and 60% at far ranges compared to IoU in the evaluation stage.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2505.22882.pdf' target='_blank'>https://arxiv.org/pdf/2505.22882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Yang, Zhixian Xie, Xuechao Zhang, Heni Ben Amor, Shan Lin, Wanxin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22882">TwinTrack: Bridging Vision and Contact Physics for Real-Time Tracking of Unknown Dynamic Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time tracking of previously unseen, highly dynamic objects in contact-rich environments -- such as during dexterous in-hand manipulation -- remains a significant challenge. Purely vision-based tracking often suffers from heavy occlusions due to the frequent contact interactions and motion blur caused by abrupt motion during contact impacts. We propose TwinTrack, a physics-aware visual tracking framework that enables robust and real-time 6-DoF pose tracking of unknown dynamic objects in a contact-rich scene by leveraging the contact physics of the observed scene. At the core of TwinTrack is an integration of Real2Sim and Sim2Real. In Real2Sim, we combine the complementary strengths of vision and contact physics to estimate object's collision geometry and physical properties: object's geometry is first reconstructed from vision, then updated along with other physical parameters from contact dynamics for physical accuracy. In Sim2Real, robust pose estimation of the object is achieved by adaptive fusion between visual tracking and prediction of the learned contact physics. TwinTrack is built on a GPU-accelerated, deeply customized physics engine to ensure real-time performance. We evaluate our method on two contact-rich scenarios: object falling with rich contact impacts against the environment, and contact-rich in-hand manipulation. Experimental results demonstrate that, compared to baseline methods, TwinTrack achieves significantly more robust, accurate, and real-time 6-DoF tracking in these challenging scenarios, with tracking speed exceeding 20 Hz. Project page: https://irislab.tech/TwinTrack-webpage/
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2505.17201.pdf' target='_blank'>https://arxiv.org/pdf/2505.17201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaim Chai Elchik, Fatemeh Karimi Nejadasl, Seyed Sahand Mohammadi Ziabari, Ali Mohammed Mansoor Alsahag
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17201">A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in computer vision has made significant advancements, yet tracking small fish in underwater environments presents unique challenges due to complex 3D motions and data noise. Traditional single-view MOT models often fall short in these settings. This thesis addresses these challenges by adapting state-of-the-art single-view MOT models, FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological studies. The core contribution of this research is the development of a multi-view framework that utilizes stereo video inputs to enhance tracking accuracy and fish behavior pattern recognition. By integrating and evaluating these models on underwater fish video datasets, the study aims to demonstrate significant improvements in precision and reliability compared to single-view approaches. The proposed framework detects fish entities with a relative accuracy of 47% and employs stereo-matching techniques to produce a novel 3D output, providing a more comprehensive understanding of fish movements and interactions
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2505.00995.pdf' target='_blank'>https://arxiv.org/pdf/2505.00995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taewook Park, Jinwoo Lee, Hyondong Oh, Won-Jae Yun, Kyu-Wha Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00995">Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the agricultural workforce declines and labor costs rise, robotic yield estimation has become increasingly important. While unmanned ground vehicles (UGVs) are commonly used for indoor farm monitoring, their deployment in greenhouses is often constrained by infrastructure limitations, sensor placement challenges, and operational inefficiencies. To address these issues, we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial odometry algorithm for precise navigation in GNSS-denied environments and utilizes a 3D multi-object tracking algorithm to estimate the count and weight of cherry tomatoes. We evaluate the system using two dataset: one from a harvesting row and another from a growing row. In the harvesting-row dataset, the proposed system achieves 94.4\% counting accuracy and 87.5\% weight estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For the growing-row dataset, which consists of occluded unripened fruits, we qualitatively analyze tracking performance and highlight future research directions for improving perception in greenhouse with strong occlusions. Our findings demonstrate the potential of UAVs for efficient robotic yield estimation in commercial greenhouses.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2505.00534.pdf' target='_blank'>https://arxiv.org/pdf/2505.00534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, Rana Hammad Raza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00534">A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2411.15811.pdf' target='_blank'>https://arxiv.org/pdf/2411.15811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Wenhui Zhao, Dingwen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15811">FastTrackTr:Towards Fast Multi-Object Tracking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based multi-object tracking (MOT) methods have captured the attention of many researchers in recent years. However, these models often suffer from slow inference speeds due to their structure or other issues. To address this problem, we revisited the Joint Detection and Tracking (JDT) method by looking back at past approaches. By integrating the original JDT approach with some advanced theories, this paper employs an efficient method of information transfer between frames on the DETR, constructing a fast and novel JDT-type MOT framework: FastTrackTr. Thanks to the superiority of this information transfer method, our approach not only reduces the number of queries required during tracking but also avoids the excessive introduction of network structures, ensuring model simplicity. Experimental results indicate that our method has the potential to achieve real-time tracking and exhibits competitive tracking accuracy across multiple datasets.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2411.13346.pdf' target='_blank'>https://arxiv.org/pdf/2411.13346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karolina Trajkovska, MatjaÅ¾ Kljun, Klen ÄopiÄ Pucihar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13346">Gaze2AOI: Open Source Deep-learning Based System for Automatic Area of Interest Annotation with Eye Tracking Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Eye gaze is considered an important indicator for understanding and predicting user behaviour, as well as directing their attention across various domains including advertisement design, human-computer interaction and film viewing. In this paper, we present a novel method to enhance the analysis of user behaviour and attention by (i) augmenting video streams with automatically annotating and labelling areas of interest (AOIs), and (ii) integrating AOIs with collected eye gaze and fixation data. The tool provides key features such as time to first fixation, dwell time, and frequency of AOI revisits. By incorporating the YOLOv8 object tracking algorithm, the tool supports over 600 different object classes, providing a comprehensive set for a variety of video streams. This tool will be made available as open-source software, thereby contributing to broader research and development efforts in the field.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2411.06197.pdf' target='_blank'>https://arxiv.org/pdf/2411.06197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shukun Jia, Shiyu Hu, Yichao Cao, Feng Yang, Xin Lu, Xiaobo Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06197">Tracking by Detection and Query: An Efficient End-to-End Framework for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is dominated by two paradigms: tracking-by-detection (TBD) and tracking-by-query (TBQ). While TBD is decoupled and efficient, its fragmented association steps and heuristic matching pipelines often compromise robustness in complex scenarios. TBQ provides stronger semantic modeling through end-to-end learning, but suffers from high training cost and slow inference due to tight coupling between detection and association. To address these challenges, we propose TBDQ-Net, a unified tracking-by-detection-and-query (TBDQ) framework that effectively combines the strengths of both paradigms. Our method efficiently integrates pretrained, high-performance detectors with an MOT-tailored associator. The associator is lightweight and directly fetches information from the inference of detectors, enhancing the overall efficiency of the framework. The associator is also learnable, making it essential for fully end-to-end optimization, ensuring robust tracking capabilities. Specifically, the associator comprises two key modules: basic information interaction (BII) for comprehensive semantic interaction, and content-position alignment (CPA) for semantic and positional consistency. TBDQ-Net's effectiveness is extensively demonstrated on DanceTrack, SportsMOT and MOT20 benchmarks. As a structurally efficient and semantically robust tracking framework, it outperforms the leading TBD method by 6.0 IDF1 points on DanceTrack and achieves at least 37.5% faster inference than prominent TBQ methods.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2410.13437.pdf' target='_blank'>https://arxiv.org/pdf/2410.13437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changcheng Xiao, Qiong Cao, Yujie Zhong, Xiang Zhang, Tao Wang, Canqun Yang, Long Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13437">Temporal-Enhanced Multimodal Transformer for Referring Multi-Object Tracking and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to locate an arbitrary number of target objects and maintain their identities referred by a language expression in a video. This intricate task involves the reasoning of linguistic and visual modalities, along with the temporal association of target objects. However, the seminal work employs only loose feature fusion and overlooks the utilization of long-term information on tracked objects. In this study, we introduce a compact Transformer-based method, termed TenRMOT. We conduct feature fusion at both encoding and decoding stages to fully exploit the advantages of Transformer architecture. Specifically, we incrementally perform cross-modal fusion layer-by-layer during the encoding phase. In the decoding phase, we utilize language-guided queries to probe memory features for accurate prediction of the desired objects. Moreover, we introduce a query update module that explicitly leverages temporal prior information of the tracked objects to enhance the consistency of their trajectories. In addition, we introduce a novel task called Referring Multi-Object Tracking and Segmentation (RMOTS) and construct a new dataset named Ref-KITTI Segmentation. Our dataset consists of 18 videos with 818 expressions, and each expression averages 10.7 masks, which poses a greater challenge compared to the typical single mask in most existing referring video segmentation datasets. TenRMOT demonstrates superior performance on both the referring multi-object tracking and the segmentation tasks.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2409.11785.pdf' target='_blank'>https://arxiv.org/pdf/2409.11785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiming Ge, Zhao Luo, Chunhui Zhang, Yingying Hua, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11785">Distilling Channels for Efficient Deep Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep trackers have proven success in visual tracking. Typically, these trackers employ optimally pre-trained deep networks to represent all diverse objects with multi-channel features from some fixed layers. The deep networks employed are usually trained to extract rich knowledge from massive data used in object classification and so they are capable to represent generic objects very well. However, these networks are too complex to represent a specific moving object, leading to poor generalization as well as high computational and memory costs. This paper presents a novel and general framework termed channel distillation to facilitate deep trackers. To validate the effectiveness of channel distillation, we take discriminative correlation filter (DCF) and ECO for example. We demonstrate that an integrated formulation can turn feature compression, response map generation, and model update into a unified energy minimization problem to adaptively select informative feature channels that improve the efficacy of tracking moving objects on the fly. Channel distillation can accurately extract good channels, alleviating the influence of noisy channels and generally reducing the number of channels, as well as adaptively generalizing to different channels and networks. The resulting deep tracker is accurate, fast, and has low memory requirements. Extensive experimental evaluations on popular benchmarks clearly demonstrate the effectiveness and generalizability of our framework.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2409.11749.pdf' target='_blank'>https://arxiv.org/pdf/2409.11749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Li, Peidong Li, Lijun Zhao, Dedong Liu, Jinghan Gao, Xian Wu, Yitao Wu, Dixiao Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11749">RockTrack: A 3D Robust Multi-Camera-Ken Multi-Object Tracking Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT) obtains significant performance improvements with the rapid advancements in 3D object detection, particularly in cost-effective multi-camera setups. However, the prevalent end-to-end training approach for multi-camera trackers results in detector-specific models, limiting their versatility. Moreover, current generic trackers overlook the unique features of multi-camera detectors, i.e., the unreliability of motion observations and the feasibility of visual information. To address these challenges, we propose RockTrack, a 3D MOT method for multi-camera detectors. Following the Tracking-By-Detection framework, RockTrack is compatible with various off-the-shelf detectors. RockTrack incorporates a confidence-guided preprocessing module to extract reliable motion and image observations from distinct representation spaces from a single detector. These observations are then fused in an association module that leverages geometric and appearance cues to minimize mismatches. The resulting matches are propagated through a staged estimation process, forming the basis for heuristic noise modeling. Additionally, we introduce a novel appearance similarity metric for explicitly characterizing object affinities in multi-camera settings. RockTrack achieves state-of-the-art performance on the nuScenes vision-only tracking leaderboard with 59.1% AMOTA while demonstrating impressive computational efficiency.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2409.03252.pdf' target='_blank'>https://arxiv.org/pdf/2409.03252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Toida, Naoki Kato, Osamu Segawa, Takeshi Nakamura, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03252">Gr-IoU: Ground-Intersection over Union for Robust Multi-Object Tracking with 3D Geometric Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a Ground IoU (Gr-IoU) to address the data association problem in multi-object tracking. When tracking objects detected by a camera, it often occurs that the same object is assigned different IDs in consecutive frames, especially when objects are close to each other or overlapping. To address this issue, we introduce Gr-IoU, which takes into account the 3D structure of the scene. Gr-IoU transforms traditional bounding boxes from the image space to the ground plane using the vanishing point geometry. The IoU calculated with these transformed bounding boxes is more sensitive to the front-to-back relationships of objects, thereby improving data association accuracy and reducing ID switches. We evaluated our Gr-IoU method on the MOT17 and MOT20 datasets, which contain diverse tracking scenarios including crowded scenes and sequences with frequent occlusions. Experimental results demonstrated that Gr-IoU outperforms conventional real-time methods without appearance features.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2408.04979.pdf' target='_blank'>https://arxiv.org/pdf/2408.04979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lennart Niecksch, Alexander Mock, Felix Igelbrink, Thomas Wiemann, Joachim Hertzberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04979">Mesh-based Object Tracking for Dynamic Semantic 3D Scene Graphs via Ray Tracing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel method for 3D geometric scene graph generation using range sensors and RGB cameras. We initially detect instance-wise keypoints with a YOLOv8s model to compute 6D pose estimates of known objects by solving PnP. We use a ray tracing approach to track a geometric scene graph consisting of mesh models of object instances. In contrast to classical point-to-point matching, this leads to more robust results, especially under occlusions between objects instances. We show that using this hybrid strategy leads to robust self-localization, pre-segmentation of the range sensor data and accurate pose tracking of objects using the same environmental representation. All detected objects are integrated into a semantic scene graph. This scene graph then serves as a front end to a semantic mapping framework to allow spatial reasoning.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2408.02263.pdf' target='_blank'>https://arxiv.org/pdf/2408.02263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Lu, Jiahao Nie, Zhiwei He, Hongjie Gu, Xudong Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02263">VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current LiDAR point cloud-based 3D single object tracking (SOT) methods typically rely on point-based representation network. Despite demonstrated success, such networks suffer from some fundamental problems: 1) It contains pooling operation to cope with inherently disordered point clouds, hindering the capture of 3D spatial information that is useful for tracking, a regression task. 2) The adopted set abstraction operation hardly handles density-inconsistent point clouds, also preventing 3D spatial information from being modeled. To solve these problems, we introduce a novel tracking framework, termed VoxelTrack. By voxelizing inherently disordered point clouds into 3D voxels and extracting their features via sparse convolution blocks, VoxelTrack effectively models precise and robust 3D spatial information, thereby guiding accurate position prediction for tracked objects. Moreover, VoxelTrack incorporates a dual-stream encoder with cross-iterative feature fusion module to further explore fine-grained 3D spatial information for tracking. Benefiting from accurate 3D spatial information being modeled, our VoxelTrack simplifies tracking pipeline with a single regression loss. Extensive experiments are conducted on three widely-adopted datasets including KITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that VoxelTrack achieves state-of-the-art performance (88.3%, 71.4% and 63.6% mean precision on the three datasets, respectively), and outperforms the existing trackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source code and model will be released.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2407.01907.pdf' target='_blank'>https://arxiv.org/pdf/2407.01907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailiang Zhang, Dian Chao, Zhihao Guan, Yang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01907">The Solution for the ICCV 2023 Perception Test Challenge 2023 -- Task 6 -- Grounded videoQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a grounded video question-answering solution. Our research reveals that the fixed official baseline method for video question answering involves two main steps: visual grounding and object tracking. However, a significant challenge emerges during the initial step, where selected frames may lack clearly identifiable target objects. Furthermore, single images cannot address questions like "Track the container from which the person pours the first time." To tackle this issue, we propose an alternative two-stage approach:(1) First, we leverage the VALOR model to answer questions based on video information.(2) concatenate the answered questions with their respective answers. Finally, we employ TubeDETR to generate bounding boxes for the targets.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2406.19844.pdf' target='_blank'>https://arxiv.org/pdf/2406.19844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaheng Zhuang, Guoan Wang, Siyu Zhang, Xiyang Wang, Hangning Zhou, Ziyao Xu, Chi Zhang, Zhiheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19844">StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object Tracking and Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking and trajectory prediction are two crucial modules in autonomous driving systems. Generally, the two tasks are handled separately in traditional paradigms and a few methods have started to explore modeling these two tasks in a joint manner recently. However, these approaches suffer from the limitations of single-frame training and inconsistent coordinate representations between tracking and prediction tasks. In this paper, we propose a streaming and unified framework for joint 3D Multi-Object Tracking and trajectory Prediction (StreamMOTP) to address the above challenges. Firstly, we construct the model in a streaming manner and exploit a memory bank to preserve and leverage the long-term latent features for tracked objects more effectively. Secondly, a relative spatio-temporal positional encoding strategy is introduced to bridge the gap of coordinate representations between the two tasks and maintain the pose-invariance for trajectory prediction. Thirdly, we further improve the quality and consistency of predicted trajectories with a dual-stream predictor. We conduct extensive experiments on popular nuSences dataset and the experimental results demonstrate the effectiveness and superiority of StreamMOTP, which outperforms previous methods significantly on both tasks. Furthermore, we also prove that the proposed framework has great potential and advantages in actual applications of autonomous driving.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2406.12081.pdf' target='_blank'>https://arxiv.org/pdf/2406.12081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matias Gran-Henriksen, Hans Andreas Lindgaard, Gabriel Kiss, Frank Lindseth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12081">Deep HM-SORT: Enhancing Multi-Object Tracking in Sports with Deep Features, Harmonic Mean, and Expansion IOU</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Deep HM-SORT, a novel online multi-object tracking algorithm specifically designed to enhance the tracking of athletes in sports scenarios. Traditional multi-object tracking methods often struggle with sports environments due to the similar appearances of players, irregular and unpredictable movements, and significant camera motion. Deep HM-SORT addresses these challenges by integrating deep features, harmonic mean, and Expansion IOU. By leveraging the harmonic mean, our method effectively balances appearance and motion cues, significantly reducing ID-swaps. Additionally, our approach retains all tracklets indefinitely, improving the re-identification of players who leave and re-enter the frame. Experimental results demonstrate that Deep HM-SORT achieves state-of-the-art performance on two large-scale public benchmarks, SportsMOT and SoccerNet Tracking Challenge 2023. Specifically, our method achieves 80.1 HOTA on the SportsMOT dataset and 85.4 HOTA on the SoccerNet-Tracking dataset, outperforming existing trackers in key metrics such as HOTA, IDF1, AssA, and MOTA. This robust solution provides enhanced accuracy and reliability for automated sports analytics, offering significant improvements over previous methods without introducing additional computational cost.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2406.01011.pdf' target='_blank'>https://arxiv.org/pdf/2406.01011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Palmer, Martin KrÃ¼ger, Richard Altendorfer, Torsten Bertram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01011">Multi-Object Tracking based on Imaging Radar 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective tracking of surrounding traffic participants allows for an accurate state estimation as a necessary ingredient for prediction of future behavior and therefore adequate planning of the ego vehicle trajectory. One approach for detecting and tracking surrounding traffic participants is the combination of a learning based object detector with a classical tracking algorithm. Learning based object detectors have been shown to work adequately on lidar and camera data, while learning based object detectors using standard radar data input have proven to be inferior. Recently, with the improvements to radar sensor technology in the form of imaging radars, the object detection performance on radar was greatly improved but is still limited compared to lidar sensors due to the sparsity of the radar point cloud. This presents a unique challenge for the task of multi-object tracking. The tracking algorithm must overcome the limited detection quality while generating consistent tracks. To this end, a comparison between different multi-object tracking methods on imaging radar data is required to investigate its potential for downstream tasks. The work at hand compares multiple approaches and analyzes their limitations when applied to imaging radar data. Furthermore, enhancements to the presented approaches in the form of probabilistic association algorithms are considered for this task.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2405.13397.pdf' target='_blank'>https://arxiv.org/pdf/2405.13397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harish Prakash, Jia Cheng Shang, Ken M. Nsiempba, Yuhao Chen, David A. Clausi, John S. Zelek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13397">Multi Player Tracking in Ice Hockey with Homographic Projections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi Object Tracking (MOT) in ice hockey pursues the combined task of localizing and associating players across a given sequence to maintain their identities. Tracking players from monocular broadcast feeds is an important computer vision problem offering various downstream analytics and enhanced viewership experience. However, existing trackers encounter significant difficulties in dealing with occlusions, blurs, and agile player movements prevalent in telecast feeds. In this work, we propose a novel tracking approach by formulating MOT as a bipartite graph matching problem infused with homography. We disentangle the positional representations of occluded and overlapping players in broadcast view, by mapping their foot keypoints to an overhead rink template, and encode these projected positions into the graph network. This ensures reliable spatial context for consistent player tracking and unfragmented tracklet prediction. Our results show considerable improvements in both the IDsw and IDF1 metrics on the two available broadcast ice hockey datasets.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2405.08909.pdf' target='_blank'>https://arxiv.org/pdf/2405.08909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuxiao Ding, Lukas Schneider, Marius Cordts, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08909">ADA-Track++: End-to-End Multi-Camera 3D Multi-Object Tracking with Alternating Detection and Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the tracking-by-attention paradigm, utilizing track queries for identity-consistent detection and object queries for identity-agnostic track spawning. Tracking-by-attention, however, entangles detection and tracking queries in one embedding for both the detection and tracking task, which is sub-optimal. Other approaches resemble the tracking-by-detection paradigm and detect objects using decoupled track and detection queries followed by a subsequent association. These methods, however, do not leverage synergies between the detection and association task. Combining the strengths of both paradigms, we introduce ADA-Track++, a novel end-to-end framework for 3D MOT from multi-view cameras. We introduce a learnable data association module based on edge-augmented cross-attention, leveraging appearance and geometric features. We also propose an auxiliary token in this attention-based association module, which helps mitigate disproportionately high attention to incorrect association targets caused by attention normalization. Furthermore, we integrate this association module into the decoder layer of a DETR-based 3D detector, enabling simultaneous DETR-like query-to-image cross-attention for detection and query-to-query cross-attention for data association. By stacking these decoder layers, queries are refined for the detection and association task alternately, effectively harnessing the task dependencies. We evaluate our method on the nuScenes dataset and demonstrate the advantage of our approach compared to the two previous paradigms.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2405.06600.pdf' target='_blank'>https://arxiv.org/pdf/2405.06600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzhe Wang, Kang Ma, Qiankun Liu, Yunhao Zou, Ying Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06600">Multi-Object Tracking in the Dark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-light scenes are prevalent in real-world applications (e.g. autonomous driving and surveillance at night). Recently, multi-object tracking in various practical use cases have received much attention, but multi-object tracking in dark scenes is rarely considered. In this paper, we focus on multi-object tracking in dark scenes. To address the lack of datasets, we first build a Low-light Multi-Object Tracking (LMOT) dataset. LMOT provides well-aligned low-light video pairs captured by our dual-camera system, and high-quality multi-object tracking annotations for all videos. Then, we propose a low-light multi-object tracking method, termed as LTrack. We introduce the adaptive low-pass downsample module to enhance low-frequency components of images outside the sensor noises. The degradation suppression learning strategy enables the model to learn invariant information under noise disturbance and image quality degradation. These components improve the robustness of multi-object tracking in dark scenes. We conducted a comprehensive analysis of our LMOT dataset and proposed LTrack. Experimental results demonstrate the superiority of the proposed method and its competitiveness in real night low-light scenes. Dataset and Code: https: //github.com/ying-fu/LMOT
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2403.15831.pdf' target='_blank'>https://arxiv.org/pdf/2403.15831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoyu Sun, Chunyang Wang, Xuelian Liu, Chunhao Shi, Yueyang Ding, Guan Xi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15831">Spatio-Temporal Bi-directional Cross-frame Memory for Distractor Filtering Point Cloud Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking within LIDAR point clouds is a pivotal task in computer vision, with profound implications for autonomous driving and robotics. However, existing methods, which depend solely on appearance matching via Siamese networks or utilize motion information from successive frames, encounter significant challenges. Issues such as similar objects nearby or occlusions can result in tracker drift. To mitigate these challenges, we design an innovative spatio-temporal bi-directional cross-frame distractor filtering tracker, named STMD-Tracker. Our first step involves the creation of a 4D multi-frame spatio-temporal graph convolution backbone. This design separates KNN graph spatial embedding and incorporates 1D temporal convolution, effectively capturing temporal fluctuations and spatio-temporal information. Subsequently, we devise a novel bi-directional cross-frame memory procedure. This integrates future and synthetic past frame memory to enhance the current memory, thereby improving the accuracy of iteration-based tracking. This iterative memory update mechanism allows our tracker to dynamically compensate for information in the current frame, effectively reducing tracker drift. Lastly, we construct spatially reliable Gaussian masks on the fused features to eliminate distractor points. This is further supplemented by an object-aware sampling strategy, which bolsters the efficiency and precision of object localization, thereby reducing tracking errors caused by distractors. Our extensive experiments on KITTI, NuScenes and Waymo datasets demonstrate that our approach significantly surpasses the current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2403.11978.pdf' target='_blank'>https://arxiv.org/pdf/2403.11978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrejÄÃ­, Oliver Kost, OndÅej Straka, JindÅich DunÃ­k
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11978">Pedestrian Tracking with Monocular Camera using Unconstrained 3D Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A first-principle single-object model is proposed for pedestrian tracking. It is assumed that the extent of the moving object can be described via known statistics in 3D, such as pedestrian height. The proposed model thus need not constrain the object motion in 3D to a common ground plane, which is usual in 3D visual tracking applications. A nonlinear filter for this model is implemented using the unscented Kalman filter (UKF) and tested using the publicly available MOT-17 dataset. The proposed solution yields promising results in 3D while maintaining perfect results when projected into the 2D image. Moreover, the estimation error covariance matches the true one. Unlike conventional methods, the introduced model parameters have convenient meaning and can readily be adjusted for a problem.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2402.13146.pdf' target='_blank'>https://arxiv.org/pdf/2402.13146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adnen Abdessaied, Manuel von Hochmeister, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13146">OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Object Language Video Transformer (OLViT) - a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2401.15288.pdf' target='_blank'>https://arxiv.org/pdf/2401.15288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ragini Gupta, Lingzhi Zhao, Jiaxi Li, Volodymyr Vakhniuk, Claudiu Danilov, Josh Eckhardt, Keyshla Bernard, Klara Nahrstedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15288">STAC: Leveraging Spatio-Temporal Data Associations For Efficient Cross-Camera Streaming and Analytics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In IoT based distributed network of cameras, real-time multi-camera video analytics is challenged by high bandwidth demands and redundant visual data, creating a fundamental tension where reducing data saves network overhead but can degrade model performance, and vice versa. We present STAC, a cross-cameras surveillance system that leverages spatio-temporal associations for efficient object tracking under constrained network conditions. STAC integrates multi-resolution feature learning, ensuring robustness under variable networked system level optimizations such as frame filtering, FFmpeg-based compression, and Region-of-Interest (RoI) masking, to eliminate redundant content across distributed video streams while preserving downstream model accuracy for object identification and tracking. Evaluated on NVIDIA's AICity Challenge dataset, STAC achieves a 76\% improvement in tracking accuracy and an 8.6x reduction in inference latency over a standard multi-object multi-camera tracking baseline (using YOLOv4 and DeepSORT). Furthermore, 29\% of redundant frames are filtered, significantly reducing data volume without compromising inference quality.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2401.11204.pdf' target='_blank'>https://arxiv.org/pdf/2401.11204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Nie, Zhiwei He, Xudong Lv, Xueyi Zhou, Dong-Kyu Chae, Fei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11204">Towards Category Unification of 3D Single Object Tracking on Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Category-specific models are provenly valuable methods in 3D single object tracking (SOT) regardless of Siamese or motion-centric paradigms. However, such over-specialized model designs incur redundant parameters, thus limiting the broader applicability of 3D SOT task. This paper first introduces unified models that can simultaneously track objects across all categories using a single network with shared model parameters. Specifically, we propose to explicitly encode distinct attributes associated to different object categories, enabling the model to adapt to cross-category data. We find that the attribute variances of point cloud objects primarily occur from the varying size and shape (e.g., large and square vehicles v.s. small and slender humans). Based on this observation, we design a novel point set representation learning network inheriting transformer architecture, termed AdaFormer, which adaptively encodes the dynamically varying shape and size information from cross-category data in a unified manner. We further incorporate the size and shape prior derived from the known template targets into the model's inputs and learning objective, facilitating the learning of unified representation. Equipped with such designs, we construct two category-unified models SiamCUT and MoCUT.Extensive experiments demonstrate that SiamCUT and MoCUT exhibit strong generalization and training stability. Furthermore, our category-unified models outperform the category-specific counterparts by a significant margin (e.g., on KITTI dataset, 12% and 3% performance gains on the Siamese and motion paradigms). Our code will be available.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2512.07776.pdf' target='_blank'>https://arxiv.org/pdf/2512.07776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Schall, Felix Leonard Knöfel, Noah Elias König, Jan Jonas Kubeler, Maximilian von Klinski, Joan Wilhelm Linnemann, Xiaoshi Liu, Iven Jelle Schlegelmilch, Ole Woyciniuk, Alexandra Schild, Dante Wasmuht, Magdalena Bermejo Espinet, German Illera Basas, Gerard de Melo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07776">GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2512.02668.pdf' target='_blank'>https://arxiv.org/pdf/2512.02668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qionglin Ren, Dawei Zhang, Chunxu Tian, Dan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02668">UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2511.17681.pdf' target='_blank'>https://arxiv.org/pdf/2511.17681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiyi Lv, Ning Zhang, Hanyang Sun, Haoran Jiang, Kai Zhao, Jing Xiao, Dan Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17681">Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2511.09070.pdf' target='_blank'>https://arxiv.org/pdf/2511.09070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wing Shing Wong, Chung Shue Chen, Yuan-Hsun Lo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09070">Color Multiset Codes based on Sunmao Construction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present results on coding using multisets instead of ordered sequences. The study is motivated by a moving object tracking problem in a sensor network and can find applications in settings where the order of the symbols in a codeword cannot be maintained or observed. In this paper a multiset coding scheme is proposed on source data that can be organized as a flat or cyclic multi-dimensional integer lattice (grid). A fundamental idea in the solution approach is to decompose the original source data grid into sub-grids. The original multiset coding problem can then be restricted to each of the sub-grid. Solutions for the sub-grids are subsequently piece together to form the desired solution. We name this circle of idea as sunmao construction in reference to woodwork construction method with ancient origin. Braid codes are specific solutions defined using the sunmao construction. They are easy to define for multi-dimensional grids. Moreover for a code of a given code set size and multiset cardinality, if we measure coding efficiency by the number of distinct symbols required, then braid codes have asymptotic order equal to those that are optimal. We also show that braid codes have interesting inherent error correction properties.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2510.13235.pdf' target='_blank'>https://arxiv.org/pdf/2510.13235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukuan Zhang, Jiarui Zhao, Shangqing Nie, Jin Kuang, Shengsheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13235">EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal semantic cues, such as textual descriptions, have shown strong potential in enhancing target perception for tracking. However, existing methods rely on static textual descriptions from large language models, which lack adaptability to real-time target state changes and prone to hallucinations. To address these challenges, we propose a unified multimodal vision-language tracking framework, named EPIPTrack, which leverages explicit and implicit prompts for dynamic target modeling and semantic alignment. Specifically, explicit prompts transform spatial motion information into natural language descriptions to provide spatiotemporal guidance. Implicit prompts combine pseudo-words with learnable descriptors to construct individualized knowledge representations capturing appearance attributes. Both prompts undergo dynamic adjustment via the CLIP text encoder to respond to changes in target state. Furthermore, we design a Discriminative Feature Augmentor to enhance visual and cross-modal representations. Extensive experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack outperforms existing trackers in diverse scenarios, exhibiting robust adaptability and superior performance.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2508.12777.pdf' target='_blank'>https://arxiv.org/pdf/2508.12777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenguang Tao, Xiaotian Wang, Tian Yan, Jie Yan, Guodong Li, Kun Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12777">SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a key research direction in the field of multi-object tracking (MOT), UAV-based multi-object tracking has significant application value in the analysis and understanding of urban intelligent transportation systems. However, in complex UAV perspectives, challenges such as small target scale variations, occlusions, nonlinear crossing motions, and motion blur severely hinder the stability of multi-object tracking. To address these challenges, this paper proposes a novel multi-object tracking framework, SocialTrack, aimed at enhancing the tracking accuracy and robustness of small targets in complex urban traffic environments. The specialized small-target detector enhances the detection performance by employing a multi-scale feature enhancement mechanism. The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of trajectory prediction by incorporating a velocity dynamic modeling mechanism. The Group Motion Compensation Strategy (GMCS) models social group motion priors to provide stable state update references for low-quality tracks, significantly improving the target association accuracy in complex dynamic environments. Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical trajectory information to predict the future state of low-quality tracks, effectively mitigating identity switching issues. Extensive experiments on the UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing state-of-the-art (SOTA) methods across several key metrics. Significant improvements in MOTA and IDF1, among other core performance indicators, highlight its superior robustness and adaptability. Additionally, SocialTrack is highly modular and compatible, allowing for seamless integration with existing trackers to further enhance performance.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2508.02238.pdf' target='_blank'>https://arxiv.org/pdf/2508.02238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Dong, Yiwei Zhang, Yangjie Cui, Jinwu Xiang, Daochun Li, Zhan Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02238">An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras offer significant advantages, including a wide dynamic range, high temporal resolution, and immunity to motion blur, making them highly promising for addressing challenging visual conditions. Extracting and utilizing effective information from asynchronous event streams is essential for the onboard implementation of event cameras. In this paper, we propose a streamlined event-based intensity reconstruction scheme, event-based single integration (ESI), to address such implementation challenges. This method guarantees the portability of conventional frame-based vision methods to event-based scenarios and maintains the intrinsic advantages of event cameras. The ESI approach reconstructs intensity images by performing a single integration of the event streams combined with an enhanced decay algorithm. Such a method enables real-time intensity reconstruction at a high frame rate, typically 100 FPS. Furthermore, the relatively low computation load of ESI fits onboard implementation suitably, such as in UAV-based visual tracking scenarios. Extensive experiments have been conducted to evaluate the performance comparison of ESI and state-of-the-art algorithms. Compared to state-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency improvements, superior reconstruction quality, and a high frame rate. As a result, ESI enhances UAV onboard perception significantly under visual adversary surroundings. In-flight tests, ESI demonstrates effective performance for UAV onboard visual tracking under extremely low illumination conditions(2-10lux), whereas other comparative algorithms fail due to insufficient frame rate, poor image quality, or limited real-time performance.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2507.17793.pdf' target='_blank'>https://arxiv.org/pdf/2507.17793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joel Brogan, Matthew Yohe, David Cornett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17793">CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>What if you could piece together your own custom biometrics and AI analysis system, a bit like LEGO blocks? We aim to bring that technology to field operators in the field who require flexible, high-performance edge AI system that can be adapted on a moment's notice. This paper introduces CHAMP (Configurable Hot-swappable Architecture for Machine Perception), a modular edge computing platform that allows operators to dynamically swap in specialized AI "capability cartridges" for tasks like face recognition, object tracking, and document analysis. CHAMP leverages low-power FPGA-based accelerators on a high-throughput bus, orchestrated by a custom operating system (VDiSK) to enable plug-and-play AI pipelines and cryptographically secured biometric datasets. In this paper we describe the CHAMP design, including its modular scaling with multiple accelerators and the VDiSK operating system for runtime reconfiguration, along with its cryptographic capabilities to keep data stored on modules safe and private. Experiments demonstrate near-linear throughput scaling from 1 to 5 neural compute accelerators, highlighting both the performance gains and saturation limits of the USB3-based bus. Finally, we discuss applications of CHAMP in field biometrics, surveillance, and disaster response, and outline future improvements in bus protocols, cartridge capabilities, and system software.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2506.09159.pdf' target='_blank'>https://arxiv.org/pdf/2506.09159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Calagna, Yenchia Yu, Paolo Giaccone, Carla Fabiana Chiasserini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09159">MOSE: A Novel Orchestration Framework for Stateful Microservice Migration at the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stateful migration has emerged as the dominant technology to support microservice mobility at the network edge while ensuring a satisfying experience to mobile end users. This work addresses two pivotal challenges, namely, the implementation and the orchestration of the migration process. We first introduce a novel framework that efficiently implements stateful migration and effectively orchestrates the migration process by fulfilling both network and application KPI targets. Through experimental validation using realistic microservices, we then show that our solution (i) greatly improves migration performance, yielding up to 77% decrease of the migration downtime with respect to the state of the art, and (ii) successfully addresses the strict user QoE requirements of critical scenarios featuring latency-sensitive microservices. Further, we consider two practical use cases, featuring, respectively, a UAV autopilot microservice and a multi-object tracking task, and demonstrate how our framework outperforms current state-of-the-art approaches in configuring the migration process and in meeting KPI targets.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2505.18727.pdf' target='_blank'>https://arxiv.org/pdf/2505.18727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohe Li, Pengfei Li, Zide Fan, Ying Geng, Fangli Mou, Haohua Wu, Yunping Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18727">FusionTrack: End-to-End Multi-Object Tracking in Arbitrary Multi-View Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view multi-object tracking (MVMOT) has found widespread applications in intelligent transportation, surveillance systems, and urban management. However, existing studies rarely address genuinely free-viewpoint MVMOT systems, which could significantly enhance the flexibility and scalability of cooperative tracking systems. To bridge this gap, we first construct the Multi-Drone Multi-Object Tracking (MDMOT) dataset, captured by mobile drone swarms across diverse real-world scenarios, initially establishing the first benchmark for multi-object tracking in arbitrary multi-view environment. Building upon this foundation, we propose \textbf{FusionTrack}, an end-to-end framework that reasonably integrates tracking and re-identification to leverage multi-view information for robust trajectory association. Extensive experiments on our MDMOT and other benchmark datasets demonstrate that FusionTrack achieves state-of-the-art performance in both single-view and multi-view tracking.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2505.08126.pdf' target='_blank'>https://arxiv.org/pdf/2505.08126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angus Apps, Ziwei Wang, Vladimir Perejogin, Timothy Molloy, Robert Mahony
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08126">Asynchronous Multi-Object Tracking with an Event Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Events cameras are ideal sensors for enabling robots to detect and track objects in highly dynamic environments due to their low latency output, high temporal resolution, and high dynamic range. In this paper, we present the Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and tracking multiple objects by processing individual raw events asynchronously. AEMOT detects salient event blob features by identifying regions of consistent optical flow using a novel Field of Active Flow Directions built from the Surface of Active Events. Detected features are tracked as candidate objects using the recently proposed Asynchronous Event Blob (AEB) tracker in order to construct small intensity patches of each candidate object. A novel learnt validation stage promotes or discards candidate objects based on classification of their intensity patches, with promoted objects having their position, velocity, size, and orientation estimated at their event rate. We evaluate AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with precision and recall performance exceeding that of alternative event-based detection and tracking algorithms by over 37%. Source code and the labelled event Bee Swarm Dataset will be open sourced
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2505.07110.pdf' target='_blank'>https://arxiv.org/pdf/2505.07110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhang, Fenghua Shao, Runsheng Zhang, Yifan Zhuang, Liuqingqing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07110">DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Based on the DeepSORT algorithm, this study explores the application of visual tracking technology in intelligent human-computer interaction, especially in the field of gesture recognition and tracking. With the rapid development of artificial intelligence and deep learning technology, visual-based interaction has gradually replaced traditional input devices and become an important way for intelligent systems to interact with users. The DeepSORT algorithm can achieve accurate target tracking in dynamic environments by combining Kalman filters and deep learning feature extraction methods. It is especially suitable for complex scenes with multi-target tracking and fast movements. This study experimentally verifies the superior performance of DeepSORT in gesture recognition and tracking. It can accurately capture and track the user's gesture trajectory and is superior to traditional tracking methods in terms of real-time and accuracy. In addition, this study also combines gesture recognition experiments to evaluate the recognition ability and feedback response of the DeepSORT algorithm under different gestures (such as sliding, clicking, and zooming). The experimental results show that DeepSORT can not only effectively deal with target occlusion and motion blur but also can stably track in a multi-target environment, achieving a smooth user interaction experience. Finally, this paper looks forward to the future development direction of intelligent human-computer interaction systems based on visual tracking and proposes future research focuses such as algorithm optimization, data fusion, and multimodal interaction in order to promote a more intelligent and personalized interactive experience. Keywords-DeepSORT, visual tracking, gesture recognition, human-computer interaction
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2504.18708.pdf' target='_blank'>https://arxiv.org/pdf/2504.18708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longfei Han, Klaus KefferpÃ¼tz, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18708">Decentralized Fusion of 3D Extended Object Tracking based on a B-Spline Shape Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended Object Tracking (EOT) exploits the high resolution of modern sensors for detailed environmental perception. Combined with decentralized fusion, it contributes to a more scalable and robust perception system. This paper investigates the decentralized fusion of 3D EOT using a B-spline curve based model. The spline curve is used to represent the side-view profile, which is then extruded with a width to form a 3D shape. We use covariance intersection (CI) for the decentralized fusion and discuss the challenge of applying it to EOT. We further evaluate the tracking result of the decentralized fusion with simulated and real datasets of traffic scenarios. We show that the CI-based fusion can significantly improve the tracking performance for sensors with unfavorable perspective.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2504.03258.pdf' target='_blank'>https://arxiv.org/pdf/2504.03258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuxiao Ding, Yutong Yang, Julian Wiederer, Markus Braun, Peizheng Li, Juergen Gall, Bin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03258">TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Query denoising has become a standard training strategy for DETR-based detectors by addressing the slow convergence issue. Besides that, query denoising can be used to increase the diversity of training samples for modeling complex scenarios which is critical for Multi-Object Tracking (MOT), showing its potential in MOT application. Existing approaches integrate query denoising within the tracking-by-attention paradigm. However, as the denoising process only happens within the single frame, it cannot benefit the tracker to learn temporal-related information. In addition, the attention mask in query denoising prevents information exchange between denoising and object queries, limiting its potential in improving association using self-attention. To address these issues, we propose TQD-Track, which introduces Temporal Query Denoising (TQD) tailored for MOT, enabling denoising queries to carry temporal information and instance-specific feature representation. We introduce diverse noise types onto denoising queries that simulate real-world challenges in MOT. We analyze our proposed TQD for different tracking paradigms, and find out the paradigm with explicit learned data association module, e.g. tracking-by-detection or alternating detection and association, benefit from TQD by a larger margin. For these paradigms, we further design an association mask in the association module to ensure the consistent interaction between track and detection queries as during inference. Extensive experiments on the nuScenes dataset demonstrate that our approach consistently enhances different tracking methods by only changing the training process, especially the paradigms with explicit association module.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2503.10730.pdf' target='_blank'>https://arxiv.org/pdf/2503.10730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longfei Han, Klaus KefferpÃ¼tz, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10730">3D Extended Object Tracking based on Extruded B-Spline Side View Profiles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is an essential task for autonomous systems. With the advancement of 3D sensors, these systems can better perceive their surroundings using effective 3D Extended Object Tracking (EOT) methods. Based on the observation that common road users are symmetrical on the right and left sides in the traveling direction, we focus on the side view profile of the object. In order to leverage of the development in 2D EOT and balance the number of parameters of a shape model in the tracking algorithms, we propose a method for 3D extended object tracking (EOT) by describing the side view profile of the object with B-spline curves and forming an extrusion to obtain a 3D extent. The use of B-spline curves exploits their flexible representation power by allowing the control points to move freely. The algorithm is developed into an Extended Kalman Filter (EKF). For a through evaluation of this method, we use simulated traffic scenario of different vehicle models and realworld open dataset containing both radar and lidar data.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2503.09807.pdf' target='_blank'>https://arxiv.org/pdf/2503.09807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingwu Liu, Nicolas Saunier, Guillaume-Alexandre Bilodeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09807">How good are deep learning methods for automated road safety analysis using video data? An experimental study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based multi-object detection (MOD) and multi-object tracking (MOT) are advancing at a fast pace. A variety of 2D and 3D MOD and MOT methods have been developed for monocular and stereo cameras. Road safety analysis can benefit from those advancements. As crashes are rare events, surrogate measures of safety (SMoS) have been developed for safety analyses. (Semi-)Automated safety analysis methods extract road user trajectories to compute safety indicators, for example, Time-to-Collision (TTC) and Post-encroachment Time (PET). Inspired by the success of deep learning in MOD and MOT, we investigate three MOT methods, including one based on a stereo-camera, using the annotated KITTI traffic video dataset. Two post-processing steps, IDsplit and SS, are developed to improve the tracking results and investigate the factors influencing the TTC. The experimental results show that, despite some advantages in terms of the numbers of interactions or similarity to the TTC distributions, all the tested methods systematically over-estimate the number of interactions and under-estimate the TTC: they report more interactions and more severe interactions, making the road user interactions appear less safe than they are. Further efforts will be directed towards testing more methods and more data, in particular from roadside sensors, to verify the results and improve the performance.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2502.13875.pdf' target='_blank'>https://arxiv.org/pdf/2502.13875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huu-Thien Tran, Phuoc-Sang Pham, Thai-Son Tran, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13875">MEX: Memory-efficient Approach to Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) is a relatively new concept that has rapidly gained traction as a promising research direction at the intersection of computer vision and natural language processing. Unlike traditional multi-object tracking, RMOT identifies and tracks objects and incorporates textual descriptions for object class names, making the approach more intuitive. Various techniques have been proposed to address this challenging problem; however, most require the training of the entire network due to their end-to-end nature. Among these methods, iKUN has emerged as a particularly promising solution. Therefore, we further explore its pipeline and enhance its performance. In this paper, we introduce a practical module dubbed Memory-Efficient Cross-modality -- MEX. This memory-efficient technique can be directly applied to off-the-shelf trackers like iKUN, resulting in significant architectural improvements. Our method proves effective during inference on a single GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI dataset, which offers diverse autonomous driving scenes with relevant language expressions, is particularly useful for studying this problem. Empirically, our method demonstrates effectiveness and efficiency regarding HOTA tracking scores, substantially improving memory allocation and processing speed.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2501.13710.pdf' target='_blank'>https://arxiv.org/pdf/2501.13710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>IÃ±aki Erregue, Kamal Nasrollahi, Sergio Escalera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13710">YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised Re-ID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT) solution that combines real-time object detection with self-supervised Re-Identification (Re-ID). By incorporating a dedicated Re-ID branch into YOLO11s, our model performs Joint Detection and Embedding (JDE), generating appearance features for each detection. The Re-ID branch is trained in a fully self-supervised setting while simultaneously training for detection, eliminating the need for costly identity-labeled datasets. The triplet loss, with hard positive and semi-hard negative mining strategies, is used for learning discriminative embeddings. Data association is enhanced with a custom tracking implementation that successfully integrates motion, appearance, and location cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20 benchmarks, surpassing existing JDE methods in terms of FPS and using up to ten times fewer parameters. Thus, making our method a highly attractive solution for real-world applications.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2412.13273.pdf' target='_blank'>https://arxiv.org/pdf/2412.13273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Znobishchev, Valerii Filev, Oleg Kudashev, Nikita Orlov, Humphrey Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13273">CompactFlowNet: Efficient Real-time Optical Flow Estimation on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CompactFlowNet, the first real-time mobile neural network for optical flow prediction, which involves determining the displacement of each pixel in an initial frame relative to the corresponding pixel in a subsequent frame. Optical flow serves as a fundamental building block for various video-related tasks, such as video restoration, motion estimation, video stabilization, object tracking, action recognition, and video generation. While current state-of-the-art methods prioritize accuracy, they often overlook constraints regarding speed and memory usage. Existing light models typically focus on reducing size but still exhibit high latency, compromise significantly on quality, or are optimized for high-performance GPUs, resulting in sub-optimal performance on mobile devices. This study aims to develop a mobile-optimized optical flow model by proposing a novel mobile device-compatible architecture, as well as enhancements to the training pipeline, which optimize the model for reduced weight, low memory utilization, and increased speed while maintaining minimal error. Our approach demonstrates superior or comparable performance to the state-of-the-art lightweight models on the challenging KITTI and Sintel benchmarks. Furthermore, it attains a significantly accelerated inference speed, thereby yielding real-time operational efficiency on the iPhone 8, while surpassing real-time performance levels on more advanced mobile devices.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2411.09020.pdf' target='_blank'>https://arxiv.org/pdf/2411.09020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirvan Dutta, Etienne Burdet, Mohsen Kaboli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09020">Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive exploration of the unknown physical properties of objects such as stiffness, mass, center of mass, friction coefficient, and shape is crucial for autonomous robotic systems operating continuously in unstructured environments. Precise identification of these properties is essential to manipulate objects in a stable and controlled way, and is also required to anticipate the outcomes of (prehensile or non-prehensile) manipulation actions such as pushing, pulling, lifting, etc. Our study focuses on autonomously inferring the physical properties of a diverse set of various homogeneous, heterogeneous, and articulated objects utilizing a robotic system equipped with vision and tactile sensors. We propose a novel predictive perception framework for identifying object properties of the diverse objects by leveraging versatile exploratory actions: non-prehensile pushing and prehensile pulling. As part of the framework, we propose a novel active shape perception to seamlessly initiate exploration. Our innovative dual differentiable filtering with Graph Neural Networks learns the object-robot interaction and performs consistent inference of indirectly observable time-invariant object properties. In addition, we formulate a $N$-step information gain approach to actively select the most informative actions for efficient learning and inference. Extensive real-robot experiments with planar objects show that our predictive perception framework results in better performance than the state-of-the-art baseline and demonstrate our framework in three major applications for i) object tracking, ii) goal-driven task, and iii) change in environment detection.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2411.08335.pdf' target='_blank'>https://arxiv.org/pdf/2411.08335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muttahirul Islam, Nazmul Haque, Md. Hadiuzzaman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08335">DEEGITS: Deep Learning based Framework for Measuring Heterogenous Traffic State in Challenging Traffic Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents DEEGITS (Deep Learning Based Heterogeneous Traffic State Measurement), a comprehensive framework that leverages state-of-the-art convolutional neural network (CNN) techniques to accurately and rapidly detect vehicles and pedestrians, as well as to measure traffic states in challenging scenarios (i.e., congestion, occlusion). In this study, we enhance the training dataset through data fusion, enabling simultaneous detection of vehicles and pedestrians. Image preprocessing and augmentation are subsequently performed to improve the quality and quantity of the dataset. Transfer learning is applied on the YOLOv8 pretrained model to increase the model's capability to identify a diverse array of vehicles. Optimal hyperparameters are obtained using the Grid Search algorithm, with the Stochastic Gradient Descent (SGD) optimizer outperforming other optimizers under these settings. Extensive experimentation and evaluation demonstrate substantial accuracy within the detection framework, with the model achieving 0.794 mAP@0.5 on the validation set and 0.786 mAP@0.5 on the test set, surpassing previous benchmarks on similar datasets. The DeepSORT multi-object tracking algorithm is incorporated to track detected vehicles and pedestrians in this study. Finally, the framework is tested to measure heterogeneous traffic states in mixed traffic conditions. Two locations with differing traffic compositions and congestion levels are selected: one motorized-dominant location with moderate density and one non-motorized-dominant location with higher density. Errors are statistically insignificant for both cases, showing correlations from 0.99 to 0.88 and 0.91 to 0.97 for heterogeneous traffic flow and speed measurements, respectively.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2411.03702.pdf' target='_blank'>https://arxiv.org/pdf/2411.03702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Depanshu Sani, Saket Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03702">Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for robust scene understanding in mobile robotics and autonomous driving has highlighted the importance of integrating multiple sensing modalities. By combining data from diverse sensors like cameras and LIDARs, fusion techniques can overcome the limitations of individual sensors, enabling a more complete and accurate perception of the environment. We introduce a novel approach to multi-modal sensor fusion, focusing on developing a graph-based state representation that supports critical decision-making processes in autonomous driving. We present a Sensor-Agnostic Graph-Aware Kalman Filter [3], the first online state estimation technique designed to fuse multi-modal graphs derived from noisy multi-sensor data. The estimated graph-based state representations serve as a foundation for advanced applications like Multi-Object Tracking (MOT), offering a comprehensive framework for enhancing the situational awareness and safety of autonomous systems. We validate the effectiveness of our proposed framework through extensive experiments conducted on both synthetic and real-world driving datasets (nuScenes). Our results showcase an improvement in MOTA and a reduction in estimated position errors (MOTP) and identity switches (IDS) for tracked objects using the SAGA-KF. Furthermore, we highlight the capability of such a framework to develop methods that can leverage heterogeneous information (like semantic objects and geometric structures) from various sensing modalities, enabling a more holistic approach to scene understanding and enhancing the safety and effectiveness of autonomous systems.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2410.15428.pdf' target='_blank'>https://arxiv.org/pdf/2410.15428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chung Shue Chen, Wing Shing Wong, Yuan-Hsun Lo, Tsai-Lien Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15428">Multiset Combinatorial Gray Codes with Application to Proximity Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate coding schemes that map source symbols into multisets of an alphabet set. Such a formulation of source coding is an alternative approach to the traditional framework and is inspired by an object tracking problem over proximity sensor networks. We define a \textit{multiset combinatorial Gray code} as a mulitset code with fixed multiset cardinality that possesses combinatorial Gray code characteristic. For source codes that are organized as a grid, namely an integer lattice, we propose a solution by first constructing a mapping from the grid to the alphabet set, the codes are then defined as the images of rectangular blocks in the grid of fixed dimensions. We refer to the mapping as a \textit{color mapping} and the code as a \textit{color multiset code}. We propose the idea of product multiset code that enables us to construct codes for high dimensional grids based on 1-dimensional (1D) grids. We provide a detailed analysis of color multiset codes on 1D grids, focusing on codes that require the minimal number of colors. To illustrate the application of such a coding scheme, we consider an object tracking problem on 2D grids and show its efficiency, which comes from exploiting transmission parallelism. Some numerical results are presented to conclude the paper.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2409.07904.pdf' target='_blank'>https://arxiv.org/pdf/2409.07904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongzihan Song, Zhenyu Weng, Huiping Zhuang, Jinchang Ren, Yongming Chen, Zhiping Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07904">FACT: Feature Adaptive Continual-learning Tracker for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) involves identifying multiple targets and assigning them corresponding IDs within a video sequence, where occlusions are often encountered. Recent methods address occlusions using appearance cues through online learning techniques to improve adaptivity or offline learning techniques to utilize temporal information from videos. However, most existing online learning-based MOT methods are unable to learn from all past tracking information to improve adaptivity on long-term occlusions while maintaining real-time tracking speed. On the other hand, temporal information-based offline learning methods maintain a long-term memory to store past tracking information, but this approach restricts them to use only local past information during tracking. To address these challenges, we propose a new MOT framework called the Feature Adaptive Continual-learning Tracker (FACT), which enables real-time tracking and feature learning for targets by utilizing all past tracking information. We demonstrate that the framework can be integrated with various state-of-the-art feature-based trackers, thereby improving their tracking ability. Specifically, we develop the feature adaptive continual-learning (FAC) module, a neural network that can be trained online to learn features adaptively using all past tracking information during tracking. Moreover, we also introduce a two-stage association module specifically designed for the proposed continual learning-based tracking. Extensive experiment results demonstrate that the proposed method achieves state-of-the-art online tracking performance on MOT17 and MOT20 benchmarks. The code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2409.00339.pdf' target='_blank'>https://arxiv.org/pdf/2409.00339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Makoto M. Itoh, Qingrui Hu, Takayuki Niizato, Hiroaki Kawashima, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00339">Fish Tracking Challenge 2024: A Multi-Object Tracking Competition with Sweetfish Schooling Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of collective animal behavior, especially in aquatic environments, presents unique challenges and opportunities for understanding movement and interaction patterns in the field of ethology, ecology, and bio-navigation. The Fish Tracking Challenge 2024 (https://ftc-2024.github.io/) introduces a multi-object tracking competition focused on the intricate behaviors of schooling sweetfish. Using the SweetFish dataset, participants are tasked with developing advanced tracking models to accurately monitor the locations of 10 sweetfishes simultaneously. This paper introduces the competition's background, objectives, the SweetFish dataset, and the appraoches of the 1st to 3rd winners and our baseline. By leveraging video data and bounding box annotations, the competition aims to foster innovation in automatic detection and tracking algorithms, addressing the complexities of aquatic animal movements. The challenge provides the importance of multi-object tracking for discovering the dynamics of collective animal behavior, with the potential to significantly advance scientific understanding in the above fields.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2407.05518.pdf' target='_blank'>https://arxiv.org/pdf/2407.05518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Athena Psalta, Vasileios Tsironis, Andreas El Saer, Konstantinos Karantzalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05518">Addressing single object tracking in satellite imagery through prompt-engineered solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking in satellite videos remains a complex endeavor in remote sensing due to the intricate and dynamic nature of satellite imagery. Existing state-of-the-art trackers in computer vision integrate sophisticated architectures, attention mechanisms, and multi-modal fusion to enhance tracking accuracy across diverse environments. However, the challenges posed by satellite imagery, such as background variations, atmospheric disturbances, and low-resolution object delineation, significantly impede the precision and reliability of traditional Single Object Tracking (SOT) techniques. Our study delves into these challenges and proposes prompt engineering methodologies, leveraging the Segment Anything Model (SAM) and TAPIR (Tracking Any Point with per-frame Initialization and temporal Refinement), to create a training-free point-based tracking method for small-scale objects on satellite videos. Experiments on the VISO dataset validate our strategy, marking a significant advancement in robust tracking solutions tailored for satellite imagery in remote sensing applications.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2407.03084.pdf' target='_blank'>https://arxiv.org/pdf/2407.03084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longfei Han, Qiuyu Xu, Klaus KefferpÃ¼tz, Gordon Elger, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03084">Applying Extended Object Tracking for Self-Localization of Roadside Radar Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent Transportation Systems (ITS) can benefit from roadside 4D mmWave radar sensors for large-scale traffic monitoring due to their weatherproof functionality, long sensing range and low manufacturing cost. However, the localization method using external measurement devices has limitations in urban environments. Furthermore, if the sensor mount exhibits changes due to environmental influences, they cannot be corrected when the measurement is performed only during the installation. In this paper, we propose self-localization of roadside radar data using Extended Object Tracking (EOT). The method analyses both the tracked trajectories of the vehicles observed by the sensor and the aerial laser scan of city streets, assigns labels of driving behaviors such as "straight ahead", "left turn", "right turn" to trajectory sections and road segments, and performs Semantic Iterative Closest Points (SICP) algorithm to register the point cloud. The method exploits the result from a down stream task -- object tracking -- for localization. We demonstrate high accuracy in the sub-meter range along with very low orientation error. The method also shows good data efficiency. The evaluation is done in both simulation and real-world tests.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2406.20024.pdf' target='_blank'>https://arxiv.org/pdf/2406.20024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Chen, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.20024">eMoE-Tracker: Environmental MoE-based Transformer for Robust Event-guided Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The unique complementarity of frame-based and event cameras for high frame rate object tracking has recently inspired some research attempts to develop multi-modal fusion approaches. However, these methods directly fuse both modalities and thus ignore the environmental attributes, e.g., motion blur, illumination variance, occlusion, scale variation, etc. Meanwhile, insufficient interaction between search and template features makes distinguishing target objects and backgrounds difficult. As a result, performance degradation is induced especially in challenging conditions. This paper proposes a novel and effective Transformer-based event-guided tracking framework, called eMoE-Tracker, which achieves new SOTA performance under various conditions. Our key idea is to disentangle the environment into several learnable attributes to dynamically learn the attribute-specific features and strengthen the target information by improving the interaction between the target template and search regions. To achieve the goal, we first propose an environmental Mix-of-Experts (eMoE) module that is built upon the environmental Attributes Disentanglement to learn attribute-specific features and environmental Attributes Assembling to assemble the attribute-specific features by the learnable attribute scores dynamically. The eMoE module is a subtle router that prompt-tunes the transformer backbone more efficiently. We then introduce a contrastive relation modeling (CRM) module to emphasize target information by leveraging a contrastive learning strategy between the target template and search regions. Extensive experiments on diverse event-based benchmark datasets showcase the superior performance of our eMoE-Tracker compared to the prior arts.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2403.08018.pdf' target='_blank'>https://arxiv.org/pdf/2403.08018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Miah, Guillaume-Alexandre Bilodeau, Nicolas Saunier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08018">Learning Data Association for Multi-Object Tracking using Only Coordinates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel Transformer-based module to address the data association problem for multi-object tracking. From detections obtained by a pretrained detector, this module uses only coordinates from bounding boxes to estimate an affinity score between pairs of tracks extracted from two distinct temporal windows. This module, named TWiX, is trained on sets of tracks with the objective of discriminating pairs of tracks coming from the same object from those which are not. Our module does not use the intersection over union measure, nor does it requires any motion priors or any camera motion compensation technique. By inserting TWiX within an online cascade matching pipeline, our tracker C-TWiX achieves state-of-the-art performance on the DanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17 dataset. The code will be made available upon publication.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2403.02075.pdf' target='_blank'>https://arxiv.org/pdf/2403.02075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02075">DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes. Tacking-by-detection (TBD) trackers with Kalman Filter motion prediction work well in pedestrian-dominant scenarios but fall short in complex situations when multiple objects perform non-linear and diverse motion simultaneously. To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT. Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D$^2$MP). It models the entire distribution of various motion presented by the data as a whole. It also predicts an individual object's motion conditioning on an individual's historical motion information. Furthermore, it optimizes the diffusion process with much fewer sampling steps. As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with $62.3\%$ and $76.2\%$ in HOTA metrics, respectively. To the best of our knowledge, DiffMOT is the first to introduce a diffusion probabilistic model into the MOT to tackle non-linear motion prediction.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2512.14998.pdf' target='_blank'>https://arxiv.org/pdf/2512.14998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sibi Parivendan, Kashfia Sailunaz, Suresh Neethirajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14998">Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2511.19936.pdf' target='_blank'>https://arxiv.org/pdf/2511.19936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngseo Kim, Dohyun Kim, Geonhee Han, Paul Hongsuck Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19936">Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image diffusion models, though originally developed for image generation, implicitly capture rich semantic structures that enable various recognition and localization tasks beyond synthesis. In this work, we investigate their self-attention maps can be reinterpreted as semantic label propagation kernels, providing robust pixel-level correspondences between relevant image regions. Extending this mechanism across frames yields a temporal propagation kernel that enables zero-shot object tracking via segmentation in videos. We further demonstrate the effectiveness of test-time optimization strategies-DDIM inversion, textual inversion, and adaptive head weighting-in adapting diffusion features for robust and consistent label propagation. Building on these findings, we introduce DRIFT, a framework for object tracking in videos leveraging a pretrained image diffusion model with SAM-guided mask refinement, achieving state-of-the-art zero-shot performance on standard video object segmentation benchmarks.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2511.13904.pdf' target='_blank'>https://arxiv.org/pdf/2511.13904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqiang Lin, Sam Lockyer, Florian Stanek, Markus Zarbock, Adrian Evans, Wenbin Li, Nic Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13904">SAE-MCVT: A Real-Time and Scalable Multi-Camera Vehicle Tracking Framework Powered by Edge Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In modern Intelligent Transportation Systems (ITS), cameras are a key component due to their ability to provide valuable information for multiple stakeholders. A central task is Multi-Camera Vehicle Tracking (MCVT), which generates vehicle trajectories and enables applications such as anomaly detection, traffic density estimation, and suspect vehicle tracking. However, most existing studies on MCVT emphasize accuracy while overlooking real-time performance and scalability. These two aspects are essential for real-world deployment and become increasingly challenging in city-scale applications as the number of cameras grows. To address this issue, we propose SAE-MCVT, the first scalable real-time MCVT framework. The system includes several edge devices that interact with one central workstation separately. On the edge side, live RTSP video streams are serialized and processed through modules including object detection, object tracking, geo-mapping, and feature extraction. Only lightweight metadata -- vehicle locations and deep appearance features -- are transmitted to the central workstation. On the central side, cross-camera association is calculated under the constraint of spatial-temporal relations between adjacent cameras, which are learned through a self-supervised camera link model. Experiments on the RoundaboutHD dataset show that SAE-MCVT maintains real-time operation on 2K 15 FPS video streams and achieves an IDF1 score of 61.2. To the best of our knowledge, this is the first scalable real-time MCVT framework suitable for city-scale deployment.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2507.16639.pdf' target='_blank'>https://arxiv.org/pdf/2507.16639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Henrich, Christian Post, Maximilian Zilke, Parth Shiroya, Emma Chanut, Amir Mollazadeh Yamchi, Ramin Yahyapour, Thomas Kneib, Imke Traulsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16639">Benchmarking pig detection and tracking under diverse and challenging conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To ensure animal welfare and effective management in pig farming, monitoring individual behavior is a crucial prerequisite. While monitoring tasks have traditionally been carried out manually, advances in machine learning have made it possible to collect individualized information in an increasingly automated way. Central to these methods is the localization of animals across space (object detection) and time (multi-object tracking). Despite extensive research of these two tasks in pig farming, a systematic benchmarking study has not yet been conducted. In this work, we address this gap by curating two datasets: PigDetect for object detection and PigTrack for multi-object tracking. The datasets are based on diverse image and video material from realistic barn conditions, and include challenging scenarios such as occlusions or bad visibility. For object detection, we show that challenging training images improve detection performance beyond what is achievable with randomly sampled images alone. Comparing different approaches, we found that state-of-the-art models offer substantial improvements in detection quality over real-time alternatives. For multi-object tracking, we observed that SORT-based methods achieve superior detection performance compared to end-to-end trainable models. However, end-to-end models show better association performance, suggesting they could become strong alternatives in the future. We also investigate characteristic failure cases of end-to-end models, providing guidance for future improvements. The detection and tracking models trained on our datasets perform well in unseen pens, suggesting good generalization capabilities. This highlights the importance of high-quality training data. The datasets and research code are made publicly available to facilitate reproducibility, re-use and further development.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2506.19341.pdf' target='_blank'>https://arxiv.org/pdf/2506.19341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongping Dong, Liming Chen, Mohand Tahar Kechadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19341">Trajectory Prediction in Dynamic Object Tracking: A Critical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study provides a detailed analysis of current advancements in dynamic object tracking (DOT) and trajectory prediction (TP) methodologies, including their applications and challenges. It covers various approaches, such as feature-based, segmentation-based, estimation-based, and learning-based methods, evaluating their effectiveness, deployment, and limitations in real-world scenarios. The study highlights the significant impact of these technologies in automotive and autonomous vehicles, surveillance and security, healthcare, and industrial automation, contributing to safety and efficiency. Despite the progress, challenges such as improved generalization, computational efficiency, reduced data dependency, and ethical considerations still exist. The study suggests future research directions to address these challenges, emphasizing the importance of multimodal data integration, semantic information fusion, and developing context-aware systems, along with ethical and privacy-preserving frameworks.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2505.18795.pdf' target='_blank'>https://arxiv.org/pdf/2505.18795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Li, Runze Gan, James R. Hopgood, Michael E. Davies, Simon J. Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18795">Distributed Expectation Propagation for Multi-Object Tracking over Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel distributed expectation propagation algorithm for multiple sensors, multiple objects tracking in cluttered environments. The proposed framework enables each sensor to operate locally while collaboratively exchanging moment estimates with other sensors, thus eliminating the need to transmit all data to a central processing node. Specifically, we introduce a fast and parallelisable Rao-Blackwellised Gibbs sampling scheme to approximate the tilted distributions, which enhances the accuracy and efficiency of expectation propagation updates. Results demonstrate that the proposed algorithm improves both communication and inference efficiency for multi-object tracking tasks with dynamic sensor connectivity and varying clutter levels.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2505.16029.pdf' target='_blank'>https://arxiv.org/pdf/2505.16029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shichao Li, Peiliang Li, Qing Lian, Peng Yun, Xiaozhi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16029">Learning better representations for crowded pedestrians in offboard LiDAR-camera 3D tracking-by-detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving pedestrians in highly crowded urban environments is a difficult long-tail problem for learning-based autonomous perception. Speeding up 3D ground truth generation for such challenging scenes is performance-critical yet very challenging. The difficulties include the sparsity of the captured pedestrian point cloud and a lack of suitable benchmarks for a specific system design study. To tackle the challenges, we first collect a new multi-view LiDAR-camera 3D multiple-object-tracking benchmark of highly crowded pedestrians for in-depth analysis. We then build an offboard auto-labeling system that reconstructs pedestrian trajectories from LiDAR point cloud and multi-view images. To improve the generalization power for crowded scenes and the performance for small objects, we propose to learn high-resolution representations that are density-aware and relationship-aware. Extensive experiments validate that our approach significantly improves the 3D pedestrian tracking performance towards higher auto-labeling efficiency. The code will be publicly available at this HTTP URL.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2504.12506.pdf' target='_blank'>https://arxiv.org/pdf/2504.12506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Nan Fernandez-Ayala, Jorge Silva, Meng Guo, Dimos V. Dimarogonas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12506">Robust Visual Servoing under Human Supervision for Assembly Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a framework enabling mobile manipulators to reliably complete pick-and-place tasks for assembling structures from construction blocks. The picking uses an eye-in-hand visual servoing controller for object tracking with Control Barrier Functions (CBFs) to ensure fiducial markers in the blocks remain visible. An additional robot with an eye-to-hand setup ensures precise placement, critical for structural stability. We integrate human-in-the-loop capabilities for flexibility and fault correction and analyze robustness to camera pose errors, proposing adapted barrier functions to handle them. Lastly, experiments validate the framework on 6-DoF mobile arms.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2504.09328.pdf' target='_blank'>https://arxiv.org/pdf/2504.09328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sonia Laguna, Alberto Garcia-Garcia, Marie-Julie Rakotosaona, Stylianos Moschoglou, Leonhard Helminger, Sergio Orts-Escolano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09328">Text To 3D Object Generation For Scalable Room Assembly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2504.07163.pdf' target='_blank'>https://arxiv.org/pdf/2504.07163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordi Serra, Anton Aguilar, Ebrahim Abu-Helalah, RaÃºl Parada, Paolo Dini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07163">Multi-Object Tracking for Collision Avoidance Using Multiple Cameras in Open RAN Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper deals with the multi-object detection and tracking problem, within the scope of open Radio Access Network (RAN), for collision avoidance in vehicular scenarios. To this end, a set of distributed intelligent agents collocated with cameras are considered. The fusion of detected objects is done at an edge service, considering Open RAN connectivity. Then, the edge service predicts the objects trajectories for collision avoidance. Compared to the related work a more realistic Open RAN network is implemented and multiple cameras are used.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2504.01457.pdf' target='_blank'>https://arxiv.org/pdf/2504.01457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Meng, Chunyun Fu, Xiangyan Yan, Zheng Liang, Pan Ji, Jianwen Wang, Tao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01457">Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking plays a crucial role in various applications, such as autonomous driving and security surveillance. This study introduces Deep LG-Track, a novel multi-object tracker that incorporates three key enhancements to improve the tracking accuracy and robustness. First, an adaptive Kalman filter is developed to dynamically update the covariance of measurement noise based on detection confidence and trajectory disappearance. Second, a novel cost matrix is formulated to adaptively fuse motion and appearance information, leveraging localization confidence and detection confidence as weighting factors. Third, a dynamic appearance feature updating strategy is introduced, adjusting the relative weighting of historical and current appearance features based on appearance clarity and localization accuracy. Comprehensive evaluations on the MOT17 and MOT20 datasets demonstrate that the proposed Deep LG-Track consistently outperforms state-of-the-art trackers across multiple performance metrics, highlighting its effectiveness in multi-object tracking tasks.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2412.08313.pdf' target='_blank'>https://arxiv.org/pdf/2412.08313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gergely SzabÃ³, ZsÃ³fia MolnÃ¡r, AndrÃ¡s HorvÃ¡th
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08313">Post-Hoc MOTS: Exploring the Capabilities of Time-Symmetric Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal forward-tracking has been the dominant approach for multi-object segmentation and tracking (MOTS). However, a novel time-symmetric tracking methodology has recently been introduced for the detection, segmentation, and tracking of budding yeast cells in pre-recorded samples. Although this architecture has demonstrated a unique perspective on stable and consistent tracking, as well as missed instance re-interpolation, its evaluation has so far been largely confined to settings related to videomicroscopic environments. In this work, we aim to reveal the broader capabilities, advantages, and potential challenges of this architecture across various specifically designed scenarios, including a pedestrian tracking dataset. We also conduct an ablation study comparing the model against its restricted variants and the widely used Kalman filter. Furthermore, we present an attention analysis of the tracking architecture for both pretrained and non-pretrained models
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2412.01041.pdf' target='_blank'>https://arxiv.org/pdf/2412.01041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Susu Fang, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01041">LiDAR SLAMMOT based on Confidence-guided Data Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of autonomous driving or robotics, simultaneous localization and mapping (SLAM) and multi-object tracking (MOT) are two fundamental problems and are generally applied separately. Solutions to SLAM and MOT usually rely on certain assumptions, such as the static environment assumption for SLAM and the accurate ego-vehicle pose assumption for MOT. But in complex dynamic environments, it is difficult or even impossible to meet these assumptions. Therefore, the SLAMMOT, i.e., simultaneous localization, mapping, and moving object tracking, integrated system of SLAM and object tracking, has emerged for autonomous vehicles in dynamic environments. However, many conventional SLAMMOT solutions directly perform data association on the predictions and detections for object tracking, but ignore their quality. In practice, inaccurate predictions caused by continuous multi-frame missed detections in temporary occlusion scenarios, may degrade the performance of tracking, thereby affecting SLAMMOT. To address this challenge, this paper presents a LiDAR SLAMMOT based on confidence-guided data association (Conf SLAMMOT) method, which tightly couples the LiDAR SLAM and the confidence-guided data association based multi-object tracking into a graph optimization backend for estimating the state of the ego-vehicle and objects simultaneously. The confidence of prediction and detection are applied in the factor graph-based multi-object tracking for its data association, which not only avoids the performance degradation caused by incorrect initial assignments in some filter-based methods but also handles issues such as continuous missed detection in tracking while also improving the overall performance of SLAMMOT. Various comparative experiments demonstrate the superior advantages of Conf SLAMMOT, especially in scenes with some missed detections.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2411.10072.pdf' target='_blank'>https://arxiv.org/pdf/2411.10072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ishrath Ahamed, Chamith Dilshan Ranathunga, Dinuka Sandun Udayantha, Benny Kai Kiat Ng, Chau Yuen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10072">Real-Time AI-Driven People Tracking and Counting Using Overhead Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate people counting in smart buildings and intelligent transportation systems is crucial for energy management, safety protocols, and resource allocation. This is especially critical during emergencies, where precise occupant counts are vital for safe evacuation. Existing methods struggle with large crowds, often losing accuracy with even a few additional people. To address this limitation, this study proposes a novel approach combining a new object tracking algorithm, a novel counting algorithm, and a fine-tuned object detection model. This method achieves 97% accuracy in real-time people counting with a frame rate of 20-27 FPS on a low-power edge computer.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2410.13240.pdf' target='_blank'>https://arxiv.org/pdf/2410.13240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanpeng Jia, Ting Wang, Xieyuanli Chen, Shiliang Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13240">TRLO: An Efficient LiDAR Odometry with 3D Dynamic Object Tracking and Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous state estimation and mapping is an essential capability for mobile robots working in dynamic urban environment. The majority of existing SLAM solutions heavily rely on a primarily static assumption. However, due to the presence of moving vehicles and pedestrians, this assumption does not always hold, leading to localization accuracy decreased and maps distorted. To address this challenge, we propose TRLO, a dynamic LiDAR odometry that efficiently improves the accuracy of state estimation and generates a cleaner point cloud map. To efficiently detect dynamic objects in the surrounding environment, a deep learning-based method is applied, generating detection bounding boxes. We then design a 3D multi-object tracker based on Unscented Kalman Filter (UKF) and nearest neighbor (NN) strategy to reliably identify and remove dynamic objects. Subsequently, a fast two-stage iterative nearest point solver is employed to solve the state estimation using cleaned static point cloud. Note that a novel hash-based keyframe database management is proposed for fast access to search keyframes. Furthermore, all the detected object bounding boxes are leveraged to impose posture consistency constraint to further refine the final state estimation. Extensive evaluations and ablation studies conducted on the KITTI and UrbanLoco datasets demonstrate that our approach not only achieves more accurate state estimation but also generates cleaner maps, compared with baselines.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2409.19891.pdf' target='_blank'>https://arxiv.org/pdf/2409.19891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Ishige, Yasuhiro Yoshimura, Ryo Yonetani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19891">Opt-in Camera: Person Identification in Video via UWB Localization and Its Application to Opt-in Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents opt-in camera, a concept of privacy-preserving camera systems capable of recording only specific individuals in a crowd who explicitly consent to be recorded. Our system utilizes a mobile wireless communication tag attached to personal belongings as proof of opt-in and as a means of localizing tag carriers in video footage. Specifically, the on-ground positions of the wireless tag are first tracked over time using the unscented Kalman filter (UKF). The tag trajectory is then matched against visual tracking results for pedestrians found in videos to identify the tag carrier. Technically, we devise a dedicated trajectory matching technique based on constrained linear optimization, as well as a novel calibration technique that handles wireless tag-camera calibration and hyperparameter tuning for the UKF, which mitigates the non-line-of-sight (NLoS) issue in wireless localization. We implemented the proposed opt-in camera system using ultra-wideband (UWB) devices and an off-the-shelf webcam. Experimental results demonstrate that our system can perform opt-in recording of individuals in real-time at 10 fps, with reliable identification accuracy in crowds of 8-23 people in a confined space.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2408.13689.pdf' target='_blank'>https://arxiv.org/pdf/2408.13689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Li, Runze Gan, Simon Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13689">Decentralised Variational Inference Frameworks for Multi-object Tracking on Sensor Networks: Additional Notes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tackles the challenge of multi-sensor multi-object tracking by proposing various decentralised Variational Inference (VI) schemes that match the tracking performance of centralised sensor fusion with only local message exchanges among neighboring sensors. We first establish a centralised VI sensor fusion scheme as a benchmark and analyse the limitations of its decentralised counterpart, which requires sensors to await consensus at each VI iteration. Therefore, we propose a decentralised gradient-based VI framework that optimises the Locally Maximised Evidence Lower Bound (LM-ELBO) instead of the standard ELBO, which reduces the parameter search space and enables faster convergence, making it particularly beneficial for decentralised tracking. This proposed framework is inherently self-evolving, improving with advancements in decentralised optimisation techniques for convergence guarantees and efficiency. Further, we enhance the convergence speed of proposed decentralised schemes using natural gradients and gradient tracking strategies. Results verify that our decentralised VI schemes are empirically equivalent to centralised fusion in tracking performance. Notably, the decentralised natural gradient VI method is the most communication-efficient, with communication costs comparable to suboptimal decentralised strategies while delivering notably higher tracking accuracy.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2408.09178.pdf' target='_blank'>https://arxiv.org/pdf/2408.09178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changcheng Xiao, Qiong Cao, Zhigang Luo, Long Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09178">MambaTrack: A Simple Baseline for Multiple Object Tracking with State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking by detection has been the prevailing paradigm in the field of Multi-object Tracking (MOT). These methods typically rely on the Kalman Filter to estimate the future locations of objects, assuming linear object motion. However, they fall short when tracking objects exhibiting nonlinear and diverse motion in scenarios like dancing and sports. In addition, there has been limited focus on utilizing learning-based motion predictors in MOT. To address these challenges, we resort to exploring data-driven motion prediction methods. Inspired by the great expectation of state space models (SSMs), such as Mamba, in long-term sequence modeling with near-linear complexity, we introduce a Mamba-based motion model named Mamba moTion Predictor (MTP). MTP is designed to model the complex motion patterns of objects like dancers and athletes. Specifically, MTP takes the spatial-temporal location dynamics of objects as input, captures the motion pattern using a bi-Mamba encoding layer, and predicts the next motion. In real-world scenarios, objects may be missed due to occlusion or motion blur, leading to premature termination of their trajectories. To tackle this challenge, we further expand the application of MTP. We employ it in an autoregressive way to compensate for missing observations by utilizing its own predictions as inputs, thereby contributing to more consistent trajectories. Our proposed tracker, MambaTrack, demonstrates advanced performance on benchmarks such as Dancetrack and SportsMOT, which are characterized by complex motion and severe occlusion.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2407.18288.pdf' target='_blank'>https://arxiv.org/pdf/2407.18288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niels G. Faber, Seyed Sahand Mohammadi Ziabari, Fatemeh Karimi Nejadasl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18288">Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements in certain scenarios, it does not consistently outperform the original FairMOT model. These findings highlight the potential and limitations of applying foundation models in knowledge
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2407.08817.pdf' target='_blank'>https://arxiv.org/pdf/2407.08817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ish Kumar Jain, Suriyaa MM, Dinesh Bharadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08817">CommRad: Context-Aware Sensing-Driven Millimeter-Wave Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Millimeter-wave (mmWave) technology is pivotal for next-generation wireless networks, enabling high-data-rate and low-latency applications such as autonomous vehicles and XR streaming. However, maintaining directional mmWave links in dynamic mobile environments is challenging due to mobility-induced disruptions and blockage. While effective, the current 5G NR beam training methods incur significant overhead and scalability issues in multi-user scenarios. To address this, we introduce CommRad, a sensing-driven solution incorporating a radar sensor at the base station to track mobile users and maintain directional beams even under blockages. While radar provides high-resolution object tracking, it suffers from a fundamental challenge of lack of context, i.e., it cannot discern which objects in the environment represent active users, reflectors, or blockers. To obtain this contextual awareness, CommRad unites wireless sensing capabilities of bi-static radio communication with the mono-static radar sensor, allowing radios to provide initial context to radar sensors. Subsequently, the radar aids in user tracking and sustains mobile links even in obstructed scenarios, resulting in robust and high-throughput directional connections for all mobile users at all times. We evaluate this collaborative radar-radio framework using a 28 GHz mmWave testbed integrated with a radar sensor in various indoor and outdoor scenarios, demonstrating a 2.5x improvement in median throughput compared to a non-collaborative baseline.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2407.06337.pdf' target='_blank'>https://arxiv.org/pdf/2407.06337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jon Crall, Connor Greenwell, David Joy, Matthew Leotta, Aashish Chaudhary, Anthony Hoogs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06337">GeoWATCH for Detecting Heavy Construction in Heterogeneous Time Series of Satellite Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from multiple sensors is challenging due to spatio-temporal misalignment and differences in resolution and captured spectra. To that end, we introduce GeoWATCH, a flexible framework for training models on long sequences of satellite images sourced from multiple sensor platforms, which is designed to handle image classification, activity recognition, object detection, or object tracking tasks. Our system includes a novel partial weight loading mechanism based on sub-graph isomorphism which allows for continually training and modifying a network over many training cycles. This has allowed us to train a lineage of models over a long period of time, which we have observed has improved performance as we adjust configurations while maintaining a core backbone.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2406.17183.pdf' target='_blank'>https://arxiv.org/pdf/2406.17183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Srebrnjak Yang, Dheeraj Khanna, John S. Zelek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17183">POPCat: Propagation of particles for complex annotation tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Novel dataset creation for all multi-object tracking, crowd-counting, and industrial-based videos is arduous and time-consuming when faced with a unique class that densely populates a video sequence. We propose a time efficient method called POPCat that exploits the multi-target and temporal features of video data to produce a semi-supervised pipeline for segmentation or box-based video annotation. The method retains the accuracy level associated with human level annotation while generating a large volume of semi-supervised annotations for greater generalization. The method capitalizes on temporal features through the use of a particle tracker to expand the domain of human-provided target points. This is done through the use of a particle tracker to reassociate the initial points to a set of images that follow the labeled frame. A YOLO model is then trained with this generated data, and then rapidly infers on the target video. Evaluations are conducted on GMOT-40, AnimalTrack, and Visdrone-2019 benchmarks. These multi-target video tracking/detection sets contain multiple similar-looking targets, camera movements, and other features that would commonly be seen in "wild" situations. We specifically choose these difficult datasets to demonstrate the efficacy of the pipeline and for comparison purposes. The method applied on GMOT-40, AnimalTrack, and Visdrone shows a margin of improvement on recall/mAP50/mAP over the best results by a value of 24.5%/9.6%/4.8%, -/43.1%/27.8%, and 7.5%/9.4%/7.5% where metrics were collected.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2406.14973.pdf' target='_blank'>https://arxiv.org/pdf/2406.14973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Yang, Jisheng Xu, Zhiliang Lin, Jianping He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14973">LU2Net: A Lightweight Network for Real-time Underwater Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer vision techniques have empowered underwater robots to effectively undertake a multitude of tasks, including object tracking and path planning. However, underwater optical factors like light refraction and absorption present challenges to underwater vision, which cause degradation of underwater images. A variety of underwater image enhancement methods have been proposed to improve the effectiveness of underwater vision perception. Nevertheless, for real-time vision tasks on underwater robots, it is necessary to overcome the challenges associated with algorithmic efficiency and real-time capabilities. In this paper, we introduce Lightweight Underwater Unet (LU2Net), a novel U-shape network designed specifically for real-time enhancement of underwater images. The proposed model incorporates axial depthwise convolution and the channel attention module, enabling it to significantly reduce computational demands and model parameters, thereby improving processing speed. The extensive experiments conducted on the dataset and real-world underwater robots demonstrate the exceptional performance and speed of proposed model. It is capable of providing well-enhanced underwater images at a speed 8 times faster than the current state-of-the-art underwater image enhancement method. Moreover, LU2Net is able to handle real-time underwater video enhancement.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2405.15755.pdf' target='_blank'>https://arxiv.org/pdf/2405.15755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Han, Nobuyuki Oishi, Yueying Tian, Elif Ucurum, Rupert Young, Chris Chatwin, Philip Birch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15755">ETTrack: Enhanced Temporal Motion Predictor for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many Multi-Object Tracking (MOT) approaches exploit motion information to associate all the detected objects across frames. However, many methods that rely on filtering-based algorithms, such as the Kalman Filter, often work well in linear motion scenarios but struggle to accurately predict the locations of objects undergoing complex and non-linear movements. To tackle these scenarios, we propose a motion-based MOT approach with an enhanced temporal motion predictor, ETTrack. Specifically, the motion predictor integrates a transformer model and a Temporal Convolutional Network (TCN) to capture short-term and long-term motion patterns, and it predicts the future motion of individual objects based on the historical motion information. Additionally, we propose a novel Momentum Correction Loss function that provides additional information regarding the motion direction of objects during training. This allows the motion predictor rapidly adapt to motion variations and more accurately predict future motion. Our experimental results demonstrate that ETTrack achieves a competitive performance compared with state-of-the-art trackers on DanceTrack and SportsMOT, scoring 56.4% and 74.4% in HOTA metrics, respectively.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2403.16834.pdf' target='_blank'>https://arxiv.org/pdf/2403.16834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Luo, Xiqing Guo, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16834">From Two-Stream to One-Stream: Efficient RGB-T Tracking via Mutual Prompt Learning and Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the complementary nature of visible light and thermal infrared modalities, object tracking based on the fusion of visible light images and thermal images (referred to as RGB-T tracking) has received increasing attention from researchers in recent years. How to achieve more comprehensive fusion of information from the two modalities at a lower cost has been an issue that researchers have been exploring. Inspired by visual prompt learning, we designed a novel two-stream RGB-T tracking architecture based on cross-modal mutual prompt learning, and used this model as a teacher to guide a one-stream student model for rapid learning through knowledge distillation techniques. Extensive experiments have shown that, compared to similar RGB-T trackers, our designed teacher model achieved the highest precision rate, while the student model, with comparable precision rate to the teacher model, realized an inference speed more than three times faster than the teacher model.(Codes will be available if accepted.)
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2402.17892.pdf' target='_blank'>https://arxiv.org/pdf/2402.17892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandro Papais, Robert Ren, Steven Waslander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17892">SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern robotic systems are required to operate in dense dynamic environments, requiring highly accurate real-time track identification and estimation. For 3D multi-object tracking, recent approaches process a single measurement frame recursively with greedy association and are prone to errors in ambiguous association decisions. Our method, Sliding Window Tracker (SWTrack), yields more accurate association and state estimation by batch processing many frames of sensor data while being capable of running online in real-time. The most probable track associations are identified by evaluating all possible track hypotheses across the temporal sliding window. A novel graph optimization approach is formulated to solve the multidimensional assignment problem with lifted graph edges introduced to account for missed detections and graph sparsity enforced to retain real-time efficiency. We evaluate our SWTrack implementation$^{2}$ on the NuScenes autonomous driving dataset to demonstrate improved tracking performance.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2401.02960.pdf' target='_blank'>https://arxiv.org/pdf/2401.02960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anton Jeran Ratnarajah, Sahani Goonetilleke, Dumindu Tissera, Kapilan Balagopalan, Ranga Rodrigo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02960">Forensic Video Analytic Software</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Law enforcement officials heavily depend on Forensic Video Analytic (FVA) Software in their evidence extraction process. However present-day FVA software are complex, time consuming, equipment dependent and expensive. Developing countries struggle to gain access to this gateway to a secure haven. The term forensic pertains the application of scientific methods to the investigation of crime through post-processing, whereas surveillance is the close monitoring of real-time feeds.
  The principle objective of this Final Year Project was to develop an efficient and effective FVA Software, addressing the shortcomings through a stringent and systematic review of scholarly research papers, online databases and legal documentation. The scope spans multiple object detection, multiple object tracking, anomaly detection, activity recognition, tampering detection, general and specific image enhancement and video synopsis.
  Methods employed include many machine learning techniques, GPU acceleration and efficient, integrated architecture development both for real-time and postprocessing. For this CNN, GMM, multithreading and OpenCV C++ coding were used. The implications of the proposed methodology would rapidly speed up the FVA process especially through the novel video synopsis research arena. This project has resulted in three research outcomes Moving Object Based Collision Free Video Synopsis, Forensic and Surveillance Analytic Tool Architecture and Tampering Detection Inter-Frame Forgery.
  The results include forensic and surveillance panel outcomes with emphasis on video synopsis and Sri Lankan context. Principal conclusions include the optimization and efficient algorithm integration to overcome limitations in processing power, memory and compromise between real-time performance and accuracy.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2512.01934.pdf' target='_blank'>https://arxiv.org/pdf/2512.01934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyi Wang, Yanmao Man, Raymond Muller, Ming Li, Z. Berkay Celik, Ryan Gerdes, Jonathan Petit
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01934">Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2511.23405.pdf' target='_blank'>https://arxiv.org/pdf/2511.23405.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suhas Srinath, Hemang Jamadagni, Aditya Chadrasekar, Prathosh AP
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23405">MANTA: Physics-Informed Generalized Underwater Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater object tracking is challenging due to wavelength dependent attenuation and scattering, which severely distort appearance across depths and water conditions. Existing trackers trained on terrestrial data fail to generalize to these physics-driven degradations. We present MANTA, a physics-informed framework integrating representation learning with tracking design for underwater scenarios. We propose a dual-positive contrastive learning strategy coupling temporal consistency with Beer-Lambert augmentations to yield features robust to both temporal and underwater distortions. We further introduce a multi-stage pipeline augmenting motion-based tracking with a physics-informed secondary association algorithm that integrates geometric consistency and appearance similarity for re-identification under occlusion and drift. To complement standard IoU metrics, we propose Center-Scale Consistency (CSC) and Geometric Alignment Score (GAS) to assess geometric fidelity. Experiments on four underwater benchmarks (WebUOT-1M, UOT32, UTB180, UWCOT220) show that MANTA achieves state-of-the-art performance, improving Success AUC by up to 6 percent, while ensuring stable long-term generalized underwater tracking and efficient runtime.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2511.20294.pdf' target='_blank'>https://arxiv.org/pdf/2511.20294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dnyandeep Mandaokar, Bernhard Rinner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20294">SAFE-IMM: Robust and Lightweight Radar-Based Object Tracking on Mobile Platforms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking maneuvering targets requires estimators that are both responsive and robust. Interacting Multiple Model (IMM) filters are a standard tracking approach, but fusing models via Gaussian mixtures can lag during maneuvers. Recent winnertakes-all (WTA) approaches react quickly but may produce discontinuities. We propose SAFE-IMM, a lightweight IMM variant for tracking on mobile and resource-limited platforms with a safe covariance-aware gate that permits WTA only when the implied jump from the mixture to the winner is provably bounded. In simulations and on nuScenes front-radar data, SAFE-IMM achieves high accuracy at real-time rates, reducing ID switches while maintaining competitive performance. The method is simple to integrate, numerically stable, and clutter-robust, offering a practical balance between responsiveness and smoothness.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2511.19396.pdf' target='_blank'>https://arxiv.org/pdf/2511.19396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jorge Ortigoso-Narro, Jose A. Belloch, Adrian Amor-Martin, Sandra Roger, Maximo Cobos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19396">Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2511.17508.pdf' target='_blank'>https://arxiv.org/pdf/2511.17508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alice Smith, Bob Johnson, Xiaoyu Zhu, Carol Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.17508">Deep Learning-based Lightweight RGB Object Tracking for Augmented Reality Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented Reality (AR) applications often require robust real-time tracking of objects in the user's environment to correctly overlay virtual content. Recent advances in computer vision have produced highly accurate deep learning-based object trackers, but these models are typically too heavy in computation and memory for wearable AR devices. In this paper, we present a lightweight RGB object tracking algorithm designed specifically for resource-constrained AR platforms. The proposed tracker employs a compact Siamese neural network architecture and incorporates optimization techniques such as model pruning, quantization, and knowledge distillation to drastically reduce model size and inference cost while maintaining high tracking accuracy. We train the tracker offline on large video datasets using deep convolutional neural networks and then deploy it on-device for real-time tracking. Experimental results on standard tracking benchmarks show that our approach achieves comparable accuracy to state-of-the-art trackers, yet runs in real-time on a mobile AR headset at around 30 FPS -- more than an order of magnitude faster than prior high-performance trackers on the same hardware. This work enables practical, robust object tracking for AR use-cases, opening the door to more interactive and dynamic AR experiences on lightweight devices.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2511.10442.pdf' target='_blank'>https://arxiv.org/pdf/2511.10442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aarush Agarwal, Raymond He, Jan Kieseler, Matteo Cremonesi, Shah Rukh Qasim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10442">FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2511.09455.pdf' target='_blank'>https://arxiv.org/pdf/2511.09455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rintaro Otsubo, Kanta Sawafuji, Hideo Saito
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09455">Hand Held Multi-Object Tracking Dataset in American Football</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) plays a critical role in analyzing player behavior from videos, enabling performance evaluation. Current MOT methods are often evaluated using publicly available datasets. However, most of these focus on everyday scenarios such as pedestrian tracking or are tailored to specific sports, including soccer and basketball. Despite the inherent challenges of tracking players in American football, such as frequent occlusion and physical contact, no standardized dataset has been publicly available, making fair comparisons between methods difficult. To address this gap, we constructed the first dedicated detection and tracking dataset for the American football players and conducted a comparative evaluation of various detection and tracking methods. Our results demonstrate that accurate detection and tracking can be achieved even in crowded scenarios. Fine-tuning detection models improved performance over pre-trained models. Furthermore, when these fine-tuned detectors and re-identification models were integrated into tracking systems, we observed notable improvements in tracking accuracy compared to existing approaches. This work thus enables robust detection and tracking of American football players in challenging, high-density scenarios previously underserved by conventional methods.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2510.21482.pdf' target='_blank'>https://arxiv.org/pdf/2510.21482.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marek Socha, Michał Marczyk, Aleksander Kempski, Michał Cogiel, Paweł Foszner, Radosław Zawiski, Michał Staniszewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21482">GRAP-MOT: Unsupervised Graph-based Position Weighted Person Multi-camera Multi-object Tracking in a Highly Congested Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>GRAP-MOT is a new approach for solving the person MOT problem dedicated to videos of closed areas with overlapping multi-camera views, where person occlusion frequently occurs. Our novel graph-weighted solution updates a person's identification label online based on tracks and the person's characteristic features. To find the best solution, we deeply investigated all elements of the MOT process, including feature extraction, tracking, and community search. Furthermore, GRAP-MOT is equipped with a person's position estimation module, which gives additional key information to the MOT method, ensuring better results than methods without position data. We tested GRAP-MOT on recordings acquired in a closed-area model and on publicly available real datasets that fulfil the requirement of a highly congested space, showing the superiority of our proposition. Finally, we analyzed existing metrics used to compare MOT algorithms and concluded that IDF1 is more adequate than MOTA in such comparisons. We made our code, along with the acquired dataset, publicly available.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2508.09585.pdf' target='_blank'>https://arxiv.org/pdf/2508.09585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Haag, Bharanidhar Duraisamy, Felix Govaers, Wolfgang Koch, Martin Fritzsche, Juergen Dickmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09585">Offline Auto Labeling: BAAS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces BAAS, a new Extended Object Tracking (EOT) and fusion-based label annotation framework for radar detections in autonomous driving. Our framework utilizes Bayesian-based tracking, smoothing and eventually fusion methods to provide veritable and precise object trajectories along with shape estimation to provide annotation labels on the detection level under various supervision levels. Simultaneously, the framework provides evaluation of tracking performance and label annotation. If manually labeled data is available, each processing module can be analyzed independently or combined with other modules to enable closed-loop continuous improvements. The framework performance is evaluated in a challenging urban real-world scenario in terms of tracking performance and the label annotation errors. We demonstrate the functionality of the proposed approach for varying dynamic objects and class types
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2508.05514.pdf' target='_blank'>https://arxiv.org/pdf/2508.05514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Wu, CÃ©sar Teixeira, Wei Ke, Zhang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05514">Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual pedestrian tracking represents a promising research field, with extensive applications in intelligent surveillance, behavior analysis, and human-computer interaction. However, real-world applications face significant occlusion challenges. When multiple pedestrians interact or overlap, the loss of target features severely compromises the tracker's ability to maintain stable trajectories. Traditional tracking methods, which typically rely on full-body bounding box features extracted from {Re-ID} models and linear constant-velocity motion assumptions, often struggle in severe occlusion scenarios. To address these limitations, this work proposes an enhanced tracking framework that leverages richer feature representations and a more robust motion model. Specifically, the proposed method incorporates detection features from both the regression and classification branches of an object detector, embedding spatial and positional information directly into the feature representations. To further mitigate occlusion challenges, a head keypoint detection model is introduced, as the head is less prone to occlusion compared to the full body. In terms of motion modeling, we propose an iterative Kalman filtering approach designed to align with modern detector assumptions, integrating 3D priors to better complete motion trajectories in complex scenes. By combining these advancements in appearance and motion modeling, the proposed method offers a more robust solution for multi-object tracking in crowded environments where occlusions are prevalent.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2508.05172.pdf' target='_blank'>https://arxiv.org/pdf/2508.05172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Wu, Longhao Wang, Cui Wang, CÃ©sar Teixeira, Wei Ke, Zhang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05172">Multi-tracklet Tracking for Generic Targets with Adaptive Detection Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking specific targets, such as pedestrians and vehicles, has been the focus of recent vision-based multitarget tracking studies. However, in some real-world scenarios, unseen categories often challenge existing methods due to low-confidence detections, weak motion and appearance constraints, and long-term occlusions. To address these issues, this article proposes a tracklet-enhanced tracker called Multi-Tracklet Tracking (MTT) that integrates flexible tracklet generation into a multi-tracklet association framework. This framework first adaptively clusters the detection results according to their short-term spatio-temporal correlation into robust tracklets and then estimates the best tracklet partitions using multiple clues, such as location and appearance over time to mitigate error propagation in long-term association. Finally, extensive experiments on the benchmark for generic multiple object tracking demonstrate the competitiveness of the proposed framework.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2508.02067.pdf' target='_blank'>https://arxiv.org/pdf/2508.02067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manikanta Kotthapalli, Deepika Ravipati, Reshma Bhatia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02067">YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, object detection has advanced significantly, with the YOLO (You Only Look Once) family of models transforming the landscape of real-time vision applications through unified, end-to-end detection frameworks. From YOLOv1's pioneering regression-based detection to the latest YOLOv9, each version has systematically enhanced the balance between speed, accuracy, and deployment efficiency through continuous architectural and algorithmic advancements.. Beyond core object detection, modern YOLO architectures have expanded to support tasks such as instance segmentation, pose estimation, object tracking, and domain-specific applications including medical imaging and industrial automation. This paper offers a comprehensive review of the YOLO family, highlighting architectural innovations, performance benchmarks, extended capabilities, and real-world use cases. We critically analyze the evolution of YOLO models and discuss emerging research directions that extend their impact across diverse computer vision domains.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2507.08548.pdf' target='_blank'>https://arxiv.org/pdf/2507.08548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alen Adamyan, TomÃ¡Å¡ ÄÃ­Å¾ek, Matej Straka, Klara Janouskova, Martin Schmid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08548">SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks and has become the state-of-the-art for visual object tracking. The model stores information from previous frames in a memory bank, enabling temporal consistency across video sequences. Recent methods augment SAM 2 with hand-crafted update rules to better handle distractors, occlusions, and object motion. We propose a fundamentally different approach using reinforcement learning for optimizing memory updates in SAM 2 by framing memory control as a sequential decision-making problem. In an overfitting setup with a separate agent per video, our method achieves a relative improvement over SAM 2 that exceeds by more than three times the gains of existing heuristics. These results reveal the untapped potential of the memory bank and highlight reinforcement learning as a powerful alternative to hand-crafted update rules for memory control in visual object tracking.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2507.04116.pdf' target='_blank'>https://arxiv.org/pdf/2507.04116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fred Lydeard, Bashar I. Ahmad, Simon Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04116">Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a computationally efficient multi-object tracking approach that can minimise track breaks (e.g., in challenging environments and against agile targets), learn the measurement model parameters on-line (e.g., in dynamically changing scenes) and infer the class of the tracked objects, if joint tracking and kinematic behaviour classification is sought. It capitalises on the flexibilities offered by the integrated Gaussian process as a motion model and the convenient statistical properties of non-homogeneous Poisson processes as a suitable observation model. This can be combined with the proposed effective track revival / stitching mechanism. We accordingly introduce the two robust and adaptive trackers, Gaussian and Poisson Process with Classification (GaPP-Class) and GaPP with Revival and Classification (GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme that efficiently integrates track management and hyperparameter learning (including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class with the addition of a Markov Chain Monte Carlo kernel applied to each particle permitting track revival and stitching (e.g., within a few time steps after deleting a trajectory). Performance evaluation and benchmarking using synthetic and real data show that GaPP-Class and GaPP-ReaCtion outperform other state-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly reduces track breaks (e.g., by around 30% from real radar data and markedly more from simulated data).
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2507.02408.pdf' target='_blank'>https://arxiv.org/pdf/2507.02408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duong Nguyen-Ngoc Tran, Long Hoang Pham, Chi Dai Tran, Quoc Pham-Nam Ho, Huy-Hung Nguyen, Jae Wook Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02408">A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking in thermal images is essential for surveillance systems, particularly in challenging environments where RGB cameras struggle due to low visibility or poor lighting conditions. Thermal sensors enhance recognition tasks by capturing infrared signatures, but a major challenge is their low-level feature representation, which makes it difficult to accurately detect and track pedestrians. To address this, the paper introduces a novel tuning method for pedestrian tracking, specifically designed to handle the complex motion patterns in thermal imagery. The proposed framework optimizes two-stages, ensuring that each stage is tuned with the most suitable hyperparameters to maximize tracking performance. By fine-tuning hyperparameters for real-time tracking, the method achieves high accuracy without relying on complex reidentification or motion models. Extensive experiments on PBVS Thermal MOT dataset demonstrate that the approach is highly effective across various thermal camera conditions, making it a robust solution for real-world surveillance applications.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2506.22562.pdf' target='_blank'>https://arxiv.org/pdf/2506.22562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhineet Singh, Nilanjan Ray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22562">Improving Token-based Object Detection with Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper improves upon the Pix2Seq object detector by extending it for videos. In the process, it introduces a new way to perform end-to-end video object detection that improves upon existing video detectors in two key ways. First, by representing objects as variable-length sequences of discrete tokens, we can succinctly represent widely varying numbers of video objects, with diverse shapes and locations, without having to inject any localization cues in the training process. This eliminates the need to sample the space of all possible boxes that constrains conventional detectors and thus solves the dual problems of loss sparsity during training and heuristics-based postprocessing during inference. Second, it conceptualizes and outputs the video objects as fully integrated and indivisible 3D boxes or tracklets instead of generating image-specific 2D boxes and linking these boxes together to construct the video object, as done in most conventional detectors. This allows it to scale effortlessly with available computational resources by simply increasing the length of the video subsequence that the network takes as input, even generalizing to multi-object tracking if the subsequence can span the entire video. We compare our video detector with the baseline Pix2Seq static detector on several datasets and demonstrate consistent improvement, although with strong signs of being bottlenecked by our limited computational resources. We also compare it with several video detectors on UA-DETRAC to show that it is competitive with the current state of the art even with the computational bottleneck. We make our code and models publicly available.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2505.07336.pdf' target='_blank'>https://arxiv.org/pdf/2505.07336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Zhang, Xiaopeng Li, Qi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07336">SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2505.04392.pdf' target='_blank'>https://arxiv.org/pdf/2505.04392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Petr Jahoda, Jan Cech
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04392">Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. The method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. The method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. Anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. A challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. Therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. Our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. The method is effective and performs in real time on standard consumer hardware.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2504.21692.pdf' target='_blank'>https://arxiv.org/pdf/2504.21692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Zhou, Changrui Dai, Aibo Song, Xiaolin Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21692">Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful video analysis relies on accurate recognition of pixels across frames, and frame reconstruction methods based on video correspondence learning are popular due to their efficiency. Existing frame reconstruction methods, while efficient, neglect the value of direct involvement of multiple reference frames for reconstruction and decision-making aspects, especially in complex situations such as occlusion or fast movement. In this paper, we introduce a Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple reference frames to concisely and directly enhance frame reconstruction. Its core component is a Reference Frame Memory Engine that dynamically selects frames based on object pixel features to improve tracking accuracy. In addition, a Bidirectional Target Prediction Network is built to utilize multiple reference frames to improve the robustness of the model. Through experiments, our algorithm outperforms the state-of-the-art self-supervised techniques on two fine-grained video object tracking tasks: object segmentation and keypoint tracking.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2504.20234.pdf' target='_blank'>https://arxiv.org/pdf/2504.20234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bartosz Ptak, Marek Kraft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20234">Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone-based crowd monitoring is the key technology for applications in surveillance, public safety, and event management. However, maintaining tracking continuity and consistency remains a significant challenge. Traditional detection-assignment tracking methods struggle with false positives, false negatives, and frequent identity switches, leading to degraded counting accuracy and making in-depth analysis impossible. This paper introduces a point-oriented online tracking algorithm that improves trajectory continuity and counting reliability in drone-based crowd monitoring. Our method builds on the Simple Online and Real-time Tracking (SORT) framework, replacing the original bounding-box assignment with a point-distance metric. The algorithm is enhanced with three cost-effective techniques: camera motion compensation, altitude-aware assignment, and classification-based trajectory validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use spatial feature maps from localisation algorithms for increased computational efficiency through neural network resource sharing are integrated to refine object tracking by reducing noise and handling missed detections. The proposed method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets, demonstrating substantial improvements in tracking metrics, reducing counting errors to 23% and 15%, respectively. The results also indicate a significant reduction of identity switches while maintaining high tracking accuracy, outperforming baseline online trackers and even an offline greedy optimisation method.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2504.19719.pdf' target='_blank'>https://arxiv.org/pdf/2504.19719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Folkman, Quynh LK Vo, Colin Johnston, Bela Stantic, Kylie A Pitt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19719">A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing demand for aquaculture production necessitates the development of innovative, intelligent tools to effectively monitor and manage fish health and welfare. While non-invasive video monitoring has become a common practice in finfish aquaculture, existing intelligent monitoring methods predominantly focus on assessing body condition or fish swimming patterns and are often developed and evaluated in controlled tank environments, without demonstrating their applicability to real-world aquaculture settings in open sea farms. This underscores the necessity for methods that can monitor physiological traits directly within the production environment of sea fish farms. To this end, we have developed a computer vision method for monitoring ventilation rates of Atlantic salmon (Salmo salar), which was specifically designed for videos recorded in the production environment of commercial sea fish farms using the existing infrastructure. Our approach uses a fish head detection model, which classifies the mouth state as either open or closed using a convolutional neural network. This is followed with multiple object tracking to create temporal sequences of fish swimming across the field of view of the underwater video camera to estimate ventilation rates. The method demonstrated high efficiency, achieving a Pearson correlation coefficient of 0.82 between ground truth and predicted ventilation rates in a test set of 100 fish collected independently of the training data. By accurately identifying pens where fish exhibit signs of respiratory distress, our method offers broad applicability and the potential to transform fish health and welfare monitoring in finfish aquaculture.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2503.16768.pdf' target='_blank'>https://arxiv.org/pdf/2503.16768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Zhou, Jiadong Xie, Mingsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16768">Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mainstream visual object tracking frameworks predominantly rely on template matching paradigms. Their performance heavily depends on the quality of template features, which becomes increasingly challenging to maintain in complex scenarios involving target deformation, occlusion, and background clutter. While existing spatiotemporal memory-based trackers emphasize memory capacity expansion, they lack effective mechanisms for dynamic feature selection and adaptive fusion. To address this gap, we propose a Dynamic Attention Mechanism in Spatiotemporal Memory Network (DASTM) with two key innovations: 1) A differentiable dynamic attention mechanism that adaptively adjusts channel-spatial attention weights by analyzing spatiotemporal correlations between the templates and memory features; 2) A lightweight gating network that autonomously allocates computational resources based on target motion states, prioritizing high-discriminability features in challenging scenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K benchmarks demonstrate our DASTM's superiority, achieving state-of-the-art performance in success rate, robustness, and real-time efficiency, thereby offering a novel solution for real-time tracking in complex environments.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2503.16318.pdf' target='_blank'>https://arxiv.org/pdf/2503.16318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Sucar, Zihang Lai, Eldar Insafutdinov, Andrea Vedaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16318">Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DUSt3R has recently shown that one can reduce many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing the scene in 3D, and establishing image correspondences, to the prediction of a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. This formulation is elegant and powerful, but unable to tackle dynamic scenes. To address this challenge, we introduce the concept of Dynamic Point Maps (DPM), extending standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key intuition is that, when time is introduced, there are several possible spatial and time references that can be used to define the point maps. We identify a minimal subset of such combinations that can be regressed by a network to solve the sub tasks mentioned above. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks for video depth prediction, dynamic point cloud reconstruction, 3D scene flow and object pose tracking, achieving state-of-the-art performance. Code, models and additional results are available at https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2503.09951.pdf' target='_blank'>https://arxiv.org/pdf/2503.09951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinglong Sun, Haijiang Sun, Shan Jiang, Jiacheng Wang, Jiasong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09951">Target-aware Bidirectional Fusion Transformer for Aerial Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The trackers based on lightweight neural networks have achieved great success in the field of aerial remote sensing, most of which aggregate multi-stage deep features to lift the tracking quality. However, existing algorithms usually only generate single-stage fusion features for state decision, which ignore that diverse kinds of features are required for identifying and locating the object, limiting the robustness and precision of tracking. In this paper, we propose a novel target-aware Bidirectional Fusion transformer (BFTrans) for UAV tracking. Specifically, we first present a two-stream fusion network based on linear self and cross attentions, which can combine the shallow and the deep features from both forward and backward directions, providing the adjusted local details for location and global semantics for recognition. Besides, a target-aware positional encoding strategy is designed for the above fusion model, which is helpful to perceive the object-related attributes during the fusion phase. Finally, the proposed method is evaluated on several popular UAV benchmarks, including UAV-123, UAV20L and UAVTrack112. Massive experimental results demonstrate that our approach can exceed other state-of-the-art trackers and run with an average speed of 30.5 FPS on embedded platform, which is appropriate for practical drone deployments.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2502.19705.pdf' target='_blank'>https://arxiv.org/pdf/2502.19705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juntao Liang, Jun Hou, Weijun Zhang, Yong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19705">CFTrack: Enhancing Lightweight Visual Tracking through Contrastive Learning and Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving both efficiency and strong discriminative ability in lightweight visual tracking is a challenge, especially on mobile and edge devices with limited computational resources. Conventional lightweight trackers often struggle with robustness under occlusion and interference, while deep trackers, when compressed to meet resource constraints, suffer from performance degradation. To address these issues, we introduce CFTrack, a lightweight tracker that integrates contrastive learning and feature matching to enhance discriminative feature representations. CFTrack dynamically assesses target similarity during prediction through a novel contrastive feature matching module optimized with an adaptive contrastive loss, thereby improving tracking accuracy. Extensive experiments on LaSOT, OTB100, and UAV123 show that CFTrack surpasses many state-of-the-art lightweight trackers, operating at 136 frames per second on the NVIDIA Jetson NX platform. Results on the HOOT dataset further demonstrate CFTrack's strong discriminative ability under heavy occlusion.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2502.03760.pdf' target='_blank'>https://arxiv.org/pdf/2502.03760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat-Tan Do, Nhi Ngoc-Yen Nguyen, Dieu-Phuong Nguyen, Trong-Hop Do
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03760">RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on Deep Learning and Big Data Technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in UAV-based video is challenging due to variations in viewpoint, low resolution, and the presence of small objects. While other research on MOT dedicated to aerial videos primarily focuses on the academic aspect by developing sophisticated algorithms, there is a lack of attention to the practical aspect of these systems. In this paper, we propose a novel real-time MOT framework that integrates Apache Kafka and Apache Spark for efficient and fault-tolerant video stream processing, along with state-of-the-art deep learning models YOLOv8/YOLOv10 and BYTETRACK/BoTSORT for accurate object detection and tracking. Our work highlights the importance of not only the advanced algorithms but also the integration of these methods with scalable and distributed systems. By leveraging these technologies, our system achieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set while maintaining a real-time processing speed of 28 FPS on a single GPU. Our work demonstrates the potential of big data technologies and deep learning for addressing the challenges of MOT in UAV applications.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2501.14587.pdf' target='_blank'>https://arxiv.org/pdf/2501.14587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viktor KozÃ¡k, Karel KoÅ¡nar, Jan Chudoba, Miroslav Kulich, Libor PÅeuÄil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14587">Visual Localization via Semantic Structures in Autonomous Photovoltaic Power Plant Inspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspection systems utilizing unmanned aerial vehicles (UAVs) equipped with thermal cameras are increasingly popular for the maintenance of photovoltaic (PV) power plants. However, automation of the inspection task is a challenging problem as it requires precise navigation to capture images from optimal distances and viewing angles.
  This paper presents a novel localization pipeline that directly integrates PV module detection with UAV navigation, allowing precise positioning during inspection. Detections are used to identify the power plant structures in the image and associate these with the power plant model. We define visually recognizable anchor points for the initial association and use object tracking to discern global associations. We present three distinct methods for visual segmentation of PV modules based on traditional computer vision, deep learning, and their fusion, and we evaluate their performance in relation to the proposed localization pipeline.
  The presented methods were verified and evaluated using custom aerial inspection data sets, demonstrating their robustness and applicability for real-time navigation. Additionally, we evaluate the influence of the power plant model's precision on the localization methods.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2501.03874.pdf' target='_blank'>https://arxiv.org/pdf/2501.03874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Zhang, Timothy Shea, Arto Nurmikko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03874">Neuromorphic Optical Tracking and Imaging of Randomly Moving Targets through Strongly Scattering Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking and acquiring simultaneous optical images of randomly moving targets obscured by scattering media remains a challenging problem of importance to many applications that require precise object localization and identification. In this work we develop an end-to-end neuromorphic optical engineering and computational approach to demonstrate how to track and image normally invisible objects by combining an event detecting camera with a multistage neuromorphic deep learning strategy. Photons emerging from dense scattering media are detected by the event camera and converted to pixel-wise asynchronized spike trains - a first step in isolating object-specific information from the dominant uninformative background. Spiking data is fed into a deep spiking neural network (SNN) engine where object tracking and image reconstruction are performed by two separate yet interconnected modules running in parallel in discrete time steps over the event duration. Through benchtop experiments we demonstrate tracking and imaging randomly moving objects in dense turbid media as well as image reconstruction of spatially stationary but optically dynamic objects. Standardized character sets serve as representative proxies for geometrically complex objects, underscoring the method's generality. The results highlight the advantages of a fully neuromorphic approach in meeting a major imaging technology with high computational efficiency and low power consumption.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2412.14088.pdf' target='_blank'>https://arxiv.org/pdf/2412.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Dal'Col, Miguel Oliveira, VÃ­tor Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14088">Joint Perception and Prediction for Autonomous Driving: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception and prediction modules are critical components of autonomous driving systems, enabling vehicles to navigate safely through complex environments. The perception module is responsible for perceiving the environment, including static and dynamic objects, while the prediction module is responsible for predicting the future behavior of these objects. These modules are typically divided into three tasks: object detection, object tracking, and motion prediction. Traditionally, these tasks are developed and optimized independently, with outputs passed sequentially from one to the next. However, this approach has significant limitations: computational resources are not shared across tasks, the lack of joint optimization can amplify errors as they propagate throughout the pipeline, and uncertainty is rarely propagated between modules, resulting in significant information loss. To address these challenges, the joint perception and prediction paradigm has emerged, integrating perception and prediction into a unified model through multi-task learning. This strategy not only overcomes the limitations of previous methods, but also enables the three tasks to have direct access to raw sensor data, allowing richer and more nuanced environmental interpretations. This paper presents the first comprehensive survey of joint perception and prediction for autonomous driving. We propose a taxonomy that categorizes approaches based on input representation, scene context modeling, and output representation, highlighting their contributions and limitations. Additionally, we present a qualitative analysis and quantitative comparison of existing methods. Finally, we discuss future research directions based on identified gaps in the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2412.08121.pdf' target='_blank'>https://arxiv.org/pdf/2412.08121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel NordstrÃ¶m, BjÃ¶rn Lindquist, George Nikolakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08121">DTAA: A Detect, Track and Avoid Architecture for navigation in spaces with Multiple Velocity Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proactive collision avoidance measures are imperative in environments where humans and robots coexist. Moreover, the introduction of high quality legged robots into workplaces highlighted the crucial role of a robust, fully autonomous safety solution for robots to be viable in shared spaces or in co-existence with humans. This article establishes for the first time ever an innovative Detect-Track-and-Avoid Architecture (DTAA) to enhance safety and overall mission performance. The proposed novel architectyre has the merit ot integrating object detection using YOLOv8, utilizing Ultralytics embedded object tracking, and state estimation of tracked objects through Kalman filters. Moreover, a novel heuristic clustering is employed to facilitate active avoidance of multiple closely positioned objects with similar velocities, creating sets of unsafe spaces for the Nonlinear Model Predictive Controller (NMPC) to navigate around. The NMPC identifies the most hazardous unsafe space, considering not only their current positions but also their predicted future locations. In the sequel, the NMPC calculates maneuvers to guide the robot along a path planned by D$^{*}_{+}$ towards its intended destination, while maintaining a safe distance to all identified obstacles. The efficacy of the novelly suggested DTAA framework is being validated by Real-life experiments featuring a Boston Dynamics Spot robot that demonstrates the robot's capability to consistently maintain a safe distance from humans in dynamic subterranean, urban indoor, and outdoor environments.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2411.06896.pdf' target='_blank'>https://arxiv.org/pdf/2411.06896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hemal Naik, Junran Yang, Dipin Das, Margaret C Crofoot, Akanksha Rathore, Vivek Hari Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06896">BuckTales : A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding animal behaviour is central to predicting, understanding, and mitigating impacts of natural and anthropogenic changes on animal populations and ecosystems. However, the challenges of acquiring and processing long-term, ecologically relevant data in wild settings have constrained the scope of behavioural research. The increasing availability of Unmanned Aerial Vehicles (UAVs), coupled with advances in machine learning, has opened new opportunities for wildlife monitoring using aerial tracking. However, limited availability of datasets with wild animals in natural habitats has hindered progress in automated computer vision solutions for long-term animal tracking. Here we introduce BuckTales, the first large-scale UAV dataset designed to solve multi-object tracking (MOT) and re-identification (Re-ID) problem in wild animals, specifically the mating behaviour (or lekking) of blackbuck antelopes. Collected in collaboration with biologists, the MOT dataset includes over 1.2 million annotations including 680 tracks across 12 high-resolution (5.4K) videos, each averaging 66 seconds and featuring 30 to 130 individuals. The Re-ID dataset includes 730 individuals captured with two UAVs simultaneously. The dataset is designed to drive scalable, long-term animal behaviour tracking using multiple camera sensors. By providing baseline performance with two detectors, and benchmarking several state-of-the-art tracking methods, our dataset reflects the real-world challenges of tracking wild animals in socially and ecologically relevant contexts. In making these data widely available, we hope to catalyze progress in MOT and Re-ID for wild animals, fostering insights into animal behaviour, conservation efforts, and ecosystem dynamics through automated, long-term monitoring.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2409.18901.pdf' target='_blank'>https://arxiv.org/pdf/2409.18901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18901">Improving Visual Object Tracking through Visual Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a discriminative model to distinguish a target from its surrounding distractors is essential to generic visual object tracking. Dynamic target representation adaptation against distractors is challenging due to the limited discriminative capabilities of prevailing trackers. We present a new visual Prompting mechanism for generic Visual Object Tracking (PiVOT) to address this issue. PiVOT proposes a prompt generation network with the pre-trained foundation model CLIP to automatically generate and refine visual prompts, enabling the transfer of foundation model knowledge for tracking. While CLIP offers broad category-level knowledge, the tracker, trained on instance-specific data, excels at recognizing unique object instances. Thus, PiVOT first compiles a visual prompt highlighting potential target locations. To transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP to refine the visual prompt based on the similarities between candidate objects and the reference templates across potential targets. Once the visual prompt is refined, it can better highlight potential target locations, thereby reducing irrelevant prompt information. With the proposed prompting mechanism, the tracker can generate improved instance-aware feature maps through the guidance of the visual prompt, thus effectively reducing distractors. The proposed method does not involve CLIP during training, thereby keeping the same training complexity and preserving the generalization capability of the pretrained foundation model. Extensive experiments across multiple benchmarks indicate that PiVOT, using the proposed prompting method can suppress distracting objects and enhance the tracker.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2409.14220.pdf' target='_blank'>https://arxiv.org/pdf/2409.14220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomasz Stanczyk, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14220">Temporally Propagated Masks and Bounding Boxes: Combining the Best of Both Worlds for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) involves identifying and consistently tracking objects across video sequences. Traditional tracking-by-detection methods, while effective, often require extensive tuning and lack generalizability. On the other hand, segmentation mask-based methods are more generic but struggle with tracking management, making them unsuitable for MOT. We propose a novel approach, McByte, which incorporates a temporally propagated segmentation mask as a strong association cue within a tracking-by-detection framework. By combining bounding box and propagated mask information, McByte enhances robustness and generalizability without per-sequence tuning. Evaluated on four benchmark datasets - DanceTrack, MOT17, SoccerNet-tracking 2022, and KITTI-tracking - McByte demonstrates performance gain in all cases examined. At the same time, it outperforms existing mask-based methods. Implementation code will be provided upon acceptance.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2408.16190.pdf' target='_blank'>https://arxiv.org/pdf/2408.16190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanner D. Harms, Steven L. Brunton, Beverley J. McKeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16190">Estimating Dynamic Flow Features in Groups of Tracked Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interpreting motion captured in image sequences is crucial for a wide range of computer vision applications. Typical estimation approaches include optical flow (OF), which approximates the apparent motion instantaneously in a scene, and multiple object tracking (MOT), which tracks the motion of subjects over time. Often, the motion of objects in a scene is governed by some underlying dynamical system which could be inferred by analyzing the motion of groups of objects. Standard motion analyses, however, are not designed to intuit flow dynamics from trajectory data, making such measurements difficult in practice. The goal of this work is to extend gradient-based dynamical systems analyses to real-world applications characterized by complex, feature-rich image sequences with imperfect tracers. The tracer trajectories are tracked using deep vision networks and gradients are approximated using Lagrangian gradient regression (LGR), a tool designed to estimate spatial gradients from sparse data. From gradients, dynamical features such as regions of coherent rotation and transport barriers are identified. The proposed approach is affordably implemented and enables advanced studies including the motion analysis of two distinct object classes in a single image sequence. Two examples of the method are presented on data sets for which standard gradient-based analyses do not apply.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2407.19430.pdf' target='_blank'>https://arxiv.org/pdf/2407.19430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Li, Kanlun Tan, Qiao Liu, Di Yuan, Xin Li, Yunpeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19430">Progressive Domain Adaptation for Thermal Infrared Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the lack of large-scale labeled Thermal InfraRed (TIR) training datasets, most existing TIR trackers are trained directly on RGB datasets. However, tracking methods trained on RGB datasets suffer a significant drop-off in TIR data due to the domain shift issue. To this end, in this work, we propose a Progressive Domain Adaptation framework for TIR Tracking (PDAT), which transfers useful knowledge learned from RGB tracking to TIR tracking. The framework makes full use of large-scale labeled RGB datasets without requiring time-consuming and labor-intensive labeling of large-scale TIR data. Specifically, we first propose an adversarial-based global domain adaptation module to reduce domain gap on the feature level coarsely. Second, we design a clustering-based subdomain adaptation method to further align the feature distributions of the RGB and TIR datasets finely. These two domain adaptation modules gradually eliminate the discrepancy between the two domains, and thus learn domain-invariant fine-grained features through progressive training. Additionally, we collect a largescale TIR dataset with over 1.48 million unlabeled TIR images for training the proposed domain adaptation framework. Experimental results on five TIR tracking benchmarks show that the proposed method gains a nearly 6% success rate, demonstrating its effectiveness.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2407.04327.pdf' target='_blank'>https://arxiv.org/pdf/2407.04327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thuc Nguyen-Quang, Minh-Triet Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04327">TF-SASM: Training-free Spatial-aware Sparse Memory for Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in computer vision remains a significant challenge, requiring precise localization and continuous tracking of multiple objects in video sequences. The emergence of data sets that emphasize robust reidentification, such as DanceTrack, has highlighted the need for effective solutions. While memory-based approaches have shown promise, they often suffer from high computational complexity and memory usage due to storing feature at every single frame. In this paper, we propose a novel memory-based approach that selectively stores critical features based on object motion and overlapping awareness, aiming to enhance efficiency while minimizing redundancy. As a result, our method not only store longer temporal information with limited number of stored features in the memory, but also diversify states of a particular object to enhance the association performance. Our approach significantly improves over MOTRv2 in the DanceTrack test set, demonstrating a gain of 2.0% AssA score and 2.1% in IDF1 score.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2407.04308.pdf' target='_blank'>https://arxiv.org/pdf/2407.04308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Griffin Golias, Masa Nakura-Fan, Vitaly Ablavsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04308">SSP-GNN: Learning to Track via Bilevel Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a graph-based tracking formulation for multi-object tracking (MOT) where target detections contain kinematic information and re-identification features (attributes). Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant. The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization guided by our novel loss function. We evaluate our algorithm on simulated scenarios to understand its sensitivity to scenario aspects and model hyperparameters. Across varied scenario complexities, our method compares favorably to a strong baseline.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2407.00830.pdf' target='_blank'>https://arxiv.org/pdf/2407.00830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ogulcan Eryuksel, Kamil Anil Ozfuttu, Fatih Cagatay Akyon, Kadir Sahin, Efe Buyukborekci, Devrim Cavusoglu, Sinan Altinuc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00830">DroBoost: An Intelligent Score and Model Boosting Method for Drone Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone detection is a challenging object detection task where visibility conditions and quality of the images may be unfavorable, and detections might become difficult due to complex backgrounds, small visible objects, and hard to distinguish objects. Both provide high confidence for drone detections, and eliminating false detections requires efficient algorithms and approaches. Our previous work, which uses YOLOv5, uses both real and synthetic data and a Kalman-based tracker to track the detections and increase their confidence using temporal information. Our current work improves on the previous approach by combining several improvements. We used a more diverse dataset combining multiple sources and combined with synthetic samples chosen from a large synthetic dataset based on the error analysis of the base model. Also, to obtain more resilient confidence scores for objects, we introduced a classification component that discriminates whether the object is a drone or not. Finally, we developed a more advanced scoring algorithm for object tracking that we use to adjust localization confidence. Furthermore, the proposed technique won 1st Place in the Drone vs. Bird Challenge (Workshop on Small-Drone Surveillance, Detection and Counteraction Techniques at ICIAP 2021).
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2407.00738.pdf' target='_blank'>https://arxiv.org/pdf/2407.00738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir AdÅ¾emoviÄ, Predrag TadiÄ, Andrija PetroviÄ, Mladen NikoliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00738">Engineering an Efficient Object Tracker for Non-Linear Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of multi-object tracking is to detect and track all objects in a scene while maintaining unique identifiers for each, by associating their bounding boxes across video frames. This association relies on matching motion and appearance patterns of detected objects. This task is especially hard in case of scenarios involving dynamic and non-linear motion patterns. In this paper, we introduce DeepMoveSORT, a novel, carefully engineered multi-object tracker designed specifically for such scenarios. In addition to standard methods of appearance-based association, we improve motion-based association by employing deep learnable filters (instead of the most commonly used Kalman filter) and a rich set of newly proposed heuristics. Our improvements to motion-based association methods are severalfold. First, we propose a new transformer-based filter architecture, TransFilter, which uses an object's motion history for both motion prediction and noise filtering. We further enhance the filter's performance by careful handling of its motion history and accounting for camera motion. Second, we propose a set of heuristics that exploit cues from the position, shape, and confidence of detected bounding boxes to improve association performance. Our experimental evaluation demonstrates that DeepMoveSORT outperforms existing trackers in scenarios featuring non-linear motion, surpassing state-of-the-art results on three such datasets. We also perform a thorough ablation study to evaluate the contributions of different tracker components which we proposed. Based on our study, we conclude that using a learnable filter instead of the Kalman filter, along with appearance-based association is key to achieving strong general tracking performance.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2405.10439.pdf' target='_blank'>https://arxiv.org/pdf/2405.10439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Abdelaziz, Mohamed Shehata, Mohamed Mohamed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10439">Beyond Traditional Single Object Tracking: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking is a vital task of many applications in critical fields. However, it is still considered one of the most challenging vision tasks. In recent years, computer vision, especially object tracking, witnessed the introduction or adoption of many novel techniques, setting new fronts for performance. In this survey, we visit some of the cutting-edge techniques in vision, such as Sequence Models, Generative Models, Self-supervised Learning, Unsupervised Learning, Reinforcement Learning, Meta-Learning, Continual Learning, and Domain Adaptation, focusing on their application in single object tracking. We propose a novel categorization of single object tracking methods based on novel techniques and trends. Also, we conduct a comparative analysis of the performance reported by the methods presented on popular tracking benchmarks. Moreover, we analyze the pros and cons of the presented approaches and present a guide for non-traditional techniques in single object tracking. Finally, we suggest potential avenues for future research in single-object tracking.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2405.04290.pdf' target='_blank'>https://arxiv.org/pdf/2405.04290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xia, Erik Stenborg, Junsheng Fu, Gustaf Hendeby
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04290">Bayesian Simultaneous Localization and Multi-Lane Tracking Using Onboard Sensors and a SD Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-definition map with accurate lane-level information is crucial for autonomous driving, but the creation of these maps is a resource-intensive process. To this end, we present a cost-effective solution to create lane-level roadmaps using only the global navigation satellite system (GNSS) and a camera on customer vehicles. Our proposed solution utilizes a prior standard-definition (SD) map, GNSS measurements, visual odometry, and lane marking edge detection points, to simultaneously estimate the vehicle's 6D pose, its position within a SD map, and also the 3D geometry of traffic lines. This is achieved using a Bayesian simultaneous localization and multi-object tracking filter, where the estimation of traffic lines is formulated as a multiple extended object tracking problem, solved using a trajectory Poisson multi-Bernoulli mixture (TPMBM) filter. In TPMBM filtering, traffic lines are modeled using B-spline trajectories, and each trajectory is parameterized by a sequence of control points. The proposed solution has been evaluated using experimental data collected by a test vehicle driving on highway. Preliminary results show that the traffic line estimates, overlaid on the satellite image, generally align with the lane markings up to some lateral offsets.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2404.18720.pdf' target='_blank'>https://arxiv.org/pdf/2404.18720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shimian Zhang, Qiuhong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18720">Innovative Integration of Visual Foundation Model with a Robotic Arm on a Mobile Platform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly advancing field of robotics, the fusion of state-of-the-art visual technologies with mobile robotic arms has emerged as a critical integration. This paper introduces a novel system that combines the Segment Anything model (SAM) -- a transformer-based visual foundation model -- with a robotic arm on a mobile platform. The design of integrating a depth camera on the robotic arm's end-effector ensures continuous object tracking, significantly mitigating environmental uncertainties. By deploying on a mobile platform, our grasping system has an enhanced mobility, playing a key role in dynamic environments where adaptability are critical. This synthesis enables dynamic object segmentation, tracking, and grasping. It also elevates user interaction, allowing the robot to intuitively respond to various modalities such as clicks, drawings, or voice commands, beyond traditional robotic systems. Empirical assessments in both simulated and real-world demonstrate the system's capabilities. This configuration opens avenues for wide-ranging applications, from industrial settings, agriculture, and household tasks, to specialized assignments and beyond.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2403.16395.pdf' target='_blank'>https://arxiv.org/pdf/2403.16395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinglong Sun, Haijiang Sun, Shan Jiang, Jiacheng Wang, Xilai Wei, Zhonghe Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16395">Multi-attention Associate Prediction Network for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classification-regression prediction networks have realized impressive success in several modern deep trackers. However, there is an inherent difference between classification and regression tasks, so they have diverse even opposite demands for feature matching. Existed models always ignore the key issue and only employ a unified matching block in two task branches, decaying the decision quality. Besides, these models also struggle with decision misalignment situation. In this paper, we propose a multi-attention associate prediction network (MAPNet) to tackle the above problems. Concretely, two novel matchers, i.e., category-aware matcher and spatial-aware matcher, are first designed for feature comparison by integrating self, cross, channel or spatial attentions organically. They are capable of fully capturing the category-related semantics for classification and the local spatial contexts for regression, respectively. Then, we present a dual alignment module to enhance the correspondences between two branches, which is useful to find the optimal tracking solution. Finally, we describe a Siamese tracker built upon the proposed prediction network, which achieves the leading performance on five tracking benchmarks, consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and UAV123, and surpasses other state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2402.17976.pdf' target='_blank'>https://arxiv.org/pdf/2402.17976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhewei Wu, Ruilong Yu, Qihe Liu, Shuying Cheng, Shilin Qiu, Shijie Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17976">Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. However, there is still a lack of research on designing adversarial defense methods for object tracking. To address these issues, we propose an effective auxiliary pre-processing defense network, AADN, which performs defensive transformations on the input images before feeding them into the tracker. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without parameter adjustments. We train AADN using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that AADN maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios. Moreover, when transferring the defense network to heterogeneous trackers, it exhibits reliable transferability. Finally, AADN achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2402.16246.pdf' target='_blank'>https://arxiv.org/pdf/2402.16246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Zhu, Yanqiang Wang, Yadong An, Hong Yang, Yiming Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16246">Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video. By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform. For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized. The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds. This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices. It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2402.09865.pdf' target='_blank'>https://arxiv.org/pdf/2402.09865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir AdÅ¾emoviÄ, Predrag TadiÄ, Andrija PetroviÄ, Mladen NikoliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09865">Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2512.20975.pdf' target='_blank'>https://arxiv.org/pdf/2512.20975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujin Noh, Inho Jake Park, Chigon Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.20975">SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2512.17939.pdf' target='_blank'>https://arxiv.org/pdf/2512.17939.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuncheng Lu, Yucen Shi, Aobo Li, Zehao Li, Junying Li, Bo Wang, Tony Tae-Hyoung Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17939">A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2512.02789.pdf' target='_blank'>https://arxiv.org/pdf/2512.02789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tang Haonan, Chen Yanjun, Jiang Lezhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.02789">TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2511.20418.pdf' target='_blank'>https://arxiv.org/pdf/2511.20418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matvei Shelukhan, Timur Mamedov, Karina Kvanchiani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.20418">StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2508.13507.pdf' target='_blank'>https://arxiv.org/pdf/2508.13507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungheon Baek, Jinhyuk Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13507">Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Badminton is known as one of the fastest racket sports in the world. Despite doubles matches being more prevalent in international tournaments than singles, previous research has mainly focused on singles due to the challenges in data availability and multi-person tracking. To address this gap, we designed an approach that transfers singles-trained models to doubles analysis. We extracted keypoints from the ShuttleSet single matches dataset using ViT-Pose and embedded them through a contrastive learning framework based on ST-GCN. To improve tracking stability, we incorporated a custom multi-object tracking algorithm that resolves ID switching issues from fast and overlapping player movements. A Transformer-based classifier then determines shot occurrences based on the learned embeddings. Our findings demonstrate the feasibility of extending pose-based shot recognition to doubles badminton, broadening analytics capabilities. This work establishes a foundation for doubles-specific datasets to enhance understanding of this predominant yet understudied format of the fast racket sport.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2508.09796.pdf' target='_blank'>https://arxiv.org/pdf/2508.09796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Wang, Zhixing Wang, Le Zheng, Tianxiao Liu, Roujing Li, Xueyao Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09796">MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in human-dominant scenarios, which involves continuously tracking multiple people within video sequences, remains a significant challenge in computer vision due to targets' complex motion and severe occlusions. Conventional tracking-by-detection methods are fundamentally limited by their reliance on Kalman filter (KF) and rigid Intersection over Union (IoU)-based association. The motion model in KF often mismatches real-world object dynamics, causing filtering errors, while rigid association struggles under occlusions, leading to identity switches or target loss. To address these issues, we propose MeMoSORT, a simple, online, and real-time MOT tracker with two key innovations. First, the Memory-assisted Kalman filter (MeKF) uses memory-augmented neural networks to compensate for mismatches between assumed and actual object motion. Second, the Motion-adaptive IoU (Mo-IoU) adaptively expands the matching space and incorporates height similarity to reduce the influence of detection errors and association failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of 67.9\% and 82.1\%, respectively.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2508.01752.pdf' target='_blank'>https://arxiv.org/pdf/2508.01752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumail Abbas, Zeeshan Afzal, Aqeel Raza, Taha Mansouri, Andrew W. Dowsey, Chaidate Inchaisri, Ali Alameer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01752">Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Activity and behaviour correlate with dairy cow health and welfare, making continual and accurate monitoring crucial for disease identification and farm productivity. Manual observation and frequent assessments are laborious and inconsistent for activity monitoring. In this study, we developed a unique multi-camera, real-time tracking system for indoor-housed Holstein Friesian dairy cows. This technology uses cutting-edge computer vision techniques, including instance segmentation and tracking algorithms to monitor cow activity seamlessly and accurately. An integrated top-down barn panorama was created by geometrically aligning six camera feeds using homographic transformations. The detection phase used a refined YOLO11-m model trained on an overhead cow dataset, obtaining high accuracy (mAP\@0.50 = 0.97, F1 = 0.95). SAMURAI, an upgraded Segment Anything Model 2.1, generated pixel-precise cow masks for instance segmentation utilizing zero-shot learning and motion-aware memory. Even with occlusion and fluctuating posture, a motion-aware Linear Kalman filter and IoU-based data association reliably identified cows over time for object tracking. The proposed system significantly outperformed Deep SORT Realtime. Multi-Object Tracking Accuracy (MOTA) was 98.7% and 99.3% in two benchmark video sequences, with IDF1 scores above 99% and near-zero identity switches. This unified multi-camera system can track dairy cows in complex interior surroundings in real time, according to our data. The system reduces redundant detections across overlapping cameras, maintains continuity as cows move between viewpoints, with the aim of improving early sickness prediction through activity quantification and behavioural classification.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2507.05229.pdf' target='_blank'>https://arxiv.org/pdf/2507.05229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markiyan Kostiv, Anatolii Adamovskyi, Yevhen Cherniavskyi, Mykyta Varenyk, Ostap Viniavskyi, Igor Krashenyi, Oles Dobosevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05229">Self-Supervised Real-Time Tracking of Military Vehicles in Low-FPS UAV Footage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to maintain consistent identities of objects across video frames. Associating objects in low-frame-rate videos captured by moving unmanned aerial vehicles (UAVs) in actual combat scenarios is complex due to rapid changes in object appearance and position within the frame. The task becomes even more challenging due to image degradation caused by cloud video streaming and compression algorithms. We present how instance association learning from single-frame annotations can overcome these challenges. We show that global features of the scene provide crucial context for low-FPS instance association, allowing our solution to be robust to distractors and gaps in detections. We also demonstrate that such a tracking approach maintains high association quality even when reducing the input image resolution and latent representation size for faster inference. Finally, we present a benchmark dataset of annotated military vehicles collected from publicly available data sources. This paper was initially presented at the NATO Science and Technology Organization Symposium (ICMCIS) organized by the Information Systems Technology (IST)Scientific and Technical Committee, IST-209-RSY - the ICMCIS, held in Oeiras, Portugal, 13-14 May 2025.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2506.21980.pdf' target='_blank'>https://arxiv.org/pdf/2506.21980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Wang, Wenwen Li, Jiawei Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21980">R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual single object tracking aims to continuously localize and estimate the scale of a target in subsequent video frames, given only its initial state in the first frame. This task has traditionally been framed as a template matching problem, evolving through major phases including correlation filters, two-stream networks, and one-stream networks with significant progress achieved. However, these methods typically require explicit classification and regression modeling, depend on supervised training with large-scale datasets, and are limited to the single task of tracking, lacking flexibility. In recent years, multi-modal large language models (MLLMs) have advanced rapidly. Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational capabilities, demonstrate excellent performance in grounding tasks. This has spurred interest in applying such models directly to visual tracking. However, experiments reveal that Qwen2.5-VL struggles with template matching between image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement learning method on a small-scale dataset with a rule-based reward function. The resulting model, R1-Track, achieved notable performance on the GOT-10k benchmark. R1-Track supports flexible initialization via bounding boxes or text descriptions while retaining most of the original model's general capabilities. And we further discuss potential improvements for R1-Track. This rough technical report summarizes our findings as of May 2025.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2506.19154.pdf' target='_blank'>https://arxiv.org/pdf/2506.19154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Falaki, Maria A. Amer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19154">Lightweight RGB-T Tracking with Mobile Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-modality object tracking (e.g., RGB-only) encounters difficulties in challenging imaging conditions, such as low illumination and adverse weather conditions. To solve this, multimodal tracking (e.g., RGB-T models) aims to leverage complementary data such as thermal infrared features. While recent Vision Transformer-based multimodal trackers achieve strong performance, they are often computationally expensive due to large model sizes. In this work, we propose a novel lightweight RGB-T tracking algorithm based on Mobile Vision Transformers (MobileViT). Our tracker introduces a progressive fusion framework that jointly learns intra-modal and inter-modal interactions between the template and search regions using separable attention. This design produces effective feature representations that support more accurate target localization while achieving a small model size and fast inference speed. Compared to state-of-the-art efficient multimodal trackers, our model achieves comparable accuracy while offering significantly lower parameter counts (less than 4 million) and the fastest GPU inference speed of 122 frames per second. This paper is the first to propose a tracker using Mobile Vision Transformers for RGB-T tracking and multimodal tracking at large. Tracker code and model weights will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2506.14256.pdf' target='_blank'>https://arxiv.org/pdf/2506.14256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deepak Ghimire, Joonwhoan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14256">Comparison of Two Methods for Stationary Incident Detection Based on Background Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2506.07850.pdf' target='_blank'>https://arxiv.org/pdf/2506.07850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arash Rocky, Q. M. Jonathan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07850">SAM2Auto: Auto Annotation Using FLASH</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) lag behind Large Language Models due to the scarcity of annotated datasets, as creating paired visual-textual annotations is labor-intensive and expensive. To address this bottleneck, we introduce SAM2Auto, the first fully automated annotation pipeline for video datasets requiring no human intervention or dataset-specific training. Our approach consists of two key components: SMART-OD, a robust object detection system that combines automatic mask generation with open-world object detection capabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a multi-object real-time video instance segmentation (VIS) that maintains consistent object identification across video frames even with intermittent detection gaps. Unlike existing open-world detection methods that require frame-specific hyperparameter tuning and suffer from numerous false positives, our system employs statistical approaches to minimize detection errors while ensuring consistent object tracking throughout entire video sequences. Extensive experimental validation demonstrates that SAM2Auto achieves comparable accuracy to manual annotation while dramatically reducing annotation time and eliminating labor costs. The system successfully handles diverse datasets without requiring retraining or extensive parameter adjustments, making it a practical solution for large-scale dataset creation. Our work establishes a new baseline for automated video annotation and provides a pathway for accelerating VLM development by addressing the fundamental dataset bottleneck that has constrained progress in vision-language understanding.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2505.22677.pdf' target='_blank'>https://arxiv.org/pdf/2505.22677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jisu Kim, Alex Mattingly, Eung-Joo Lee, Benjamin S. Riggan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22677">Using Cross-Domain Detection Loss to Infer Multi-Scale Information for Improved Tiny Head Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head detection and tracking are essential for downstream tasks, but current methods often require large computational budgets, which increase latencies and ties up resources (e.g., processors, memory, and bandwidth). To address this, we propose a framework to enhance tiny head detection and tracking by optimizing the balance between performance and efficiency. Our framework integrates (1) a cross-domain detection loss, (2) a multi-scale module, and (3) a small receptive field detection mechanism. These innovations enhance detection by bridging the gap between large and small detectors, capturing high-frequency details at multiple scales during training, and using filters with small receptive fields to detect tiny heads. Evaluations on the CroHD and CrowdHuman datasets show improved Multiple Object Tracking Accuracy (MOTA) and mean Average Precision (mAP), demonstrating the effectiveness of our approach in crowded scenes.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2504.18165.pdf' target='_blank'>https://arxiv.org/pdf/2504.18165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michel Gokan Khan, Renan Guarese, Fabian Johnson, Xi Vincent Wang, Anders Bergman, Benjamin Edvinsson, Mario Romero, JÃ©rÃ©my Vachier, Jan Kronqvist
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18165">PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2504.11310.pdf' target='_blank'>https://arxiv.org/pdf/2504.11310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayong Liu, Qingrui Zhang, Zeyang Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11310">Intelligent driving vehicle front multi-target tracking and detection based on YOLOv5 and point cloud 3D projection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-target tracking and detection tasks, it is necessary to continuously track multiple targets, such as vehicles, pedestrians, etc. To achieve this goal, the system must be able to continuously acquire and process image frames containing these targets. These consecutive frame images enable the algorithm to update the position and state of the target in real-time in each frame of the image. How to accurately associate the detected target with the target in the previous or next frame to form a stable trajectory is a complex problem. Therefore, a multi object tracking and detection method for intelligent driving vehicles based on YOLOv5 and point cloud 3D projection is proposed. Using Retinex algorithm to enhance the image of the environment in front of the vehicle, remove light interference in the image, and build an intelligent detection model based on YOLOv5 network structure. The enhanced image is input into the model, and multiple targets in front of the vehicle are identified through feature extraction and target localization. By combining point cloud 3D projection technology, the correlation between the position changes of adjacent frame images in the projection coordinate system can be inferred. By sequentially projecting the multi-target recognition results of multiple consecutive frame images into the 3D laser point cloud environment, effective tracking of the motion trajectories of all targets in front of the vehicle can be achieved. The experimental results show that the application of this method for intelligent driving vehicle front multi-target tracking and detection yields a MOTA (Tracking Accuracy) value greater than 30, demonstrating its superior tracking and detection performance.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2504.02519.pdf' target='_blank'>https://arxiv.org/pdf/2504.02519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Alexander Holz, Christian Bader, Markus Enzweiler, Matthias DrÃ¼ppel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02519">Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents novel Machine Learning (ML) methodologies for Multi-Object Tracking (MOT), specifically designed to meet the increasing complexity and precision demands of Advanced Driver Assistance Systems (ADAS). We introduce three Neural Network (NN) models that address key challenges in MOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii) the Single-Association Network (SANT) for mapping individual Sensor Object (SO) to existing tracks, and (iii) the Multi-Association Network (MANTa) for associating multiple SOs to multiple tracks. These models are seamlessly integrated into a traditional Kalman Filter (KF) framework, maintaining the system's modularity by replacing relevant components without disrupting the overall architecture. Importantly, all three networks are designed to be run in a realtime, embedded environment. Each network contains less than 50k trainable parameters. Our evaluation, conducted on the public KITTI tracking dataset, demonstrates significant improvements in tracking performance. SPENT reduces the Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT and MANTa achieve up to 95% accuracy in sensor object-to-track assignments. These results underscore the effectiveness of incorporating task-specific NNs into traditional tracking systems, boosting performance and robustness while preserving modularity, maintainability, and interpretability.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2503.04500.pdf' target='_blank'>https://arxiv.org/pdf/2503.04500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Hsi Chen, Chin-Tien Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04500">ReynoldsFlow: Exquisite Flow Estimation via Reynolds Transport Theorem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical flow is a fundamental technique for motion estimation, widely applied in video stabilization, interpolation, and object tracking. Traditional optical flow estimation methods rely on restrictive assumptions like brightness constancy and slow motion constraints. Recent deep learning-based flow estimations require extensive training on large domain-specific datasets, making them computationally demanding. Also, artificial intelligence (AI) advances have enabled deep learning models to take advantage of optical flow as an important feature for object tracking and motion analysis. Since optical flow is commonly encoded in HSV for visualization, its conversion to RGB for neural network processing is nonlinear and may introduce perceptual distortions. These transformations amplify the sensitivity to estimation errors, potentially affecting the predictive accuracy of the networks. To address these challenges that are influential to the performance of downstream network models, we propose Reynolds flow, a novel training-free flow estimation inspired by the Reynolds transport theorem, offering a principled approach to modeling complex motion dynamics. In addition to conventional HSV-based visualization of Reynolds flow, we also introduce an RGB-encoded representation of Reynolds flow designed to improve flow visualization and feature enhancement for neural networks. We evaluated the effectiveness of Reynolds flow in video-based tasks. Experimental results on three benchmarks, tiny object detection on UAVDB, infrared object detection on Anti-UAV, and pose estimation on GolfDB, demonstrate that networks trained with RGB-encoded Reynolds flow achieve SOTA performance, exhibiting improved robustness and efficiency across all tasks.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2502.18867.pdf' target='_blank'>https://arxiv.org/pdf/2502.18867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akhil Penta, Vaibhav Adwani, Ankush Chopra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18867">Enhanced Transformer-Based Tracking for Skiing Events: Overcoming Multi-Camera Challenges, Scale Variations and Rapid Motion -- SkiTB Visual Tracking Challenge 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate skier tracking is essential for performance analysis, injury prevention, and optimizing training strategies in alpine sports. Traditional tracking methods often struggle with occlusions, dynamic movements, and varying environmental conditions, limiting their effectiveness. In this work, we used STARK (Spatio-Temporal Transformer Network for Visual Tracking), a transformer-based model, to track skiers. We adapted STARK to address domain-specific challenges such as camera movements, camera changes, occlusions, etc. by optimizing the model's architecture and hyperparameters to better suit the dataset.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2502.04478.pdf' target='_blank'>https://arxiv.org/pdf/2502.04478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luiz C. S. de Araujo, Carlos M. S. Figueiredo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04478">OneTrack-M: A multitask approach to transformer-based MOT models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) is a critical problem in computer vision, essential for understanding how objects move and interact in videos. This field faces significant challenges such as occlusions and complex environmental dynamics, impacting model accuracy and efficiency. While traditional approaches have relied on Convolutional Neural Networks (CNNs), introducing transformers has brought substantial advancements. This work introduces OneTrack-M, a transformer-based MOT model designed to enhance tracking computational efficiency and accuracy. Our approach simplifies the typical transformer-based architecture by eliminating the need for a decoder model for object detection and tracking. Instead, the encoder alone serves as the backbone for temporal data interpretation, significantly reducing processing time and increasing inference speed. Additionally, we employ innovative data pre-processing and multitask training techniques to address occlusion and diverse objective challenges within a single set of weights. Experimental results demonstrate that OneTrack-M achieves at least 25% faster inference times compared to state-of-the-art models in the literature while maintaining or improving tracking accuracy metrics. These improvements highlight the potential of the proposed solution for real-time applications such as autonomous vehicles, surveillance systems, and robotics, where rapid responses are crucial for system effectiveness.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2412.10453.pdf' target='_blank'>https://arxiv.org/pdf/2412.10453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailas PS, Selvakumaran R, Palani Murugan, Ramesh Kumar, Malaya Kumar Biswal M
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10453">Analysis of Object Detection Models for Tiny Object in Satellite Imagery: A Dataset-Centric Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, significant advancements have been made in deep learning-based object detection algorithms, revolutionizing basic computer vision tasks, notably in object detection, tracking, and segmentation. This paper delves into the intricate domain of Small-Object-Detection (SOD) within satellite imagery, highlighting the unique challenges stemming from wide imaging ranges, object distribution, and their varying appearances in bird's-eye-view satellite images. Traditional object detection models face difficulties in detecting small objects due to limited contextual information and class imbalances. To address this, our research presents a meticulously curated dataset comprising 3000 images showcasing cars, ships, and airplanes in satellite imagery. Our study aims to provide valuable insights into small object detection in satellite imagery by empirically evaluating state-of-the-art models. Furthermore, we tackle the challenges of satellite video-based object tracking, employing the Byte Track algorithm on the SAT-MTB dataset. Through rigorous experimentation, we aim to offer a comprehensive understanding of the efficacy of state-of-the-art models in Small-Object-Detection for satellite applications. Our findings shed light on the effectiveness of these models and pave the way for future advancements in satellite imagery analysis.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2412.01119.pdf' target='_blank'>https://arxiv.org/pdf/2412.01119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mojtaba S. Fazli, Shannon Quinn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01119">Object Tracking in a $360^o$ View: A Novel Perspective on Bridging the Gap to Biomedical Advancements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a fundamental tool in modern innovation, with applications in defense systems, autonomous vehicles, and biomedical research. It enables precise identification, monitoring, and spatiotemporal analysis of objects across sequential frames, providing insights into dynamic behaviors. In cell biology, object tracking is vital for uncovering cellular mechanisms, such as migration, interactions, and responses to drugs or pathogens. These insights drive breakthroughs in understanding disease progression and therapeutic interventions.
  Over time, object tracking methods have evolved from traditional feature-based approaches to advanced machine learning and deep learning frameworks. While classical methods are reliable in controlled settings, they struggle in complex environments with occlusions, variable lighting, and high object density. Deep learning models address these challenges by delivering greater accuracy, adaptability, and robustness.
  This review categorizes object tracking techniques into traditional, statistical, feature-based, and machine learning paradigms, with a focus on biomedical applications. These methods are essential for tracking cells and subcellular structures, advancing our understanding of health and disease. Key performance metrics, including accuracy, efficiency, and adaptability, are discussed. The paper explores limitations of current methods and highlights emerging trends to guide the development of next-generation tracking systems for biomedical research and broader scientific domains.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2411.19134.pdf' target='_blank'>https://arxiv.org/pdf/2411.19134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peilin Tian, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19134">Visual SLAMMOT Considering Multiple Motion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous Localization and Mapping (SLAM) and Multi-Object Tracking (MOT) are pivotal tasks in the realm of autonomous driving, attracting considerable research attention. While SLAM endeavors to generate real-time maps and determine the vehicle's pose in unfamiliar settings, MOT focuses on the real-time identification and tracking of multiple dynamic objects. Despite their importance, the prevalent approach treats SLAM and MOT as independent modules within an autonomous vehicle system, leading to inherent limitations. Classical SLAM methodologies often rely on a static environment assumption, suitable for indoor rather than dynamic outdoor scenarios. Conversely, conventional MOT techniques typically rely on the vehicle's known state, constraining the accuracy of object state estimations based on this prior. To address these challenges, previous efforts introduced the unified SLAMMOT paradigm, yet primarily focused on simplistic motion patterns. In our team's previous work IMM-SLAMMOT\cite{IMM-SLAMMOT}, we present a novel methodology incorporating consideration of multiple motion models into SLAMMOT i.e. tightly coupled SLAM and MOT, demonstrating its efficacy in LiDAR-based systems. This paper studies feasibility and advantages of instantiating this methodology as visual SLAMMOT, bridging the gap between LiDAR and vision-based sensing mechanisms. Specifically, we propose a solution of visual SLAMMOT considering multiple motion models and validate the inherent advantages of IMM-SLAMMOT in the visual domain.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2411.18443.pdf' target='_blank'>https://arxiv.org/pdf/2411.18443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Lichtenfeld, Kevin Daun, Oskar von Stryk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18443">Efficient Dynamic LiDAR Odometry for Mobile Robots with Structured Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a real-time dynamic LiDAR odometry pipeline for mobile robots in Urban Search and Rescue (USAR) scenarios. Existing approaches to dynamic object detection often rely on pretrained learned networks or computationally expensive volumetric maps. To enhance efficiency on computationally limited robots, we reuse data between the odometry and detection module. Utilizing a range image segmentation technique and a novel residual-based heuristic, our method distinguishes dynamic from static objects before integrating them into the point cloud map. The approach demonstrates robust object tracking and improved map accuracy in environments with numerous dynamic objects. Even highly non-rigid objects, such as running humans, are accurately detected at point level without prior downsampling of the point cloud and hence, without loss of information. Evaluation on simulated and real-world data validates its computational efficiency. Compared to a state-of-the-art volumetric method, our approach shows comparable detection performance at a fraction of the processing time, adding only 14 ms to the odometry module for dynamic object detection and tracking. The implementation and a new real-world dataset are available as open-source for further research.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2410.20079.pdf' target='_blank'>https://arxiv.org/pdf/2410.20079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>InPyo Song, Jangwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20079">SFTrack: A Robust Scale and Motion Adaptive Algorithm for Tracking Small and Fast Moving Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of multi-object tracking in Unmanned Aerial Vehicle (UAV) footage. It plays a critical role in various UAV applications, including traffic monitoring systems and real-time suspect tracking by the police. However, this task is highly challenging due to the fast motion of UAVs, as well as the small size of target objects in the videos caused by the high-altitude and wide angle views of drones. In this study, we thus introduce a simple yet more effective method compared to previous work to overcome these challenges. Our approach involves a new tracking strategy, which initiates the tracking of target objects from low-confidence detections commonly encountered in UAV application scenarios. Additionally, we propose revisiting traditional appearance-based matching algorithms to improve the association of low-confidence detections. To evaluate the effectiveness of our method, we conducted benchmark evaluations on two UAV-specific datasets (VisDrone2019, UAVDT) and one general object tracking dataset (MOT17). The results demonstrate that our approach surpasses current state-of-the art methodologies, highlighting its robustness and adaptability in diverse tracking environments. Furthermore, we have improved the annotation of the UAVDT dataset by rectifying several errors and addressing omissions found in the original annotations. We will provide this refined version of the dataset to facilitate better benchmarking in the field.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2410.08769.pdf' target='_blank'>https://arxiv.org/pdf/2410.08769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan MÃ¼ller, Adrian Pigors
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08769">Efficient Multi-Object Tracking on Edge Devices via Reconstruction-Based Channel Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of multi-object tracking (MOT) technologies presents the dual challenge of maintaining high performance while addressing critical security and privacy concerns. In applications such as pedestrian tracking, where sensitive personal data is involved, the potential for privacy violations and data misuse becomes a significant issue if data is transmitted to external servers. To mitigate these risks, processing data directly on an edge device, such as a smart camera, has emerged as a viable solution. Edge computing ensures that sensitive information remains local, thereby aligning with stringent privacy principles and significantly reducing network latency. However, the implementation of MOT on edge devices is not without its challenges. Edge devices typically possess limited computational resources, necessitating the development of highly optimized algorithms capable of delivering real-time performance under these constraints. The disparity between the computational requirements of state-of-the-art MOT algorithms and the capabilities of edge devices emphasizes a significant obstacle. To address these challenges, we propose a neural network pruning method specifically tailored to compress complex networks, such as those used in modern MOT systems. This approach optimizes MOT performance by ensuring high accuracy and efficiency within the constraints of limited edge devices, such as NVIDIA's Jetson Orin Nano. By applying our pruning method, we achieve model size reductions of up to 70% while maintaining a high level of accuracy and further improving performance on the Jetson Orin Nano, demonstrating the effectiveness of our approach for edge computing applications.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2409.17533.pdf' target='_blank'>https://arxiv.org/pdf/2409.17533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Limanta, Kuniaki Uto, Koichi Shinoda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17533">CAMOT: Camera Angle-aware Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes CAMOT, a simple camera angle estimator for multi-object tracking to tackle two problems: 1) occlusion and 2) inaccurate distance estimation in the depth direction. Under the assumption that multiple objects are located on a flat plane in each video frame, CAMOT estimates the camera angle using object detection. In addition, it gives the depth of each object, enabling pseudo-3D MOT. We evaluated its performance by adding it to various 2D MOT methods on the MOT17 and MOT20 datasets and confirmed its effectiveness. Applying CAMOT to ByteTrack, we obtained 63.8% HOTA, 80.6% MOTA, and 78.5% IDF1 in MOT17, which are state-of-the-art results. Its computational cost is significantly lower than the existing deep-learning-based depth estimators for tracking.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2409.09841.pdf' target='_blank'>https://arxiv.org/pdf/2409.09841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oriel Perl, Ido Leshem, Uria Franko, Yuval Goldman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09841">Tracking Virtual Meetings in the Wild: Re-identification in Multi-Participant Virtual Meetings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, workplaces and educational institutes have widely adopted virtual meeting platforms. This has led to a growing interest in analyzing and extracting insights from these meetings, which requires effective detection and tracking of unique individuals. In practice, there is no standardization in video meetings recording layout, and how they are captured across the different platforms and services. This, in turn, creates a challenge in acquiring this data stream and analyzing it in a uniform fashion. Our approach provides a solution to the most general form of video recording, usually consisting of a grid of participants (\cref{fig:videomeeting}) from a single video source with no metadata on participant locations, while using the least amount of constraints and assumptions as to how the data was acquired. Conventional approaches often use YOLO models coupled with tracking algorithms, assuming linear motion trajectories akin to that observed in CCTV footage. However, such assumptions fall short in virtual meetings, where participant video feed window can abruptly change location across the grid. In an organic video meeting setting, participants frequently join and leave, leading to sudden, non-linear movements on the video grid. This disrupts optical flow-based tracking methods that depend on linear motion. Consequently, standard object detection and tracking methods might mistakenly assign multiple participants to the same tracker. In this paper, we introduce a novel approach to track and re-identify participants in remote video meetings, by utilizing the spatio-temporal priors arising from the data in our domain. This, in turn, increases tracking capabilities compared to the use of general object tracking. Our approach reduces the error rate by 95% on average compared to YOLO-based tracking methods as a baseline.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2408.13243.pdf' target='_blank'>https://arxiv.org/pdf/2408.13243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandru Niculescu-Mizil, Deep Patel, Iain Melvin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13243">MCTR: Multi Camera Tracking Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-camera tracking plays a pivotal role in various real-world applications. While end-to-end methods have gained significant interest in single-camera tracking, multi-camera tracking remains predominantly reliant on heuristic techniques. In response to this gap, this paper introduces Multi-Camera Tracking tRansformer (MCTR), a novel end-to-end approach tailored for multi-object detection and tracking across multiple cameras with overlapping fields of view. MCTR leverages end-to-end detectors like DEtector TRansformer (DETR) to produce detections and detection embeddings independently for each camera view. The framework maintains set of track embeddings that encaplusate global information about the tracked objects, and updates them at every frame by integrating the local information from the view-specific detection embeddings. The track embeddings are probabilistically associated with detections in every camera view and frame to generate consistent object tracks. The soft probabilistic association facilitates the design of differentiable losses that enable end-to-end training of the entire system. To validate our approach, we conduct experiments on MMPTrack and AI City Challenge, two recently introduced large-scale multi-camera multi-object tracking datasets.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2408.07344.pdf' target='_blank'>https://arxiv.org/pdf/2408.07344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Guo, Rujie Liu, Narishige Abe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07344">RTAT: A Robust Two-stage Association Tracker for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data association is an essential part in the tracking-by-detection based Multi-Object Tracking (MOT). Most trackers focus on how to design a better data association strategy to improve the tracking performance. The rule-based handcrafted association methods are simple and highly efficient but lack generalization capability to deal with complex scenes. While the learnt association methods can learn high-order contextual information to deal with various complex scenes, but they have the limitations of higher complexity and cost. To address these limitations, we propose a Robust Two-stage Association Tracker, named RTAT. The first-stage association is performed between tracklets and detections to generate tracklets with high purity, and the second-stage association is performed between tracklets to form complete trajectories. For the first-stage association, we use a simple data association strategy to generate tracklets with high purity by setting a low threshold for the matching cost in the assignment process. We conduct the tracklet association in the second-stage based on the framework of message-passing GNN. Our method models the tracklet association as a series of edge classification problem in hierarchical graphs, which can recursively merge short tracklets into longer ones. Our tracker RTAT ranks first on the test set of MOT17 and MOT20 benchmarks in most of the main MOT metrics: HOTA, IDF1, and AssA. We achieve 67.2 HOTA, 84.7 IDF1, and 69.7 AssA on MOT17, and 66.2 HOTA, 82.5 IDF1, and 68.1 AssA on MOT20.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2408.00417.pdf' target='_blank'>https://arxiv.org/pdf/2408.00417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Gramsch, Shishan Yang, Hosam Alqaderi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00417">A Batch Update Using Multiplicative Noise Modelling for Extended Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the tracking of multiple extended targets demands for sophisticated algorithms to handle the high complexity inherent to the task, it also requires low runtime for online execution in real-world scenarios. In this work, we derive a batch update for the recently introduced elliptical-target tracker called MEM-EKF*. The MEM-EKF* is based on the same likelihood as the well-established random matrix approach but is derived from the multiplicative error model (MEM) and uses an extended Kalman filter (EKF) to update the target state sequentially, i.e., measurement-by-measurement. Our batch variant updates the target state in a single step based on straightforward sums over all measurements and the MEM-specific pseudo-measurements. This drastically reduces the scaling constant for typical implementations and indeed we find a speedup of roughly 100x in our numerical experiments. At the same time, the estimation error which we measure using the Gaussian Wasserstein distance stays significantly below that of the random matrix approach in coordinated turn scenarios while being comparable otherwise.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2407.20623.pdf' target='_blank'>https://arxiv.org/pdf/2407.20623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Varini, Joel H. Gayford, Jeremy Jenrette, Matthew J. Witt, Francesco Garzon, Francesco Ferretti, Sophie Wilday, Mark E. Bond, Michael R. Heithaus, Danielle Robinson, Devon Carter, Najee Gumbs, Vincent Webster, Ben Glocker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20623">SharkTrack: an accurate, generalisable software for streamlining shark and ray underwater video analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Elasmobranchs (shark sand rays) represent a critical component of marine ecosystems. Yet, they are experiencing global population declines and effective monitoring of populations is essential to their protection. Underwater stationary videos, such as those from Baited Remote Underwater Video Stations (BRUVS), are critical for understanding elasmobranch spatial ecology and abundance. However, processing these videos requires time-consuming manual analysis that can delay conservation. To address this challenge, we developed SharkTrack, a semi-automatic underwater video analysis software. SharkTrack uses Convolutional Neural Networks (CNN) and Multi-Object Tracking to automatically detect and track elasmobranchs and provides an annotation pipeline to manually classify elasmobranch species and compute species-specific MaxN (ssMaxN), the standard metric of relative abundance. When tested on BRUVS footage from locations unseen by the CNN model during training, SharkTrack computed ssMaxN with 89% accuracy over 207 hours of footage. The semi-automatic SharkTrack pipeline required two minutes of manual classification per hour of video, an estimated 95% reduction of manual analysis time compared to traditional methods. Furthermore, we demonstrate SharkTrack accuracy across diverse marine ecosystems and elasmobranch species, an advancement compared to previous models, which were limited to specific species or locations. SharkTrack applications extend beyond BRUVS, facilitating the analysis of any underwater stationary video. By making video analysis faster and more accessible, SharkTrack enables research and conservation organisations to monitor elasmobranch populations more efficiently, thereby improving conservation efforts. To further support these goals, we provide public access to the SharkTrack software.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2407.20446.pdf' target='_blank'>https://arxiv.org/pdf/2407.20446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaid A. El Shair, Samir A. Rawashdeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20446">MEVDT: Multi-Modal Event-Based Vehicle Detection and Tracking Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this data article, we introduce the Multi-Modal Event-based Vehicle Detection and Tracking (MEVDT) dataset. This dataset provides a synchronized stream of event data and grayscale images of traffic scenes, captured using the Dynamic and Active-Pixel Vision Sensor (DAVIS) 240c hybrid event-based camera. MEVDT comprises 63 multi-modal sequences with approximately 13k images, 5M events, 10k object labels, and 85 unique object tracking trajectories. Additionally, MEVDT includes manually annotated ground truth labels $\unicode{x2014}$ consisting of object classifications, pixel-precise bounding boxes, and unique object IDs $\unicode{x2014}$ which are provided at a labeling frequency of 24 Hz. Designed to advance the research in the domain of event-based vision, MEVDT aims to address the critical need for high-quality, real-world annotated datasets that enable the development and evaluation of object detection and tracking algorithms in automotive environments.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2407.17521.pdf' target='_blank'>https://arxiv.org/pdf/2407.17521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edoardo Cittadini, Alessandro De Siena, Giorgio Buttazzo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17521">CORT: Class-Oriented Real-time Tracking for Embedded Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ever-increasing use of artificial intelligence in autonomous systems has significantly contributed to advance the research on multi-object tracking, adopted in several real-time applications (e.g., autonomous driving, surveillance drones, robotics) to localize and follow the trajectory of multiple objects moving in front of a camera. Current tracking algorithms can be divided into two main categories: some approaches introduce complex heuristics and re-identification models to improve the tracking accuracy and reduce the number of identification switches, without particular attention to the timing performance, whereas other approaches are aimed at reducing response times by removing the re-identification phase, thus penalizing the tracking accuracy. This work proposes a new approach to multi-class object tracking that allows achieving smaller and more predictable execution times, without penalizing the tracking performance. The idea is to reduce the problem of matching predictions with detections into smaller sub-problems by splitting the Hungarian matrix by class and invoking the second re-identification stage only when strictly necessary for a smaller number of elements. The proposed solution was evaluated in complex urban scenarios with several objects of different types (as cars, trucks, bikes, and pedestrians), showing the effectiveness of the multi-class approach with respect to state of the art trackers.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2407.12614.pdf' target='_blank'>https://arxiv.org/pdf/2407.12614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Liu, Congliang Zhou, Won Suk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12614">Strawberry detection and counting based on YOLOv7 pruning and information based tracking algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The strawberry industry yields significant economic benefits for Florida, yet the process of monitoring strawberry growth and yield is labor-intensive and costly. The development of machine learning-based detection and tracking methodologies has been used for helping automated monitoring and prediction of strawberry yield, still, enhancement has been limited as previous studies only applied the deep learning method for flower and fruit detection, which did not consider the unique characteristics of image datasets collected by the machine vision system. This study proposed an optimal pruning of detection heads of the deep learning model (YOLOv7 and its variants) that could achieve fast and precise strawberry flower, immature fruit, and mature fruit detection. Thereafter, an enhanced object tracking algorithm, which is called the Information Based Tracking Algorithm (IBTA) utilized the best detection result, removed the Kalman Filter, and integrated moving direction, velocity, and spatial information to improve the precision in strawberry flower and fruit tracking. The proposed pruning of detection heads across YOLOv7 variants, notably Pruning-YOLOv7-tiny with detection head 3 and Pruning-YOLOv7-tiny with heads 2 and 3 achieved the best inference speed (163.9 frames per second) and detection accuracy (89.1%), respectively. On the other hand, the effect of IBTA was proved by comparing it with the centroid tracking algorithm (CTA), the Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP) of IBTA were 12.3% and 6.0% higher than that of CTA, accordingly. In addition, other object-tracking evaluation metrics, including IDF1, IDR, IDP, MT, and IDs, show that IBTA performed better than CTA in strawberry flower and fruit tracking.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2406.08294.pdf' target='_blank'>https://arxiv.org/pdf/2406.08294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasod Ginige, Ransika Gunasekara, Darsha Hewavitharana, Manjula Ariyarathne, Ranga Rodrigo, Peshala Jayasekara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08294">Vessel Re-identification and Activity Detection in Thermal Domain for Maritime Surveillance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maritime surveillance is vital to mitigate illegal activities such as drug smuggling, illegal fishing, and human trafficking. Vision-based maritime surveillance is challenging mainly due to visibility issues at night, which results in failures in re-identifying vessels and detecting suspicious activities. In this paper, we introduce a thermal, vision-based approach for maritime surveillance with object tracking, vessel re-identification, and suspicious activity detection capabilities. For vessel re-identification, we propose a novel viewpoint-independent algorithm which compares features of the sides of the vessel separately (separate side-spaces) leveraging shape information in the absence of color features. We propose techniques to adapt tracking and activity detection algorithms for the thermal domain and train them using a thermal dataset we created. This dataset will be the first publicly available benchmark dataset for thermal maritime surveillance. Our system is capable of re-identifying vessels with an 81.8% Top1 score and identifying suspicious activities with a 72.4\% frame mAP score; a new benchmark for each task in the thermal domain.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2406.07680.pdf' target='_blank'>https://arxiv.org/pdf/2406.07680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duc Pham, Matthew Hansen, FÃ©licie Dhellemmes, Jens Krause, Pia Bideau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07680">Watching Swarm Dynamics from Above: A Framework for Advanced Object Tracking in Drone Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Easily accessible sensors, like drones with diverse onboard sensors, have greatly expanded studying animal behavior in natural environments. Yet, analyzing vast, unlabeled video data, often spanning hours, remains a challenge for machine learning, especially in computer vision. Existing approaches often analyze only a few frames. Our focus is on long-term animal behavior analysis. To address this challenge, we utilize classical probabilistic methods for state estimation, such as particle filtering. By incorporating recent advancements in semantic object segmentation, we enable continuous tracking of rapidly evolving object formations, even in scenarios with limited data availability. Particle filters offer a provably optimal algorithmic structure for recursively adding new incoming information. We propose a novel approach for tracking schools of fish in the open ocean from drone videos. Our framework not only performs classical object tracking in 2D, instead it tracks the position and spatial expansion of the fish school in world coordinates by fusing video data and the drone's on board sensor information (GPS and IMU). The presented framework for the first time allows researchers to study collective behavior of fish schools in its natural social and environmental context in a non-invasive and scalable way.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2405.14195.pdf' target='_blank'>https://arxiv.org/pdf/2405.14195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wei, Yujie He, Zhanchuan Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14195">Enhanced Object Tracking by Self-Supervised Auxiliary Depth Estimation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-D tracking significantly improves the accuracy of object tracking. However, its dependency on real depth inputs and the complexity involved in multi-modal fusion limit its applicability across various scenarios. The utilization of depth information in RGB-D tracking inspired us to propose a new method, named MDETrack, which trains a tracking network with an additional capability to understand the depth of scenes, through supervised or self-supervised auxiliary Monocular Depth Estimation learning. The outputs of MDETrack's unified feature extractor are fed to the side-by-side tracking head and auxiliary depth estimation head, respectively. The auxiliary module will be discarded in inference, thus keeping the same inference speed. We evaluated our models with various training strategies on multiple datasets, and the results show an improved tracking accuracy even without real depth. Through these findings we highlight the potential of depth estimation in enhancing object tracking performance.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2405.11655.pdf' target='_blank'>https://arxiv.org/pdf/2405.11655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tharun V. Puthanveettil, Fnu Obaid ur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11655">Track Anything Rapter(TAR)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a fundamental task in computer vision with broad practical applications across various domains, including traffic monitoring, robotics, and autonomous vehicle tracking. In this project, we aim to develop a sophisticated aerial vehicle system known as Track Anything Rapter (TAR), designed to detect, segment, and track objects of interest based on user-provided multimodal queries, such as text, images, and clicks. TAR utilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate the relative pose of the queried object. The tracking problem is approached as a Visual Servoing task, enabling the UAV to consistently focus on the object through advanced motion planning and control algorithms. We showcase how the integration of these foundational models with a custom high-level control algorithm results in a highly stable and precise tracking system deployed on a custom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the tracking algorithm's performance, we compare it against Vicon-based ground truth. Additionally, we evaluate the reliability of the foundational models in aiding tracking in scenarios involving occlusions. Finally, we test and validate the model's ability to work seamlessly with multiple modalities, such as click, bounding box, and image templates.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2405.10868.pdf' target='_blank'>https://arxiv.org/pdf/2405.10868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>P. Sarveswarasarma, T. Sathulakjan, V. J. V. Godfrey, Thanuja D. Ambegoda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10868">Air Signing and Privacy-Preserving Signature Verification for Digital Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to the digital signing of electronic documents through the use of a camera-based interaction system, single-finger tracking for sign recognition, and multi commands executing hand gestures. The proposed solution, referred to as "Air Signature," involves writing the signature in front of the camera, rather than relying on traditional methods such as mouse drawing or physically signing on paper and showing it to a web camera. The goal is to develop a state-of-the-art method for detecting and tracking gestures and objects in real-time. The proposed methods include applying existing gesture recognition and object tracking systems, improving accuracy through smoothing and line drawing, and maintaining continuity during fast finger movements. An evaluation of the fingertip detection, sketching, and overall signing process is performed to assess the effectiveness of the proposed solution. The secondary objective of this research is to develop a model that can effectively recognize the unique signature of a user. This type of signature can be verified by neural cores that analyze the movement, speed, and stroke pixels of the signing in real time. The neural cores use machine learning algorithms to match air signatures to the individual's stored signatures, providing a secure and efficient method of verification. Our proposed System does not require sensors or any hardware other than the camera.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2405.10444.pdf' target='_blank'>https://arxiv.org/pdf/2405.10444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Abdelaziz, Mohamed Sami Shehata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10444">A Novel Bounding Box Regression Method for Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Locating an object in a sequence of frames, given its appearance in the first frame of the sequence, is a hard problem that involves many stages. Usually, state-of-the-art methods focus on bringing novel ideas in the visual encoding or relational modelling phases. However, in this work, we show that bounding box regression from learned joint search and template features is of high importance as well. While previous methods relied heavily on well-learned features representing interactions between search and template, we hypothesize that the receptive field of the input convolutional bounding box network plays an important role in accurately determining the object location. To this end, we introduce two novel bounding box regression networks: inception and deformable. Experiments and ablation studies show that our inception module installed on the recent ODTrack outperforms the latter on three benchmarks: the GOT-10k, the UAV123 and the OTB2015.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2405.08996.pdf' target='_blank'>https://arxiv.org/pdf/2405.08996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Priya Sundaresan, Aditya Ganapathi, Harry Zhang, Shivin Devgon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08996">Learning Correspondence for Deformable Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the problem of pixelwise correspondence for deformable objects, namely cloth and rope, by comparing both classical and learning-based methods. We choose cloth and rope because they are traditionally some of the most difficult deformable objects to analytically model with their large configuration space, and they are meaningful in the context of robotic tasks like cloth folding, rope knot-tying, T-shirt folding, curtain closing, etc. The correspondence problem is heavily motivated in robotics, with wide-ranging applications including semantic grasping, object tracking, and manipulation policies built on top of correspondences. We present an exhaustive survey of existing classical methods for doing correspondence via feature-matching, including SIFT, SURF, and ORB, and two recently published learning-based methods including TimeCycle and Dense Object Nets. We make three main contributions: (1) a framework for simulating and rendering synthetic images of deformable objects, with qualitative results demonstrating transfer between our simulated and real domains (2) a new learning-based correspondence method extending Dense Object Nets, and (3) a standardized comparison across state-of-the-art correspondence methods. Our proposed method provides a flexible, general formulation for learning temporally and spatially continuous correspondences for nonrigid (and rigid) objects. We report root mean squared error statistics for all methods and find that Dense Object Nets outperforms baseline classical methods for correspondence, and our proposed extension of Dense Object Nets performs similarly.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2405.07272.pdf' target='_blank'>https://arxiv.org/pdf/2405.07272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Chen, Chunhua Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07272">MAML MOT: Multiple Object Tracking based on Meta-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of video analysis technology, the multi-object tracking (MOT) problem in complex scenes involving pedestrians is gaining increasing importance. This challenge primarily involves two key tasks: pedestrian detection and re-identification. While significant progress has been achieved in pedestrian detection tasks in recent years, enhancing the effectiveness of re-identification tasks remains a persistent challenge. This difficulty arises from the large total number of pedestrian samples in multi-object tracking datasets and the scarcity of individual instance samples. Motivated by recent rapid advancements in meta-learning techniques, we introduce MAML MOT, a meta-learning-based training approach for multi-object tracking. This approach leverages the rapid learning capability of meta-learning to tackle the issue of sample scarcity in pedestrian re-identification tasks, aiming to improve the model's generalization performance and robustness. Experimental results demonstrate that the proposed method achieves high accuracy on mainstream datasets in the MOT Challenge. This offers new perspectives and solutions for research in the field of pedestrian multi-object tracking.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2405.00023.pdf' target='_blank'>https://arxiv.org/pdf/2405.00023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A. Hossam, A. Ramadan, M. Magdy, R. Abdelwahab, S. Ashraf, Z. Mohamed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00023">Revolutionizing Retail Analytics: Advancing Inventory and Customer Insight with AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In response to the significant challenges facing the retail sector, including inefficient queue management, poor demand forecasting, and ineffective marketing, this paper introduces an innovative approach utilizing cutting-edge machine learning technologies. We aim to create an advanced smart retail analytics system (SRAS), leveraging these technologies to enhance retail efficiency and customer engagement. To enhance customer tracking capabilities, a new hybrid architecture is proposed integrating several predictive models. In the first stage of the proposed hybrid architecture for customer tracking, we fine-tuned the YOLOV8 algorithm using a diverse set of parameters, achieving exceptional results across various performance metrics. This fine-tuning process utilized actual surveillance footage from retail environments, ensuring its practical applicability. In the second stage, we explored integrating two sophisticated object-tracking models, BOT-SORT and ByteTrack, with the labels detected by YOLOV8. This integration is crucial for tracing customer paths within stores, which facilitates the creation of accurate visitor counts and heat maps. These insights are invaluable for understanding consumer behavior and improving store operations. To optimize inventory management, we delved into various predictive models, optimizing and contrasting their performance against complex retail data patterns. The GRU model, with its ability to interpret time-series data with long-range temporal dependencies, consistently surpassed other models like Linear Regression, showing 2.873% and 29.31% improvements in R2-score and mAPE, respectively.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2404.12133.pdf' target='_blank'>https://arxiv.org/pdf/2404.12133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julia Vinogradova, Gabor Fodor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12133">On Target Detection in the Presence of Clutter in Joint Communication and Sensing Cellular Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works on joint communication and sensing (JCAS) cellular networks have proposed to use time division mode (TDM) and concurrent mode (CM), as alternative methods for sharing the resources between communication and sensing signals. While the performance of these JCAS schemes for object tracking and parameter estimation has been studied in previous works, their performance on target detection in the presence of clutter has not been analyzed. In this paper, we propose a detection scheme for estimating the number of targets in JCAS cellular networks that employ TDM or CM resource sharing. The proposed detection method allows for the presence of clutter and/or temporally correlated noise. This scheme is studied with respect to the JCAS trade-off parameters that allow to control the time slots in TDM and the power resources in CM allocated to sensing and communications. The performance of two fundamental transmit beamforming schemes, typical for JCAS, is compared in terms of the receiver operating characteristics curves. Our results indicate that in general the TDM scheme gives a somewhat better detection performance compared to the CM scheme, although both schemes outperform existing approaches provided that their respective trade-off parameters are tuned properly.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2404.10992.pdf' target='_blank'>https://arxiv.org/pdf/2404.10992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Z. Alam, Zeeshan Kaleem, Sousso Kelouwani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10992">How to deal with glare for improved perception of Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision sensors are versatile and can capture a wide range of visual cues, such as color, texture, shape, and depth. This versatility, along with the relatively inexpensive availability of machine vision cameras, played an important role in adopting vision-based environment perception systems in autonomous vehicles (AVs). However, vision-based perception systems can be easily affected by glare in the presence of a bright source of light, such as the sun or the headlights of the oncoming vehicle at night or simply by light reflecting off snow or ice-covered surfaces; scenarios encountered frequently during driving. In this paper, we investigate various glare reduction techniques, including the proposed saturated pixel-aware glare reduction technique for improved performance of the computer vision (CV) tasks employed by the perception layer of AVs. We evaluate these glare reduction methods based on various performance metrics of the CV algorithms used by the perception layer. Specifically, we considered object detection, object recognition, object tracking, depth estimation, and lane detection which are crucial for autonomous driving. The experimental findings validate the efficacy of the proposed glare reduction approach, showcasing enhanced performance across diverse perception tasks and remarkable resilience against varying levels of glare.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2404.07467.pdf' target='_blank'>https://arxiv.org/pdf/2404.07467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kashish Jain, Manthan Juthani, Jash Jain, Anant V. Nimkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07467">Trashbusters: Deep Learning Approach for Litter Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The illegal disposal of trash is a major public health and environmental concern. Disposing of trash in unplanned places poses serious health and environmental risks. We should try to restrict public trash cans as much as possible. This research focuses on automating the penalization of litterbugs, addressing the persistent problem of littering in public places. Traditional approaches relying on manual intervention and witness reporting suffer from delays, inaccuracies, and anonymity issues. To overcome these challenges, this paper proposes a fully automated system that utilizes surveillance cameras and advanced computer vision algorithms for litter detection, object tracking, and face recognition. The system accurately identifies and tracks individuals engaged in littering activities, attaches their identities through face recognition, and enables efficient enforcement of anti-littering policies. By reducing reliance on manual intervention, minimizing human error, and providing prompt identification, the proposed system offers significant advantages in addressing littering incidents. The primary contribution of this research lies in the implementation of the proposed system, leveraging advanced technologies to enhance surveillance operations and automate the penalization of litterbugs.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2403.18193.pdf' target='_blank'>https://arxiv.org/pdf/2403.18193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiming Wang, Yongqiang Bai, Hongxing Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18193">Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-T tracking, a vital downstream task of object tracking, has made remarkable progress in recent years. Yet, it remains hindered by two major challenges: 1) the trade-off between performance and efficiency; 2) the scarcity of training data. To address the latter challenge, some recent methods employ prompts to fine-tune pre-trained RGB tracking models and leverage upstream knowledge in a parameter-efficient manner. However, these methods inadequately explore modality-independent patterns and disregard the dynamic reliability of different modalities in open scenarios. We propose M3PT, a novel RGB-T prompt tracking method that leverages middle fusion and multi-modal and multi-stage visual prompts to overcome these challenges. We pioneer the use of the adjustable middle fusion meta-framework for RGB-T tracking, which could help the tracker balance the performance with efficiency, to meet various demands of application. Furthermore, based on the meta-framework, we utilize multiple flexible prompt strategies to adapt the pre-trained model to comprehensive exploration of uni-modal patterns and improved modeling of fusion-modal features in diverse modality-priority scenarios, harnessing the potential of prompt learning in RGB-T tracking. Evaluating on 6 existing challenging benchmarks, our method surpasses previous state-of-the-art prompt fine-tuning methods while maintaining great competitiveness against excellent full-parameter fine-tuning methods, with only 0.34M fine-tuned parameters.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2403.07914.pdf' target='_blank'>https://arxiv.org/pdf/2403.07914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushan Han, Kaer Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07914">ACTrack: Adding Spatio-Temporal Condition for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently modeling spatio-temporal relations of objects is a key challenge in visual object tracking (VOT). Existing methods track by appearance-based similarity or long-term relation modeling, resulting in rich temporal contexts between consecutive frames being easily overlooked. Moreover, training trackers from scratch or fine-tuning large pre-trained models needs more time and memory consumption. In this paper, we present ACTrack, a new tracking framework with additive spatio-temporal conditions. It preserves the quality and capabilities of the pre-trained Transformer backbone by freezing its parameters, and makes a trainable lightweight additive net to model spatio-temporal relations in tracking. We design an additive siamese convolutional network to ensure the integrity of spatial features and perform temporal sequence modeling to simplify the tracking pipeline. Experimental results on several benchmarks prove that ACTrack could balance training efficiency and tracking performance.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2403.07635.pdf' target='_blank'>https://arxiv.org/pdf/2403.07635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelos Dimakos, Daniel Woodhall, Seemal Asif
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07635">A Study on Centralised and Decentralised Swarm Robotics Architecture for Part Delivery System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drones are also known as UAVs are originally designed for military purposes. With the technological advances, they can be seen in most of the aspects of life from filming to logistics. The increased use of drones made it sometimes essential to form a collaboration between them to perform the task efficiently in a defined process. This paper investigates the use of a combined centralised and decentralised architecture for the collaborative operation of drones in a parts delivery scenario to enable and expedite the operation of the factories of the future. The centralised and decentralised approaches were extensively researched, with experimentation being undertaken to determine the appropriateness of each approach for this use-case. Decentralised control was utilised to remove the need for excessive communication during the operation of the drones, resulting in smoother operations. Initial results suggested that the decentralised approach is more appropriate for this use-case. The individual functionalities necessary for the implementation of a decentralised architecture were proven and assessed, determining that a combination of multiple individual functionalities, namely VSLAM, dynamic collision avoidance and object tracking, would give an appropriate solution for use in an industrial setting. A final architecture for the parts delivery system was proposed for future work, using a combined centralised and decentralised approach to combat the limitations inherent in each architecture.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2403.02767.pdf' target='_blank'>https://arxiv.org/pdf/2403.02767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Huang, Shoudong Han, Mengyu He, Wenbo Zheng, Yuhao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02767">DeconfuseTrack:Dealing with Confusion for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT). However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association. To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA). DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues. Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections. Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT. Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers. Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2402.12968.pdf' target='_blank'>https://arxiv.org/pdf/2402.12968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Wang, Ruohui Zhang, Chenglin Chen, Min Yang, Yun Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12968">MapTrack: Tracking in the Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target. Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities. Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations. In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes. Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter. The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons. Trajectories of undetected targets that are still within the probability map are extended by state estimations directly. The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter. The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20. Despite its superior performance, our method remains simple, online, and real-time. The code will be open-sourced later.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2401.17874.pdf' target='_blank'>https://arxiv.org/pdf/2401.17874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyan Zhang, Rahul Chaudhari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17874">VR-based generation of photorealistic synthetic data for training hand-object tracking models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images. To address these issues, we present "blender-hoisynth", an interactive synthetic data generator based on the Blender software. Blender-hoisynth can scalably generate and automatically annotate visual HOI training data. Other competing approaches usually generate synthetic HOI data compeletely without human input. While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent. With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware. The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it. We show that there is no significant degradation in the model performance despite the data replacement.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2401.12182.pdf' target='_blank'>https://arxiv.org/pdf/2401.12182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Robinson, Michael Stein, Henry S. Owen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12182">Tracking before detection using partial orders and optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article addresses the problem of multi-object tracking by using a non-deterministic model of target behaviors with hard constraints. To capture the evolution of target features as well as their locations, we permit objects to lie in a general topological target configuration space, rather than a Euclidean space. We obtain tracker performance bounds based on sample rates, and derive a flexible, agnostic tracking algorithm. We demonstrate our algorithm on two scenarios involving laboratory and field data.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2401.07929.pdf' target='_blank'>https://arxiv.org/pdf/2401.07929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rakibul Karim Akanda, Joshua Reynolds, Treylin Jackson, Milijah Gray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07929">Machine Learning Based Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning based object detection as well as tracking that object have been performed in this paper. The authors were able to set a range of interest (ROI) around an object using Open Computer Vision, better known as OpenCV. Next a tracking algorithm has been used to maintain tracking on an object while simultaneously operating two servo motors to keep the object centered in the frame. Detailed procedure and code are included in this paper.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2512.05171.pdf' target='_blank'>https://arxiv.org/pdf/2512.05171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandr Abramov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.05171">Two-Stage Camera Calibration Method for Multi-Camera Systems Using Scene Geometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Calibration of multi-camera systems is a key task for accurate object tracking. However, it remains a challenging problem in real-world conditions, where traditional methods are not applicable due to the lack of accurate floor plans, physical access to place calibration patterns, or synchronized video streams. This paper presents a novel two-stage calibration method that overcomes these limitations. In the first stage, partial calibration of individual cameras is performed based on an operator's annotation of natural geometric primitives (parallel, perpendicular, and vertical lines, or line segments of equal length). This allows estimating key parameters (roll, pitch, focal length) and projecting the camera's Effective Field of View (EFOV) onto the horizontal plane in a base 3D coordinate system. In the second stage, precise system calibration is achieved through interactive manipulation of the projected EFOV polygons. The operator adjusts their position, scale, and rotation to align them with the floor plan or, in its absence, using virtual calibration elements projected onto all cameras in the system. This determines the remaining extrinsic parameters (camera position and yaw). Calibration requires only a static image from each camera, eliminating the need for physical access or synchronized video. The method is implemented as a practical web service. Comparative analysis and demonstration videos confirm the method's applicability, accuracy, and flexibility, enabling the deployment of precise multi-camera tracking systems in scenarios previously considered infeasible.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2509.01095.pdf' target='_blank'>https://arxiv.org/pdf/2509.01095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihong Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01095">An End-to-End Framework for Video Multi-Person Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based human pose estimation models aim to address scenarios that cannot be effectively solved by static image models such as motion blur, out-of-focus and occlusion. Most existing approaches consist of two stages: detecting human instances in each image frame and then using a temporal model for single-person pose estimation. This approach separates the spatial and temporal dimensions and cannot capture the global spatio-temporal context between spatial instances for end-to-end optimization. In addition, it relies on separate detectors and complex post-processing such as RoI cropping and NMS, which reduces the inference efficiency of the video scene. To address the above problems, we propose VEPE (Video End-to-End Pose Estimation), a simple and flexible framework for end-to-end pose estimation in video. The framework utilizes three crucial spatio-temporal Transformer components: the Spatio-Temporal Pose Encoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and the Spatio-Temporal Pose Decoder (STPD). These components are designed to effectively utilize temporal context for optimizing human body pose estimation. Furthermore, to reduce the mismatch problem during the cross-frame pose query matching process, we propose an instance consistency mechanism, which aims to enhance the consistency and discrepancy of the cross-frame instance query and realize the instance tracking function, which in turn accurately guides the pose query to perform cross-frame matching. Extensive experiments on the Posetrack dataset show that our approach outperforms most two-stage models and improves inference efficiency by 300%.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2508.08269.pdf' target='_blank'>https://arxiv.org/pdf/2508.08269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagar Verma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08269">emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tendon-driven robotic hands offer unparalleled dexterity for manipulation tasks, but learning control policies for such systems presents unique challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a direct one-to-one mapping between motion capture (mocap) data and tendon controls, making the learning process complex and expensive. Additionally, visual tracking methods for real-world applications are prone to occlusions and inaccuracies, further complicating joint tracking. Wrist-wearable surface electromyography (sEMG) sensors present an inexpensive, robust alternative to capture hand motion. However, mapping sEMG signals to tendon control remains a significant challenge despite the availability of EMG-to-pose data sets and regression-based models in the existing literature.
  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic hands, extending the emg2pose dataset, which includes recordings from 193 subjects, spanning 370 hours and 29 stages with diverse gestures. This dataset incorporates tendon control signals derived using the MyoSuite MyoHand model, addressing limitations such as invalid poses in prior methods. We provide three baseline regression models to demonstrate emg2tendon utility and propose a novel diffusion-based regression model for predicting tendon control from sEMG recordings. This dataset and modeling framework marks a significant step forward for tendon-driven dexterous robotic manipulation, laying the groundwork for scalable and accurate tendon control in robotic hands. https://emg2tendon.github.io/
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2508.00272.pdf' target='_blank'>https://arxiv.org/pdf/2508.00272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyue Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00272">Towards Robust Semantic Correspondence: A Benchmark and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic correspondence aims to identify semantically meaningful relationships between different images and is a fundamental challenge in computer vision. It forms the foundation for numerous tasks such as 3D reconstruction, object tracking, and image editing. With the progress of large-scale vision models, semantic correspondence has achieved remarkable performance in controlled and high-quality conditions. However, the robustness of semantic correspondence in challenging scenarios is much less investigated. In this work, we establish a novel benchmark for evaluating semantic correspondence in adverse conditions. The benchmark dataset comprises 14 distinct challenging scenarios that reflect commonly encountered imaging issues, including geometric distortion, image blurring, digital artifacts, and environmental occlusion. Through extensive evaluations, we provide several key insights into the robustness of semantic correspondence approaches: (1) All existing methods suffer from noticeable performance drops under adverse conditions; (2) Using large-scale vision models can enhance overall robustness, but fine-tuning on these models leads to a decline in relative robustness; (3) The DINO model outperforms the Stable Diffusion in relative robustness, and their fusion achieves better absolute robustness; Moreover, We evaluate common robustness enhancement strategies for semantic correspondence and find that general data augmentations are ineffective, highlighting the need for task-specific designs. These results are consistent across both our dataset and real-world benchmarks.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2507.22421.pdf' target='_blank'>https://arxiv.org/pdf/2507.22421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahla John
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22421">Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time video analysis remains a challenging problem in computer vision, requiring efficient processing of both spatial and temporal information while maintaining computational efficiency. Existing approaches often struggle to balance accuracy and speed, particularly in resource-constrained environments. In this work, we present a unified framework that leverages advanced spatial-temporal modeling techniques for simultaneous action recognition and object tracking. Our approach builds upon recent advances in parallel sequence modeling and introduces a novel hierarchical attention mechanism that adaptively focuses on relevant spatial regions across temporal sequences. We demonstrate that our method achieves state-of-the-art performance on standard benchmarks while maintaining real-time inference speeds. Extensive experiments on UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action recognition accuracy and 2.8% in tracking precision compared to existing methods, with 40% faster inference time.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2506.13769.pdf' target='_blank'>https://arxiv.org/pdf/2506.13769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Leveni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13769">Non-planar Object Detection and Identification by Features Matching and Triangulation Growth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2506.13457.pdf' target='_blank'>https://arxiv.org/pdf/2506.13457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir AdÅ¾emoviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13457">Deep Learning-Based Multi-Object Tracking: A Comprehensive Survey from Foundations to State-of-the-Art</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a core task in computer vision that involves detecting objects in video frames and associating them across time. The rise of deep learning has significantly advanced MOT, particularly within the tracking-by-detection paradigm, which remains the dominant approach. Advancements in modern deep learning-based methods accelerated in 2022 with the introduction of ByteTrack for tracking-by-detection and MOTR for end-to-end tracking. Our survey provides an in-depth analysis of deep learning-based MOT methods, systematically categorizing tracking-by-detection approaches into five groups: joint detection and embedding, heuristic-based, motion-based, affinity learning, and offline methods. In addition, we examine end-to-end tracking methods and compare them with existing alternative approaches. We evaluate the performance of recent trackers across multiple benchmarks and specifically assess their generality by comparing results across different domains. Our findings indicate that heuristic-based methods achieve state-of-the-art results on densely populated datasets with linear object motion, while deep learning-based association methods, in both tracking-by-detection and end-to-end approaches, excel in scenarios with complex motion patterns.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2501.01206.pdf' target='_blank'>https://arxiv.org/pdf/2501.01206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karolina Prawda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01206">Sensitivity of Room Impulse Responses in Changing Acoustic Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Changes in room acoustics, such as modifications to surface absorption or the insertion of a scattering object, significantly impact measured room impulse responses (RIRs). These changes can affect the performance of systems used in echo cancellation and active acoustics and support tasks such as navigation and object tracking. Recognizing and quantifying such changes is, therefore, critical for advancing technologies based on room acoustics. This study introduces a method for analyzing acoustic environment changes by evaluating the similarity of consecutively recorded RIRs. Short-time coherence is employed to characterize modifications, including changes in wall absorption or the presence of a moving person in the room. A sensitivity rating is further used to quantify the magnitude of these changes. The results clearly differentiate between types of modifications -- atmospheric variation, changes in absorption, and human presence. The methods described provide a novel approach to analyzing and interpreting room acoustics, emphasizing RIR similarity and extracting information from temporal and spectral signal properties.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2501.00843.pdf' target='_blank'>https://arxiv.org/pdf/2501.00843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathanael L. Baisa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00843">FusionSORT: Fusion Methods for Online Multi-object Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2412.07966.pdf' target='_blank'>https://arxiv.org/pdf/2412.07966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kurt H. W. Stolle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07966">Balancing Shared and Task-Specific Representations: A Hybrid Approach to Depth-Aware Video Panoptic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Multiformer, a novel approach to depth-aware video panoptic segmentation (DVPS) based on the mask transformer paradigm. Our method learns object representations that are shared across segmentation, monocular depth estimation, and object tracking subtasks. In contrast to recent unified approaches that progressively refine a common object representation, we propose a hybrid method using task-specific branches within each decoder block, ultimately fusing them into a shared representation at the block interfaces. Extensive experiments on the Cityscapes-DVPS and SemKITTI-DVPS datasets demonstrate that Multiformer achieves state-of-the-art performance across all DVPS metrics, outperforming previous methods by substantial margins. With a ResNet-50 backbone, Multiformer surpasses the previous best result by 3.0 DVPQ points while also improving depth estimation accuracy. Using a Swin-B backbone, Multiformer further improves performance by 4.0 DVPQ points. Multiformer also provides valuable insights into the design of multi-task decoder architectures.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2411.10028.pdf' target='_blank'>https://arxiv.org/pdf/2411.10028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanzhao Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10028">MOT FCG++: Enhanced Representation of Spatio-temporal Motion and Appearance Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of multi-object tracking (MOT) is to detect and track all objects in a scene across frames, while maintaining a unique identity for each object. Most existing methods rely on the spatial-temporal motion features and appearance embedding features of the detected objects in consecutive frames. Effectively and robustly representing the spatial and appearance features of long trajectories has become a critical factor affecting the performance of MOT. We propose a novel approach for appearance and spatial-temporal motion feature representation, improving upon the hierarchical clustering association method MOT FCG. For spatialtemporal motion features, we first propose Diagonal Modulated GIoU, which more accurately represents the relationship between the position and shape of the objects. Second, Mean Constant Velocity Modeling is proposed to reduce the effect of observation noise on target motion state estimation. For appearance features, we utilize a dynamic appearance representation that incorporates confidence information, enabling the trajectory appearance features to be more robust and global. Based on the baseline model MOT FCG, we have realized further improvements in the performance of all. we achieved 63.1 HOTA, 76.9 MOTA and 78.2 IDF1 on the MOT17 test set, and also achieved competitive performance on the MOT20 and DanceTrack sets.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2407.08885.pdf' target='_blank'>https://arxiv.org/pdf/2407.08885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mihir Godbole
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08885">Manipulating a Tetris-Inspired 3D Video Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Synopsis is a technique that performs video compression in a way that preserves the activity in the video. This technique is particularly useful in surveillance and monitoring applications. Although it is still a nascent field of research, there have been several approaches proposed over the last two decades varying with the application, optimization type, nature of data feed, etc. The primary data required for these algorithms arises from some sort of object tracking method. In this paper, we discuss different spatio-temporal data representations suitable for different applications. We also present a formal definition for the video synopsis algorithm. We further discuss the assumptions and modifications to this definition required for a simpler version of the problem. We explore the application of a packing algorithm to solve the problem of video synopsis. Since the nature of the data is three dimensional, we consider 3D packing problems in the discussion. This paper also provides an extensive literature review of different video synopsis methods and packing problems. Lastly, we examine the different applications of this algorithm and how the different data representations discussed earlier can make the problem simpler. We also discuss the future directions of research that can be explored following this discussion.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2406.16784.pdf' target='_blank'>https://arxiv.org/pdf/2406.16784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhi Kamboj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16784">The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The transformer neural network architecture allows for autoregressive sequence-to-sequence modeling through the use of attention layers. It was originally created with the application of machine translation but has revolutionized natural language processing. Recently, transformers have also been applied across a wide variety of pattern recognition tasks, particularly in computer vision. In this literature review, we describe major advances in computer vision utilizing transformers. We then focus specifically on Multi-Object Tracking (MOT) and discuss how transformers are increasingly becoming competitive in state-of-the-art MOT works, yet still lag behind traditional deep learning methods.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2406.09914.pdf' target='_blank'>https://arxiv.org/pdf/2406.09914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandeep Singh Sengar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09914">Robust compressive tracking via online weighted multiple instance learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing a robust object tracker is a challenging task due to factors such as occlusion, motion blur, fast motion, illumination variations, rotation, background clutter, low resolution and deformation across the frames. In the literature, lots of good approaches based on sparse representation have already been presented to tackle the above problems. However, most of the algorithms do not focus on the learning of sparse representation. They only consider the modeling of target appearance and therefore drift away from the target with the imprecise training samples. By considering all the above factors in mind, we have proposed a visual object tracking algorithm by integrating a coarse-to-fine search strategy based on sparse representation and the weighted multiple instance learning (WMIL) algorithm. Compared with the other trackers, our approach has more information of the original signal with less complexity due to the coarse-to-fine search method, and also has weights for important samples. Thus, it can easily discriminate the background features from the foreground. Furthermore, we have also selected the samples from the un-occluded sub-regions to efficiently develop the strong classifier. As a consequence, a stable and robust object tracker is achieved to tackle all the aforementioned problems. Experimental results with quantitative as well as qualitative analysis on challenging benchmark datasets show the accuracy and efficiency of our method.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2406.07228.pdf' target='_blank'>https://arxiv.org/pdf/2406.07228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07228">Haptic Repurposing with GenAI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed Reality aims to merge the digital and physical worlds to create immersive human-computer interactions. Despite notable advancements, the absence of realistic haptic feedback often breaks the immersive experience by creating a disconnect between visual and tactile perceptions. This paper introduces Haptic Repurposing with GenAI, an innovative approach to enhance MR interactions by transforming any physical objects into adaptive haptic interfaces for AI-generated virtual assets. Utilizing state-of-the-art generative AI models, this system captures both 2D and 3D features of physical objects and, through user-directed prompts, generates corresponding virtual objects that maintain the physical form of the original objects. Through model-based object tracking, the system dynamically anchors virtual assets to physical props in real time, allowing objects to visually morph into any user-specified virtual object. This paper details the system's development, presents findings from usability studies that validate its effectiveness, and explores its potential to significantly enhance interactive MR environments. The hope is this work can lay a foundation for further research into AI-driven spatial transformation in immersive and haptic technologies.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2404.00085.pdf' target='_blank'>https://arxiv.org/pdf/2404.00085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bahman Moraffah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00085">Bayesian Nonparametrics: An Alternative to Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian nonparametric models offer a flexible and powerful framework for statistical model selection, enabling the adaptation of model complexity to the intricacies of diverse datasets. This survey intends to delve into the significance of Bayesian nonparametrics, particularly in addressing complex challenges across various domains such as statistics, computer science, and electrical engineering. By elucidating the basic properties and theoretical foundations of these nonparametric models, this survey aims to provide a comprehensive understanding of Bayesian nonparametrics and their relevance in addressing complex problems, particularly in the domain of multi-object tracking. Through this exploration, we uncover the versatility and efficacy of Bayesian nonparametric methodologies, paving the way for innovative solutions to intricate challenges across diverse disciplines.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2403.18908.pdf' target='_blank'>https://arxiv.org/pdf/2403.18908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasuyuki Ihara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18908">Enhancing Multiple Object Tracking Accuracy via Quantum Annealing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT), a key task in image recognition, presents a persistent challenge in balancing processing speed and tracking accuracy. This study introduces a novel approach that leverages quantum annealing (QA) to expedite computation speed, while enhancing tracking accuracy through the ensembling of object tracking processes. A method to improve the matching integration process is also proposed. By utilizing the sequential nature of MOT, this study further augments the tracking method via reverse annealing (RA). Experimental validation confirms the maintenance of high accuracy with an annealing time of a mere 3 $Î¼$s per tracking process. The proposed method holds significant potential for real-time MOT applications, including traffic flow measurement for urban traffic light control, collision prediction for autonomous robots and vehicles, and management of products mass-produced in factories.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2402.04275.pdf' target='_blank'>https://arxiv.org/pdf/2402.04275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenping Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04275">Motion Mapping Cognition: A Nondecomposable Primary Process in Human Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human intelligence seems so mysterious that we have not successfully understood its foundation until now. Here, I want to present a basic cognitive process, motion mapping cognition (MMC), which should be a nondecomposable primary function in human vision. Wherein, I point out that, MMC process can be used to explain most of human visual functions in fundamental, but can not be effectively modelled by traditional visual processing ways including image segmentation, object recognition, object tracking etc. Furthermore, I state that MMC may be looked as an extension of Chen's theory of topological perception on human vision, and seems to be unsolvable using existing intelligent algorithm skills. Finally, along with the requirements of MMC problem, an interesting computational model, quantized topological matching principle can be derived by developing the idea of optimal transport theory. Above results may give us huge inspiration to develop more robust and interpretable machine vision models.
