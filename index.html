<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.07134.pdf' target='_blank'>https://arxiv.org/pdf/2510.07134.pdf</a></span>   <span><a href='https://pku-epic.github.io/TrackVLA-plus-plus-Web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07134">TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2510.07134.pdf' target='_blank'>https://arxiv.org/pdf/2510.07134.pdf</a></span>   <span><a href='https://pku-epic.github.io/TrackVLA-plus-plus-Web/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07134">TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2510.06619.pdf' target='_blank'>https://arxiv.org/pdf/2510.06619.pdf</a></span>   <span><a href='https://github.com/Fengtao191/MSITrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Feng, Tingfa Xu, Haolin Qin, Tianhao Li, Shuaihao Han, Xuyang Zou, Zhan Lv, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06619">MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking in real-world scenarios presents numerous challenges including occlusion, interference from similar objects and complex backgrounds-all of which limit the effectiveness of RGB-based trackers. Multispectral imagery, which captures pixel-level spectral reflectance, enhances target discriminability. However, the availability of multispectral tracking datasets remains limited. To bridge this gap, we introduce MSITrack, the largest and most diverse multispectral single object tracking dataset to date. MSITrack offers the following key features: (i) More Challenging Attributes-including interference from similar objects and similarity in color and texture between targets and backgrounds in natural scenarios, along with a wide range of real-world tracking challenges; (ii) Richer and More Natural Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack far exceeds the scope of existing benchmarks. Many of these scenes and categories are introduced to the multispectral tracking domain for the first time; (iii) Larger Scale-300 videos comprising over 129k frames of multispectral imagery. To ensure annotation precision, each frame has undergone meticulous processing, manual labeling and multi-stage verification. Extensive evaluations using representative trackers demonstrate that the multispectral data in MSITrack significantly improves performance over RGB-only baselines, highlighting its potential to drive future advancements in the field. The MSITrack dataset is publicly available at: https://github.com/Fengtao191/MSITrack.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2510.06619.pdf' target='_blank'>https://arxiv.org/pdf/2510.06619.pdf</a></span>   <span><a href='https://github.com/Fengtao191/MSITrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Feng, Tingfa Xu, Haolin Qin, Tianhao Li, Shuaihao Han, Xuyang Zou, Zhan Lv, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06619">MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking in real-world scenarios presents numerous challenges including occlusion, interference from similar objects and complex backgrounds-all of which limit the effectiveness of RGB-based trackers. Multispectral imagery, which captures pixel-level spectral reflectance, enhances target discriminability. However, the availability of multispectral tracking datasets remains limited. To bridge this gap, we introduce MSITrack, the largest and most diverse multispectral single object tracking dataset to date. MSITrack offers the following key features: (i) More Challenging Attributes-including interference from similar objects and similarity in color and texture between targets and backgrounds in natural scenarios, along with a wide range of real-world tracking challenges; (ii) Richer and More Natural Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack far exceeds the scope of existing benchmarks. Many of these scenes and categories are introduced to the multispectral tracking domain for the first time; (iii) Larger Scale-300 videos comprising over 129k frames of multispectral imagery. To ensure annotation precision, each frame has undergone meticulous processing, manual labeling and multi-stage verification. Extensive evaluations using representative trackers demonstrate that the multispectral data in MSITrack significantly improves performance over RGB-only baselines, highlighting its potential to drive future advancements in the field. The MSITrack dataset is publicly available at: https://github.com/Fengtao191/MSITrack.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2509.17323.pdf' target='_blank'>https://arxiv.org/pdf/2509.17323.pdf</a></span>   <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17323">DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Multi-Object Tracking (MOT) is a crucial component of robotic perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D cues, such as bounding boxes and motion modeling, which struggle under occlusions and close-proximity interactions. Trackers relying on these 2D cues are particularly unreliable in robotic environments, where dense targets and frequent occlusions are common. While depth information has the potential to alleviate these issues, most existing MOT datasets lack depth annotations, leading to its underexploited role in the domain. To unveil the potential of depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based detector enhanced with instance-level depth information. Specifically, we propose two key innovations: (i) foundation model-based instance-level soft depth label supervision, which refines depth prediction, and (ii) the distillation of dense depth maps to maintain global depth consistency. These strategies enable DepTR-MOT to output instance-level depth during inference, without requiring foundation models and without additional computational cost. By incorporating depth cues, our method enhances the robustness of the TBD paradigm, effectively resolving occlusion and close-proximity challenges. Experiments on both the QuadTrack and DanceTrack datasets demonstrate the effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47, respectively. In particular, results on QuadTrack, a robotic platform MOT dataset, highlight the advantages of our method in handling occlusion and close-proximity challenges in robotic tracking. The source code will be made publicly available at https://github.com/warriordby/DepTR-MOT.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2509.17323.pdf' target='_blank'>https://arxiv.org/pdf/2509.17323.pdf</a></span>   <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/warriordby/DepTR-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Buyin Deng, Lingxin Huang, Kai Luo, Fei Teng, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17323">DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Multi-Object Tracking (MOT) is a crucial component of robotic perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D cues, such as bounding boxes and motion modeling, which struggle under occlusions and close-proximity interactions. Trackers relying on these 2D cues are particularly unreliable in robotic environments, where dense targets and frequent occlusions are common. While depth information has the potential to alleviate these issues, most existing MOT datasets lack depth annotations, leading to its underexploited role in the domain. To unveil the potential of depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based detector enhanced with instance-level depth information. Specifically, we propose two key innovations: (i) foundation model-based instance-level soft depth label supervision, which refines depth prediction, and (ii) the distillation of dense depth maps to maintain global depth consistency. These strategies enable DepTR-MOT to output instance-level depth during inference, without requiring foundation models and without additional computational cost. By incorporating depth cues, our method enhances the robustness of the TBD paradigm, effectively resolving occlusion and close-proximity challenges. Experiments on both the QuadTrack and DanceTrack datasets demonstrate the effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47, respectively. In particular, results on QuadTrack, a robotic platform MOT dataset, highlight the advantages of our method in handling occlusion and close-proximity challenges in robotic tracking. The source code will be made publicly available at https://github.com/warriordby/DepTR-MOT.
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2509.16527.pdf' target='_blank'>https://arxiv.org/pdf/2509.16527.pdf</a></span>   <span><a href='https://george-zhuang.github.io/lbm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangze Zheng, Shijie Lin, Haobo Zuo, Si Si, Ming-Shan Wang, Changhong Fu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16527">Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes the Lattice Boltzmann Model (LBM) to learn real-world pixel dynamicity for visual tracking. LBM decomposes visual representations into dynamic pixel lattices and solves pixel motion states through collision-streaming processes. Specifically, the high-dimensional distribution of the target pixels is acquired through a multilayer predict-update network to estimate the pixel positions and visibility. The predict stage formulates lattice collisions among the spatial neighborhood of target pixels and develops lattice streaming within the temporal visual context. The update stage rectifies the pixel distributions with online visual representations. Compared with existing methods, LBM demonstrates practical applicability in an online and real-time manner, which can efficiently adapt to real-world visual tracking tasks. Comprehensive evaluations of real-world point tracking benchmarks such as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B further demonstrates LBM's real-world practicality.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2509.16527.pdf' target='_blank'>https://arxiv.org/pdf/2509.16527.pdf</a></span>   <span><a href='https://george-zhuang.github.io/lbm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangze Zheng, Shijie Lin, Haobo Zuo, Si Si, Ming-Shan Wang, Changhong Fu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16527">Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes the Lattice Boltzmann Model (LBM) to learn real-world pixel dynamicity for visual tracking. LBM decomposes visual representations into dynamic pixel lattices and solves pixel motion states through collision-streaming processes. Specifically, the high-dimensional distribution of the target pixels is acquired through a multilayer predict-update network to estimate the pixel positions and visibility. The predict stage formulates lattice collisions among the spatial neighborhood of target pixels and develops lattice streaming within the temporal visual context. The update stage rectifies the pixel distributions with online visual representations. Compared with existing methods, LBM demonstrates practical applicability in an online and real-time manner, which can efficiently adapt to real-world visual tracking tasks. Comprehensive evaluations of real-world point tracking benchmarks such as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B further demonstrates LBM's real-world practicality.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2509.13864.pdf' target='_blank'>https://arxiv.org/pdf/2509.13864.pdf</a></span>   <span><a href='https://github.com/jovanavidenovic/DAM4SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovana Videnovic, Matej Kristan, Alan Lukezic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13864">Distractor-Aware Memory-Based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent emergence of memory-based video segmentation methods such as SAM2 has led to models with excellent performance in segmentation tasks, achieving leading results on numerous benchmarks. However, these modes are not fully adjusted for visual object tracking, where distractors (i.e., objects visually similar to the target) pose a key challenge. In this paper we propose a distractor-aware drop-in memory module and introspection-based management method for SAM2, leading to DAM4SAM. Our design effectively reduces the tracking drift toward distractors and improves redetection capability after object occlusion. To facilitate the analysis of tracking in the presence of distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results on ten. Furthermore, integrating the proposed distractor-aware memory into a real-time tracker EfficientTAM leads to 11% improvement and matches tracking quality of the non-real-time SAM2.1-L on multiple tracking and segmentation benchmarks, while integration with edge-based tracker EdgeTAM delivers 4% performance boost, demonstrating a very good generalization across architectures.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2509.13864.pdf' target='_blank'>https://arxiv.org/pdf/2509.13864.pdf</a></span>   <span><a href='https://github.com/jovanavidenovic/DAM4SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovana Videnovic, Matej Kristan, Alan Lukezic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13864">Distractor-Aware Memory-Based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent emergence of memory-based video segmentation methods such as SAM2 has led to models with excellent performance in segmentation tasks, achieving leading results on numerous benchmarks. However, these modes are not fully adjusted for visual object tracking, where distractors (i.e., objects visually similar to the target) pose a key challenge. In this paper we propose a distractor-aware drop-in memory module and introspection-based management method for SAM2, leading to DAM4SAM. Our design effectively reduces the tracking drift toward distractors and improves redetection capability after object occlusion. To facilitate the analysis of tracking in the presence of distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results on ten. Furthermore, integrating the proposed distractor-aware memory into a real-time tracker EfficientTAM leads to 11% improvement and matches tracking quality of the non-real-time SAM2.1-L on multiple tracking and segmentation benchmarks, while integration with edge-based tracker EdgeTAM delivers 4% performance boost, demonstrating a very good generalization across architectures.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2509.12913.pdf' target='_blank'>https://arxiv.org/pdf/2509.12913.pdf</a></span>   <span><a href='https://github.com/to/be/released' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hojat Ardi, Amir Jahanshahi, Ali Diba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12913">T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial object tracking remains a challenging task due to scale variations, dynamic backgrounds, clutter, and frequent occlusions. While most existing trackers emphasize spatial cues, they often overlook temporal dependencies, resulting in limited robustness in long-term tracking and under occlusion. Furthermore, correlation-based Siamese trackers are inherently constrained by the linear nature of correlation operations, making them ineffective against complex, non-linear appearance changes. To address these limitations, we introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends the SiamTPN architecture with explicit temporal modeling. Our approach incorporates temporal feature fusion and attention-based interactions, strengthening temporal consistency and enabling richer feature representations. These enhancements yield significant improvements over the baseline and achieve performance competitive with state-of-the-art trackers. Crucially, despite the added temporal modules, T-SiamTPN preserves computational efficiency. Deployed on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1 FPS, demonstrating its suitability for real-world embedded applications without notable runtime overhead. Experimental results highlight substantial gains: compared to the baseline, T-SiamTPN improves success rate by 13.7% and precision by 14.7%. These findings underscore the importance of temporal modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and efficient solution for aerial object tracking. Code is available at: https://github.com/to/be/released
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2509.12913.pdf' target='_blank'>https://arxiv.org/pdf/2509.12913.pdf</a></span>   <span><a href='https://github.com/to/be/released' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hojat Ardi, Amir Jahanshahi, Ali Diba
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12913">T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Aerial object tracking remains a challenging task due to scale variations, dynamic backgrounds, clutter, and frequent occlusions. While most existing trackers emphasize spatial cues, they often overlook temporal dependencies, resulting in limited robustness in long-term tracking and under occlusion. Furthermore, correlation-based Siamese trackers are inherently constrained by the linear nature of correlation operations, making them ineffective against complex, non-linear appearance changes. To address these limitations, we introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends the SiamTPN architecture with explicit temporal modeling. Our approach incorporates temporal feature fusion and attention-based interactions, strengthening temporal consistency and enabling richer feature representations. These enhancements yield significant improvements over the baseline and achieve performance competitive with state-of-the-art trackers. Crucially, despite the added temporal modules, T-SiamTPN preserves computational efficiency. Deployed on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1 FPS, demonstrating its suitability for real-world embedded applications without notable runtime overhead. Experimental results highlight substantial gains: compared to the baseline, T-SiamTPN improves success rate by 13.7% and precision by 14.7%. These findings underscore the importance of temporal modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and efficient solution for aerial object tracking. Code is available at: https://github.com/to/be/released
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2509.11772.pdf' target='_blank'>https://arxiv.org/pdf/2509.11772.pdf</a></span>   <span><a href='https://github.com/hcmr-lab/Seg2Track-SAM2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Diogo MendonÃ§a, Tiago Barros, Cristiano Premebida, Urbano J. Nunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11772">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to operate reliably in dynamic environments. MOT ensures consistent object identity assignment and precise spatial delineation. Recent advances in foundation models, such as SAM2, have demonstrated strong zero-shot generalization for video segmentation, but their direct application to MOTS (MOT+Segmentation) remains limited by insufficient identity management and memory efficiency. This work introduces Seg2Track-SAM2, a framework that integrates pre-trained object detectors with SAM2 and a novel Seg2Track module to address track initialization, track management, and reinforcement. The proposed approach requires no fine-tuning and remains detector-agnostic. Experimental results on KITTI MOT and KITTI MOTS benchmarks show that Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth overall in both car and pedestrian classes on KITTI MOTS, while establishing a new benchmark in association accuracy (AssA). Furthermore, a sliding-window memory strategy reduces memory usage by up to 75% with negligible performance degradation, supporting deployment under resource constraints. These results confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot tracking, enhanced identity preservation, and efficient memory utilization. The code is available at https://github.com/hcmr-lab/Seg2Track-SAM2
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2509.11772.pdf' target='_blank'>https://arxiv.org/pdf/2509.11772.pdf</a></span>   <span><a href='https://github.com/hcmr-lab/Seg2Track-SAM2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Diogo MendonÃ§a, Tiago Barros, Cristiano Premebida, Urbano J. Nunes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11772">Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to operate reliably in dynamic environments. MOT ensures consistent object identity assignment and precise spatial delineation. Recent advances in foundation models, such as SAM2, have demonstrated strong zero-shot generalization for video segmentation, but their direct application to MOTS (MOT+Segmentation) remains limited by insufficient identity management and memory efficiency. This work introduces Seg2Track-SAM2, a framework that integrates pre-trained object detectors with SAM2 and a novel Seg2Track module to address track initialization, track management, and reinforcement. The proposed approach requires no fine-tuning and remains detector-agnostic. Experimental results on KITTI MOT and KITTI MOTS benchmarks show that Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth overall in both car and pedestrian classes on KITTI MOTS, while establishing a new benchmark in association accuracy (AssA). Furthermore, a sliding-window memory strategy reduces memory usage by up to 75% with negligible performance degradation, supporting deployment under resource constraints. These results confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot tracking, enhanced identity preservation, and efficient memory utilization. The code is available at https://github.com/hcmr-lab/Seg2Track-SAM2
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2509.11323.pdf' target='_blank'>https://arxiv.org/pdf/2509.11323.pdf</a></span>   <span><a href='https://github.com/SongJgit/filternet' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SongJgit/TBDTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Song, Wei Mei, Yunfeng Xu, Qiang Fu, Renke Kou, Lina Bu, Yucheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11323">Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion estimation is a crucial component in multi-object tracking (MOT). It predicts the trajectory of objects by analyzing the changes in their positions in consecutive frames of images, reducing tracking failures and identity switches. The Kalman filter (KF) based on the linear constant-velocity model is one of the most commonly used methods in MOT. However, it may yield unsatisfactory results when KF's parameters are mismatched and objects move in non-stationary. In this work, we utilize the learning-aided filter to handle the motion estimation of MOT. In particular, we propose a novel method named Semantic-Independent KalmanNet (SIKNet), which encodes the state vector (the input feature) using a Semantic-Independent Encoder (SIE) by two steps. First, the SIE uses a 1D convolution with a kernel size of 1, which convolves along the dimension of homogeneous-semantic elements across different state vectors to encode independent semantic information. Then it employs a fully-connected layer and a nonlinear activation layer to encode nonlinear and cross-dependency information between heterogeneous-semantic elements. To independently evaluate the performance of the motion estimation module in MOT, we constructed a large-scale semi-simulated dataset from several open-source MOT datasets. Experimental results demonstrate that the proposed SIKNet outperforms the traditional KF and achieves superior robustness and accuracy than existing learning-aided filters. The code is available at (https://github.com/SongJgit/filternet and https://github.com/SongJgit/TBDTracker).
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2509.11323.pdf' target='_blank'>https://arxiv.org/pdf/2509.11323.pdf</a></span>   <span><a href='https://github.com/SongJgit/TBDTracker' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SongJgit/filternet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Song, Wei Mei, Yunfeng Xu, Qiang Fu, Renke Kou, Lina Bu, Yucheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11323">Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion estimation is a crucial component in multi-object tracking (MOT). It predicts the trajectory of objects by analyzing the changes in their positions in consecutive frames of images, reducing tracking failures and identity switches. The Kalman filter (KF) based on the linear constant-velocity model is one of the most commonly used methods in MOT. However, it may yield unsatisfactory results when KF's parameters are mismatched and objects move in non-stationary. In this work, we utilize the learning-aided filter to handle the motion estimation of MOT. In particular, we propose a novel method named Semantic-Independent KalmanNet (SIKNet), which encodes the state vector (the input feature) using a Semantic-Independent Encoder (SIE) by two steps. First, the SIE uses a 1D convolution with a kernel size of 1, which convolves along the dimension of homogeneous-semantic elements across different state vectors to encode independent semantic information. Then it employs a fully-connected layer and a nonlinear activation layer to encode nonlinear and cross-dependency information between heterogeneous-semantic elements. To independently evaluate the performance of the motion estimation module in MOT, we constructed a large-scale semi-simulated dataset from several open-source MOT datasets. Experimental results demonstrate that the proposed SIKNet outperforms the traditional KF and achieves superior robustness and accuracy than existing learning-aided filters. The code is available at (https://github.com/SongJgit/filternet and https://github.com/SongJgit/TBDTracker).
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2509.09977.pdf' target='_blank'>https://arxiv.org/pdf/2509.09977.pdf</a></span>   <span><a href='https://github.com/lsying009/ISTASTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siying Liu, Zikai Wang, Hanle Zheng, Yifan Hu, Xilin Wang, Qingkai Yang, Jibin Wu, Hao Guo, Lei Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09977">ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-Event tracking has become a promising trend in visual object tracking to leverage the complementary strengths of both RGB images and dynamic spike events for improved performance. However, existing artificial neural networks (ANNs) struggle to fully exploit the sparse and asynchronous nature of event streams. Recent efforts toward hybrid architectures combining ANNs and spiking neural networks (SNNs) have emerged as a promising solution in RGB-Event perception, yet effectively fusing features across heterogeneous paradigms remains a challenge. In this work, we propose ISTASTrack, the first transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model employs a vision transformer to extract spatial context from RGB inputs and a spiking transformer to capture spatio-temporal dynamics from event streams. To bridge the modality and paradigm gap between ANN and SNN features, we systematically design a model-based ISTA adapter for bidirectional feature interaction between the two branches, derived from sparse representation theory by unfolding the iterative shrinkage thresholding algorithm. Additionally, we incorporate a temporal downsampling attention module within the adapter to align multi-step SNN features with single-step ANN features in the latent space, improving temporal fusion. Experimental results on RGB-Event tracking benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that ISTASTrack achieves state-of-the-art performance while maintaining high energy efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking. The code is publicly available at https://github.com/lsying009/ISTASTrack.git.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2509.09977.pdf' target='_blank'>https://arxiv.org/pdf/2509.09977.pdf</a></span>   <span><a href='https://github.com/lsying009/ISTASTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siying Liu, Zikai Wang, Hanle Zheng, Yifan Hu, Xilin Wang, Qingkai Yang, Jibin Wu, Hao Guo, Lei Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09977">ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-Event tracking has become a promising trend in visual object tracking to leverage the complementary strengths of both RGB images and dynamic spike events for improved performance. However, existing artificial neural networks (ANNs) struggle to fully exploit the sparse and asynchronous nature of event streams. Recent efforts toward hybrid architectures combining ANNs and spiking neural networks (SNNs) have emerged as a promising solution in RGB-Event perception, yet effectively fusing features across heterogeneous paradigms remains a challenge. In this work, we propose ISTASTrack, the first transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model employs a vision transformer to extract spatial context from RGB inputs and a spiking transformer to capture spatio-temporal dynamics from event streams. To bridge the modality and paradigm gap between ANN and SNN features, we systematically design a model-based ISTA adapter for bidirectional feature interaction between the two branches, derived from sparse representation theory by unfolding the iterative shrinkage thresholding algorithm. Additionally, we incorporate a temporal downsampling attention module within the adapter to align multi-step SNN features with single-step ANN features in the latent space, improving temporal fusion. Experimental results on RGB-Event tracking benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that ISTASTrack achieves state-of-the-art performance while maintaining high energy efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking. The code is publicly available at https://github.com/lsying009/ISTASTrack.git.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2509.09962.pdf' target='_blank'>https://arxiv.org/pdf/2509.09962.pdf</a></span>   <span><a href='https://github.com/ngobibibnbe/uncertain-identity-aware-tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anne Marthe Sophie Ngo Bibinbe, Chiron Bang, Patrick Gagnon, Jamie Ahloy-Dallaire, Eric R. Paquet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09962">An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The need for long-term multi-object tracking (MOT) is growing due to the demand for analyzing individual behaviors in videos that span several minutes. Unfortunately, due to identity switches between objects, the tracking performance of existing MOT approaches decreases over time, making them difficult to apply for long-term tracking. However, in many real-world applications, such as in the livestock sector, it is possible to obtain sporadic identifications for some of the animals from sources like feeders. To address the challenges of long-term MOT, we propose a new framework that combines both uncertain identities and tracking using a Hidden Markov Model (HMM) formulation. In addition to providing real-world identities to animals, our HMM framework improves the F1 score of ByteTrack, a leading MOT approach even with re-identification, on a 10 minute pig tracking dataset with 21 identifications at the pen's feeding station. We also show that our approach is robust to the uncertainty of identifications, with performance increasing as identities are provided more frequently. The improved performance of our HMM framework was also validated on the MOT17 and MOT20 benchmark datasets using both ByteTrack and FairMOT. The code for this new HMM framework and the new 10-minute pig tracking video dataset are available at: https://github.com/ngobibibnbe/uncertain-identity-aware-tracking
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2509.09962.pdf' target='_blank'>https://arxiv.org/pdf/2509.09962.pdf</a></span>   <span><a href='https://github.com/ngobibibnbe/uncertain-identity-aware-tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anne Marthe Sophie Ngo Bibinbe, Chiron Bang, Patrick Gagnon, Jamie Ahloy-Dallaire, Eric R. Paquet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09962">An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The need for long-term multi-object tracking (MOT) is growing due to the demand for analyzing individual behaviors in videos that span several minutes. Unfortunately, due to identity switches between objects, the tracking performance of existing MOT approaches decreases over time, making them difficult to apply for long-term tracking. However, in many real-world applications, such as in the livestock sector, it is possible to obtain sporadic identifications for some of the animals from sources like feeders. To address the challenges of long-term MOT, we propose a new framework that combines both uncertain identities and tracking using a Hidden Markov Model (HMM) formulation. In addition to providing real-world identities to animals, our HMM framework improves the F1 score of ByteTrack, a leading MOT approach even with re-identification, on a 10 minute pig tracking dataset with 21 identifications at the pen's feeding station. We also show that our approach is robust to the uncertainty of identifications, with performance increasing as identities are provided more frequently. The improved performance of our HMM framework was also validated on the MOT17 and MOT20 benchmark datasets using both ByteTrack and FairMOT. The code for this new HMM framework and the new 10-minute pig tracking video dataset are available at: https://github.com/ngobibibnbe/uncertain-identity-aware-tracking
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2509.08265.pdf' target='_blank'>https://arxiv.org/pdf/2509.08265.pdf</a></span>   <span><a href='https://github.com/lgao001/HyMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Gao, Yunhe Zhang, Yan Jiang, Weiying Xie, Yunsong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08265">Hyperspectral Mamba for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking holds great promise due to the rich spectral information and fine-grained material distinctions in hyperspectral images, which are beneficial in challenging scenarios. While existing hyperspectral trackers have made progress by either transforming hyperspectral data into false-color images or incorporating modality fusion strategies, they often fail to capture the intrinsic spectral information, temporal dependencies, and cross-depth interactions. To address these limitations, a new hyperspectral object tracking network equipped with Mamba (HyMamba), is proposed. It unifies spectral, cross-depth, and temporal modeling through state space modules (SSMs). The core of HyMamba lies in the Spectral State Integration (SSI) module, which enables progressive refinement and propagation of spectral features with cross-depth and temporal spectral information. Embedded within each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial and spectral information synchronously via three directional scanning SSMs. Based on SSI and HSM, HyMamba constructs joint features from false-color and hyperspectral inputs, and enhances them through interaction with original spectral features extracted from raw hyperspectral images. Extensive experiments conducted on seven benchmark datasets demonstrate that HyMamba achieves state-of-the-art performance. For instance, it achieves 73.0\% of the AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will be released at https://github.com/lgao001/HyMamba.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2509.08265.pdf' target='_blank'>https://arxiv.org/pdf/2509.08265.pdf</a></span>   <span><a href='https://github.com/lgao001/HyMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Gao, Yunhe Zhang, Yan Jiang, Weiying Xie, Yunsong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08265">Hyperspectral Mamba for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking holds great promise due to the rich spectral information and fine-grained material distinctions in hyperspectral images, which are beneficial in challenging scenarios. While existing hyperspectral trackers have made progress by either transforming hyperspectral data into false-color images or incorporating modality fusion strategies, they often fail to capture the intrinsic spectral information, temporal dependencies, and cross-depth interactions. To address these limitations, a new hyperspectral object tracking network equipped with Mamba (HyMamba), is proposed. It unifies spectral, cross-depth, and temporal modeling through state space modules (SSMs). The core of HyMamba lies in the Spectral State Integration (SSI) module, which enables progressive refinement and propagation of spectral features with cross-depth and temporal spectral information. Embedded within each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial and spectral information synchronously via three directional scanning SSMs. Based on SSI and HSM, HyMamba constructs joint features from false-color and hyperspectral inputs, and enhances them through interaction with original spectral features extracted from raw hyperspectral images. Extensive experiments conducted on seven benchmark datasets demonstrate that HyMamba achieves state-of-the-art performance. For instance, it achieves 73.0\% of the AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will be released at https://github.com/lgao001/HyMamba.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2508.11531.pdf' target='_blank'>https://arxiv.org/pdf/2508.11531.pdf</a></span>   <span><a href='https://github.com/wsumel/MST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilei Wang, Gong Cheng, Pujian Lai, Dong Gao, Junwei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11531">Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient trackers achieve faster runtime by reducing computational complexity and model parameters. However, this efficiency often compromises the expense of weakened feature representation capacity, thus limiting their ability to accurately capture target states using single-layer features. To overcome this limitation, we propose Multi-State Tracker (MST), which utilizes highly lightweight state-specific enhancement (SSE) to perform specialized enhancement on multi-state features produced by multi-state generation (MSG) and aggregates them in an interactive and adaptive manner using cross-state interaction (CSI). This design greatly enhances feature representation while incurring minimal computational overhead, leading to improved tracking robustness in complex environments. Specifically, the MSG generates multiple state representations at multiple stages during feature extraction, while SSE refines them to highlight target-specific features. The CSI module facilitates information exchange between these states and ensures the integration of complementary features. Notably, the introduced SSE and CSI modules adopt a highly lightweight hidden state adaptation-based state space duality (HSA-SSD) design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters. Experimental results demonstrate that MST outperforms all previous efficient trackers across multiple datasets, significantly improving tracking accuracy and robustness. In particular, it shows excellent runtime performance, with an AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2508.10655.pdf' target='_blank'>https://arxiv.org/pdf/2508.10655.pdf</a></span>   <span><a href='https://github.com/Zhangyong-Tang/UniBench300' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Chunyang Cheng, Tao Zhou, Xiaojun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10655">Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \textit{inconsistency} between training and testing, thus leading to performance \textit{degradation}. To address these issues, this work advances in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\%. \ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2508.10567.pdf' target='_blank'>https://arxiv.org/pdf/2508.10567.pdf</a></span>   <span><a href='https://phi-wol.github.io/sparcad/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Wolters, Johannes Gilg, Torben Teepe, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10567">SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving systems promise stronger performance through unified optimization of perception, motion forecasting, and planning. However, vision-based approaches face fundamental limitations in adverse weather conditions, partial occlusions, and precise velocity estimation - critical challenges in safety-sensitive scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. To address these limitations, we propose SpaRC-AD, a query-based end-to-end camera-radar fusion framework for planning-oriented autonomous driving. Through sparse 3D feature alignment, and doppler-based velocity estimation, we achieve strong 3D scene representations for refinement of agent anchors, map polylines and motion modelling. Our method achieves strong improvements over the state-of-the-art vision-only baselines across multiple autonomous driving tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA), online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal consistency on multiple challenging benchmarks, including real-world open-loop nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We show the effectiveness of radar-based fusion in safety-critical scenarios where accurate motion understanding and long-horizon trajectory prediction are essential for collision avoidance. The source code of all experiments is available at https://phi-wol.github.io/sparcad/
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2508.10432.pdf' target='_blank'>https://arxiv.org/pdf/2508.10432.pdf</a></span>   <span><a href='https://github.com/01upup10/CRISP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baichen Liu, Qi Lyu, Xudong Wang, Jiahua Dong, Lianqing Liu, Zhi Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10432">CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual video instance segmentation demands both the plasticity to absorb new object categories and the stability to retain previously learned ones, all while preserving temporal consistency across frames. In this work, we introduce Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier attempt tailored to address the instance-wise, category-wise, and task-wise confusion in continual video instance segmentation. For instance-wise learning, we model instance tracking and construct instance correlation loss, which emphasizes the correlation with the prior query space while strengthening the specificity of the current task query. For category-wise learning, we build an adaptive residual semantic prompt (ARSP) learning framework, which constructs a learnable semantic residual prompt pool generated by category text and uses an adjustive query-prompt matching mechanism to build a mapping relationship between the query of the current task and the semantic residual prompt. Meanwhile, a semantic consistency loss based on the contrastive learning is introduced to maintain semantic coherence between object queries and residual prompts during incremental training. For task-wise learning, to ensure the correlation at the inter-task level within the query space, we introduce a concise yet powerful initialization strategy for incremental prompts. Extensive experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that CRISP significantly outperforms existing continual segmentation methods in the long-term continual video instance segmentation task, avoiding catastrophic forgetting and effectively improving segmentation and classification performance. The code is available at https://github.com/01upup10/CRISP.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2508.07250.pdf' target='_blank'>https://arxiv.org/pdf/2508.07250.pdf</a></span>   <span><a href='https://github.com/bearshng/suit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengchao Xiong, Zhenxing Wu, Sen Jia, Yuntao Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07250">SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal structure, offer distinct advantages in challenging tracking scenarios such as cluttered backgrounds and small objects. However, existing methods primarily focus on spatial interactions between the template and search regions, often overlooking spectral interactions, leading to suboptimal performance. To address this issue, this paper investigates spectral interactions from both the architectural and training perspectives. At the architectural level, we first establish band-wise long-range spatial relationships between the template and search regions using Transformers. We then model spectral interactions using the inclusion-exclusion principle from set theory, treating them as the union of spatial interactions across all bands. This enables the effective integration of both shared and band-specific spatial cues. At the training level, we introduce a spectral loss to enforce material distribution alignment between the template and predicted regions, enhancing robustness to shape deformation and appearance variations. Extensive experiments demonstrate that our tracker achieves state-of-the-art tracking performance. The source code, trained models and results will be publicly available via https://github.com/bearshng/suit to support reproducibility.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2508.05630.pdf' target='_blank'>https://arxiv.org/pdf/2508.05630.pdf</a></span>   <span><a href='https://github.com/henghuiding/MOSE-api' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip H. S. Torr, Song Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05630">MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To bridge this gap, the coMplex video Object SEgmentation (MOSEv1) dataset was introduced to facilitate VOS research in complex scenes. Building on the foundations and insights of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces much greater scene complexity, including {more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), and scenarios requiring external knowledge.} We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops on MOSEv2. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and observe similar declines, demonstrating that MOSEv2 poses challenges across tasks. These results highlight that despite strong performance on existing datasets, current VOS methods still fall short under real-world complexities. Based on our analysis of the observed challenges, we further propose several practical tricks that enhance model performance. MOSEv2 is publicly available at https://MOSE.video.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2508.05630.pdf' target='_blank'>https://arxiv.org/pdf/2508.05630.pdf</a></span>   <span><a href='https://github.com/henghuiding/MOSE-api' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip H. S. Torr, Song Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05630">MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% J&F) on benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To bridge this gap, the coMplex video Object SEgmentation (MOSEv1) dataset was introduced to facilitate VOS research in complex scenes. Building on the foundations and insights of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces much greater scene complexity, including {more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), and scenarios requiring external knowledge.} We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops on MOSEv2. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and observe similar declines, demonstrating that MOSEv2 poses challenges across tasks. These results highlight that despite strong performance on existing datasets, current VOS methods still fall short under real-world complexities. Based on our analysis of the observed challenges, we further propose several practical tricks that enhance model performance. MOSEv2 is publicly available at https://MOSE.video.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2508.05221.pdf' target='_blank'>https://arxiv.org/pdf/2508.05221.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Open_VLTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Liye Jin, Xufeng Lou, Shiao Wang, Lan Chen, Bo Jiang, Zhipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05221">ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the model's reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on https://github.com/Event-AHU/Open_VLTrack
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2508.02512.pdf' target='_blank'>https://arxiv.org/pdf/2508.02512.pdf</a></span>   <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Wu, Fei Teng, Hao Shi, Qi Jiang, Kai Luo, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02512">QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data-caused by inherent kinematic constraints and complex sensor calibration challenges-fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer-the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available at https://github.com/losehu/QuaDreamer.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2508.02512.pdf' target='_blank'>https://arxiv.org/pdf/2508.02512.pdf</a></span>   <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Wu, Fei Teng, Hao Shi, Qi Jiang, Kai Luo, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02512">QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data-caused by inherent kinematic constraints and complex sensor calibration challenges-fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer-the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available at https://github.com/losehu/QuaDreamer.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2508.02512.pdf' target='_blank'>https://arxiv.org/pdf/2508.02512.pdf</a></span>   <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/losehu/QuaDreamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng Wu, Fei Teng, Hao Shi, Qi Jiang, Kai Luo, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02512">QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data-caused by inherent kinematic constraints and complex sensor calibration challenges-fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer-the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available at https://github.com/losehu/QuaDreamer.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2508.01802.pdf' target='_blank'>https://arxiv.org/pdf/2508.01802.pdf</a></span>   <span><a href='https://github.com/AtomScott/SoccerTrack-v2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Atom Scott, Ikuma Uchida, Kento Kuroda, Yufi Kim, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01802">SoccerTrack v2: A Full-Pitch Multi-View Soccer Dataset for Game State Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>SoccerTrack v2 is a new public dataset for advancing multi-object tracking (MOT), game state reconstruction (GSR), and ball action spotting (BAS) in soccer analytics. Unlike prior datasets that use broadcast views or limited scenarios, SoccerTrack v2 provides 10 full-length, panoramic 4K recordings of university-level matches, captured with BePro cameras for complete player visibility. Each video is annotated with GSR labels (2D pitch coordinates, jersey-based player IDs, roles, teams) and BAS labels for 12 action classes (e.g., Pass, Drive, Shot). This technical report outlines the datasets structure, collection pipeline, and annotation process. SoccerTrack v2 is designed to advance research in computer vision and soccer analytics, enabling new benchmarks and practical applications in tactical analysis and automated tools.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2507.21732.pdf' target='_blank'>https://arxiv.org/pdf/2507.21732.pdf</a></span>   <span><a href='https://github.com/Sam1224/SAMITE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianxiong Xu, Lanyun Zhu, Chenxi Liu, Guosheng Lin, Cheng Long, Ziyue Li, Rui Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21732">SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is widely used in applications like autonomous driving to continuously track targets in videos. Existing methods can be roughly categorized into template matching and autoregressive methods, where the former usually neglects the temporal dependencies across frames and the latter tends to get biased towards the object categories during training, showing weak generalizability to unseen classes. To address these issues, some methods propose to adapt the video foundation model SAM2 for VOT, where the tracking results of each frame would be encoded as memory for conditioning the rest of frames in an autoregressive manner. Nevertheless, existing methods fail to overcome the challenges of object occlusions and distractions, and do not have any measures to intercept the propagation of tracking errors. To tackle them, we present a SAMITE model, built upon SAM2 with additional modules, including: (1) Prototypical Memory Bank: We propose to quantify the feature-wise and position-wise correctness of each frame's tracking results, and select the best frames to condition subsequent frames. As the features of occluded and distracting objects are feature-wise and position-wise inaccurate, their scores would naturally be lower and thus can be filtered to intercept error propagation; (2) Positional Prompt Generator: To further reduce the impacts of distractors, we propose to generate positional mask prompts to provide explicit positional clues for the target, leading to more accurate tracking. Extensive experiments have been conducted on six benchmarks, showing the superiority of SAMITE. The code is available at https://github.com/Sam1224/SAMITE.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2507.21606.pdf' target='_blank'>https://arxiv.org/pdf/2507.21606.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/SSTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaozong Zheng, Bineng Zhong, Qihua Liang, Ning Li, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21606">Decoupled Spatio-Temporal Consistency Learning for Self-Supervised Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of visual tracking has been largely driven by datasets with manual box annotations. However, these box annotations require tremendous human effort, limiting the scale and diversity of existing tracking datasets. In this work, we present a novel Self-Supervised Tracking framework named \textbf{\tracker}, designed to eliminate the need of box annotations. Specifically, a decoupled spatio-temporal consistency training framework is proposed to learn rich target information across timestamps through global spatial localization and local temporal association. This allows for the simulation of appearance and motion variations of instances in real-world scenarios. Furthermore, an instance contrastive loss is designed to learn instance-level correspondences from a multi-view perspective, offering robust instance supervision without additional labels. This new design paradigm enables {\tracker} to effectively learn generic tracking representations in a self-supervised manner, while reducing reliance on extensive box annotations. Extensive experiments on nine benchmark datasets demonstrate that {\tracker} surpasses \textit{SOTA} self-supervised tracking methods, achieving an improvement of more than 25.3\%, 20.4\%, and 14.8\% in AUC (AO) score on the GOT10K, LaSOT, TrackingNet datasets, respectively. Code: https://github.com/GXNU-ZhongLab/SSTrack.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2507.19754.pdf' target='_blank'>https://arxiv.org/pdf/2507.19754.pdf</a></span>   <span><a href='https://github.com/Seung-Hun-Lee/LOMM' target='_blank'>  GitHub</a></span> <span><a href='https://seung-hun-lee.github.io/projects/LOMM/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seunghun Lee, Jiwan Seo, Minwoo Choi, Kiljoon Han, Jaehoon Jeong, Zane Durante, Ehsan Adeli, Sang Hyun Park, Sunghoon Im
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19754">Latest Object Memory Management for Temporally Consistent Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present Latest Object Memory Management (LOMM) for temporally consistent video instance segmentation that significantly improves long-term instance tracking. At the core of our method is Latest Object Memory (LOM), which robustly tracks and continuously updates the latest states of objects by explicitly modeling their presence in each frame. This enables consistent tracking and accurate identity management across frames, enhancing both performance and reliability through the VIS process. Moreover, we introduce Decoupled Object Association (DOA), a strategy that separately handles newly appearing and already existing objects. By leveraging our memory system, DOA accurately assigns object indices, improving matching accuracy and ensuring stable identity consistency, even in dynamic scenes where objects frequently appear and disappear. Extensive experiments and ablation studies demonstrate the superiority of our method over traditional approaches, setting a new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of 54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos. Project page: https://seung-hun-lee.github.io/projects/LOMM/
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2507.19239.pdf' target='_blank'>https://arxiv.org/pdf/2507.19239.pdf</a></span>   <span><a href='https://github.com/zhongjiaru/CoopTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaru Zhong, Jiahao Wang, Jiahui Xu, Xiaofan Li, Zaiqing Nie, Haibao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19239">CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\% mAP and 32.8\% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2507.16191.pdf' target='_blank'>https://arxiv.org/pdf/2507.16191.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/RSTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fansheng Zeng, Bineng Zhong, Haiying Xia, Yufei Tan, Xiantao Hu, Liangtao Shi, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16191">Explicit Context Reasoning with Supervision for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contextual reasoning with constraints is crucial for enhancing temporal consistency in cross-frame modeling for visual tracking. However, mainstream tracking algorithms typically associate context by merely stacking historical information without explicitly supervising the association process, making it difficult to effectively model the target's evolving dynamics. To alleviate this problem, we propose RSTrack, which explicitly models and supervises context reasoning via three core mechanisms. \textit{1) Context Reasoning Mechanism}: Constructs a target state reasoning pipeline, converting unconstrained contextual associations into a temporal reasoning process that predicts the current representation based on historical target states, thereby enhancing temporal consistency. \textit{2) Forward Supervision Strategy}: Utilizes true target features as anchors to constrain the reasoning pipeline, guiding the predicted output toward the true target distribution and suppressing drift in the context reasoning process. \textit{3) Efficient State Modeling}: Employs a compression-reconstruction mechanism to extract the core features of the target, removing redundant information across frames and preventing ineffective contextual associations. These three mechanisms collaborate to effectively alleviate the issue of contextual association divergence in traditional temporal modeling. Experimental results show that RSTrack achieves state-of-the-art performance on multiple benchmark datasets while maintaining real-time running speeds. Our code is available at https://github.com/GXNU-ZhongLab/RSTrack.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2507.15292.pdf' target='_blank'>https://arxiv.org/pdf/2507.15292.pdf</a></span>   <span><a href='https://szupc.github.io/EndoControlMag/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>An Wang, Rulin Zhou, Mengya Xu, Yiru Ye, Longfei Gou, Yiting Chang, Hao Chen, Chwee Ming Lim, Jiankun Wang, Hongliang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15292">EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visualizing subtle vascular motions in endoscopic surgery is crucial for surgical precision and decision-making, yet remains challenging due to the complex and dynamic nature of surgical scenes. To address this, we introduce EndoControlMag, a training-free, Lagrangian-based framework with mask-conditioned vascular motion magnification tailored to endoscopic environments. Our approach features two key modules: a Periodic Reference Resetting (PRR) scheme that divides videos into short overlapping clips with dynamically updated reference frames to prevent error accumulation while maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification (HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores using a pretrained visual tracking model to maintain accurate localization despite occlusions and view changes. It then applies one of two adaptive softening strategies to surrounding tissues: motion-based softening that modulates magnification strength proportional to observed tissue displacement, or distance-based exponential decay that simulates biomechanical force attenuation. This dual-mode approach accommodates diverse surgical scenarios-motion-based softening excels with complex tissue deformations while distance-based softening provides stability during unreliable optical flow conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four different surgery types and various challenging scenarios, including occlusions, instrument disturbance, view changes, and vessel deformations. Quantitative metrics, visual assessments, and expert surgeon evaluations demonstrate that EndoControlMag significantly outperforms existing methods in both magnification accuracy and visual quality while maintaining robustness across challenging surgical conditions. The code, dataset, and video results are available at https://szupc.github.io/EndoControlMag/.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2507.14613.pdf' target='_blank'>https://arxiv.org/pdf/2507.14613.pdf</a></span>   <span><a href='https://github.com/apple1986/DD-SAM2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoping Xu, Christopher Kabat, You Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14613">Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in medical image segmentation have been driven by deep learning; however, most existing methods remain limited by modality-specific designs and exhibit poor adaptability to dynamic medical imaging scenarios. The Segment Anything Model 2 (SAM2) and its related variants, which introduce a streaming memory mechanism for real-time video segmentation, present new opportunities for prompt-based, generalizable solutions. Nevertheless, adapting these models to medical video scenarios typically requires large-scale datasets for retraining or transfer learning, leading to high computational costs and the risk of catastrophic forgetting. To address these challenges, we propose DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction with minimal parameter overhead. This design enables effective fine-tuning of SAM2 on medical videos with limited training data. Unlike existing adapter-based methods focused solely on static images, DD-SAM2 fully exploits SAM2's streaming memory for medical video object tracking and segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation) and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best of our knowledge, this work provides an initial attempt at systematically exploring adapter-based SAM2 fine-tuning for medical video segmentation and tracking. Code, datasets, and models will be publicly available at https://github.com/apple1986/DD-SAM2.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2507.14613.pdf' target='_blank'>https://arxiv.org/pdf/2507.14613.pdf</a></span>   <span><a href='https://github.com/apple1986/DD-SAM2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoping Xu, Christopher Kabat, You Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14613">Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in medical image segmentation have been driven by deep learning; however, most existing methods remain limited by modality-specific designs and exhibit poor adaptability to dynamic medical imaging scenarios. The Segment Anything Model 2 (SAM2) and its related variants, which introduce a streaming memory mechanism for real-time video segmentation, present new opportunities for prompt-based, generalizable solutions. Nevertheless, adapting these models to medical video scenarios typically requires large-scale datasets for retraining or transfer learning, leading to high computational costs and the risk of catastrophic forgetting. To address these challenges, we propose DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction with minimal parameter overhead. This design enables effective fine-tuning of SAM2 on medical videos with limited training data. Unlike existing adapter-based methods focused solely on static images, DD-SAM2 fully exploits SAM2's streaming memory for medical video object tracking and segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation) and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best of our knowledge, this work provides an initial attempt at systematically exploring adapter-based SAM2 fine-tuning for medical video segmentation and tracking. Code, datasets, and models will be publicly available at https://github.com/apple1986/DD-SAM2.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2507.12087.pdf' target='_blank'>https://arxiv.org/pdf/2507.12087.pdf</a></span>   <span><a href='https://github.com/Salvatore-Love/YOLOv8-SMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Yu, Xinyao Liu, Guang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12087">YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned Aerial Vehicle (UAV) perspective is a highly challenging computer vision task. The difficulty stems from three main sources: the extreme scarcity of target appearance features, the complex motion entanglement caused by the combined dynamics of the camera and the targets themselves, and the frequent occlusions and identity ambiguity arising from dense flocking behavior. This paper details our championship-winning solution in the MVA 2025 "Finding Birds" Small Multi-Object Tracking Challenge (SMOT4SB), which adopts the tracking-by-detection paradigm with targeted innovations at both the detection and association levels. On the detection side, we propose a systematic training enhancement framework named \textbf{SliceTrain}. This framework, through the synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic augmentation, effectively addresses the problem of insufficient learning for small objects in high-resolution image training. On the tracking side, we designed a robust tracker that is completely independent of appearance information. By integrating a \textbf{motion direction maintenance (EMA)} mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding box expansion and distance penalty} into the OC-SORT framework, our tracker can stably handle irregular motion and maintain target identities. Our method achieves state-of-the-art performance on the SMOT4SB public test set, reaching an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness and advancement of our framework in solving complex real-world SMOT problems. The source code will be made available at https://github.com/Salvatore-Love/YOLOv8-SMOT.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2507.07603.pdf' target='_blank'>https://arxiv.org/pdf/2507.07603.pdf</a></span>   <span><a href='https://github.com/LouisFinner/HiM2SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixiang Chen, Guolei Sun, Yawei Li, Jie Qin, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07603">HiM2SAM: Enhancing SAM2 with Hierarchical Motion Estimation and Memory Optimization towards Long-term Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents enhancements to the SAM2 framework for video object tracking task, addressing challenges such as occlusions, background clutter, and target reappearance. We introduce a hierarchical motion estimation strategy, combining lightweight linear prediction with selective non-linear refinement to improve tracking accuracy without requiring additional training. In addition, we optimize the memory bank by distinguishing long-term and short-term memory frames, enabling more reliable tracking under long-term occlusions and appearance changes. Experimental results show consistent improvements across different model scales. Our method achieves state-of-the-art performance on LaSOT and LaSOText with the large model, achieving 9.6% and 7.2% relative improvements in AUC over the original SAM2, and demonstrates even larger relative gains on smaller models, highlighting the effectiveness of our trainless, low-overhead improvements for boosting long-term tracking performance. The code is available at https://github.com/LouisFinner/HiM2SAM.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2507.07603.pdf' target='_blank'>https://arxiv.org/pdf/2507.07603.pdf</a></span>   <span><a href='https://github.com/LouisFinner/HiM2SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixiang Chen, Guolei Sun, Yawei Li, Jie Qin, Luca Benini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07603">HiM2SAM: Enhancing SAM2 with Hierarchical Motion Estimation and Memory Optimization towards Long-term Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents enhancements to the SAM2 framework for video object tracking task, addressing challenges such as occlusions, background clutter, and target reappearance. We introduce a hierarchical motion estimation strategy, combining lightweight linear prediction with selective non-linear refinement to improve tracking accuracy without requiring additional training. In addition, we optimize the memory bank by distinguishing long-term and short-term memory frames, enabling more reliable tracking under long-term occlusions and appearance changes. Experimental results show consistent improvements across different model scales. Our method achieves state-of-the-art performance on LaSOT and LaSOText with the large model, achieving 9.6% and 7.2% relative improvements in AUC over the original SAM2, and demonstrates even larger relative gains on smaller models, highlighting the effectiveness of our trainless, low-overhead improvements for boosting long-term tracking performance. The code is available at https://github.com/LouisFinner/HiM2SAM.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2507.06543.pdf' target='_blank'>https://arxiv.org/pdf/2507.06543.pdf</a></span>   <span><a href='https://github.com/naver-ai/tobo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taekyung Kim, Dongyoon Han, Byeongho Heo, Jeongeun Park, Sangdoo Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06543">Token Bottleneck: One Token to Remember Dynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deriving compact and temporally aware visual representations from dynamic scenes is essential for successful execution of sequential scene understanding tasks such as visual tracking and robotic manipulation. In this paper, we introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised learning pipeline that squeezes a scene into a bottleneck token and predicts the subsequent scene using minimal patches as hints. The ToBo pipeline facilitates the learning of sequential scene representations by conservatively encoding the reference scene into a compact bottleneck token during the squeeze step. In the expansion step, we guide the model to capture temporal dynamics by predicting the target scene using the bottleneck token along with few target patches as hints. This design encourages the vision backbone to embed temporal dependencies, thereby enabling understanding of dynamic transitions across scenes. Extensive experiments in diverse sequential tasks, including video label propagation and robot manipulation in simulated environments demonstrate the superiority of ToBo over baselines. Moreover, deploying our pre-trained model on physical robots confirms its robustness and effectiveness in real-world environments. We further validate the scalability of ToBo across different model scales.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2507.06400.pdf' target='_blank'>https://arxiv.org/pdf/2507.06400.pdf</a></span>   <span><a href='https://vranlee.github.io/SU-T/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiran Li, Yeqiang Liu, Qiannan Guo, Yijie Wei, Hwa Liang Leo, Zhenbo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06400">When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) technology has made significant progress in terrestrial applications, but underwater tracking scenarios remain underexplored despite their importance to marine ecology and aquaculture. In this paper, we present Multiple Fish Tracking Dataset 2025 (MFT25), a comprehensive dataset specifically designed for underwater multiple fish tracking, featuring 15 diverse video sequences with 408,578 meticulously annotated bounding boxes across 48,066 frames. Our dataset captures various underwater environments, fish species, and challenging conditions including occlusions, similar appearances, and erratic motion patterns. Additionally, we introduce Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework featuring an Unscented Kalman Filter (UKF) optimized for non-linear swimming patterns of fish and a novel Fish-Intersection-over-Union (FishIoU) matching that accounts for the unique morphological characteristics of aquatic species. Extensive experiments demonstrate that our SU-T baseline achieves state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while revealing fundamental differences between fish tracking and terrestrial object tracking scenarios. The dataset and codes are released at https://vranlee.github.io/SU-T/.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2507.05899.pdf' target='_blank'>https://arxiv.org/pdf/2507.05899.pdf</a></span>   <span><a href='https://github.com/supertyd/FlexTrack/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuedong Tan, Jiawei Shao, Eduard Zamfir, Ruanjun Li, Zhaochong An, Chao Ma, Danda Paudel, Luc Van Gool, Radu Timofte, Zongwei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05899">What You Have is What You Track: Adaptive and Robust Multimodal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal data is known to be helpful for visual tracking by improving robustness to appearance variations. However, sensor synchronization challenges often compromise data availability, particularly in video settings where shortages can be temporal. Despite its importance, this area remains underexplored. In this paper, we present the first comprehensive study on tracker performance with temporally incomplete multimodal data. Unsurprisingly, under such a circumstance, existing trackers exhibit significant performance degradation, as their rigid architectures lack the adaptability needed to effectively handle missing modalities. To address these limitations, we propose a flexible framework for robust multimodal tracking. We venture that a tracker should dynamically activate computational units based on missing data rates. This is achieved through a novel Heterogeneous Mixture-of-Experts fusion mechanism with adaptive complexity, coupled with a video-level masking strategy that ensures both temporal consistency and spatial completeness which is critical for effective video tracking. Surprisingly, our model not only adapts to varying missing rates but also adjusts to scene complexity. Extensive experiments show that our model achieves SOTA performance across 9 benchmarks, excelling in both conventional complete and missing modality settings. The code and benchmark will be publicly available at https://github.com/supertyd/FlexTrack/tree/main.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2507.02479.pdf' target='_blank'>https://arxiv.org/pdf/2507.02479.pdf</a></span>   <span><a href='https://github.com/loseevaya/CrowdTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Fu, Yuwen Chen, Zhuofan Chen, Mengyang Zhao, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02479">CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2507.00648.pdf' target='_blank'>https://arxiv.org/pdf/2507.00648.pdf</a></span>   <span><a href='https://github.com/Z-Z188/UMDATrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Yao, Rui Zhu, Ziqi Wang, Wenqi Ren, Yanyang Yan, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00648">UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has gained promising progress in past decades. Most of the existing approaches focus on learning target representation in well-conditioned daytime data, while for the unconstrained real-world scenarios with adverse weather conditions, e.g. nighttime or foggy environment, the tremendous domain shift leads to significant performance degradation. In this paper, we propose UMDATrack, which is capable of maintaining high-quality target state prediction under various adverse weather conditions within a unified domain adaptation framework. Specifically, we first use a controllable scenario generator to synthesize a small amount of unlabeled videos (less than 2% frames in source daytime datasets) in multiple weather conditions under the guidance of different text prompts. Afterwards, we design a simple yet effective domain-customized adapter (DCA), allowing the target objects' representation to rapidly adapt to various weather conditions without redundant model updating. Furthermore, to enhance the localization consistency between source and target domains, we propose a target-aware confidence alignment module (TCA) following optimal transport theorem. Extensive experiments demonstrate that UMDATrack can surpass existing advanced visual trackers and lead new state-of-the-art performance by a significant margin. Our code is available at https://github.com/Z-Z188/UMDATrack.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2506.23972.pdf' target='_blank'>https://arxiv.org/pdf/2506.23972.pdf</a></span>   <span><a href='https://github.com/xuboyue1999/mmtrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyue Xu, Ruichao Hou, Tongwei Ren, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23972">Visual and Memory Dual Adapter for Multi-Modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt-learning-based multi-modal trackers have achieved promising progress by employing lightweight visual adapters to incorporate auxiliary modality features into frozen foundation models. However, existing approaches often struggle to learn reliable prompts due to limited exploitation of critical cues across frequency and temporal domains. In this paper, we propose a novel visual and memory dual adapter (VMDA) to construct more robust and discriminative representations for multi-modal tracking. Specifically, we develop a simple but effective visual adapter that adaptively transfers discriminative cues from auxiliary modality to dominant modality by jointly modeling the frequency, spatial, and channel-wise features. Additionally, we design the memory adapter inspired by the human memory mechanism, which stores global temporal cues and performs dynamic update and retrieval operations to ensure the consistent propagation of reliable temporal information across video sequences. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth, and RGB-Event tracking. Code and models are available at https://github.com/xuboyue1999/mmtrack.git.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2506.23972.pdf' target='_blank'>https://arxiv.org/pdf/2506.23972.pdf</a></span>   <span><a href='https://github.com/xuboyue1999/mmtrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyue Xu, Ruichao Hou, Tongwei Ren, Dongming zhou, Gangshan Wu, Jinde Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23972">Learning Frequency and Memory-Aware Prompts for Multi-Modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt-learning-based multi-modal trackers have made strong progress by using lightweight visual adapters to inject auxiliary-modality cues into frozen foundation models. However, they still underutilize two essentials: modality-specific frequency structure and long-range temporal dependencies. We present Learning Frequency and Memory-Aware Prompts, a dual-adapter framework that injects lightweight prompts into a frozen RGB tracker. A frequency-guided visual adapter adaptively transfers complementary cues across modalities by jointly calibrating spatial, channel, and frequency components, narrowing the modality gap without full fine-tuning. A multilevel memory adapter with short, long, and permanent memory stores, updates, and retrieves reliable temporal context, enabling consistent propagation across frames and robust recovery from occlusion, motion blur, and illumination changes. This unified design preserves the efficiency of prompt learning while strengthening cross-modal interaction and temporal coherence. Extensive experiments on RGB-Thermal, RGB-Depth, and RGB-Event benchmarks show consistent state-of-the-art results over fully fine-tuned and adapter-based baselines, together with favorable parameter efficiency and runtime. Code and models are available at https://github.com/xuboyue1999/mmtrack.git.
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2506.23972.pdf' target='_blank'>https://arxiv.org/pdf/2506.23972.pdf</a></span>   <span><a href='https://github.com/xuboyue1999/mmtrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyue Xu, Ruichao Hou, Tongwei Ren, Dongming zhou, Gangshan Wu, Jinde Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23972">Learning Frequency and Memory-Aware Prompts for Multi-Modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prompt-learning-based multi-modal trackers have made strong progress by using lightweight visual adapters to inject auxiliary-modality cues into frozen foundation models. However, they still underutilize two essentials: modality-specific frequency structure and long-range temporal dependencies. We present Learning Frequency and Memory-Aware Prompts, a dual-adapter framework that injects lightweight prompts into a frozen RGB tracker. A frequency-guided visual adapter adaptively transfers complementary cues across modalities by jointly calibrating spatial, channel, and frequency components, narrowing the modality gap without full fine-tuning. A multilevel memory adapter with short, long, and permanent memory stores, updates, and retrieves reliable temporal context, enabling consistent propagation across frames and robust recovery from occlusion, motion blur, and illumination changes. This unified design preserves the efficiency of prompt learning while strengthening cross-modal interaction and temporal coherence. Extensive experiments on RGB-Thermal, RGB-Depth, and RGB-Event benchmarks show consistent state-of-the-art results over fully fine-tuned and adapter-based baselines, together with favorable parameter efficiency and runtime. Code and models are available at https://github.com/xuboyue1999/mmtrack.git.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2506.23783.pdf' target='_blank'>https://arxiv.org/pdf/2506.23783.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Mamba_FETrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiao Wang, Ju Huang, Qingchuan Ma, Jinfeng Gao, Chunyi Xu, Xiao Wang, Lan Chen, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23783">Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining traditional RGB cameras with bio-inspired event cameras for robust object tracking has garnered increasing attention in recent years. However, most existing multimodal tracking algorithms depend heavily on high-complexity Vision Transformer architectures for feature extraction and fusion across modalities. This not only leads to substantial computational overhead but also limits the effectiveness of cross-modal interactions. In this paper, we propose an efficient RGB-Event object tracking framework based on the linear-complexity Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a lightweight Prompt Generator that utilizes embedded features from each modality, together with a shared prompt pool, to dynamically generate modality-specific learnable prompt vectors. These prompts, along with the modality-specific embedded features, are then fed into a Vision Mamba-based FEMamba backbone, which facilitates prompt-guided feature extraction, cross-modal interaction, and fusion in a unified manner. Finally, the fused representations are passed to the tracking head for accurate target localization. Extensive experimental evaluations on multiple RGB-Event tracking benchmarks, including short-term COESOT dataset and long-term datasets, i.e., FE108 and FELT V2, demonstrate the superior performance and efficiency of the proposed tracking framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/Mamba_FETrack
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2506.17119.pdf' target='_blank'>https://arxiv.org/pdf/2506.17119.pdf</a></span>   <span><a href='https://github.com/GreatenAnoymous/RGBTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Guo, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17119">RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a robust framework, RGBTrack, for real-time 6D pose estimation and tracking that operates solely on RGB data, thereby eliminating the need for depth input for such dynamic and precise object pose tracking tasks. Building on the FoundationPose architecture, we devise a novel binary search strategy combined with a render-and-compare mechanism to efficiently infer depth and generate robust pose hypotheses from true-scale CAD models. To maintain stable tracking in dynamic scenarios, including rapid movements and occlusions, RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman filter and a state machine for proactive object pose recovery. In addition, RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale using an initial depth estimate, enabling seamless integration with modern generative reconstruction techniques. Extensive evaluations on benchmark datasets demonstrate that RGBTrack's novel depth-free approach achieves competitive accuracy and real-time performance, making it a promising practical solution candidate for application areas including robotics, augmented reality, and computer vision.
  The source code for our implementation will be made publicly available at https://github.com/GreatenAnoymous/RGBTrack.git.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2506.15945.pdf' target='_blank'>https://arxiv.org/pdf/2506.15945.pdf</a></span>   <span><a href='https://github.com/arc-l/karl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kowndinya Boyalakuntla, Abdeslam Boularias, Jingjin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15945">KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic object tracking and grasping over eye-on-hand (EoH) systems, significantly expanding such systems capabilities in challenging, realistic environments. In comparison to the previous state-of-the-art, KARL (1) incorporates a novel six-stage RL curriculum that doubles the system's motion range, thereby greatly enhancing the system's grasping performance, (2) integrates a robust Kalman filter layer between the perception and reinforcement learning (RL) control modules, enabling the system to maintain an uncertain but continuous 6D pose estimate even when the target object temporarily exits the camera's field-of-view or undergoes rapid, unpredictable motion, and (3) introduces mechanisms to allow retries to gracefully recover from unavoidable policy execution failures. Extensive evaluations conducted in both simulation and real-world experiments qualitatively and quantitatively corroborate KARL's advantage over earlier systems, achieving higher grasp success rates and faster robot execution speed. Source code and supplementary materials for KARL will be made available at: https://github.com/arc-l/karl.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2506.12105.pdf' target='_blank'>https://arxiv.org/pdf/2506.12105.pdf</a></span>   <span><a href='https://github.com/softwarePupil/VSMB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxiang Chen, Wei Zhao, Rufei Zhang, Nannan Li, Dongjin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12105">Multiple Object Tracking in Video SAR: A Benchmark and Tracking Baseline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of multi-object tracking using video synthetic aperture radar (Video SAR), Doppler shifts induced by target motion result in artifacts that are easily mistaken for shadows caused by static occlusions. Moreover, appearance changes of the target caused by Doppler mismatch may lead to association failures and disrupt trajectory continuity. A major limitation in this field is the lack of public benchmark datasets for standardized algorithm evaluation. To address the above challenges, we collected and annotated 45 video SAR sequences containing moving targets, and named the Video SAR MOT Benchmark (VSMB). Specifically, to mitigate the effects of trailing and defocusing in moving targets, we introduce a line feature enhancement mechanism that emphasizes the positive role of motion shadows and reduces false alarms induced by static occlusions. In addition, to mitigate the adverse effects of target appearance variations, we propose a motion-aware clue discarding mechanism that substantially improves tracking robustness in Video SAR. The proposed model achieves state-of-the-art performance on the VSMB, and the dataset and model are released at https://github.com/softwarePupil/VSMB.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2506.02866.pdf' target='_blank'>https://arxiv.org/pdf/2506.02866.pdf</a></span>   <span><a href='https://github.com/AhsanBaidar/MVTD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahsan Baidar Bakht, Muhayy Ud Din, Sajid Javed, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.02866">MVTD: A Benchmark Dataset for Maritime Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is a fundamental task with widespread applications in autonomous navigation, surveillance, and maritime robotics. Despite significant advances in generic object tracking, maritime environments continue to present unique challenges, including specular water reflections, low-contrast targets, dynamically changing backgrounds, and frequent occlusions. These complexities significantly degrade the performance of state-of-the-art tracking algorithms, highlighting the need for domain-specific datasets. To address this gap, we introduce the Maritime Visual Tracking Dataset (MVTD), a comprehensive and publicly available benchmark specifically designed for maritime VOT. MVTD comprises 182 high-resolution video sequences, totaling approximately 150,000 frames, and includes four representative object classes: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset captures a diverse range of operational conditions and maritime scenarios, reflecting the real-world complexities of maritime environments. We evaluated 14 recent SOTA tracking algorithms on the MVTD benchmark and observed substantial performance degradation compared to their performance on general-purpose datasets. However, when fine-tuned on MVTD, these models demonstrate significant performance gains, underscoring the effectiveness of domain adaptation and the importance of transfer learning in specialized tracking contexts. The MVTD dataset fills a critical gap in the visual tracking community by providing a realistic and challenging benchmark for maritime scenarios. Dataset and Source Code can be accessed here "https://github.com/AhsanBaidar/MVTD".
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2506.01373.pdf' target='_blank'>https://arxiv.org/pdf/2506.01373.pdf</a></span>   <span><a href='https://github.com/tstanczyk95/McByte' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomasz Stanczyk, Seongro Yoon, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01373">No Train Yet Gain: Towards Generic Multi-Object Tracking in Sports and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is essential for sports analytics, enabling performance evaluation and tactical insights. However, tracking in sports is challenging due to fast movements, occlusions, and camera shifts. Traditional tracking-by-detection methods require extensive tuning, while segmentation-based approaches struggle with track processing. We propose McByte, a tracking-by-detection framework that integrates temporally propagated segmentation mask as an association cue to improve robustness without per-video tuning. Unlike many existing methods, McByte does not require training, relying solely on pre-trained models and object detectors commonly used in the community. Evaluated on SportsMOT, DanceTrack, SoccerNet-tracking 2022 and MOT17, McByte demonstrates strong performance across sports and general pedestrian tracking. Our results highlight the benefits of mask propagation for a more adaptable and generalizable MOT approach. Code will be made available at https://github.com/tstanczyk95/McByte.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2506.00774.pdf' target='_blank'>https://arxiv.org/pdf/2506.00774.pdf</a></span>   <span><a href='https://github.com/Milad-Khanchi/DepthMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Khanchi, Maria Amer, Charalambos Poullis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00774">Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current motion-based multiple object tracking (MOT) approaches rely heavily on Intersection-over-Union (IoU) for object association. Without using 3D features, they are ineffective in scenarios with occlusions or visually similar objects. To address this, our paper presents a novel depth-aware framework for MOT. We estimate depth using a zero-shot approach and incorporate it as an independent feature in the association process. Additionally, we introduce a Hierarchical Alignment Score that refines IoU by integrating both coarse bounding box overlap and fine-grained (pixel-level) alignment to improve association accuracy without requiring additional learnable parameters. To our knowledge, this is the first MOT framework to incorporate 3D features (monocular depth) as an independent decision matrix in the association step. Our framework achieves state-of-the-art results on challenging benchmarks without any training nor fine-tuning. The code is available at https://github.com/Milad-Khanchi/DepthMOT
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2506.00325.pdf' target='_blank'>https://arxiv.org/pdf/2506.00325.pdf</a></span>   <span><a href='https://github.com/pgao-lab/DiffDf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Xu, Peng Gao, Wen-Jia Tang, Fei Wang, Ru-Yue Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00325">Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although deep learning-based visual tracking methods have made significant progress, they exhibit vulnerabilities when facing carefully designed adversarial attacks, which can lead to a sharp decline in tracking performance. To address this issue, this paper proposes for the first time a novel adversarial defense method based on denoise diffusion probabilistic models, termed DiffDf, aimed at effectively improving the robustness of existing visual tracking methods against adversarial attacks. DiffDf establishes a multi-scale defense mechanism by combining pixel-level reconstruction loss, semantic consistency loss, and structural similarity loss, effectively suppressing adversarial perturbations through a gradual denoising process. Extensive experimental results on several mainstream datasets show that the DiffDf method demonstrates excellent generalization performance for trackers with different architectures, significantly improving various evaluation metrics while achieving real-time inference speeds of over 30 FPS, showcasing outstanding defense performance and efficiency. Codes are available at https://github.com/pgao-lab/DiffDf.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2505.23704.pdf' target='_blank'>https://arxiv.org/pdf/2505.23704.pdf</a></span>   <span><a href='https://github.com/HamadYA/CLDTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamad Alansari, Sajid Javed, Iyyakutti Iyappan Ganapathi, Sara Alansari, Muzammal Naseer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23704">CLDTracker: A Comprehensive Language Description for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>VOT remains a fundamental yet challenging task in computer vision due to dynamic appearance changes, occlusions, and background clutter. Traditional trackers, relying primarily on visual cues, often struggle in such complex scenarios. Recent advancements in VLMs have shown promise in semantic understanding for tasks like open-vocabulary detection and image captioning, suggesting their potential for VOT. However, the direct application of VLMs to VOT is hindered by critical limitations: the absence of a rich and comprehensive textual representation that semantically captures the target object's nuances, limiting the effective use of language information; inefficient fusion mechanisms that fail to optimally integrate visual and textual features, preventing a holistic understanding of the target; and a lack of temporal modeling of the target's evolving appearance in the language domain, leading to a disconnect between the initial description and the object's subsequent visual changes. To bridge these gaps and unlock the full potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive Language Description framework for robust visual Tracking. Our tracker introduces a dual-branch architecture consisting of a textual and a visual branch. In the textual branch, we construct a rich bag of textual descriptions derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with semantic and contextual cues to address the lack of rich textual representation. Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves SOTA performance, validating the effectiveness of leveraging robust and temporally-adaptive vision-language representations for tracking. Code and models are publicly available at: https://github.com/HamadYA/CLDTracker
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2505.23189.pdf' target='_blank'>https://arxiv.org/pdf/2505.23189.pdf</a></span>   <span><a href='https://pku-epic.github.io/TrackVLA-web' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoan Wang, Jiazhao Zhang, Minghan Li, Jiahang Liu, Anqi Li, Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng Zhang, He Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23189">TrackVLA: Embodied Visual Tracking in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed. Our project page is: https://pku-epic.github.io/TrackVLA-web.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2505.21795.pdf' target='_blank'>https://arxiv.org/pdf/2505.21795.pdf</a></span>   <span><a href='https://github.com/ClaudiaCuttano/SANSA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ClaudiaCuttano/SANSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Claudia Cuttano, Gabriele Trivigno, Giuseppe Averta, Carlo Masone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21795">SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot segmentation aims to segment unseen object categories from just a handful of annotated examples. This requires mechanisms that can both identify semantically related objects across images and accurately produce segmentation masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, offers both strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art performance on few-shot segmentation benchmarks specifically designed to assess generalization, outperforms generalist methods in the popular in-context setting, supports various prompts flexible interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code is available at https://github.com/ClaudiaCuttano/SANSA.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2505.16321.pdf' target='_blank'>https://arxiv.org/pdf/2505.16321.pdf</a></span>   <span><a href='https://github.com/zj5559/Motion-Prompt-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Zhao, Xin Chen, Yongsheng Yuan, Michael Felsberg, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16321">Efficient Motion Prompt Learning for Robust Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the challenges of processing temporal information, most trackers depend solely on visual discriminability and overlook the unique temporal coherence of video data. In this paper, we propose a lightweight and plug-and-play motion prompt tracking method. It can be easily integrated into existing vision-based trackers to build a joint tracking framework leveraging both motion and vision cues, thereby achieving robust tracking through efficient prompt learning. A motion encoder with three different positional encodings is proposed to encode the long-term motion trajectory into the visual embedding space, while a fusion decoder and an adaptive weight mechanism are designed to dynamically fuse visual and motion features. We integrate our motion module into three different trackers with five models in total. Experiments on seven challenging tracking benchmarks demonstrate that the proposed motion module significantly improves the robustness of vision-based trackers, with minimal training costs and negligible speed sacrifice. Code is available at https://github.com/zj5559/Motion-Prompt-Tracking.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2505.15928.pdf' target='_blank'>https://arxiv.org/pdf/2505.15928.pdf</a></span>   <span><a href='https://github.com/t-montes/viqagent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tony Montes, Fernando Lozano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.15928">ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Video Question Answering (VideoQA) have introduced LLM-based agents, modular frameworks, and procedural solutions, yielding promising results. These systems use dynamic agents and memory-based mechanisms to break down complex tasks and refine answers. However, significant improvements remain in tracking objects for grounding over time and decision-making based on reasoning to better align object references with language model outputs, as newer models get better at both tasks. This work presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA) that combines a Chain-of-Thought framework with grounding reasoning alongside YOLO-World to enhance object tracking and alignment. This approach establishes a new state-of-the-art in VideoQA and Video Understanding, showing enhanced performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also enables cross-checking of grounding timeframes, improving accuracy and providing valuable support for verification and increased output reliability across multiple video domains. The code is available at https://github.com/t-montes/viqagent.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2505.12903.pdf' target='_blank'>https://arxiv.org/pdf/2505.12903.pdf</a></span>   <span><a href='https://github.com/Event-AHU/SlowFast_Event_Track' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiao Wang, Xiao Wang, Liye Jin, Bo Jiang, Lin Zhu, Lan Chen, Yonghong Tian, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12903">Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing tracking algorithms typically rely on low-frame-rate RGB cameras coupled with computationally intensive deep neural network architectures to achieve effective tracking. However, such frame-based methods inherently face challenges in achieving low-latency performance and often fail in resource-constrained environments. Visual object tracking using bio-inspired event cameras has emerged as a promising research direction in recent years, offering distinct advantages for low-latency applications. In this paper, we propose a novel Slow-Fast Tracking paradigm that flexibly adapts to different operational requirements, termed SFTrack. The proposed framework supports two complementary modes, i.e., a high-precision slow tracker for scenarios with sufficient computational resources, and an efficient fast tracker tailored for latency-aware, resource-constrained environments. Specifically, our framework first performs graph-based representation learning from high-temporal-resolution event streams, and then integrates the learned graph-structured information into two FlashAttention-based vision backbones, yielding the slow and fast trackers, respectively. The fast tracker achieves low latency through a lightweight network design and by producing multiple bounding box outputs in a single forward pass. Finally, we seamlessly combine both trackers via supervised fine-tuning and further enhance the fast tracker's performance through a knowledge distillation strategy. Extensive experiments on public benchmarks, including FE240, COESOT, and EventVOT, demonstrate the effectiveness and efficiency of our proposed method across different real-world scenarios. The source code has been released on https://github.com/Event-AHU/SlowFast_Event_Track.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2505.08999.pdf' target='_blank'>https://arxiv.org/pdf/2505.08999.pdf</a></span>   <span><a href='https://github.com/pgao-lab/AMGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei-Long Tian, Peng Gao, Xiao Liu, Long Xu, Hamido Fujita, Hanan Aljuai, Mao-Li Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08999">Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, visual tracking methods based on convolutional neural networks and Transformers have achieved remarkable performance and have been successfully applied in fields such as autonomous driving. However, the numerous security issues exposed by deep learning models have gradually affected the reliable application of visual tracking methods in real-world scenarios. Therefore, how to reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks has become a critical problem that needs to be addressed. To this end, we propose an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking. This method integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing, which can significantly enhance the transferability and attack effectiveness of adversarial examples. AMGA randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model. This paradigm minimizes the gap between white- and black-box adversarial attacks, thus achieving excellent attack performance in black-box scenarios. Extensive experimental results on large-scale datasets such as OTB2015, LaSOT, and GOT-10k demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples. Codes and data are available at https://github.com/pgao-lab/AMGA.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2505.05936.pdf' target='_blank'>https://arxiv.org/pdf/2505.05936.pdf</a></span>   <span><a href='https://github.com/Nightwatch-Fox11/CGTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihong Li, Xiaoqiong Liu, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05936">CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in visual object tracking have markedly improved the capabilities of unmanned aerial vehicle (UAV) tracking, which is a critical component in real-world robotics applications. While the integration of hierarchical lightweight networks has become a prevalent strategy for enhancing efficiency in UAV tracking, it often results in a significant drop in network capacity, which further exacerbates challenges in UAV scenarios, such as frequent occlusions and extreme changes in viewing angles. To address these issues, we introduce a novel family of UAV trackers, termed CGTrack, which combines explicit and implicit techniques to expand network capacity within a coarse-to-fine framework. Specifically, we first introduce a Hierarchical Feature Cascade (HFC) module that leverages the spirit of feature reuse to increase network capacity by integrating the deep semantic cues with the rich spatial information, incurring minimal computational costs while enhancing feature representation. Based on this, we design a novel Lightweight Gated Center Head (LGCH) that utilizes gating mechanisms to decouple target-oriented coordinates from previously expanded features, which contain dense local discriminative information. Extensive experiments on three challenging UAV tracking benchmarks demonstrate that CGTrack achieves state-of-the-art performance while running fast. Code will be available at https://github.com/Nightwatch-Fox11/CGTrack.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2505.01257.pdf' target='_blank'>https://arxiv.org/pdf/2505.01257.pdf</a></span>   <span><a href='https://github.com/TrackingLaboratory/CAMELTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vladimir Somers, Baptiste Standaert, Victor Joos, Alexandre Alahi, Christophe De Vleeschouwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01257">CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online multi-object tracking has been recently dominated by tracking-by-detection (TbD) methods, where recent advances rely on increasingly sophisticated heuristics for tracklet representation, feature fusion, and multi-stage matching. The key strength of TbD lies in its modular design, enabling the integration of specialized off-the-shelf models like motion predictors and re-identification. However, the extensive usage of human-crafted rules for temporal associations makes these methods inherently limited in their ability to capture the complex interplay between various tracking cues. In this work, we introduce CAMEL, a novel association module for Context-Aware Multi-Cue ExpLoitation, that learns resilient association strategies directly from data, breaking free from hand-crafted heuristics while maintaining TbD's valuable modularity. At its core, CAMEL employs two transformer-based modules and relies on a novel association-centric training scheme to effectively model the complex interactions between tracked targets and their various association cues. Unlike end-to-end detection-by-tracking approaches, our method remains lightweight and fast to train while being able to leverage external off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack, achieves state-of-the-art performance on multiple tracking benchmarks. Our code is available at https://github.com/TrackingLaboratory/CAMELTrack.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2504.21716.pdf' target='_blank'>https://arxiv.org/pdf/2504.21716.pdf</a></span>   <span><a href='https://github.com/marc1198/chat-hsr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marc Glocker, Peter HÃ¶nig, Matthias Hirschmanner, Markus Vincze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21716">LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2504.18068.pdf' target='_blank'>https://arxiv.org/pdf/2504.18068.pdf</a></span>   <span><a href='https://github.com/bytepioneerX/s3mot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohao Yan, Shaoquan Feng, Xingxing Li, Yuxuan Zhou, Chunxi Xia, Shengyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18068">S3MOT: Monocular 3D Object Tracking with Selective State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at https://github.com/bytepioneerX/s3mot.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2504.15609.pdf' target='_blank'>https://arxiv.org/pdf/2504.15609.pdf</a></span>   <span><a href='https://github.com/LiYunfengLYF/SonarT165' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfeng Li, Bo Wang, Jiahao Wan, Xueyi Wu, Ye Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15609">SonarT165: A Large-scale Benchmark and STFTrack Framework for Acoustic Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater observation systems typically integrate optical cameras and imaging sonar systems. When underwater visibility is insufficient, only sonar systems can provide stable data, which necessitates exploration of the underwater acoustic object tracking (UAOT) task. Previous studies have explored traditional methods and Siamese networks for UAOT. However, the absence of a unified evaluation benchmark has significantly constrained the value of these methods. To alleviate this limitation, we propose the first large-scale UAOT benchmark, SonarT165, comprising 165 square sequences, 165 fan sequences, and 205K high-quality annotations. Experimental results demonstrate that SonarT165 reveals limitations in current state-of-the-art SOT trackers. To address these limitations, we propose STFTrack, an efficient framework for acoustic object tracking. It includes two novel modules, a multi-view template fusion module (MTFM) and an optimal trajectory correction module (OTCM). The MTFM module integrates multi-view feature of both the original image and the binary image of the dynamic template, and introduces a cross-attention-like layer to fuse the spatio-temporal target representations. The OTCM module introduces the acoustic-response-equivalent pixel property and proposes normalized pixel brightness response scores, thereby suppressing suboptimal matches caused by inaccurate Kalman filter prediction boxes. To further improve the model feature, STFTrack introduces a acoustic image enhancement method and a Frequency Enhancement Module (FEM) into its tracking pipeline. Comprehensive experiments show the proposed STFTrack achieves state-of-the-art performance on the proposed benchmark. The code is available at https://github.com/LiYunfengLYF/SonarT165.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2504.14423.pdf' target='_blank'>https://arxiv.org/pdf/2504.14423.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Adversarial_Attack_Defense' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiang Chen, Xiao Wang, Haowen Wang, Bo Jiang, Lin Zhu, Dawei Zhang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14423">Adversarial Attack for RGB-Event based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking is a crucial research topic in the fields of computer vision and multi-modal fusion. Among various approaches, robust visual tracking that combines RGB frames with Event streams has attracted increasing attention from researchers. While striving for high accuracy and efficiency in tracking, it is also important to explore how to effectively conduct adversarial attacks and defenses on RGB-Event stream tracking algorithms, yet research in this area remains relatively scarce. To bridge this gap, in this paper, we propose a cross-modal adversarial attack algorithm for RGB-Event visual tracking. Because of the diverse representations of Event streams, and given that Event voxels and frames are more commonly used, this paper will focus on these two representations for an in-depth study. Specifically, for the RGB-Event voxel, we first optimize the perturbation by adversarial loss to generate RGB frame adversarial examples. For discrete Event voxel representations, we propose a two-step attack strategy, more in detail, we first inject Event voxels into the target region as initialized adversarial examples, then, conduct a gradient-guided optimization by perturbing the spatial location of the Event voxels. For the RGB-Event frame based tracking, we optimize the cross-modal universal perturbation by integrating the gradient information from multimodal data. We evaluate the proposed approach against attacks on three widely used RGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive experiments show that our method significantly reduces the performance of the tracker across numerous datasets in both unimodal and multimodal scenarios. The source code will be released on https://github.com/Event-AHU/Adversarial_Attack_Defense
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2504.10165.pdf' target='_blank'>https://arxiv.org/pdf/2504.10165.pdf</a></span>   <span><a href='https://dat-nguyenvn.github.io/WildLive/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nguyen Ngoc Dat, Tom Richardson, Matthew Watson, Kilian Meier, Jenna Kline, Sid Reid, Guy Maalouf, Duncan Hine, Majid Mirmehdi, Tilo Burghardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10165">WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Live tracking of wildlife via high-resolution video processing directly onboard drones is widely unexplored and most existing solutions rely on streaming video to ground stations to support navigation. Yet, both autonomous animal-reactive flight control beyond visual line of sight and/or mission-specific individual and behaviour recognition tasks rely to some degree on this capability. In response, we introduce WildLive - a near real-time animal detection and tracking framework for high-resolution imagery running directly onboard uncrewed aerial vehicles (UAVs). The system performs multi-animal detection and tracking at 17.81fps for HD and 7.53fps on 4K video streams suitable for operation during higher altitude flights to minimise animal disturbance. Our system is optimised for Jetson Orin AGX onboard hardware. It integrates the efficiency of sparse optical flow tracking and mission-specific sampling with device-optimised and proven YOLO-driven object detection and segmentation techniques. Essentially, computational resource is focused onto spatio-temporal regions of high uncertainty to significantly improve UAV processing speeds. Alongside, we introduce our WildLive dataset, which comprises 200K+ annotated animal instances across 19K+ frames from 4K UAV videos collected at the Ol Pejeta Conservancy in Kenya. All frames contain ground truth bounding boxes, segmentation masks, as well as individual tracklets and tracking point trajectories. We compare our system against current object tracking approaches including OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments with onboard hardware confirm that near real-time high-resolution wildlife tracking is possible on UAVs whilst maintaining high accuracy levels as needed for future navigational and mission-specific animal-centric operational autonomy. Our materials are available at: https://dat-nguyenvn.github.io/WildLive/
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2504.09195.pdf' target='_blank'>https://arxiv.org/pdf/2504.09195.pdf</a></span>   <span><a href='https://github.com/Tzoulio/ReferGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tzoulio Chamiti, Leandro Di Bella, Adrian Munteanu, Nikos Deligiannis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09195">ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple objects based on textual queries is a challenging task that requires linking language understanding with object association across frames. Previous works typically train the whole process end-to-end or integrate an additional referring text module into a multi-object tracker, but they both require supervised training and potentially struggle with generalization to open-set queries. In this work, we introduce ReferGPT, a novel zero-shot referring multi-object tracking framework. We provide a multi-modal large language model (MLLM) with spatial knowledge enabling it to generate 3D-aware captions. This enhances its descriptive capabilities and supports a more flexible referring vocabulary without training. We also propose a robust query-matching strategy, leveraging CLIP-based semantic encoding and fuzzy matching to associate MLLM generated captions with user queries. Extensive experiments on Refer-KITTI, Refer-KITTIv2 and Refer-KITTI+ demonstrate that ReferGPT achieves competitive performance against trained methods, showcasing its robustness and zero-shot capabilities in autonomous driving. The codes are available on https://github.com/Tzoulio/ReferGPT
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2504.07962.pdf' target='_blank'>https://arxiv.org/pdf/2504.07962.pdf</a></span>   <span><a href='https://glus-video.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07962">GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel framework utilizing multi-modal large language models (MLLMs) for referring video object segmentation (RefVOS). Previous MLLM-based methods commonly struggle with the dilemma between "Ref" and "VOS": they either specialize in understanding a few key frames (global reasoning) or tracking objects on continuous frames (local reasoning), and rely on external VOS or frame selectors to mitigate the other end of the challenge. However, our framework GLUS shows that global and local consistency can be unified into a single video segmentation MLLM: a set of sparse "context frames" provides global information, while a stream of continuous "query frames" conducts local object tracking. This is further supported by jointly training the MLLM with a pre-trained VOS memory bank to simultaneously digest short-range and long-range temporal information. To improve the information efficiency within the limited context window of MLLMs, we introduce object contrastive learning to distinguish hard false-positive objects and a self-refined framework to identify crucial frames and perform propagation. By collectively integrating these insights, our GLUS delivers a simple yet effective baseline, achieving new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our project page is at https://glus-video.github.io/.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2504.04519.pdf' target='_blank'>https://arxiv.org/pdf/2504.04519.pdf</a></span>   <span><a href='https://github.com/TripleJoy/SAM2MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Jiang, Zelin Wang, Manqi Zhao, Yin Li, DongSheng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04519">SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segment Anything 2 (SAM2) enables robust single-object tracking using segmentation. To extend this to multi-object tracking (MOT), we propose SAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking by Detection or Tracking by Query, SAM2MOT directly generates tracking boxes from segmentation masks, reducing reliance on detection accuracy. SAM2MOT has two key advantages: zero-shot generalization, allowing it to work across datasets without fine-tuning, and strong object association, inherited from SAM2. To further improve performance, we integrate a trajectory manager system for precise object addition and removal, and a cross-object interaction module to handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show state-of-the-art results. Notably, SAM2MOT outperforms existing methods on DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT. Code is available at https://github.com/TripleJoy/SAM2MOT.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2504.01321.pdf' target='_blank'>https://arxiv.org/pdf/2504.01321.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Jialin Gao, Xin Sun, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01321">COST: Contrastive One-Stage Transformer for Vision-Language Small Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer has recently demonstrated great potential in improving vision-language (VL) tracking algorithms. However, most of the existing VL trackers rely on carefully designed mechanisms to perform the multi-stage multi-modal fusion. Additionally, direct multi-modal fusion without alignment ignores distribution discrepancy between modalities in feature space, potentially leading to suboptimal representations. In this work, we propose COST, a contrastive one-stage transformer fusion framework for VL tracking, aiming to learn semantically consistent and unified VL representations. Specifically, we introduce a contrastive alignment strategy that maximizes mutual information (MI) between a video and its corresponding language description. This enables effective cross-modal alignment, yielding semantically consistent features in the representation space. By leveraging a visual-linguistic transformer, we establish an efficient multi-modal fusion and reasoning mechanism, empirically demonstrating that a simple stack of transformer encoders effectively enables unified VL representations. Moreover, we contribute a newly collected VL tracking benchmark dataset for small object tracking, named VL-SOT500, with bounding boxes and language descriptions. Our dataset comprises two challenging subsets, VL-SOT230 and VL-SOT270, dedicated to evaluating generic and high-speed small object tracking, respectively. Small object tracking is notoriously challenging due to weak appearance and limited features, and this dataset is, to the best of our knowledge, the first to explore the usage of language cues to enhance visual representation for small object tracking. Extensive experiments demonstrate that COST achieves state-of-the-art performance on five existing VL tracking datasets, as well as on our proposed VL-SOT500 dataset. Source codes and dataset will be made publicly available.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2504.00954.pdf' target='_blank'>https://arxiv.org/pdf/2504.00954.pdf</a></span>   <span><a href='https://github.com/BwLiu01/IDMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bangwei Liu, Yicheng Bao, Shaohui Lin, Xuhong Wang, Xin Tan, Yingchun Wang, Yuan Xie, Chaochao Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00954">IDMR: Towards Instance-Driven Precise Visual Correspondence in Multimodal Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal retrieval systems are becoming increasingly vital for cutting-edge AI technologies, such as embodied AI and AI-driven digital content industries. However, current multimodal retrieval tasks lack sufficient complexity and demonstrate limited practical application value. It spires us to design Instance-Driven Multimodal Image Retrieval (IDMR), a novel task that requires models to retrieve images containing the same instance as a query image while matching a text-described scenario. Unlike existing retrieval tasks focused on global image similarity or category-level matching, IDMR demands fine-grained instance-level consistency across diverse contexts. To benchmark this capability, we develop IDMR-bench using real-world object tracking and first-person video data. Addressing the scarcity of training data, we propose a cross-domain synthesis method that creates 557K training samples by cropping objects from standard detection datasets. Our Multimodal Large Language Model (MLLM) based retrieval model, trained on 1.2M samples, outperforms state-of-the-art approaches on both traditional benchmarks and our zero-shot IDMR-bench. Experimental results demonstrate previous models' limitations in instance-aware retrieval and highlight the potential of MLLM for advanced retrieval applications. The whole training dataset, codes and models, with wide ranges of sizes, are available at https://github.com/BwLiu01/IDMR.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2503.18338.pdf' target='_blank'>https://arxiv.org/pdf/2503.18338.pdf</a></span>   <span><a href='https://github.com/WenRuiCai/SPMTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenrui Cai, Qingjie Liu, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18338">SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture of Experts for Scalable Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most state-of-the-art trackers adopt one-stream paradigm, using a single Vision Transformer for joint feature extraction and relation modeling of template and search region images. However, relation modeling between different image patches exhibits significant variations. For instance, background regions dominated by target-irrelevant information require reduced attention allocation, while foreground, particularly boundary areas, need to be be emphasized. A single model may not effectively handle all kinds of relation modeling simultaneously. In this paper, we propose a novel tracker called SPMTrack based on mixture-of-experts tailored for visual tracking task (TMoE), combining the capability of multiple experts to handle diverse relation modeling more flexibly. Benefiting from TMoE, we extend relation modeling from image pairs to spatio-temporal context, further improving tracking accuracy with minimal increase in model parameters. Moreover, we employ TMoE as a parameter-efficient fine-tuning method, substantially reducing trainable parameters, which enables us to train SPMTrack of varying scales efficiently and preserve the generalization ability of pretrained models to achieve superior performance. We conduct experiments on seven datasets, and experimental results demonstrate that our method significantly outperforms current state-of-the-art trackers. The source code is available at https://github.com/WenRuiCai/SPMTrack.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2503.18282.pdf' target='_blank'>https://arxiv.org/pdf/2503.18282.pdf</a></span>   <span><a href='https://github.com/open-starlab/TrackID3x3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazuhiro Yamada, Li Yin, Qingrui Hu, Ning Ding, Shunsuke Iwashita, Jun Ichikawa, Kiwamu Kotani, Calvin Yeung, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18282">TrackID3x3: A Dataset and Algorithm for Multi-Player Tracking with Identification and Pose Estimation in 3x3 Basketball Full-court Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking, player identification, and pose estimation are fundamental components of sports analytics, essential for analyzing player movements, performance, and tactical strategies. However, existing datasets and methodologies primarily target mainstream team sports such as soccer and conventional 5-on-5 basketball, often overlooking scenarios involving fixed-camera setups commonly used at amateur levels, less mainstream sports, or datasets that explicitly incorporate pose annotations. In this paper, we propose the TrackID3x3 dataset, the first publicly available comprehensive dataset specifically designed for multi-player tracking, player identification, and pose estimation in 3x3 basketball scenarios. The dataset comprises three distinct subsets (Indoor fixed-camera, Outdoor fixed-camera, and Drone camera footage), capturing diverse full-court camera perspectives and environments. We also introduce the Track-ID task, a simplified variant of the game state reconstruction task that excludes field detection and focuses exclusively on fixed-camera scenarios. To evaluate performance, we propose a baseline algorithm called Track-ID algorithm, tailored to assess tracking and identification quality. Furthermore, our benchmark experiments, utilizing recent multi-object tracking algorithms (e.g., BoT-SORT-ReID) and top-down pose estimation methods (HRNet, RTMPose, and SwinPose), demonstrate robust results and highlight remaining challenges. Our dataset and evaluation benchmarks provide a solid foundation for advancing automated analytics in 3x3 basketball. Dataset and code will be available at https://github.com/open-starlab/TrackID3x3.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2503.17699.pdf' target='_blank'>https://arxiv.org/pdf/2503.17699.pdf</a></span>   <span><a href='https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolin Qin, Tingfa Xu, Tianhao Li, Zhenxiang Chen, Tao Feng, Jianan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.17699">MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>UAV tracking faces significant challenges in real-world scenarios, such as small-size targets and occlusions, which limit the performance of RGB-based trackers. Multispectral images (MSI), which capture additional spectral information, offer a promising solution to these challenges. However, progress in this field has been hindered by the lack of relevant datasets. To address this gap, we introduce the first large-scale Multispectral UAV Single Object Tracking dataset (MUST), which includes 250 video sequences spanning diverse environments and challenges, providing a comprehensive data foundation for multispectral UAV tracking. We also propose a novel tracking framework, UNTrack, which encodes unified spectral, spatial, and temporal features from spectrum prompts, initial templates, and sequential searches. UNTrack employs an asymmetric transformer with a spectral background eliminate mechanism for optimal relationship modeling and an encoder that continuously updates the spectrum prompt to refine tracking, improving both accuracy and efficiency. Extensive experiments show that our proposed UNTrack outperforms state-of-the-art UAV trackers. We believe our dataset and framework will drive future research in this area. The dataset is available on https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2503.12888.pdf' target='_blank'>https://arxiv.org/pdf/2503.12888.pdf</a></span>   <span><a href='https://github.com/ManOfStory/UncTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Yao, Yang Guo, Yanyang Yan, Wenqi Ren, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12888">UncTrack: Reliable Visual Object Tracking with Uncertainty-Aware Prototype Memory Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based trackers have achieved promising success and become the dominant tracking paradigm due to their accuracy and efficiency. Despite the substantial progress, most of the existing approaches tackle object tracking as a deterministic coordinate regression problem, while the target localization uncertainty has been greatly overlooked, which hampers trackers' ability to maintain reliable target state prediction in challenging scenarios. To address this issue, we propose UncTrack, a novel uncertainty-aware transformer tracker that predicts the target localization uncertainty and incorporates this uncertainty information for accurate target state inference. Specifically, UncTrack utilizes a transformer encoder to perform feature interaction between template and search images. The output features are passed into an uncertainty-aware localization decoder (ULD) to coarsely predict the corner-based localization and the corresponding localization uncertainty. Then the localization uncertainty is sent into a prototype memory network (PMN) to excavate valuable historical information to identify whether the target state prediction is reliable or not. To enhance the template representation, the samples with high confidence are fed back into the prototype memory bank for memory updating, making the tracker more robust to challenging appearance variations. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods. Our code is available at https://github.com/ManOfStory/UncTrack.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2503.12562.pdf' target='_blank'>https://arxiv.org/pdf/2503.12562.pdf</a></span>   <span><a href='https://github.com/HELLORPG/HATReID-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruopeng Gao, Yuyao Wang, Chunxu Liu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12562">History-Aware Transformation of ReID Features for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of multiple object tracking (MOT) is to detect all objects in a video and bind them into multiple trajectories. Generally, this process is carried out in two steps: detecting objects and associating them across frames based on various cues and metrics. Many studies and applications adopt object appearance, also known as re-identification (ReID) features, for target matching through straightforward similarity calculation. However, we argue that this practice is overly naive and thus overlooks the unique characteristics of MOT tasks. Unlike regular re-identification tasks that strive to distinguish all potential targets in a general representation, multi-object tracking typically immerses itself in differentiating similar targets within the same video sequence. Therefore, we believe that seeking a more suitable feature representation space based on the different sample distributions of each sequence will enhance tracking performance. In this paper, we propose using history-aware transformations on ReID features to achieve more discriminative appearance representations. Specifically, we treat historical trajectory features as conditions and employ a tailored Fisher Linear Discriminant (FLD) to find a spatial projection matrix that maximizes the differentiation between different trajectories. Our extensive experiments reveal that this training-free projection can significantly boost feature-only trackers to achieve competitive, even superior tracking performance compared to state-of-the-art methods while also demonstrating impressive zero-shot transfer capabilities. This demonstrates the effectiveness of our proposal and further encourages future investigation into the importance and customization of ReID models in multiple object tracking. The code will be released at https://github.com/HELLORPG/HATReID-MOT.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2503.12527.pdf' target='_blank'>https://arxiv.org/pdf/2503.12527.pdf</a></span>   <span><a href='https://github.com/yiyscut/VIO-IPNet.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Yi, Kunqing Wang, Jinpu Zhang, Zhen Tan, Xiangke Wang, Hui Shen, Dewen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12527">A Plug-and-Play Learning-based IMU Bias Factor for Robust Visual-Inertial Odometry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The bias of low-cost Inertial Measurement Units (IMU) is a critical factor affecting the performance of Visual-Inertial Odometry (VIO). In particular, when visual tracking encounters errors, the optimized bias results may deviate significantly from the true values, adversely impacting the system's stability and localization precision. In this paper, we propose a novel plug-and-play framework featuring the Inertial Prior Network (IPNet), which is designed to accurately estimate IMU bias. Recognizing the substantial impact of initial bias errors in low-cost inertial devices on system performance, our network directly leverages raw IMU data to estimate the mean bias, eliminating the dependency on historical estimates in traditional recursive predictions and effectively preventing error propagation. Furthermore, we introduce an iterative approach to calculate the mean value of the bias for network training, addressing the lack of bias labels in many visual-inertial datasets. The framework is evaluated on two public datasets and one self-collected dataset. Extensive experiments demonstrate that our method significantly enhances both localization precision and robustness, with the ATE-RMSE metric improving on average by 46\%. The source code and video will be available at \textcolor{red}{https://github.com/yiyscut/VIO-IPNet.git}.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2503.12006.pdf' target='_blank'>https://arxiv.org/pdf/2503.12006.pdf</a></span>   <span><a href='https://github.com/ShanZard/ROS-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Shan, Yang Liu, Lei Zhou, Cheng Yan, Heng Wang, Xia Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12006">ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of large-scale remote sensing video data underscores the importance of high-quality interactive segmentation. However, challenges such as small object sizes, ambiguous features, and limited generalization make it difficult for current methods to achieve this goal. In this work, we propose ROS-SAM, a method designed to achieve high-quality interactive segmentation while preserving generalization across diverse remote sensing data. The ROS-SAM is built upon three key innovations: 1) LoRA-based fine-tuning, which enables efficient domain adaptation while maintaining SAM's generalization ability, 2) Enhancement of deep network layers to improve the discriminability of extracted features, thereby reducing misclassifications, and 3) Integration of global context with local boundary details in the mask decoder to generate high-quality segmentation masks. Additionally, we design the data pipeline to ensure the model learns to better handle objects at varying scales during training while focusing on high-quality predictions during inference. Experiments on remote sensing video datasets show that the redesigned data pipeline boosts the IoU by 6%, while ROS-SAM increases the IoU by 13%. Finally, when evaluated on existing remote sensing object tracking datasets, ROS-SAM demonstrates impressive zero-shot capabilities, generating masks that closely resemble manual annotations. These results confirm ROS-SAM as a powerful tool for fine-grained segmentation in remote sensing applications. Code is available at https://github.com/ShanZard/ROS-SAM.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2503.10616.pdf' target='_blank'>https://arxiv.org/pdf/2503.10616.pdf</a></span>   <span><a href='https://github.com/jinyanglii/OVTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyang Li, En Yu, Sijia Chen, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10616">OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary multiple object tracking aims to generalize trackers to unseen categories during training, enabling their application across a variety of real-world scenarios. However, the existing open-vocabulary tracker is constrained by its framework structure, isolated frame-level perception, and insufficient modal interactions, which hinder its performance in open-vocabulary classification and tracking. In this paper, we propose OVTR (End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the first end-to-end open-vocabulary tracker that models motion, appearance, and category simultaneously. To achieve stable classification and continuous tracking, we design the CIP (Category Information Propagation) strategy, which establishes multiple high-level category information priors for subsequent frames. Additionally, we introduce a dual-branch structure for generalization capability and deep multimodal interaction, and incorporate protective strategies in the decoder to enhance performance. Experimental results show that our method surpasses previous trackers on the open-vocabulary MOT benchmark while also achieving faster inference speeds and significantly reducing preprocessing requirements. Moreover, the experiment transferring the model to another dataset demonstrates its strong adaptability. Models and code are released at https://github.com/jinyanglii/OVTR.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2503.08471.pdf' target='_blank'>https://arxiv.org/pdf/2503.08471.pdf</a></span>   <span><a href='https://github.com/Tsinghua-MARS-Lab/TrackOcc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuoguang Chen, Kenan Li, Xiuyu Yang, Tao Jiang, Yiming Li, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08471">TrackOcc: Camera-based 4D Panoptic Occupancy Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Comprehensive and consistent dynamic scene understanding from camera input is essential for advanced autonomous systems. Traditional camera-based perception tasks like 3D object tracking and semantic occupancy prediction lack either spatial comprehensiveness or temporal consistency. In this work, we introduce a brand-new task, Camera-based 4D Panoptic Occupancy Tracking, which simultaneously addresses panoptic occupancy segmentation and object tracking from camera-only input. Furthermore, we propose TrackOcc, a cutting-edge approach that processes image inputs in a streaming, end-to-end manner with 4D panoptic queries to address the proposed task. Leveraging the localization-aware loss, TrackOcc enhances the accuracy of 4D panoptic occupancy tracking without bells and whistles. Experimental results demonstrate that our method achieves state-of-the-art performance on the Waymo dataset. The source code will be released at https://github.com/Tsinghua-MARS-Lab/TrackOcc.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2503.06625.pdf' target='_blank'>https://arxiv.org/pdf/2503.06625.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/SGLATrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaocan Xue, Bineng Zhong, Qihua Liang, Yaozong Zheng, Ning Li, Yuanliang Xue, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.06625">Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision transformers (ViTs) have emerged as a popular backbone for visual tracking. However, complete ViT architectures are too cumbersome to deploy for unmanned aerial vehicle (UAV) tracking which extremely emphasizes efficiency. In this study, we discover that many layers within lightweight ViT-based trackers tend to learn relatively redundant and repetitive target representations. Based on this observation, we propose a similarity-guided layer adaptation approach to optimize the structure of ViTs. Our approach dynamically disables a large number of representation-similar layers and selectively retains only a single optimal layer among them, aiming to achieve a better accuracy-speed trade-off. By incorporating this approach into existing ViTs, we tailor previously complete ViT architectures into an efficient similarity-guided layer-adaptive framework, namely SGLATrack, for real-time UAV tracking. Extensive experiments on six tracking benchmarks verify the effectiveness of the proposed approach, and show that our SGLATrack achieves a state-of-the-art real-time speed while maintaining competitive tracking precision. Codes and models are available at https://github.com/GXNU-ZhongLab/SGLATrack.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2503.04565.pdf' target='_blank'>https://arxiv.org/pdf/2503.04565.pdf</a></span>   <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xifen523/OmniTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04565">Omnidirectional Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoramic imagery, with its 360Â° field of view, offers comprehensive information to support Multi-Object Tracking (MOT) in capturing spatial and temporal relationships of surrounding objects. However, most MOT algorithms are tailored for pinhole images with limited views, impairing their effectiveness in panoramic settings. Additionally, panoramic image distortions, such as resolution loss, geometric deformation, and uneven lighting, hinder direct adaptation of existing MOT methods, leading to significant performance degradation. To address these challenges, we propose OmniTrack, an omnidirectional MOT framework that incorporates Tracklet Management to introduce temporal cues, FlexiTrack Instances for object localization and association, and the CircularStatE Module to alleviate image and geometric distortions. This integration enables tracking in panoramic field-of-view scenarios, even under rapid sensor motion. To mitigate the lack of panoramic MOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as panoramic fields of view, intense motion, and complex environments. Extensive experiments on the public JRDB dataset and the newly introduced QuadTrack benchmark demonstrate the state-of-the-art performance of the proposed framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the baseline by 6.81%. The established dataset and source code are available at https://github.com/xifen523/OmniTrack.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2503.04322.pdf' target='_blank'>https://arxiv.org/pdf/2503.04322.pdf</a></span>   <span><a href='https://github.com/LarsBredereke/object_tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lars Bredereke, Yale Hartmann, Tanja Schultz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04322">A Modular Pipeline for 3D Object Tracking Using RGB Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a key challenge of computer vision with various applications that all require different architectures. Most tracking systems have limitations such as constraining all movement to a 2D plane and they often track only one object. In this paper, we present a new modular pipeline that calculates 3D trajectories of multiple objects. It is adaptable to various settings where multiple time-synced and stationary cameras record moving objects, using off the shelf webcams. Our pipeline was tested on the Table Setting Dataset, where participants are recorded with various sensors as they set a table with tableware objects. We need to track these manipulated objects, using 6 rgb webcams. Challenges include: Detecting small objects in 9.874.699 camera frames, determining camera poses, discriminating between nearby and overlapping objects, temporary occlusions, and finally calculating a 3D trajectory using the right subset of an average of 11.12.456 pixel coordinates per 3-minute trial. We implement a robust pipeline that results in accurate trajectories with covariance of x,y,z-position as a confidence metric. It deals dynamically with appearing and disappearing objects, instantiating new Extended Kalman Filters. It scales to hundreds of table-setting trials with very little human annotation input, even with the camera poses of each trial unknown. The code is available at https://github.com/LarsBredereke/object_tracking
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2503.00516.pdf' target='_blank'>https://arxiv.org/pdf/2503.00516.pdf</a></span>   <span><a href='https://github.com/jiawen-zhu/AsymTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Zhu, Huayi Tang, Xin Chen, Xinying Wang, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00516">Two-stream Beats One-stream: Asymmetric Siamese Network for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient tracking has garnered attention for its ability to operate on resource-constrained platforms for real-world deployment beyond desktop GPUs. Current efficient trackers mainly follow precision-oriented trackers, adopting a one-stream framework with lightweight modules. However, blindly adhering to the one-stream paradigm may not be optimal, as incorporating template computation in every frame leads to redundancy, and pervasive semantic interaction between template and search region places stress on edge devices. In this work, we propose a novel asymmetric Siamese tracker named \textbf{AsymTrack} for efficient tracking. AsymTrack disentangles template and search streams into separate branches, with template computing only once during initialization to generate modulation signals. Building on this architecture, we devise an efficient template modulation mechanism to unidirectional inject crucial cues into the search features, and design an object perception enhancement module that integrates abstract semantics and local details to overcome the limited representation in lightweight tracker. Extensive experiments demonstrate that AsymTrack offers superior speed-precision trade-offs across different platforms compared to the current state-of-the-arts. For instance, AsymTrack-T achieves 60.8\% AUC on LaSOT and 224/81/84 FPS on GPU/CPU/AGX, surpassing HiT-Tiny by 6.0\% AUC with higher speeds. The code is available at https://github.com/jiawen-zhu/AsymTrack.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2502.20111.pdf' target='_blank'>https://arxiv.org/pdf/2502.20111.pdf</a></span>   <span><a href='https://mii-laboratory.github.io/MITracker/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20111">MITracker: Multi-View Integration for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a bird's eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance. The code and the new dataset will be available at https://mii-laboratory.github.io/MITracker/.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2502.18220.pdf' target='_blank'>https://arxiv.org/pdf/2502.18220.pdf</a></span>   <span><a href='https://github.com/wanghe/UASTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>He Wang, Tianyang Xu, Zhangyong Tang, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18220">UASTrack: A Unified Adaptive Selection Framework with Modality-Customization in Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal tracking is essential in single-object tracking (SOT), as different sensor types contribute unique capabilities to overcome challenges caused by variations in object appearance. However, existing unified RGB-X trackers (X represents depth, event, or thermal modality) either rely on the task-specific training strategy for individual RGB-X image pairs or fail to address the critical importance of modality-adaptive perception in real-world applications. In this work, we propose UASTrack, a unified adaptive selection framework that facilitates both model and parameter unification, as well as adaptive modality discrimination across various multi-modal tracking tasks. To achieve modality-adaptive perception in joint RGB-X pairs, we design a Discriminative Auto-Selector (DAS) capable of identifying modality labels, thereby distinguishing the data distributions of auxiliary modalities. Furthermore, we propose a Task-Customized Optimization Adapter (TCOA) tailored to various modalities in the latent space. This strategy effectively filters noise redundancy and mitigates background interference based on the specific characteristics of each modality. Extensive comparisons conducted on five benchmarks including LasHeR, GTOT, RGBT234, VisEvent, and DepthTrack, covering RGB-T, RGB-E, and RGB-D tracking scenarios, demonstrate our innovative approach achieves comparative performance by introducing only additional training parameters of 1.87M and flops of 1.95G. The code will be available at https://github.com/wanghe/UASTrack.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2502.16809.pdf' target='_blank'>https://arxiv.org/pdf/2502.16809.pdf</a></span>   <span><a href='https://github.com/ZJZhao123/CRTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijing Zhao, Jianlong Yu, Lin Zhang, Shunli Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16809">CRTrack: Low-Light Semi-Supervised Multi-object Tracking Based on Consistency Regularization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking under low-light environments is prevalent in real life. Recent years have seen rapid development in the field of multi-object tracking. However, due to the lack of datasets and the high cost of annotations, multi-object tracking under low-light environments remains a persistent challenge. In this paper, we focus on multi-object tracking under low-light conditions. To address the issues of limited data and the lack of dataset, we first constructed a low-light multi-object tracking dataset (LLMOT). This dataset comprises data from MOT17 that has been enhanced for nighttime conditions as well as multiple unannotated low-light videos. Subsequently, to tackle the high annotation costs and address the issue of image quality degradation, we propose a semi-supervised multi-object tracking method based on consistency regularization named CRTrack. First, we calibrate a consistent adaptive sampling assignment to replace the static IoU-based strategy, enabling the semi-supervised tracking method to resist noisy pseudo-bounding boxes. Then, we design a adaptive semi-supervised network update method, which effectively leverages unannotated data to enhance model performance. Dataset and Code: https://github.com/ZJZhao123/CRTrack.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2502.09672.pdf' target='_blank'>https://arxiv.org/pdf/2502.09672.pdf</a></span>   <span><a href='https://github.com/Ap01lo/IMM-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohong Liu, Xulong Zhao, Gang Liu, Zili Wu, Tao Wang, Lei Meng, Yuhan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.09672">IMM-MOT: A Novel 3D Multi-object Tracking Framework with Interacting Multiple Model Filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT) provides the trajectories of surrounding objects, assisting robots or vehicles in smarter path planning and obstacle avoidance. Existing 3D MOT methods based on the Tracking-by-Detection framework typically use a single motion model to track an object throughout its entire tracking process. However, objects may change their motion patterns due to variations in the surrounding environment. In this paper, we introduce the Interacting Multiple Model filter in IMM-MOT, which accurately fits the complex motion patterns of individual objects, overcoming the limitation of single-model tracking in existing approaches. In addition, we incorporate a Damping Window mechanism into the trajectory lifecycle management, leveraging the continuous association status of trajectories to control their creation and termination, reducing the occurrence of overlooked low-confidence true targets. Furthermore, we propose the Distance-Based Score Enhancement module, which enhances the differentiation between false positives and true positives by adjusting detection scores, thereby improving the effectiveness of the Score Filter. On the NuScenes Val dataset, IMM-MOT outperforms most other single-modal models using 3D point clouds, achieving an AMOTA of 73.8%. Our project is available at https://github.com/Ap01lo/IMM-MOT.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2502.05574.pdf' target='_blank'>https://arxiv.org/pdf/2502.05574.pdf</a></span>   <span><a href='https://github.com/Event-AHU/EventVOT_Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiao Wang, Xiao Wang, Chao Wang, Liye Jin, Lin Zhu, Bo Jiang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05574">Event Stream-based Visual Object Tracking: HDETrack V2 and A High-Definition Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We then introduce a novel hierarchical knowledge distillation strategy that incorporates the similarity matrix, feature representation, and response map-based distillation to guide the learning of the student Transformer network. We also enhance the model's ability to capture temporal dependencies by applying the temporal Fourier transform to establish temporal relationships between video frames. We adapt the network model to specific target objects during testing via a newly proposed test-time tuning strategy to achieve high performance and flexibility in target tracking. Recognizing the limitations of existing event-based tracking datasets, which are predominantly low-resolution, we propose EventVOT, the first large-scale high-resolution event-based tracking dataset. It comprises 1141 videos spanning diverse categories such as pedestrians, vehicles, UAVs, ping pong, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, FELT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. Both the benchmark dataset and source code have been released on https://github.com/Event-AHU/EventVOT_Benchmark
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2501.12386.pdf' target='_blank'>https://arxiv.org/pdf/2501.12386.pdf</a></span>   <span><a href='https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kai Chen, Wenhai Wang, Yu Qiao, Yali Wang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12386">InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to improve the performance of video multimodal large language models (MLLM) via long and rich context (LRC) modeling. As a result, we develop a new version of InternVideo2.5 with a focus on enhancing the original MLLMs' ability to perceive fine-grained details and capture long-form temporal structure in videos. Specifically, our approach incorporates dense vision task annotations into MLLMs using direct preference optimization and develops compact spatiotemporal representations through adaptive hierarchical token compression. Experimental results demonstrate this unique design of LRC greatly improves the results of video MLLM in mainstream video understanding benchmarks (short & long), enabling the MLLM to memorize significantly longer video inputs (at least 6x longer than the original), and master specialized vision capabilities like object tracking and segmentation. Our work highlights the importance of multimodal context richness (length and fineness) in empowering MLLM's innate abilites (focus and memory), providing new insights for future research on video MLLM. Code and models are available at https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2501.11288.pdf' target='_blank'>https://arxiv.org/pdf/2501.11288.pdf</a></span>   <span><a href='https://github.com/Wangyc2000/PD_SORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanchao Wang, Dawei Zhang, Run Li, Zhonglong Zheng, Minglu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11288">PD-SORT: Occlusion-Robust Multi-Object Tracking Using Pseudo-Depth Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a rising topic in video processing technologies and has important application value in consumer electronics. Currently, tracking-by-detection (TBD) is the dominant paradigm for MOT, which performs target detection and association frame by frame. However, the association performance of TBD methods degrades in complex scenes with heavy occlusions, which hinders the application of such methods in real-world scenarios.To this end, we incorporate pseudo-depth cues to enhance the association performance and propose Pseudo-Depth SORT (PD-SORT). First, we extend the Kalman filter state vector with pseudo-depth states. Second, we introduce a novel depth volume IoU (DVIoU) by combining the conventional 2D IoU with pseudo-depth. Furthermore, we develop a quantized pseudo-depth measurement (QPDM) strategy for more robust data association. Besides, we also integrate camera motion compensation (CMC) to handle dynamic camera situations. With the above designs, PD-SORT significantly alleviates the occlusion-induced ambiguous associations and achieves leading performances on DanceTrack, MOT17, and MOT20. Note that the improvement is especially obvious on DanceTrack, where objects show complex motions, similar appearances, and frequent occlusions. The code is available at https://github.com/Wangyc2000/PD_SORT.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2501.07554.pdf' target='_blank'>https://arxiv.org/pdf/2501.07554.pdf</a></span>   <span><a href='https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Varun Biyyala, Bharat Chanderprakash Kathuria, Jialu Li, Youshan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07554">SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video editing models have advanced significantly, but evaluating their performance remains challenging. Traditional metrics, such as CLIP text and image scores, often fall short: text scores are limited by inadequate training data and hierarchical dependencies, while image scores fail to assess temporal consistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation Metric), a novel evaluation framework that leverages modern Vision-Language Models (VLMs), Object Detection, and Temporal Consistency checks. SST-EM comprises four components: (1) semantic extraction from frames using a VLM, (2) primary object tracking with Object Detection, (3) focused object refinement via an LLM agent, and (4) temporal consistency assessment using a Vision Transformer (ViT). These components are integrated into a unified metric with weights derived from human evaluations and regression analysis. The name SST-EM reflects its focus on Semantic, Spatial, and Temporal aspects of video evaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and temporal smoothness in video editing. The source code is available in the \textbf{\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub Repository}}.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2501.07360.pdf' target='_blank'>https://arxiv.org/pdf/2501.07360.pdf</a></span>   <span><a href='https://github.com/timbervision/timbervision' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Steininger, Julia Simon, Andreas Trondl, Markus Murschitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07360">TimberVision: A Multi-Task Dataset and Framework for Log-Component Segmentation and Tracking in Autonomous Forestry Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Timber represents an increasingly valuable and versatile resource. However, forestry operations such as harvesting, handling and measuring logs still require substantial human labor in remote environments posing significant safety risks. Progressively automating these tasks has the potential of increasing their efficiency as well as safety, but requires an accurate detection of individual logs as well as live trees and their context. Although initial approaches have been proposed for this challenging application domain, specialized data and algorithms are still too scarce to develop robust solutions. To mitigate this gap, we introduce the TimberVision dataset, consisting of more than 2k annotated RGB images containing a total of 51k trunk components including cut and lateral surfaces, thereby surpassing any existing dataset in this domain in terms of both quantity and detail by a large margin. Based on this data, we conduct a series of ablation experiments for oriented object detection and instance segmentation and evaluate the influence of multiple scene parameters on model performance. We introduce a generic framework to fuse the components detected by our models for both tasks into unified trunk representations. Furthermore, we automatically derive geometric properties and apply multi-object tracking to further enhance robustness. Our detection and tracking approach provides highly descriptive and accurate trunk representations solely from RGB image data, even under challenging environmental conditions. Our solution is suitable for a wide range of application scenarios and can be readily combined with other sensor modalities.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2501.05453.pdf' target='_blank'>https://arxiv.org/pdf/2501.05453.pdf</a></span>   <span><a href='https://brjathu.github.io/toto/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jathushan Rajasegaran, Ilija Radosavovic, Rahul Ravishankar, Yossi Gandelsman, Christoph Feichtenhofer, Jitendra Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05453">An Empirical Study of Autoregressive Pre-training from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We empirically study autoregressive pre-training from videos. To perform our study, we construct a series of autoregressive video models, called Toto. We treat videos as sequences of visual tokens and train transformer models to autoregressively predict future tokens. Our models are pre-trained on a diverse dataset of videos and images comprising over 1 trillion visual tokens. We explore different architectural, training, and inference design choices. We evaluate the learned visual representations on a range of downstream tasks including image recognition, video classification, object tracking, and robotics. Our results demonstrate that, despite minimal inductive biases, autoregressive pre-training leads to competitive performance across all benchmarks. Finally, we find that scaling our video models results in similar scaling curves to those seen in language models, albeit with a different rate. More details at https://brjathu.github.io/toto/
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2501.01275.pdf' target='_blank'>https://arxiv.org/pdf/2501.01275.pdf</a></span>   <span><a href='https://github.com/leandro-svg/HybridTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leandro Di Bella, Yangxintong Lyu, Bruno Cornelis, Adrian Munteanu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01275">HybridTrack: A Hybrid Approach for Robust Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evolution of Advanced Driver Assistance Systems (ADAS) has increased the need for robust and generalizable algorithms for multi-object tracking. Traditional statistical model-based tracking methods rely on predefined motion models and assumptions about system noise distributions. Although computationally efficient, they often lack adaptability to varying traffic scenarios and require extensive manual design and parameter tuning. To address these issues, we propose a novel 3D multi-object tracking approach for vehicles, HybridTrack, which integrates a data-driven Kalman Filter (KF) within a tracking-by-detection paradigm. In particular, it learns the transition residual and Kalman gain directly from data, which eliminates the need for manual motion and stochastic parameter modeling. Validated on the real-world KITTI dataset, HybridTrack achieves 82.72% HOTA accuracy, significantly outperforming state-of-the-art methods. We also evaluate our method under different configurations, achieving the fastest processing speed of 112 FPS. Consequently, HybridTrack eliminates the dependency on scene-specific designs while improving performance and maintaining real-time efficiency. The code is publicly available at: https://github.com/leandro-svg/HybridTrack.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2412.20002.pdf' target='_blank'>https://arxiv.org/pdf/2412.20002.pdf</a></span>   <span><a href='https://github.com/wuyou3474/AVTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>You Wu, Yongxin Li, Mengyuan Liu, Xucheng Wang, Xiangyang Yang, Hengzhou Ye, Dan Zeng, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20002">Learning an Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based models have improved visual tracking, but most still cannot run in real time on resource-limited devices, especially for unmanned aerial vehicle (UAV) tracking. To achieve a better balance between performance and efficiency, we propose AVTrack, an adaptive computation tracking framework that adaptively activates transformer blocks through an Activation Module (AM), which dynamically optimizes the ViT architecture by selectively engaging relevant components. To address extreme viewpoint variations, we propose to learn view-invariant representations via mutual information (MI) maximization. In addition, we propose AVTrack-MD, an enhanced tracker incorporating a novel MI maximization-based multi-teacher knowledge distillation framework. Leveraging multiple off-the-shelf AVTrack models as teachers, we maximize the MI between their aggregated softened features and the corresponding softened feature of the student model, improving the generalization and performance of the student, especially under noisy conditions. Extensive experiments show that AVTrack-MD achieves performance comparable to AVTrack's performance while reducing model complexity and boosting average tracking speed by over 17\%. Codes is available at: https://github.com/wuyou3474/AVTrack.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2412.17807.pdf' target='_blank'>https://arxiv.org/pdf/2412.17807.pdf</a></span>   <span><a href='https://github.com/chen-si-jia/CRMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Chen, En Yu, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17807">Cross-View Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) is an important topic in the current tracking field. Its task form is to guide the tracker to track objects that match the language description. Current research mainly focuses on referring multi-object tracking under single-view, which refers to a view sequence or multiple unrelated view sequences. However, in the single-view, some appearances of objects are easily invisible, resulting in incorrect matching of objects with the language description. In this work, we propose a new task, called Cross-view Referring Multi-Object Tracking (CRMOT). It introduces the cross-view to obtain the appearances of objects from multiple views, avoiding the problem of the invisible appearances of objects in RMOT task. CRMOT is a more challenging task of accurately tracking the objects that match the language description and maintaining the identity consistency of objects in each cross-view. To advance CRMOT task, we construct a cross-view referring multi-object tracking benchmark based on CAMPUS and DIVOTrack datasets, named CRTrack. Specifically, it provides 13 different scenes and 221 language descriptions. Furthermore, we propose an end-to-end cross-view referring multi-object tracking method, named CRTracker. Extensive experiments on the CRTrack benchmark verify the effectiveness of our method. The dataset and code are available at https://github.com/chen-si-jia/CRMOT.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2412.15691.pdf' target='_blank'>https://arxiv.org/pdf/2412.15691.pdf</a></span>   <span><a href='https://github.com/NJU-PCALab/STTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiantao Hu, Ying Tai, Xu Zhao, Chen Zhao, Zhenyu Zhang, Jun Li, Bineng Zhong, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15691">Exploiting Multimodal Spatial-temporal Patterns for Video Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal tracking has garnered widespread attention as a result of its ability to effectively address the inherent limitations of traditional RGB tracking. However, existing multimodal trackers mainly focus on the fusion and enhancement of spatial features or merely leverage the sparse temporal relationships between video frames. These approaches do not fully exploit the temporal correlations in multimodal videos, making it difficult to capture the dynamic changes and motion information of targets in complex scenarios. To alleviate this problem, we propose a unified multimodal spatial-temporal tracking approach named STTrack. In contrast to previous paradigms that solely relied on updating reference information, we introduced a temporal state generator (TSG) that continuously generates a sequence of tokens containing multimodal temporal information. These temporal information tokens are used to guide the localization of the target in the next time state, establish long-range contextual relationships between video frames, and capture the temporal trajectory of the target. Furthermore, at the spatial level, we introduced the mamba fusion and background suppression interactive (BSI) modules. These modules establish a dual-stage mechanism for coordinating information interaction and fusion between modalities. Extensive comparisons on five benchmark datasets illustrate that STTrack achieves state-of-the-art performance across various multimodal tracking scenarios. Code is available at: https://github.com/NJU-PCALab/STTrack.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2412.15212.pdf' target='_blank'>https://arxiv.org/pdf/2412.15212.pdf</a></span>   <span><a href='https://github.com/google-deepmind/representations4d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro VÃ©lez, Luisa PolanÃ­a, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica PÄtrÄucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15212">Scaling 4D Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scaling has not yet been convincingly demonstrated for pure self-supervised learning from video. However, prior work has focused evaluations on semantic-related tasks $\unicode{x2013}$ action classification, ImageNet classification, etc. In this paper we focus on evaluating self-supervised learning on non-semantic vision tasks that are more spatial (3D) and temporal (+1D = 4D), such as camera pose estimation, point and object tracking, and depth estimation. We show that by learning from very large video datasets, masked auto-encoding (MAE) with transformer video models actually scales, consistently improving performance on these 4D tasks, as model size increases from 20M all the way to the largest by far reported self-supervised video model $\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with many recent image and video models demonstrates the benefits of scaling 4D representations. Pretrained models are available at https://github.com/google-deepmind/representations4d .
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2412.11023.pdf' target='_blank'>https://arxiv.org/pdf/2412.11023.pdf</a></span>   <span><a href='https://github.com/kangben258/MCITrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Kang, Xin Chen, Simiao Lai, Yang Liu, Yi Liu, Dong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11023">Exploring Enhanced Contextual Information for Video-Level Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Contextual information at the video level has become increasingly crucial for visual object tracking. However, existing methods typically use only a few tokens to convey this information, which can lead to information loss and limit their ability to fully capture the context. To address this issue, we propose a new video-level visual object tracking framework called MCITrack. It leverages Mamba's hidden states to continuously record and transmit extensive contextual information throughout the video stream, resulting in more robust object tracking. The core component of MCITrack is the Contextual Information Fusion module, which consists of the mamba layer and the cross-attention layer. The mamba layer stores historical contextual information, while the cross-attention layer integrates this information into the current visual features of each backbone block. This module enhances the model's ability to capture and utilize contextual information at multiple levels through deep integration with the backbone. Experiments demonstrate that MCITrack achieves competitive performance across numerous benchmarks. For instance, it gets 76.6% AUC on LaSOT and 80.0% AO on GOT-10k, establishing a new state-of-the-art performance. Code and models are available at https://github.com/kangben258/MCITrack.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2412.10861.pdf' target='_blank'>https://arxiv.org/pdf/2412.10861.pdf</a></span>   <span><a href='https://github.com/xuqingyu26/HGTMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyu Xu, Longguang Wang, Weidong Sheng, Yingqian Wang, Chao Xiao, Chao Ma, Wei An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10861">Heterogeneous Graph Transformer for Multiple Tiny Object Tracking in RGB-T Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple tiny objects is highly challenging due to their weak appearance and limited features. Existing multi-object tracking algorithms generally focus on single-modality scenes, and overlook the complementary characteristics of tiny objects captured by multiple remote sensors. To enhance tracking performance by integrating complementary information from multiple sources, we propose a novel framework called {HGT-Track (Heterogeneous Graph Transformer based Multi-Tiny-Object Tracking)}. Specifically, we first employ a Transformer-based encoder to embed images from different modalities. Subsequently, we utilize Heterogeneous Graph Transformer to aggregate spatial and temporal information from multiple modalities to generate detection and tracking features. Additionally, we introduce a target re-detection module (ReDet) to ensure tracklet continuity by maintaining consistency across different modalities. Furthermore, this paper introduces the first benchmark VT-Tiny-MOT (Visible-Thermal Tiny Multi-Object Tracking) for RGB-T fused multiple tiny object tracking. Extensive experiments are conducted on VT-Tiny-MOT, and the results have demonstrated the effectiveness of our method. Compared to other state-of-the-art methods, our method achieves better performance in terms of MOTA (Multiple-Object Tracking Accuracy) and ID-F1 score. The code and dataset will be made available at https://github.com/xuqingyu26/HGTMT.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2412.09617.pdf' target='_blank'>https://arxiv.org/pdf/2412.09617.pdf</a></span>   <span><a href='https://joehjhuang.github.io/normalflow' target='_blank'>  GitHub</a></span> <span><a href='https://joehjhuang.github.io/normalflow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hung-Jui Huang, Michael Kaess, Wenzhen Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09617">NormalFlow: Fast, Robust, and Accurate Contact-based Object 6DoF Pose Tracking with Vision-based Tactile Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile sensing is crucial for robots aiming to achieve human-level dexterity. Among tactile-dependent skills, tactile-based object tracking serves as the cornerstone for many tasks, including manipulation, in-hand manipulation, and 3D reconstruction. In this work, we introduce NormalFlow, a fast, robust, and real-time tactile-based 6DoF tracking algorithm. Leveraging the precise surface normal estimation of vision-based tactile sensors, NormalFlow determines object movements by minimizing discrepancies between the tactile-derived surface normals. Our results show that NormalFlow consistently outperforms competitive baselines and can track low-texture objects like table surfaces. For long-horizon tracking, we demonstrate when rolling the sensor around a bead for 360 degrees, NormalFlow maintains a rotational tracking error of 2.5 degrees. Additionally, we present state-of-the-art tactile-based 3D reconstruction results, showcasing the high accuracy of NormalFlow. We believe NormalFlow unlocks new possibilities for high-precision perception and manipulation tasks that involve interacting with objects using hands. The video demo, code, and dataset are available on our website: https://joehjhuang.github.io/normalflow.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2412.05548.pdf' target='_blank'>https://arxiv.org/pdf/2412.05548.pdf</a></span>   <span><a href='https://lolrudy.github.io/No3DTrackSG/' target='_blank'>  GitHub</a></span> <span><a href='https://lolrudy.github.io/No3DTrackSG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruida Zhang, Chengxi Li, Chenyangguang Zhang, Xingyu Liu, Haili Yuan, Yanyan Li, Xiangyang Ji, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05548">Street Gaussians without 3D Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic scene reconstruction in driving scenarios poses significant challenges due to fast-moving objects. Most existing methods rely on labor-intensive manual labeling of object poses to reconstruct dynamic objects in canonical space and move them based on these poses during rendering. While some approaches attempt to use 3D object trackers to replace manual annotations, the limited generalization of 3D trackers -- caused by the scarcity of large-scale 3D datasets -- results in inferior reconstructions in real-world settings. In contrast, 2D foundation models demonstrate strong generalization capabilities. To eliminate the reliance on 3D trackers and enhance robustness across diverse environments, we propose a stable object tracking module by leveraging associations from 2D deep trackers within a 3D object fusion strategy. We address inevitable tracking errors by further introducing a motion learning strategy in an implicit feature space that autonomously corrects trajectory errors and recovers missed detections. Experimental results on Waymo-NOTR and KITTI show that our method outperforms existing approaches. Our code will be released on https://lolrudy.github.io/No3DTrackSG/.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2412.05548.pdf' target='_blank'>https://arxiv.org/pdf/2412.05548.pdf</a></span>   <span><a href='https://lolrudy.github.io/No3DTrackSG/' target='_blank'>  GitHub</a></span> <span><a href='https://lolrudy.github.io/No3DTrackSG/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruida Zhang, Chengxi Li, Chenyangguang Zhang, Xingyu Liu, Haili Yuan, Yanyan Li, Xiangyang Ji, Gim Hee Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05548">Street Gaussians without 3D Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Realistic scene reconstruction in driving scenarios poses significant challenges due to fast-moving objects. Most existing methods rely on labor-intensive manual labeling of object poses to reconstruct dynamic objects in canonical space and move them based on these poses during rendering. While some approaches attempt to use 3D object trackers to replace manual annotations, the limited generalization of 3D trackers -- caused by the scarcity of large-scale 3D datasets -- results in inferior reconstructions in real-world settings. In contrast, 2D foundation models demonstrate strong generalization capabilities. To eliminate the reliance on 3D trackers and enhance robustness across diverse environments, we propose a stable object tracking module by leveraging associations from 2D deep trackers within a 3D object fusion strategy. We address inevitable tracking errors by further introducing a motion learning strategy in an implicit feature space that autonomously corrects trajectory errors and recovers missed detections. Experimental results on Waymo-NOTR and KITTI show that our method outperforms existing approaches. Our code will be released on https://lolrudy.github.io/No3DTrackSG/.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2412.04945.pdf' target='_blank'>https://arxiv.org/pdf/2412.04945.pdf</a></span>   <span><a href='https://github.com/mschwimmbeck/HOLa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Schwimmbeck, Serouj Khajarian, Konstantin Holzapfel, Johannes Schmidt, Stefanie Remmele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04945">HOLa: HoloLens Object Labeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the context of medical Augmented Reality (AR) applications, object tracking is a key challenge and requires a significant amount of annotation masks. As segmentation foundation models like the Segment Anything Model (SAM) begin to emerge, zero-shot segmentation requires only minimal human participation obtaining high-quality object masks. We introduce a HoloLens-Object-Labeling (HOLa) Unity and Python application based on the SAM-Track algorithm that offers fully automatic single object annotation for HoloLens 2 while requiring minimal human participation. HOLa does not have to be adjusted to a specific image appearance and could thus alleviate AR research in any application field. We evaluate HOLa for different degrees of image complexity in open liver surgery and in medical phantom experiments. Using HOLa for image annotation can increase the labeling speed by more than 500 times while providing Dice scores between 0.875 and 0.982, which are comparable to human annotators. Our code is publicly available at: https://github.com/mschwimmbeck/HOLa
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2412.03512.pdf' target='_blank'>https://arxiv.org/pdf/2412.03512.pdf</a></span>   <span><a href='https://compvis.github.io/distilldift' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank Fundel, Johannes Schusterbauer, Vincent Tao Hu, BjÃ¶rn Ommer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03512">Distillation of Diffusion Features for Semantic Correspondence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic correspondence, the task of determining relationships between different parts of images, underpins various applications including 3D reconstruction, image-to-image translation, object tracking, and visual place recognition. Recent studies have begun to explore representations learned in large generative image models for semantic correspondence, demonstrating promising results. Building on this progress, current state-of-the-art methods rely on combining multiple large models, resulting in high computational demands and reduced efficiency. In this work, we address this challenge by proposing a more computationally efficient approach. We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency. We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost. Furthermore, we demonstrate that by incorporating 3D data, we are able to further improve performance, without the need for human-annotated correspondences. Overall, our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications, such as semantic video correspondence. Our code and weights are publicly available on our project page.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2412.02129.pdf' target='_blank'>https://arxiv.org/pdf/2412.02129.pdf</a></span>   <span><a href='https://github.com/ailovejinx/GSOT3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Jiao, Yunhao Li, Junhua Ding, Qing Yang, Song Fu, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02129">GSOT3D: Towards Generic 3D Single Object Tracking in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel benchmark, GSOT3D, that aims at facilitating development of generic 3D single object tracking (SOT) in the wild. Specifically, GSOT3D offers 620 sequences with 123K frames, and covers a wide selection of 54 object categories. Each sequence is offered with multiple modalities, including the point cloud (PC), RGB image, and depth. This allows GSOT3D to support various 3D tracking tasks, such as single-modal 3D SOT on PC and multi-modal 3D SOT on RGB-PC or RGB-D, and thus greatly broadens research directions for 3D object tracking. To provide highquality per-frame 3D annotations, all sequences are labeled manually with multiple rounds of meticulous inspection and refinement. To our best knowledge, GSOT3D is the largest benchmark dedicated to various generic 3D object tracking tasks. To understand how existing 3D trackers perform and to provide comparisons for future research on GSOT3D, we assess eight representative point cloud-based tracking models. Our evaluation results exhibit that these models heavily degrade on GSOT3D, and more efforts are required for robust and generic 3D object tracking. Besides, to encourage future research, we present a simple yet effective generic 3D tracker, named PROT3D, that localizes the target object via a progressive spatial-temporal network and outperforms all current solutions by a large margin. By releasing GSOT3D, we expect to advance further 3D tracking in future research and applications. Our benchmark and model as well as the evaluation results will be publicly released at our webpage https://github.com/ailovejinx/GSOT3D.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2412.01147.pdf' target='_blank'>https://arxiv.org/pdf/2412.01147.pdf</a></span>   <span><a href='https://uark-aicv.github.io/A2VIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Tran, Thang Pham, Winston Bounsavy, Tri Nguyen, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01147">A2VIS: Amodal-Aware Approach to Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Handling occlusion remains a significant challenge for video instance-level tasks like Multiple Object Tracking (MOT) and Video Instance Segmentation (VIS). In this paper, we propose a novel framework, Amodal-Aware Video Instance Segmentation (A2VIS), which incorporates amodal representations to achieve a reliable and comprehensive understanding of both visible and occluded parts of objects in a video. The key intuition is that awareness of amodal segmentation through spatiotemporal dimension enables a stable stream of object information. In scenarios where objects are partially or completely hidden from view, amodal segmentation offers more consistency and less dramatic changes along the temporal axis compared to visible segmentation. Hence, both amodal and visible information from all clips can be integrated into one global instance prototype. To effectively address the challenge of video amodal segmentation, we introduce the spatiotemporal-prior Amodal Mask Head, which leverages visible information intra clips while extracting amodal characteristics inter clips. Through extensive experiments and ablation studies, we show that A2VIS excels in both MOT and VIS tasks in identifying and tracking object instances with a keen understanding of their full shape.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2412.01132.pdf' target='_blank'>https://arxiv.org/pdf/2412.01132.pdf</a></span>   <span><a href='https://github.com/joe-rabbit/VideoQA_Pilot_Study' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Raj Vishal, Divesh Basina, Aarya Choudhary, Bharatesh Chakravarthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01132">Eyes on the Road: State-of-the-Art Video Question Answering Models Assessment for Traffic Monitoring Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in video question answering (VideoQA) offer promising applications, especially in traffic monitoring, where efficient video interpretation is critical. Within ITS, answering complex, real-time queries like "How many red cars passed in the last 10 minutes?" or "Was there an incident between 3:00 PM and 3:05 PM?" enhances situational awareness and decision-making. Despite progress in vision-language models, VideoQA remains challenging, especially in dynamic environments involving multiple objects and intricate spatiotemporal relationships. This study evaluates state-of-the-art VideoQA models using non-benchmark synthetic and real-world traffic sequences. The framework leverages GPT-4o to assess accuracy, relevance, and consistency across basic detection, temporal reasoning, and decomposition queries. VideoLLaMA-2 excelled with 57% accuracy, particularly in compositional reasoning and consistent answers. However, all models, including VideoLLaMA-2, faced limitations in multi-object tracking, temporal coherence, and complex scene interpretation, highlighting gaps in current architectures. These findings underscore VideoQA's potential in traffic monitoring but also emphasize the need for improvements in multi-object tracking, temporal reasoning, and compositional capabilities. Enhancing these areas could make VideoQA indispensable for incident detection, traffic flow management, and responsive urban planning. The study's code and framework are open-sourced for further exploration: https://github.com/joe-rabbit/VideoQA_Pilot_Study
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2411.18855.pdf' target='_blank'>https://arxiv.org/pdf/2411.18855.pdf</a></span>   <span><a href='https://wvuvl.github.io/SiamABC/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ram Zaveri, Shivang Patel, Yu Gu, Gianfranco Doretto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18855">Improving Accuracy and Generalization for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient visual trackers overfit to their training distributions and lack generalization abilities, resulting in them performing well on their respective in-distribution (ID) test sets and not as well on out-of-distribution (OOD) sequences, imposing limitations to their deployment in-the-wild under constrained resources. We introduce SiamABC, a highly efficient Siamese tracker that significantly improves tracking performance, even on OOD sequences. SiamABC takes advantage of new architectural designs in the way it bridges the dynamic variability of the target, and of new losses for training. Also, it directly addresses OOD tracking generalization by including a fast backward-free dynamic test-time adaptation method that continuously adapts the model according to the dynamic visual changes of the target. Our extensive experiments suggest that SiamABC shows remarkable performance gains in OOD sets while maintaining accurate performance on the ID benchmarks. SiamABC outperforms MixFormerV2-S by 7.6\% on the OOD AVisT benchmark while being 3x faster (100 FPS) on a CPU. Our code and models are available at https://wvuvl.github.io/SiamABC/.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2411.17576.pdf' target='_blank'>https://arxiv.org/pdf/2411.17576.pdf</a></span>   <span><a href='https://github.com/jovanavidenovic/DAM4SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovana Videnovic, Alan Lukezic, Matej Kristan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17576">A Distractor-Aware Memory for Visual Object Tracking with SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Memory-based trackers are video object segmentation methods that form the target model by concatenating recently tracked frames into a memory buffer and localize the target by attending the current image to the buffered frames. While already achieving top performance on many benchmarks, it was the recent release of SAM2 that placed memory-based trackers into focus of the visual object tracking community. Nevertheless, modern trackers still struggle in the presence of distractors. We argue that a more sophisticated memory model is required, and propose a new distractor-aware memory model for SAM2 and an introspection-based update strategy that jointly addresses the segmentation accuracy as well as tracking robustness. The resulting tracker is denoted as SAM2.1++. We also propose a new distractor-distilled DiDi dataset to study the distractor problem better. SAM2.1++ outperforms SAM2.1 and related SAM memory extensions on seven benchmarks and sets a solid new state-of-the-art on six of them.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2411.15761.pdf' target='_blank'>https://arxiv.org/pdf/2411.15761.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15761">MambaTrack: Exploiting Dual-Enhancement for Night UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Night unmanned aerial vehicle (UAV) tracking is impeded by the challenges of poor illumination, with previous daylight-optimized methods demonstrating suboptimal performance in low-light conditions, limiting the utility of UAV applications. To this end, we propose an efficient mamba-based tracker, leveraging dual enhancement techniques to boost night UAV tracking. The mamba-based low-light enhancer, equipped with an illumination estimator and a damage restorer, achieves global image enhancement while preserving the details and structure of low-light images. Additionally, we advance a cross-modal mamba network to achieve efficient interactive learning between vision and language modalities. Extensive experiments showcase that our method achieves advanced performance and exhibits significantly improved computation and memory efficiency. For instance, our method is 2.8$\times$ faster than CiteTracker and reduces 50.2$\%$ GPU memory. Our codes are available at \url{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2411.12943.pdf' target='_blank'>https://arxiv.org/pdf/2411.12943.pdf</a></span>   <span><a href='https://github.com/wassimea/thermalMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wassim El Ahmar, Dhanvin Kolhatkar, Farzan Nowruzi, Robert Laganiere
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12943">Enhancing Thermal MOT: A Novel Box Association Method Leveraging Thermal Identity and Motion Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) in thermal imaging presents unique challenges due to the lack of visual features and the complexity of motion patterns. This paper introduces an innovative approach to improve MOT in the thermal domain by developing a novel box association method that utilizes both thermal object identity and motion similarity. Our method merges thermal feature sparsity and dynamic object tracking, enabling more accurate and robust MOT performance. Additionally, we present a new dataset comprised of a large-scale collection of thermal and RGB images captured in diverse urban environments, serving as both a benchmark for our method and a new resource for thermal imaging. We conduct extensive experiments to demonstrate the superiority of our approach over existing methods, showing significant improvements in tracking accuracy and robustness under various conditions. Our findings suggest that incorporating thermal identity with motion data enhances MOT performance. The newly collected dataset and source code is available at https://github.com/wassimea/thermalMOT
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2411.11922.pdf' target='_blank'>https://arxiv.org/pdf/2411.11922.pdf</a></span>   <span><a href='https://yangchris11.github.io/samurai/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11922">SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks but faces challenges in visual object tracking, particularly when managing crowded scenes with fast-moving or self-occluding objects. Furthermore, the fixed-window memory approach in the original model does not consider the quality of memories selected to condition the image features for the next frame, leading to error propagation in videos. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 specifically designed for visual object tracking. By incorporating temporal motion cues with the proposed motion-aware memory selection mechanism, SAMURAI effectively predicts object motion and refines mask selection, achieving robust, accurate tracking without the need for retraining or fine-tuning. SAMURAI operates in real-time and demonstrates strong zero-shot performance across diverse benchmark datasets, showcasing its ability to generalize without fine-tuning. In evaluations, SAMURAI achieves significant improvements in success rate and precision over existing trackers, with a 7.1% AUC gain on LaSOT$_{\text{ext}}$ and a 3.5% AO gain on GOT-10k. Moreover, it achieves competitive results compared to fully supervised methods on LaSOT, underscoring its robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2411.10564.pdf' target='_blank'>https://arxiv.org/pdf/2411.10564.pdf</a></span>   <span><a href='https://github.com/MahmudulHasan11085/Vision-Eagle-Attention.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahmudul Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10564">Vision Eagle Attention: a new lens for advancing image classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In computer vision tasks, the ability to focus on relevant regions within an image is crucial for improving model performance, particularly when key features are small, subtle, or spatially dispersed. Convolutional neural networks (CNNs) typically treat all regions of an image equally, which can lead to inefficient feature extraction. To address this challenge, I have introduced Vision Eagle Attention, a novel attention mechanism that enhances visual feature extraction using convolutional spatial attention. The model applies convolution to capture local spatial features and generates an attention map that selectively emphasizes the most informative regions of the image. This attention mechanism enables the model to focus on discriminative features while suppressing irrelevant background information. I have integrated Vision Eagle Attention into a lightweight ResNet-18 architecture, demonstrating that this combination results in an efficient and powerful model. I have evaluated the performance of the proposed model on three widely used benchmark datasets: FashionMNIST, Intel Image Classification, and OracleMNIST, with a primary focus on image classification. Experimental results show that the proposed approach improves classification accuracy. Additionally, this method has the potential to be extended to other vision tasks, such as object detection, segmentation, and visual tracking, offering a computationally efficient solution for a wide range of vision-based applications. Code is available at: https://github.com/MahmudulHasan11085/Vision-Eagle-Attention.git
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2411.09551.pdf' target='_blank'>https://arxiv.org/pdf/2411.09551.pdf</a></span>   <span><a href='https://github.com/serycjon/MFTIQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Serych, Michal Neoral, Jiri Matas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09551">MFTIQ: Multi-Flow Tracker with Independent Matching Quality Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present MFTIQ, a novel dense long-term tracking model that advances the Multi-Flow Tracker (MFT) framework to address challenges in point-level visual tracking in video sequences. MFTIQ builds upon the flow-chaining concepts of MFT, integrating an Independent Quality (IQ) module that separates correspondence quality estimation from optical flow computations. This decoupling significantly enhances the accuracy and flexibility of the tracking process, allowing MFTIQ to maintain reliable trajectory predictions even in scenarios of prolonged occlusions and complex dynamics. Designed to be "plug-and-play", MFTIQ can be employed with any off-the-shelf optical flow method without the need for fine-tuning or architectural modifications. Experimental validations on the TAP-Vid Davis dataset show that MFTIQ with RoMa optical flow not only surpasses MFT but also performs comparably to state-of-the-art trackers while having substantially faster processing speed. Code and models available at https://github.com/serycjon/MFTIQ .
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2411.08216.pdf' target='_blank'>https://arxiv.org/pdf/2411.08216.pdf</a></span>   <span><a href='https://github.com/sjc042/gta-link.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Sun, Hsiang-Wei Huang, Cheng-Yen Yang, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08216">GTA: Global Tracklet Association for Multi-Object Tracking in Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking in sports scenarios has become one of the focal points in computer vision, experiencing significant advancements through the integration of deep learning techniques. Despite these breakthroughs, challenges remain, such as accurately re-identifying players upon re-entry into the scene and minimizing ID switches. In this paper, we propose an appearance-based global tracklet association algorithm designed to enhance tracking performance by splitting tracklets containing multiple identities and connecting tracklets seemingly from the same identity. This method can serve as a plug-and-play refinement tool for any multi-object tracker to further boost their performance. The proposed method achieved a new state-of-the-art performance on the SportsMOT dataset with HOTA score of 81.04%. Similarly, on the SoccerNet dataset, our method enhanced multiple trackers' performance, consistently increasing the HOTA score from 79.41% to 83.11%. These significant and consistent improvements across different trackers and datasets underscore our proposed method's potential impact on the application of sports player tracking. We open-source our project codebase at https://github.com/sjc042/gta-link.git.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2411.06378.pdf' target='_blank'>https://arxiv.org/pdf/2411.06378.pdf</a></span>   <span><a href='https://github.com/hwcao17/pkf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwen Cao, George J. Pappas, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06378">PKF: Probabilistic Data Association Kalman Filter for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we derive a new Kalman filter with probabilistic data association between measurements and states. We formulate a variational inference problem to approximate the posterior density of the state conditioned on the measurement data. We view the unknown data association as a latent variable and apply Expectation Maximization (EM) to obtain a filter with update step in the same form as the Kalman filter but with expanded measurement vector of all potential associations. We show that the association probabilities can be computed as permanents of matrices with measurement likelihood entries. We also propose an ambiguity check that associates only a subset of ambiguous measurements and states probabilistically, thus reducing the association time and preventing low-probability measurements from harming the estimation accuracy. Experiments in simulation show that our filter achieves lower tracking errors than the well-established joint probabilistic data association filter (JPDAF), while running at comparable rate. We also demonstrate the effectiveness of our filter in multi-object tracking (MOT) on multiple real-world datasets, including MOT17, MOT20, and DanceTrack. We achieve better higher order tracking accuracy (HOTA) than previous Kalman-filter methods and remain real-time. Associating only bounding boxes without deep features or velocities, our method ranks top-10 on both MOT17 and MOT20 in terms of HOTA. Given offline detections, our algorithm tracks at 250+ fps on a single laptop CPU. Code is available at https://github.com/hwcao17/pkf.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2411.06378.pdf' target='_blank'>https://arxiv.org/pdf/2411.06378.pdf</a></span>   <span><a href='https://github.com/hwcao17/pkf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanwen Cao, George J. Pappas, Nikolay Atanasov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06378">PKF: Probabilistic Data Association Kalman Filter for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we derive a new Kalman filter with probabilistic data association between measurements and states. We formulate a variational inference problem to approximate the posterior density of the state conditioned on the measurement data. We view the unknown data association as a latent variable and apply Expectation Maximization (EM) to obtain a filter with update step in the same form as the Kalman filter but with expanded measurement vector of all potential associations. We show that the association probabilities can be computed as permanents of matrices with measurement likelihood entries. We also propose an ambiguity check that associates only a subset of ambiguous measurements and states probabilistically, thus reducing the association time and preventing low-probability measurements from harming the estimation accuracy. Experiments in simulation show that our filter achieves lower tracking errors than the well-established joint probabilistic data association filter (JPDAF), while running at comparable rate. We also demonstrate the effectiveness of our filter in multi-object tracking (MOT) on multiple real-world datasets, including MOT17, MOT20, and DanceTrack. We achieve better higher order tracking accuracy (HOTA) than previous Kalman-filter methods and remain real-time. Associating only bounding boxes without deep features or velocities, our method ranks top-10 on both MOT17 and MOT20 in terms of HOTA. Given offline detections, our algorithm tracks at 250+ fps on a single laptop CPU. Code is available at https://github.com/hwcao17/pkf.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2410.20421.pdf' target='_blank'>https://arxiv.org/pdf/2410.20421.pdf</a></span>   <span><a href='https://github.com/LiuYuML/NV-VOT211' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Arif Mahmood, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20421">NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many current visual object tracking benchmarks such as OTB100, NfS, UAV123, LaSOT, and GOT-10K, predominantly contain day-time scenarios while the challenges posed by the night-time has been less investigated. It is primarily because of the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. To this end, this paper presents NT-VOT211, a new benchmark tailored for evaluating visual object tracking algorithms in the challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. To the best of our knowledge, it is the largest night-time tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and distractors inherent to night-time tracking scenarios. Through a comprehensive analysis of results obtained from 42 diverse tracking algorithms on NT-VOT211, we uncover the strengths and limitations of these algorithms, highlighting opportunities for enhancements in visual object tracking, particularly in environments with suboptimal lighting. Besides, a leaderboard for revealing performance rankings, annotation tools, comprehensive meta-information and all the necessary code for reproducibility of results is made publicly available. We believe that our NT-VOT211 benchmark will not only be instrumental in facilitating field deployment of VOT algorithms, but will also help VOT enhancements and it will unlock new real-world tracking applications. Our dataset and other assets can be found at: {https://github.com/LiuYuML/NV-VOT211.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2410.20395.pdf' target='_blank'>https://arxiv.org/pdf/2410.20395.pdf</a></span>   <span><a href='https://github.com/LiuYuML/Depth-Attention' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Arif Mahmood, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20395">Depth Attention for Robust RGB Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB video object tracking is a fundamental task in computer vision. Its effectiveness can be improved using depth information, particularly for handling motion-blurred target. However, depth information is often missing in commonly used tracking benchmarks. In this work, we propose a new framework that leverages monocular depth estimation to counter the challenges of tracking targets that are out of view or affected by motion blur in RGB video sequences. Specifically, our work introduces following contributions. To the best of our knowledge, we are the first to propose a depth attention mechanism and to formulate a simple framework that allows seamlessly integration of depth information with state of the art tracking algorithms, without RGB-D cameras, elevating accuracy and robustness. We provide extensive experiments on six challenging tracking benchmarks. Our results demonstrate that our approach provides consistent gains over several strong baselines and achieves new SOTA performance. We believe that our method will open up new possibilities for more sophisticated VOT solutions in real-world scenarios. Our code and models are publicly released: https://github.com/LiuYuML/Depth-Attention.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2410.17856.pdf' target='_blank'>https://arxiv.org/pdf/2410.17856.pdf</a></span>   <span><a href='https://craftjarvis.github.io/ROCKET-1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17856">ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models (VLMs) have excelled in multimodal tasks, but adapting them to embodied decision-making in open-world environments presents challenges. One critical issue is bridging the gap between discrete entities in low-level observations and the abstract concepts required for effective planning. A common solution is building hierarchical agents, where VLMs serve as high-level reasoners that break down tasks into executable sub-tasks, typically specified using language. However, language suffers from the inability to communicate detailed spatial information. We propose visual-temporal context prompting, a novel communication protocol between VLMs and policy models. This protocol leverages object segmentation from past observations to guide policy-environment interactions. Using this approach, we train ROCKET-1, a low-level policy that predicts actions based on concatenated visual observations and segmentation masks, supported by real-time object tracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to tackle complex tasks that demand spatial reasoning. Experiments in Minecraft show that our approach enables agents to achieve previously unattainable tasks, with a $\mathbf{76}\%$ absolute improvement in open-world interaction performance. Codes and demos are now available on the project page: https://craftjarvis.github.io/ROCKET-1.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2410.17534.pdf' target='_blank'>https://arxiv.org/pdf/2410.17534.pdf</a></span>   <span><a href='https://github.com/Coo1Sea/OVT-B-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiji Liang, Ruize Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17534">OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary object perception has become an important topic in artificial intelligence, which aims to identify objects with novel classes that have not been seen during training. Under this setting, open-vocabulary object detection (OVD) in a single image has been studied in many literature. However, open-vocabulary object tracking (OVT) from a video has been studied less, and one reason is the shortage of benchmarks. In this work, we have built a new large-scale benchmark for open-vocabulary multi-object tracking namely OVT-B. OVT-B contains 1,048 categories of objects and 1,973 videos with 637,608 bounding box annotations, which is much larger than the sole open-vocabulary tracking dataset, i.e., OVTAO-val dataset (200+ categories, 900+ videos). The proposed OVT-B can be used as a new benchmark to pave the way for OVT research. We also develop a simple yet effective baseline method for OVT. It integrates the motion features for object tracking, which is an important feature for MOT but is ignored in previous OVT methods. Experimental results have verified the usefulness of the proposed benchmark and the effectiveness of our method. We have released the benchmark to the public at https://github.com/Coo1Sea/OVT-B-Dataset.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2410.14977.pdf' target='_blank'>https://arxiv.org/pdf/2410.14977.pdf</a></span>   <span><a href='https://github.com/linh-gist/ms-glmb-nuScenes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Van Ma, Muhammad Ishfaq Hussain, Kin-Choong Yow, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14977">3D Multi-Object Tracking Employing MS-GLMB Filter for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The MS-GLMB filter offers a robust framework for tracking multiple objects through the use of multi-sensor data. Building on this, the MV-GLMB and MV-GLMB-AB filters enhance the MS-GLMB capabilities by employing cameras for 3D multi-sensor multi-object tracking, effectively addressing occlusions. However, both filters depend on overlapping fields of view from the cameras to combine complementary information. In this paper, we introduce an improved approach that integrates an additional sensor, such as LiDAR, into the MS-GLMB framework for 3D multi-object tracking. Specifically, we present a new LiDAR measurement model, along with a multi-camera and LiDAR multi-object measurement model. Our experimental results demonstrate a significant improvement in tracking performance compared to existing MS-GLMB-based methods. Importantly, our method eliminates the need for overlapping fields of view, broadening the applicability of the MS-GLMB filter. Our source code for nuScenes dataset is available at https://github.com/linh-gist/ms-glmb-nuScenes.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2410.11125.pdf' target='_blank'>https://arxiv.org/pdf/2410.11125.pdf</a></span>   <span><a href='https://huiyegit.github.io/UAV3D_Benchmark/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Ye, Rajshekhar Sunderraman, Shihao Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.11125">UAV3D: A Large-scale 3D Perception Benchmark for Unmanned Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned Aerial Vehicles (UAVs), equipped with cameras, are employed in numerous applications, including aerial photography, surveillance, and agriculture. In these applications, robust object detection and tracking are essential for the effective deployment of UAVs. However, existing benchmarks for UAV applications are mainly designed for traditional 2D perception tasks, restricting the development of real-world applications that require a 3D understanding of the environment. Furthermore, despite recent advancements in single-UAV perception, limited views of a single UAV platform significantly constrain its perception capabilities over long distances or in occluded areas. To address these challenges, we introduce UAV3D, a benchmark designed to advance research in both 3D and collaborative 3D perception tasks with UAVs. UAV3D comprises 1,000 scenes, each of which has 20 frames with fully annotated 3D bounding boxes on vehicles. We provide the benchmark for four 3D perception tasks: single-UAV 3D object detection, single-UAV object tracking, collaborative-UAV 3D object detection, and collaborative-UAV object tracking. Our dataset and code are available at https://huiyegit.github.io/UAV3D_Benchmark/.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2410.10409.pdf' target='_blank'>https://arxiv.org/pdf/2410.10409.pdf</a></span>   <span><a href='https://github.com/mzahana/SMART-TRACK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Khaled Gabr, Mohamed Abdelkader, Imen Jarraya, Abdullah AlMusalami, Anis Koubaa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10409">SMART-TRACK: A Novel Kalman Filter-Guided Sensor Fusion For Robust UAV Object Tracking in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of sensor fusion and state estimation for object detection and localization, ensuring accurate tracking in dynamic environments poses significant challenges. Traditional methods like the Kalman Filter (KF) often fail when measurements are intermittent, leading to rapid divergence in state estimations. To address this, we introduce SMART (Sensor Measurement Augmentation and Reacquisition Tracker), a novel approach that leverages high-frequency state estimates from the KF to guide the search for new measurements, maintaining tracking continuity even when direct measurements falter. This is crucial for dynamic environments where traditional methods struggle. Our contributions include: 1) Versatile Measurement Augmentation Using KF Feedback: We implement a versatile measurement augmentation system that serves as a backup when primary object detectors fail intermittently. This system is adaptable to various sensors, demonstrated using depth cameras where KF's 3D predictions are projected into 2D depth image coordinates, integrating nonlinear covariance propagation techniques simplified to first-order approximations. 2) Open-source ROS2 Implementation: We provide an open-source ROS2 implementation of the SMART-TRACK framework, validated in a realistic simulation environment using Gazebo and ROS2, fostering broader adaptation and further research. Our results showcase significant enhancements in tracking stability, with estimation RMSE as low as 0.04 m during measurement disruptions, advancing the robustness of UAV tracking and expanding the potential for reliable autonomous UAV operations in complex scenarios. The implementation is available at https://github.com/mzahana/SMART-TRACK.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2410.09243.pdf' target='_blank'>https://arxiv.org/pdf/2410.09243.pdf</a></span>   <span><a href='https://UARK-AICV.github.io/G2MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duy Le Dinh Anh, Kim Hoang Tran, Quang-Thuc Nguyen, Ngan Hoang Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09243">Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent progress, Multi-Object Tracking (MOT) continues to face significant challenges, particularly its dependence on prior knowledge and predefined categories, complicating the tracking of unfamiliar objects. Generic Multiple Object Tracking (GMOT) emerges as a promising solution, requiring less prior information. Nevertheless, existing GMOT methods, primarily designed as OneShot-GMOT, rely heavily on initial bounding boxes and often struggle with variations in viewpoint, lighting, occlusion, and scale. To overcome the limitations inherent in both MOT and GMOT when it comes to tracking objects with specific generic attributes, we introduce Grounded-GMOT, an innovative tracking paradigm that enables users to track multiple generic objects in videos through natural language descriptors.
  Our contributions begin with the introduction of the G2MOT dataset, which includes a collection of videos featuring a wide variety of generic objects, each accompanied by detailed textual descriptions of their attributes. Following this, we propose a novel tracking method, KAM-SORT, which not only effectively integrates visual appearance with motion cues but also enhances the Kalman filter. KAM-SORT proves particularly advantageous when dealing with objects of high visual similarity from the same generic category in GMOT scenarios. Through comprehensive experiments, we demonstrate that Grounded-GMOT outperforms existing OneShot-GMOT approaches. Additionally, our extensive comparisons between various trackers highlight KAM-SORT's efficacy in GMOT, further establishing its significance in the field. Project page: https://UARK-AICV.github.io/G2MOT. The source code and dataset will be made publicly available.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2410.04250.pdf' target='_blank'>https://arxiv.org/pdf/2410.04250.pdf</a></span>   <span><a href='https://github.com/leggedrobotics/rsl_panoptic_mapping' target='_blank'>  GitHub</a></span> <span><a href='https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Terenzi, Julian Nubert, Pol Eyschen, Pascal Roth, Simin Fei, Edo Jelavic, Marco Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04250">ETHcavation: A Dataset and Pipeline for Panoptic Scene Understanding and Object Tracking in Dynamic Construction Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Construction sites are challenging environments for autonomous systems due to their unstructured nature and the presence of dynamic actors, such as workers and machinery. This work presents a comprehensive panoptic scene understanding solution designed to handle the complexities of such environments by integrating 2D panoptic segmentation with 3D LiDAR mapping. Our system generates detailed environmental representations in real-time by combining semantic and geometric data, supported by Kalman Filter-based tracking for dynamic object detection. We introduce a fine-tuning method that adapts large pre-trained panoptic segmentation models for construction site applications using a limited number of domain-specific samples. For this use case, we release a first-of-its-kind dataset of 502 hand-labeled sample images with panoptic annotations from construction sites. In addition, we propose a dynamic panoptic mapping technique that enhances scene understanding in unstructured environments. As a case study, we demonstrate the system's application for autonomous navigation, utilizing real-time RRT* for reactive path planning in dynamic scenarios. The dataset (https://leggedrobotics.github.io/panoptic-scene-understanding.github.io/) and code (https://github.com/leggedrobotics/rsl_panoptic_mapping) for training and deployment are publicly available to support future research.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2410.02249.pdf' target='_blank'>https://arxiv.org/pdf/2410.02249.pdf</a></span>   <span><a href='https://github.com/AndyCao1125/SpikeSlicer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahang Cao, Mingyuan Sun, Ziqing Wang, Hao Cheng, Qiang Zhang, Shibo Zhou, Renjing Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02249">Spiking Neural Network as Adaptive Event Stream Slicer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event-based cameras are attracting significant interest as they provide rich edge information, high dynamic range, and high temporal resolution. Many state-of-the-art event-based algorithms rely on splitting the events into fixed groups, resulting in the omission of crucial temporal information, particularly when dealing with diverse motion scenarios (\eg, high/low speed).In this work, we propose SpikeSlicer, a novel-designed plug-and-play event processing method capable of splitting events stream adaptively.SpikeSlicer utilizes a low-energy spiking neural network (SNN) to trigger event slicing. To guide the SNN to fire spikes at optimal time steps, we propose the Spiking Position-aware Loss (SPA-Loss) to modulate the neuron's state. Additionally, we develop a Feedback-Update training strategy that refines the slicing decisions using feedback from the downstream artificial neural network (ANN). Extensive experiments demonstrate that our method yields significant performance improvements in event-based object tracking and recognition. Notably, SpikeSlicer provides a brand-new SNN-ANN cooperation paradigm, where the SNN acts as an efficient, low-energy data processor to assist the ANN in improving downstream performance, injecting new perspectives and potential avenues of exploration. Our code is available at https://github.com/AndyCao1125/SpikeSlicer.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2409.19603.pdf' target='_blank'>https://arxiv.org/pdf/2409.19603.pdf</a></span>   <span><a href='https://github.com/showlab/VideoLISA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19603">One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2409.17564.pdf' target='_blank'>https://arxiv.org/pdf/2409.17564.pdf</a></span>   <span><a href='https://github.com/LingyiHongfd/CompressTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyi Hong, Jinglun Li, Xinyu Zhou, Shilin Yan, Pinxue Guo, Kaixun Jiang, Zhaoyu Chen, Shuyong Gao, Runze Li, Xingdong Sheng, Wei Zhang, Hong Lu, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17564">General Compression Framework for Efficient Transformer Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous works have attempted to improve tracking efficiency through lightweight architecture design or knowledge distillation from teacher models to compact student trackers. However, these solutions often sacrifice accuracy for speed to a great extent, and also have the problems of complex training process and structural limitations. Thus, we propose a general model compression framework for efficient transformer object tracking, named CompressTracker, to reduce model size while preserving tracking accuracy. Our approach features a novel stage division strategy that segments the transformer layers of the teacher model into distinct stages to break the limitation of model structure. Additionally, we also design a unique replacement training technique that randomly substitutes specific stages in the student model with those from the teacher model, as opposed to training the student model in isolation. Replacement training enhances the student model's ability to replicate the teacher model's behavior and simplifies the training process. To further forcing student model to emulate teacher model, we incorporate prediction guidance and stage-wise feature mimicking to provide additional supervision during the teacher model's compression process. CompressTracker is structurally agnostic, making it compatible with any transformer architecture. We conduct a series of experiment to verify the effectiveness and generalizability of our CompressTracker. Our CompressTracker-SUTrack, compressed from SUTrack, retains about 99 performance on LaSOT (72.2 AUC) while achieves 2.42x speed up. Code is available at https://github.com/LingyiHongfd/CompressTracker.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2409.16902.pdf' target='_blank'>https://arxiv.org/pdf/2409.16902.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Guanjie Huang, Zhipeng Zhang, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16902">Underwater Camouflaged Object Tracking Meets Vision-Language SAM2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale datasets. However, these datasets have primarily focused on open-air scenarios and have largely overlooked underwater animal tracking-especially the complex challenges posed by camouflaged marine animals. To bridge this gap, we take a step forward by proposing the first large-scale multi-modal underwater camouflaged object tracking dataset, namely UW-COT220. Based on the proposed dataset, this work first comprehensively evaluates current advanced visual object tracking methods, including SAM- and SAM2-based trackers, in challenging underwater environments, \eg, coral reefs. Our findings highlight the improvements of SAM2 over SAM, demonstrating its enhanced ability to handle the complexities of underwater camouflaged objects. Furthermore, we propose a novel vision-language tracking framework called VL-SAM2, based on the video foundation model SAM2. Extensive experimental results demonstrate that the proposed VL-SAM2 achieves state-of-the-art performance across underwater and open-air object tracking datasets. The dataset and codes are available at~{\color{magenta}{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}}.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2409.16834.pdf' target='_blank'>https://arxiv.org/pdf/2409.16834.pdf</a></span>   <span><a href='https://github.com/vision4robotics/CGDenoiser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Wang, Changhong Fu, Kunhan Lu, Liangliang Yao, Haobo Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16834">Conditional Generative Denoiser for Nighttime UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art (SOTA) visual object tracking methods have significantly enhanced the autonomy of unmanned aerial vehicles (UAVs). However, in low-light conditions, the presence of irregular real noise from the environments severely degrades the performance of these SOTA methods. Moreover, existing SOTA denoising techniques often fail to meet the real-time processing requirements when deployed as plug-and-play denoisers for UAV tracking. To address this challenge, this work proposes a novel conditional generative denoiser (CGDenoiser), which breaks free from the limitations of traditional deterministic paradigms and generates the noise conditioning on the input, subsequently removing it. To better align the input dimensions and accelerate inference, a novel nested residual Transformer conditionalizer is developed. Furthermore, an innovative multi-kernel conditional refiner is designed to pertinently refine the denoised output. Extensive experiments show that CGDenoiser promotes the tracking precision of the SOTA tracker by 18.18\% on DarkTrack2021 whereas working 5.8 times faster than the second well-performed denoiser. Real-world tests with complex challenges also prove the effectiveness and practicality of CGDenoiser. Code, video demo and supplementary proof for CGDenoier are now available at: \url{https://github.com/vision4robotics/CGDenoiser}.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2409.16652.pdf' target='_blank'>https://arxiv.org/pdf/2409.16652.pdf</a></span>   <span><a href='https://github.com/vision4robotics/PRL-Track' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changhong Fu, Xiang Lei, Haobo Zuo, Liangliang Yao, Guangze Zheng, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16652">Progressive Representation Learning for Real-Time UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has significantly promoted autonomous applications for unmanned aerial vehicles (UAVs). However, learning robust object representations for UAV tracking is especially challenging in complex dynamic environments, when confronted with aspect ratio change and occlusion. These challenges severely alter the original information of the object. To handle the above issues, this work proposes a novel progressive representation learning framework for UAV tracking, i.e., PRL-Track. Specifically, PRL-Track is divided into coarse representation learning and fine representation learning. For coarse representation learning, two innovative regulators, which rely on appearance and semantic information, are designed to mitigate appearance interference and capture semantic information. Furthermore, for fine representation learning, a new hierarchical modeling generator is developed to intertwine coarse object representations. Exhaustive experiments demonstrate that the proposed PRL-Track delivers exceptional performance on three authoritative UAV tracking benchmarks. Real-world tests indicate that the proposed PRL-Track realizes superior tracking performance with 42.6 frames per second on the typical UAV platform equipped with an edge smart camera. The code, model, and demo videos are available at \url{https://github.com/vision4robotics/PRL-Track}.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2409.16631.pdf' target='_blank'>https://arxiv.org/pdf/2409.16631.pdf</a></span>   <span><a href='https://github.com/vision4robotics/LDEnhancer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangliang Yao, Changhong Fu, Yiheng Wang, Haobo Zuo, Kunhan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16631">Enhancing Nighttime UAV Tracking with Light Distribution Suppression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has boosted extensive intelligent applications for unmanned aerial vehicles (UAVs). However, the state-of-the-art (SOTA) enhancers for nighttime UAV tracking always neglect the uneven light distribution in low-light images, inevitably leading to excessive enhancement in scenarios with complex illumination. To address these issues, this work proposes a novel enhancer, i.e., LDEnhancer, enhancing nighttime UAV tracking with light distribution suppression. Specifically, a novel image content refinement module is developed to decompose the light distribution information and image content information in the feature space, allowing for the targeted enhancement of the image content information. Then this work designs a new light distribution generation module to capture light distribution effectively. The features with light distribution information and image content information are fed into the different parameter estimation modules, respectively, for the parameter map prediction. Finally, leveraging two parameter maps, an innovative interweave iteration adjustment is proposed for the collaborative pixel-wise adjustment of low-light images. Additionally, a challenging nighttime UAV tracking dataset with uneven light distribution, namely NAT2024-2, is constructed to provide a comprehensive evaluation, which contains 40 challenging sequences with over 74K frames in total. Experimental results on the authoritative UAV benchmarks and the proposed NAT2024-2 demonstrate that LDEnhancer outperforms other SOTA low-light enhancers for nighttime UAV tracking. Furthermore, real-world tests on a typical UAV platform with an NVIDIA Orin NX confirm the practicality and efficiency of LDEnhancer. The code is available at https://github.com/vision4robotics/LDEnhancer.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2409.16149.pdf' target='_blank'>https://arxiv.org/pdf/2409.16149.pdf</a></span>   <span><a href='https://github.com/megvii-research/MCTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyang Wang, Shouzheng Qi, Jieyou Zhao, Hangning Zhou, Siyu Zhang, Guoan Wang, Kai Tu, Songlin Guo, Jianbo Zhao, Jian Li, Mu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16149">MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces MCTrack, a new 3D multi-object tracking method that achieves state-of-the-art (SOTA) performance across KITTI, nuScenes, and Waymo datasets. Addressing the gap in existing tracking paradigms, which often perform well on specific datasets but lack generalizability, MCTrack offers a unified solution. Additionally, we have standardized the format of perceptual results across various datasets, termed BaseVersion, facilitating researchers in the field of multi-object tracking (MOT) to concentrate on the core algorithmic development without the undue burden of data preprocessing. Finally, recognizing the limitations of current evaluation metrics, we propose a novel set that assesses motion information output, such as velocity and acceleration, crucial for downstream tasks. The source codes of the proposed method are available at this link: https://github.com/megvii-research/MCTrack}{https://github.com/megvii-research/MCTrack
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2409.12159.pdf' target='_blank'>https://arxiv.org/pdf/2409.12159.pdf</a></span>   <span><a href='https://github.com/Walleclipse/WeHelp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abulikemu Abuduweili, Alice Wu, Tianhao Wei, Weiye Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12159">WeHelp: A Shared Autonomy System for Wheelchair Users</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is a large population of wheelchair users. Most of the wheelchair users need help with daily tasks. However, according to recent reports, their needs are not properly satisfied due to the lack of caregivers. Therefore, in this project, we develop WeHelp, a shared autonomy system aimed for wheelchair users. A robot with a WeHelp system has three modes, following mode, remote control mode and tele-operation mode. In the following mode, the robot follows the wheelchair user automatically via visual tracking. The wheelchair user can ask the robot to follow them from behind, by the left or by the right. When the wheelchair user asks for help, the robot will recognize the command via speech recognition, and then switch to the teleoperation mode or remote control mode. In the teleoperation mode, the wheelchair user takes over the robot with a joy stick and controls the robot to complete some complex tasks for their needs, such as opening doors, moving obstacles on the way, reaching objects on a high shelf or on the low ground, etc. In the remote control mode, a remote assistant takes over the robot and helps the wheelchair user complete some complex tasks for their needs. Our evaluation shows that the pipeline is useful and practical for wheelchair users. Source code and demo of the paper are available at \url{https://github.com/Walleclipse/WeHelp}.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2409.11235.pdf' target='_blank'>https://arxiv.org/pdf/2409.11235.pdf</a></span>   <span><a href='https://github.com/siyuanliii/SLAck' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Li, Lei Ke, Yung-Hsu Yang, Luigi Piccinelli, Mattia SegÃ¹, Martin Danelljan, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11235">SLAck: Semantic, Location, and Appearance Aware Open-Vocabulary Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary Multiple Object Tracking (MOT) aims to generalize trackers to novel categories not in the training set. Currently, the best-performing methods are mainly based on pure appearance matching. Due to the complexity of motion patterns in the large-vocabulary scenarios and unstable classification of the novel objects, the motion and semantics cues are either ignored or applied based on heuristics in the final matching steps by existing methods. In this paper, we present a unified framework SLAck that jointly considers semantics, location, and appearance priors in the early steps of association and learns how to integrate all valuable information through a lightweight spatial and temporal object graph. Our method eliminates complex post-processing heuristics for fusing different cues and boosts the association performance significantly for large-scale open-vocabulary tracking. Without bells and whistles, we outperform previous state-of-the-art methods for novel classes tracking on the open-vocabulary MOT and TAO TETA benchmarks. Our code is available at \href{https://github.com/siyuanliii/SLAck}{github.com/siyuanliii/SLAck}.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2409.11234.pdf' target='_blank'>https://arxiv.org/pdf/2409.11234.pdf</a></span>   <span><a href='https://github.com/ydhcg-BoBo/STCMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Ma, Chuanming Tang, Fei Wu, Can Zhao, Jianlin Zhang, Zhiyong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11234">STCMOT: Spatio-Temporal Cohesion Learning for UAV-Based Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) in Unmanned Aerial Vehicle (UAV) videos is important for diverse applications in computer vision. Current MOT trackers rely on accurate object detection results and precise matching of target reidentification (ReID). These methods focus on optimizing target spatial attributes while overlooking temporal cues in modelling object relationships, especially for challenging tracking conditions such as object deformation and blurring, etc. To address the above-mentioned issues, we propose a novel Spatio-Temporal Cohesion Multiple Object Tracking framework (STCMOT), which utilizes historical embedding features to model the representation of ReID and detection features in a sequential order. Concretely, a temporal embedding boosting module is introduced to enhance the discriminability of individual embedding based on adjacent frame cooperation. While the trajectory embedding is then propagated by a temporal detection refinement module to mine salient target locations in the temporal field. Extensive experiments on the VisDrone2019 and UAVDT datasets demonstrate our STCMOT sets a new state-of-the-art performance in MOTA and IDF1 metrics. The source codes are released at https://github.com/ydhcg-BoBo/STCMOT.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2409.09293.pdf' target='_blank'>https://arxiv.org/pdf/2409.09293.pdf</a></span>   <span><a href='https://github.com/balabooooo/AED' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zimeng Fang, Chao Liang, Xue Zhou, Shuyuan Zhu, Xi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09293">Associate Everything Detected: Facilitating Tracking-by-Detection to the Unknown</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) emerges as a pivotal and highly promising branch in the field of computer vision. Classical closed-vocabulary MOT (CV-MOT) methods aim to track objects of predefined categories. Recently, some open-vocabulary MOT (OV-MOT) methods have successfully addressed the problem of tracking unknown categories. However, we found that the CV-MOT and OV-MOT methods each struggle to excel in the tasks of the other. In this paper, we present a unified framework, Associate Everything Detected (AED), that simultaneously tackles CV-MOT and OV-MOT by integrating with any off-the-shelf detector and supports unknown categories. Different from existing tracking-by-detection MOT methods, AED gets rid of prior knowledge (e.g. motion cues) and relies solely on highly robust feature learning to handle complex trajectories in OV-MOT tasks while keeping excellent performance in CV-MOT tasks. Specifically, we model the association task as a similarity decoding problem and propose a sim-decoder with an association-centric learning mechanism. The sim-decoder calculates similarities in three aspects: spatial, temporal, and cross-clip. Subsequently, association-centric learning leverages these threefold similarities to ensure that the extracted features are appropriate for continuous tracking and robust enough to generalize to unknown categories. Compared with existing powerful OV-MOT and CV-MOT methods, AED achieves superior performance on TAO, SportsMOT, and DanceTrack without any prior knowledge. Our code is available at https://github.com/balabooooo/AED.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2409.06617.pdf' target='_blank'>https://arxiv.org/pdf/2409.06617.pdf</a></span>   <span><a href='https://github.com/emirhanbayar/Fast-StrongSORT,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/emirhanbayar/Fast-Deep-OC-SORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Emirhan Bayar, Cemal Aker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06617">When to Extract ReID Features: A Selective Approach for Improved Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting and matching Re-Identification (ReID) features is used by many state-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularly effective against frequent and long-term occlusions. While end-to-end object detection and tracking have been the main focus of recent research, they have yet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus, from an application standpoint, methods with separate detection and embedding remain the best option for accuracy, modularity, and ease of implementation, though they are impractical for edge devices due to the overhead involved. In this paper, we investigate a selective approach to minimize the overhead of feature extraction while preserving accuracy, modularity, and ease of implementation. This approach can be integrated into various SOTA methods. We demonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT. Experiments on MOT17, MOT20, and DanceTrack datasets show that our mechanism retains the advantages of feature extraction during occlusions while significantly reducing runtime. Additionally, it improves accuracy by preventing confusion in the feature-matching stage, particularly in cases of deformation and appearance similarity, which are common in DanceTrack. https://github.com/emirhanbayar/Fast-StrongSORT, https://github.com/emirhanbayar/Fast-Deep-OC-SORT
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2409.04187.pdf' target='_blank'>https://arxiv.org/pdf/2409.04187.pdf</a></span>   <span><a href='https://github.com/Jumabek/LITE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jumabek Alikhanov, Dilshod Obidov, Hakil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04187">LITE: A Paradigm Shift in Multi-Object Tracking with Efficient ReID Feature Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Lightweight Integrated Tracking-Feature Extraction (LITE) paradigm is introduced as a novel multi-object tracking (MOT) approach. It enhances ReID-based trackers by eliminating inference, pre-processing, post-processing, and ReID model training costs. LITE uses real-time appearance features without compromising speed. By integrating appearance feature extraction directly into the tracking pipeline using standard CNN-based detectors such as YOLOv8m, LITE demonstrates significant performance improvements. The simplest implementation of LITE on top of classic DeepSORT achieves a HOTA score of 43.03% at 28.3 FPS on the MOT17 benchmark, making it twice as fast as DeepSORT on MOT17 and four times faster on the more crowded MOT20 dataset, while maintaining similar accuracy. Additionally, a new evaluation framework for tracking-by-detection approaches reveals that conventional trackers like DeepSORT remain competitive with modern state-of-the-art trackers when evaluated under fair conditions. The code will be available post-publication at https://github.com/Jumabek/LITE.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2409.02490.pdf' target='_blank'>https://arxiv.org/pdf/2409.02490.pdf</a></span>   <span><a href='https://fsoft-aic.github.io/TP-GMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duy Le Dinh Anh, Kim Hoang Tran, Ngan Hoang Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02490">TP-GMOT: Tracking Generic Multiple Object by Textual Prompt with Motion-Appearance Cost (MAC) SORT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Multi-Object Tracking (MOT) has made substantial advancements, it is limited by heavy reliance on prior knowledge and limited to predefined categories. In contrast, Generic Multiple Object Tracking (GMOT), tracking multiple objects with similar appearance, requires less prior information about the targets but faces challenges with variants like viewpoint, lighting, occlusion, and resolution. Our contributions commence with the introduction of the \textbf{\text{Refer-GMOT dataset}} a collection of videos, each accompanied by fine-grained textual descriptions of their attributes. Subsequently, we introduce a novel text prompt-based open-vocabulary GMOT framework, called \textbf{\text{TP-GMOT}}, which can track never-seen object categories with zero training examples. Within \text{TP-GMOT} framework, we introduce two novel components: (i) {\textbf{\text{TP-OD}}, an object detection by a textual prompt}, for accurately detecting unseen objects with specific characteristics. (ii) Motion-Appearance Cost SORT \textbf{\text{MAC-SORT}}, a novel object association approach that adeptly integrates motion and appearance-based matching strategies to tackle the complex task of tracking multiple generic objects with high similarity. Our contributions are benchmarked on the \text{Refer-GMOT} dataset for GMOT task. Additionally, to assess the generalizability of the proposed \text{TP-GMOT} framework and the effectiveness of \text{MAC-SORT} tracker, we conduct ablation studies on the DanceTrack and MOT20 datasets for the MOT task. Our dataset, code, and models will be publicly available at: https://fsoft-aic.github.io/TP-GMOT
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2409.00487.pdf' target='_blank'>https://arxiv.org/pdf/2409.00487.pdf</a></span>   <span><a href='https://github.com/Xavier-Lin/TrackSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Hu, Run Luo, Zelin Liu, Cheng Wang, Wenyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00487">TrackSSM: A General Motion Predictor by State-Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal motion modeling has always been a key component in multiple object tracking (MOT) which can ensure smooth trajectory movement and provide accurate positional information to enhance association precision. However, current motion models struggle to be both efficient and effective across different application scenarios. To this end, we propose TrackSSM inspired by the recently popular state space models (SSM), a unified encoder-decoder motion framework that uses data-dependent state space model to perform temporal motion of trajectories. Specifically, we propose Flow-SSM, a module that utilizes the position and motion information from historical trajectories to guide the temporal state transition of object bounding boxes. Based on Flow-SSM, we design a flow decoder. It is composed of a cascaded motion decoding module employing Flow-SSM, which can use the encoded flow information to complete the temporal position prediction of trajectories. Additionally, we propose a Step-by-Step Linear (S$^2$L) training strategy. By performing linear interpolation between the positions of the object in the previous frame and the current frame, we construct the pseudo labels of step-by-step linear training, ensuring that the trajectory flow information can better guide the object bounding box in completing temporal transitions. TrackSSM utilizes a simple Mamba-Block to build a motion encoder for historical trajectories, forming a temporal motion model with an encoder-decoder structure in conjunction with the flow decoder. TrackSSM is applicable to various tracking scenarios and achieves excellent tracking performance across multiple benchmarks, further extending the potential of SSM-like temporal motion models in multi-object tracking tasks. Code and models are publicly available at \url{https://github.com/Xavier-Lin/TrackSSM}.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2408.15548.pdf' target='_blank'>https://arxiv.org/pdf/2408.15548.pdf</a></span>   <span><a href='https://github.com/Tankowa/ConsistencyTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lifan Jiang, Zhihui Wang, Siqi Yin, Guangxiao Ma, Peng Zhang, Boxi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15548">ConsistencyTrack: A Robust Multi-Object Tracker with a Generation Strategy of Consistency Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a critical technology in computer vision, designed to detect multiple targets in video sequences and assign each target a unique ID per frame. Existed MOT methods excel at accurately tracking multiple objects in real-time across various scenarios. However, these methods still face challenges such as poor noise resistance and frequent ID switches. In this research, we propose a novel ConsistencyTrack, joint detection and tracking(JDT) framework that formulates detection and association as a denoising diffusion process on perturbed bounding boxes. This progressive denoising strategy significantly improves the model's noise resistance. During the training phase, paired object boxes within two adjacent frames are diffused from ground-truth boxes to a random distribution, and then the model learns to detect and track by reversing this process. In inference, the model refines randomly generated boxes into detection and tracking results through minimal denoising steps. ConsistencyTrack also introduces an innovative target association strategy to address target occlusion. Experiments on the MOT17 and DanceTrack datasets demonstrate that ConsistencyTrack outperforms other compared methods, especially better than DiffusionTrack in inference speed and other performance metrics. Our code is available at https://github.com/Tankowa/ConsistencyTrack.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2408.13877.pdf' target='_blank'>https://arxiv.org/pdf/2408.13877.pdf</a></span>   <span><a href='https://github.com/openat25/HIPTrack-MLS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Guo, Pengzhi Zhong, Hao Zhang, Defeng Huang, Huikai Shao, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13877">Camouflaged Object Tracking: A Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual tracking has seen remarkable advancements, largely driven by the availability of large-scale training datasets that have enabled the development of highly accurate and robust algorithms. While significant progress has been made in tracking general objects, research on more challenging scenarios, such as tracking camouflaged objects, remains limited. Camouflaged objects, which blend seamlessly with their surroundings or other objects, present unique challenges for detection and tracking in complex environments. This challenge is particularly critical in applications such as military, security, agriculture, and marine monitoring, where precise tracking of camouflaged objects is essential. To address this gap, we introduce the Camouflaged Object Tracking Dataset (COTD), a specialized benchmark designed specifically for evaluating camouflaged object tracking methods. The COTD dataset comprises 200 sequences and approximately 80,000 frames, each annotated with detailed bounding boxes. Our evaluation of 20 existing tracking algorithms reveals significant deficiencies in their performance with camouflaged objects. To address these issues, we propose a novel tracking framework, HiPTrack-MLS, which demonstrates promising results in improving tracking performance for camouflaged objects. COTD and code are avialable at https://github.com/openat25/HIPTrack-MLS.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2408.13003.pdf' target='_blank'>https://arxiv.org/pdf/2408.13003.pdf</a></span>   <span><a href='https://github.com/vukasin-stanojevic/BoostTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>VukaÅ¡in StanojeviÄ, Branimir TodoroviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13003">BoostTrack++: using tracklet information to detect more objects in multiple object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) depends heavily on selection of true positive detected bounding boxes. However, this aspect of the problem is mostly overlooked or mitigated by employing two-stage association and utilizing low confidence detections in the second stage. Recently proposed BoostTrack attempts to avoid the drawbacks of multiple stage association approach and use low-confidence detections by applying detection confidence boosting. In this paper, we identify the limitations of the confidence boost used in BoostTrack and propose a method to improve its performance. To construct a richer similarity measure and enable a better selection of true positive detections, we propose to use a combination of shape, Mahalanobis distance and novel soft BIoU similarity. We propose a soft detection confidence boost technique which calculates new confidence scores based on the similarity measure and the previous confidence scores, and we introduce varying similarity threshold to account for lower similarity measure between detections and tracklets which are not regularly updated. The proposed additions are mutually independent and can be used in any MOT algorithm.
  Combined with the BoostTrack+ baseline, our method achieves near state of the art results on the MOT17 dataset and new state of the art HOTA and IDF1 scores on the MOT20 dataset.
  The source code is available at: https://github.com/vukasin-stanojevic/BoostTrack .
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2408.11571.pdf' target='_blank'>https://arxiv.org/pdf/2408.11571.pdf</a></span>   <span><a href='https://github.com/CellTrackingChallenge/py-ctcmetrics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Timo Kaiser, Vladimir Ulman, Bodo Rosenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11571">CHOTA: A Higher Order Accuracy Metric for Cell Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The evaluation of cell tracking results steers the development of tracking methods, significantly impacting biomedical research. This is quantitatively achieved by means of evaluation metrics. Unfortunately, current metrics favor local correctness and weakly reward global coherence, impeding high-level biological analysis. To also foster global coherence, we propose the CHOTA metric (Cell-specific Higher Order Tracking Accuracy) which unifies the evaluation of all relevant aspects of cell tracking: cell detections and local associations, global coherence, and lineage tracking. We achieve this by introducing a new definition of the term 'trajectory' that includes the entire cell lineage and by including this into the well-established HOTA metric from general multiple object tracking. Furthermore, we provide a detailed survey of contemporary cell tracking metrics to compare our novel CHOTA metric and to show its advantages. All metrics are extensively evaluated on state-of-the-art real-data cell tracking results and synthetic results that simulate specific tracking errors. We show that CHOTA is sensitive to all tracking errors and gives a good indication of the biologically relevant capability of a method to reconstruct the full lineage of cells. It introduces a robust and comprehensive alternative to the currently used metrics in cell tracking. Python code is available at https://github.com/CellTrackingChallenge/py-ctcmetrics .
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2408.11463.pdf' target='_blank'>https://arxiv.org/pdf/2408.11463.pdf</a></span>   <span><a href='https://github.com/OpenCodeGithub/H-DCPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengzhi Zhong, Xiaoyu Guo, Defeng Huang, Xiaojun Peng, Yian Li, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.11463">Low-Light Object Tracking: A Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the field of visual tracking has made significant progress with the application of large-scale training datasets. These datasets have supported the development of sophisticated algorithms, enhancing the accuracy and stability of visual object tracking. However, most research has primarily focused on favorable illumination circumstances, neglecting the challenges of tracking in low-ligh environments. In low-light scenes, lighting may change dramatically, targets may lack distinct texture features, and in some scenarios, targets may not be directly observable. These factors can lead to a severe decline in tracking performance. To address this issue, we introduce LLOT, a benchmark specifically designed for Low-Light Object Tracking. LLOT comprises 269 challenging sequences with a total of over 132K frames, each carefully annotated with bounding boxes. This specially designed dataset aims to promote innovation and advancement in object tracking techniques for low-light conditions, addressing challenges not adequately covered by existing benchmarks. To assess the performance of existing methods on LLOT, we conducted extensive tests on 39 state-of-the-art tracking algorithms. The results highlight a considerable gap in low-light tracking performance. In response, we propose H-DCPT, a novel tracker that incorporates historical and darkness clue prompts to set a stronger baseline. H-DCPT outperformed all 39 evaluated methods in our experiments, demonstrating significant improvements. We hope that our benchmark and H-DCPT will stimulate the development of novel and accurate methods for tracking objects in low-light conditions. The LLOT and code are available at https://github.com/OpenCodeGithub/H-DCPT.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2408.10487.pdf' target='_blank'>https://arxiv.org/pdf/2408.10487.pdf</a></span>   <span><a href='https://github.com/Event-AHU/MambaEVT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Chao wang, Shiao Wang, Xixi Wang, Zhicheng Zhao, Lin Zhu, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10487">MambaEVT: Event Stream based Visual Object Tracking using State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event camera-based visual tracking has drawn more and more attention in recent years due to the unique imaging principle and advantages of low energy consumption, high dynamic range, and dense temporal resolution. Current event-based tracking algorithms are gradually hitting their performance bottlenecks, due to the utilization of vision Transformer and the static template for target object localization. In this paper, we propose a novel Mamba-based visual tracking framework that adopts the state space model with linear complexity as a backbone network. The search regions and target template are fed into the vision Mamba network for simultaneous feature extraction and interaction. The output tokens of search regions will be fed into the tracking head for target localization. More importantly, we consider introducing a dynamic template update strategy into the tracking framework using the Memory Mamba network. By considering the diversity of samples in the target template library and making appropriate adjustments to the template memory module, a more effective dynamic template can be integrated. The effective combination of dynamic and static templates allows our Mamba-based tracking algorithm to achieve a good balance between accuracy and computational cost on multiple large-scale datasets, including EventVOT, VisEvent, and FE240hz. The source code will be released on https://github.com/Event-AHU/MambaEVT
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2408.07605.pdf' target='_blank'>https://arxiv.org/pdf/2408.07605.pdf</a></span>   <span><a href='https://panacea-ad.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Wen, Yucheng Zhao, Yingfei Liu, Binyuan Huang, Fan Jia, Yanhui Wang, Chi Zhang, Tiancai Wang, Xiaoyan Sun, Xiangyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07605">Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of autonomous driving increasingly demands high-quality annotated video training data. In this paper, we propose Panacea+, a powerful and universally applicable framework for generating video data in driving scenes. Built upon the foundation of our previous work, Panacea, Panacea+ adopts a multi-view appearance noise prior mechanism and a super-resolution module for enhanced consistency and increased resolution. Extensive experiments show that the generated video samples from Panacea+ greatly benefit a wide range of tasks on different datasets, including 3D object tracking, 3D object detection, and lane detection tasks on the nuScenes and Argoverse 2 dataset. These results strongly prove Panacea+ to be a valuable data generation framework for autonomous driving.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2408.06190.pdf' target='_blank'>https://arxiv.org/pdf/2408.06190.pdf</a></span>   <span><a href='https://meyerls.github.io/fruit_nerf/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Meyer, Andreas Gilson, Ute Schmid, Marc Stamminger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.06190">FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FruitNeRF, a unified novel fruit counting framework that leverages state-of-the-art view synthesis methods to count any fruit type directly in 3D. Our framework takes an unordered set of posed images captured by a monocular camera and segments fruit in each image. To make our system independent of the fruit type, we employ a foundation model that generates binary segmentation masks for any fruit. Utilizing both modalities, RGB and semantic, we train a semantic neural radiance field. Through uniform volume sampling of the implicit Fruit Field, we obtain fruit-only point clouds. By applying cascaded clustering on the extracted point cloud, our approach achieves precise fruit count.The use of neural radiance fields provides significant advantages over conventional methods such as object tracking or optical flow, as the counting itself is lifted into 3D. Our method prevents double counting fruit and avoids counting irrelevant fruit.We evaluate our methodology using both real-world and synthetic datasets. The real-world dataset consists of three apple trees with manually counted ground truths, a benchmark apple dataset with one row and ground truth fruit location, while the synthetic dataset comprises various fruit types including apple, plum, lemon, pear, peach, and mango.Additionally, we assess the performance of fruit counting using the foundation model compared to a U-Net.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2408.01688.pdf' target='_blank'>https://arxiv.org/pdf/2408.01688.pdf</a></span>   <span><a href='https://github.com/HDU-VRLab/SiamMo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Yang, Yingqi Deng, Jing Zhang, Hongjie Gu, Zhekang Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01688">SiamMo: Siamese Motion-Centric 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current 3D single object tracking methods primarily rely on the Siamese matching-based paradigm, which struggles with textureless and incomplete LiDAR point clouds. Conversely, the motion-centric paradigm avoids appearance matching, thus overcoming these issues. However, its complex multi-stage pipeline and the limited temporal modeling capability of a single-stream architecture constrain its potential. In this paper, we introduce SiamMo, a novel and simple Siamese motion-centric tracking approach. Unlike the traditional single-stream architecture, we employ Siamese feature extraction for motion-centric tracking. This decouples feature extraction from temporal fusion, significantly enhancing tracking performance. Additionally, we design a Spatio-Temporal Feature Aggregation module to integrate Siamese features at multiple scales, capturing motion information effectively. We also introduce a Box-aware Feature Encoding module to encode object size priors into motion estimation. SiamMo is a purely motion-centric tracker that eliminates the need for additional processes like segmentation and box refinement. Without whistles and bells, SiamMo not only surpasses state-of-the-art methods across multiple benchmarks but also demonstrates exceptional robustness in challenging scenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\% precision while maintaining a high inference speed of 108 FPS. The code will be released at https://github.com/HDU-VRLab/SiamMo.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2408.00969.pdf' target='_blank'>https://arxiv.org/pdf/2408.00969.pdf</a></span>   <span><a href='https://github.com/wqw123wqw/PFTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yabin Zhu, Qianwu Wang, Chenglong Li, Jin Tang, Zhixiang Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00969">Visible-Thermal Multiple Object Tracking: Large-scale Video Dataset and Progressive Fusion Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complementary benefits from visible and thermal infrared data are widely utilized in various computer vision task, such as visual tracking, semantic segmentation and object detection, but rarely explored in Multiple Object Tracking (MOT). In this work, we contribute a large-scale Visible-Thermal video benchmark for MOT, called VT-MOT. VT-MOT has the following main advantages. 1) The data is large scale and high diversity. VT-MOT includes 582 video sequence pairs, 401k frame pairs from surveillance, drone, and handheld platforms. 2) The cross-modal alignment is highly accurate. We invite several professionals to perform both spatial and temporal alignment frame by frame. 3) The annotation is dense and high-quality. VT-MOT has 3.99 million annotation boxes annotated and double-checked by professionals, including heavy occlusion and object re-acquisition (object disappear and reappear) challenges. To provide a strong baseline, we design a simple yet effective tracking framework, which effectively fuses temporal information and complementary information of two modalities in a progressive manner, for robust visible-thermal MOT. A comprehensive experiment are conducted on VT-MOT and the results prove the superiority and effectiveness of the proposed method compared with state-of-the-art methods. From the evaluation results and analysis, we specify several potential future directions for visible-thermal MOT. The project is released in https://github.com/wqw123wqw/PFTrack.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2408.00874.pdf' target='_blank'>https://arxiv.org/pdf/2408.00874.pdf</a></span>   <span><a href='https://supermedintel.github.io/Medical-SAM2/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayuan Zhu, Abdullah Hamdi, Yunli Qi, Yueming Jin, Junde Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00874">Medical SAM 2: Segment medical images as video via Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical image segmentation plays a pivotal role in clinical diagnostics and treatment planning, yet existing models often face challenges in generalization and in handling both 2D and 3D data uniformly. In this paper, we introduce Medical SAM 2 (MedSAM-2), a generalized auto-tracking model for universal 2D and 3D medical image segmentation. The core concept is to leverage the Segment Anything Model 2 (SAM2) pipeline to treat all 2D and 3D medical segmentation tasks as a video object tracking problem. To put it into practice, we propose a novel \emph{self-sorting memory bank} mechanism that dynamically selects informative embeddings based on confidence and dissimilarity, regardless of temporal order. This mechanism not only significantly improves performance in 3D medical image segmentation but also unlocks a \emph{One-Prompt Segmentation} capability for 2D images, allowing segmentation across multiple images from a single prompt without temporal relationships. We evaluated MedSAM-2 on five 2D tasks and nine 3D tasks, including white blood cells, optic cups, retinal vessels, mandibles, coronary arteries, kidney tumors, liver tumors, breast cancer, nasopharynx cancer, vestibular schwannoma, mediastinal lymph nodules, cerebral artery, inferior alveolar nerve, and abdominal organs, comparing it against state-of-the-art (SOTA) models in task-tailored, general and interactive segmentation settings. Our findings demonstrate that MedSAM-2 surpasses a wide range of existing models and updates new SOTA on several benchmarks. The code is released on the project page: https://supermedintel.github.io/Medical-SAM2/.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2407.15199.pdf' target='_blank'>https://arxiv.org/pdf/2407.15199.pdf</a></span>   <span><a href='https://github.com/SpaceTimeLab/360_object_tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingwei Guo, Yitai Cheng, Meihui Wang, Ilya Ilyankou, Natchapon Jongwiriyanurak, Xiaowei Gao, Nicola Christie, James Haworth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.15199">Multiple Object Detection and Tracking in Panoramic Videos for Cycling Safety Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cyclists face a disproportionate risk of injury, yet conventional crash records are too limited to reconstruct the circumstances of incidents or to diagnose risk at the finer spatial and temporal detail needed for targeted interventions. Recently, naturalistic studies have gained traction as a way to capture the complex behavioural and infrastructural factors that contribute to crashes. These approaches typically involve the collection and analysis of video data. A video promising format is panoramic video, which can record 360-degree views around a rider. However, its use is limited by severe distortions, large numbers of small objects and boundary continuity. This study addresses these challenges by proposing a novel three-step framework: (1) enhancing object detection accuracy on panoramic imagery by segmenting and projecting the original 360-degree images into four perspective sub-images, thus reducing distortion; (2) modifying multi-object tracking models to incorporate boundary continuity and object category information for improved tracking consistency; and (3) validating the proposed approach through a real-world application focused on detecting overtaking manoeuvres by vehicles around cyclists. The methodology is evaluated using panoramic videos recorded by cyclists on London's roadways under diverse conditions. Experimental results demonstrate notable improvements over baseline methods, achieving higher average precision across varying image resolutions. Moreover, the enhanced tracking approach yields a 3.0% increase in multi-object tracking accuracy and a 4.6% improvement in identification F-score. The overtaking detection task achieves a high F-score of 0.81, illustrating the practical effectiveness of the proposed method in real-world cycling safety scenarios. The code is available on GitHub (https://github.com/SpaceTimeLab/360_object_tracking) to ensure reproducibility.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2407.14086.pdf' target='_blank'>https://arxiv.org/pdf/2407.14086.pdf</a></span>   <span><a href='https://github.com/yfzhang1214/TCBTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfei Zhang, Chao Liang, Jin Gao, Zhipeng Zhang, Weiming Hu, Stephen Maybank, Xue Zhou, Liang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14086">Temporal Correlation Meets Embedding: Towards a 2nd Generation of JDE-based Real-Time Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Joint Detection and Embedding (JDE) trackers have demonstrated excellent performance in Multi-Object Tracking (MOT) tasks by incorporating the extraction of appearance features as auxiliary tasks through embedding Re-Identification task (ReID) into the detector, achieving a balance between inference speed and tracking performance. However, solving the competition between the detector and the feature extractor has always been a challenge. Meanwhile, the issue of directly embedding the ReID task into MOT has remained unresolved. The lack of high discriminability in appearance features results in their limited utility. In this paper, a new learning approach using cross-correlation to capture temporal information of objects is proposed. The feature extraction network is no longer trained solely on appearance features from each frame but learns richer motion features by utilizing feature heatmaps from consecutive frames, which addresses the challenge of inter-class feature similarity. Furthermore, our learning approach is applied to a more lightweight feature extraction network, and treat the feature matching scores as strong cues rather than auxiliary cues, with an appropriate weight calculation to reflect the compatibility between our obtained features and the MOT task. Our tracker, named TCBTrack, achieves state-of-the-art performance on multiple public benchmarks, i.e., MOT17, MOT20, and DanceTrack datasets. Specifically, on the DanceTrack test set, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA, making it the best online tracker capable of achieving real-time performance. Comparative evaluations with other trackers prove that our tracker achieves the best balance between speed, robustness and accuracy. Code is available at https://github.com/yfzhang1214/TCBTrack.
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2407.10151.pdf' target='_blank'>https://arxiv.org/pdf/2407.10151.pdf</a></span>   <span><a href='https://github.com/lorenzovaquero/BUSCA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lorenzovaquero/BUSCA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Vaquero, Yihong Xu, Xavier Alameda-Pineda, Victor M. Brea, Manuel Mucientes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10151">Lost and Found: Overcoming Detector Failures in Online Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) endeavors to precisely estimate the positions and identities of multiple objects over time. The prevailing approach, tracking-by-detection (TbD), first detects objects and then links detections, resulting in a simple yet effective method. However, contemporary detectors may occasionally miss some objects in certain frames, causing trackers to cease tracking prematurely. To tackle this issue, we propose BUSCA, meaning `to search', a versatile framework compatible with any online TbD system, enhancing its ability to persistently track those objects missed by the detector, primarily due to occlusions. Remarkably, this is accomplished without modifying past tracking results or accessing future frames, i.e., in a fully online manner. BUSCA generates proposals based on neighboring tracks, motion, and learned tokens. Utilizing a decision Transformer that integrates multimodal visual and spatiotemporal information, it addresses the object-proposal association as a multi-choice question-answering task. BUSCA is trained independently of the underlying tracker, solely on synthetic data, without requiring fine-tuning. Through BUSCA, we showcase consistent performance enhancements across five different trackers and establish a new state-of-the-art baseline across three different benchmarks. Code available at: https://github.com/lorenzovaquero/BUSCA.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2407.08872.pdf' target='_blank'>https://arxiv.org/pdf/2407.08872.pdf</a></span>   <span><a href='https://github.com/linh-gist/mv-glmb-ab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Van Ma, Tran Thien Dat Nguyen, Changbeom Shim, Du Yong Kim, Namkoo Ha, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08872">Visual Multi-Object Tracking with Re-Identification and Occlusion Handling using Labeled Random Finite Sets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes an online visual multi-object tracking (MOT) algorithm that resolves object appearance-reappearance and occlusion. Our solution is based on the labeled random finite set (LRFS) filtering approach, which in principle, addresses disappearance, appearance, reappearance, and occlusion via a single Bayesian recursion. However, in practice, existing numerical approximations cause reappearing objects to be initialized as new tracks, especially after long periods of being undetected. In occlusion handling, the filter's efficacy is dictated by trade-offs between the sophistication of the occlusion model and computational demand. Our contribution is a novel modeling method that exploits object features to address reappearing objects whilst maintaining a linear complexity in the number of detections. Moreover, to improve the filter's occlusion handling, we propose a fuzzy detection model that takes into consideration the overlapping areas between tracks and their sizes. We also develop a fast version of the filter to further reduce the computational time. The source code is publicly available at https://github.com/linh-gist/mv-glmb-ab.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2407.05383.pdf' target='_blank'>https://arxiv.org/pdf/2407.05383.pdf</a></span>   <span><a href='https://github.com/wuyou3474/BDTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>You Wu, Xucheng Wang, Dan Zeng, Hengzhou Ye, Xiaolan Xie, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05383">Learning Motion Blur Robust Vision Transformers for Real-Time UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unmanned aerial vehicle (UAV) tracking is critical for applications like surveillance, search-and-rescue, and autonomous navigation. However, the high-speed movement of UAVs and targets introduces unique challenges, including real-time processing demands and severe motion blur, which degrade the performance of existing generic trackers. While single-stream vision transformer (ViT) architectures have shown promise in visual tracking, their computational inefficiency and lack of UAV-specific optimizations limit their practicality in this domain. In this paper, we boost the efficiency of this framework by tailoring it into an adaptive computation framework that dynamically exits Transformer blocks for real-time UAV tracking. The motivation behind this is that tracking tasks with fewer challenges can be adequately addressed using low-level feature representations. Simpler tasks can often be handled with less demanding, lower-level features. This approach allows the model use computational resources more efficiently by focusing on complex tasks and conserving resources for easier ones. Another significant enhancement introduced in this paper is the improved effectiveness of ViTs in handling motion blur, a common issue in UAV tracking caused by the fast movements of either the UAV, the tracked objects, or both. This is achieved by acquiring motion blur robust representations through enforcing invariance in the feature representation of the target with respect to simulated motion blur. We refer to our proposed approach as BDTrack. Extensive experiments conducted on four tracking benchmarks validate the effectiveness and versatility of our approach, demonstrating its potential as a practical and effective approach for real-time UAV tracking. Code is released at: https://github.com/wuyou3474/BDTrack.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2407.05238.pdf' target='_blank'>https://arxiv.org/pdf/2407.05238.pdf</a></span>   <span><a href='https://github.com/haooozi/P2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Nie, Fei Xie, Sifan Zhou, Xueyi Zhou, Dong-Kyu Chae, Zhiwei He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05238">P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for LiDAR Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) methods based on appearance matching has long suffered from insufficient appearance information incurred by incomplete, textureless and semantically deficient LiDAR point clouds. While motion paradigm exploits motion cues instead of appearance matching for tracking, it incurs complex multi-stage processing and segmentation module. In this paper, we first provide in-depth explorations on motion paradigm, which proves that (\textbf{i}) it is feasible to directly infer target relative motion from point clouds across consecutive frames; (\textbf{ii}) fine-grained information comparison between consecutive point clouds facilitates target motion modeling. We thereby propose to perform part-to-part motion modeling for consecutive point clouds and introduce a novel tracking framework, termed \textbf{P2P}. The novel framework fuses each corresponding part information between consecutive point clouds, effectively exploring detailed information changes and thus modeling accurate target-related motion cues. Following this framework, we present P2P-point and P2P-voxel models, incorporating implicit and explicit part-to-part motion modeling by point- and voxel-based representation, respectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art performance ($\sim$\textbf{89\%}, \textbf{72\%} and \textbf{63\%} precision on KITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same point-based representation, P2P-point outperforms the previous motion tracker M$^2$Track by \textbf{3.3\%} and \textbf{6.7\%} on the KITTI and NuScenes, while running at a considerably high speed of \textbf{107 Fps} on a single RTX3090 GPU. The source code and pre-trained models are available at https://github.com/haooozi/P2P.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2407.05235.pdf' target='_blank'>https://arxiv.org/pdf/2407.05235.pdf</a></span>   <span><a href='https://github.com/OpenCodeGithub/HIP-HaTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Guo, Pengzhi Zhong, Lizhi Lin, Hao Zhang, Ling Huang, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05235">Tracking Reflected Objects: A Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual tracking has advanced significantly in recent years, mainly due to the availability of large-scale training datasets. These datasets have enabled the development of numerous algorithms that can track objects with high accuracy and robustness.However, the majority of current research has been directed towards tracking generic objects, with less emphasis on more specialized and challenging scenarios. One such challenging scenario involves tracking reflected objects. Reflections can significantly distort the appearance of objects, creating ambiguous visual cues that complicate the tracking process. This issue is particularly pertinent in applications such as autonomous driving, security, smart homes, and industrial production, where accurately tracking objects reflected in surfaces like mirrors or glass is crucial. To address this gap, we introduce TRO, a benchmark specifically for Tracking Reflected Objects. TRO includes 200 sequences with around 70,000 frames, each carefully annotated with bounding boxes. This dataset aims to encourage the development of new, accurate methods for tracking reflected objects, which present unique challenges not sufficiently covered by existing benchmarks. We evaluated 20 state-of-the-art trackers and found that they struggle with the complexities of reflections. To provide a stronger baseline, we propose a new tracker, HiP-HaTrack, which uses hierarchical features to improve performance, significantly outperforming existing algorithms. We believe our benchmark, evaluation, and HiP-HaTrack will inspire further research and applications in tracking reflected objects. The TRO and code are available at https://github.com/OpenCodeGithub/HIP-HaTrack.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2406.16837.pdf' target='_blank'>https://arxiv.org/pdf/2406.16837.pdf</a></span>   <span><a href='https://github.com/MIT-SPARK/certifiable_tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lorenzo Shaikewitz, Samuel Ubellacker, Luca Carlone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16837">A Certifiable Algorithm for Simultaneous Shape Estimation and Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Applications from manipulation to autonomous vehicles rely on robust and general object tracking to safely perform tasks in dynamic environments. We propose the first certifiably optimal category-level approach for simultaneous shape estimation and pose tracking of an object of known category (e.g. a car). Our approach uses 3D semantic keypoint measurements extracted from an RGB-D image sequence, and phrases the estimation as a fixed-lag smoothing problem. Temporal constraints enforce the object's rigidity (fixed shape) and smooth motion according to a constant-twist motion model. The solutions to this problem are the estimates of the object's state (poses, velocities) and shape (paramaterized according to the active shape model) over the smoothing horizon. Our key contribution is to show that despite the non-convexity of the fixed-lag smoothing problem, we can solve it to certifiable optimality using a small-size semidefinite relaxation. We also present a fast outlier rejection scheme that filters out incorrect keypoint detections with shape and time compatibility tests, and wrap our certifiable solver in a graduated non-convexity scheme. We evaluate the proposed approach on synthetic and real data, showcasing its performance in a table-top manipulation scenario and a drone-based vehicle tracking application.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2406.13271.pdf' target='_blank'>https://arxiv.org/pdf/2406.13271.pdf</a></span>   <span><a href='https://github.com/dyhBUPT/HIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Du, Zhicheng Zhao, Fei Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13271">Hierarchical IoU Tracking based on Interval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) aims to detect and associate all targets of given classes across frames. Current dominant solutions, e.g. ByteTrack and StrongSORT++, follow the hybrid pipeline, which first accomplish most of the associations in an online manner, and then refine the results using offline tricks such as interpolation and global link. While this paradigm offers flexibility in application, the disjoint design between the two stages results in suboptimal performance. In this paper, we propose the Hierarchical IoU Tracking framework, dubbed HIT, which achieves unified hierarchical tracking by utilizing tracklet intervals as priors. To ensure the conciseness, only IoU is utilized for association, while discarding the heavy appearance models, tricky auxiliary cues, and learning-based association modules. We further identify three inconsistency issues regarding target size, camera movement and hierarchical cues, and design corresponding solutions to guarantee the reliability of associations. Though its simplicity, our method achieves promising performance on four datasets, i.e., MOT17, KITTI, DanceTrack and VisDrone, providing a strong baseline for future tracking method design. Moreover, we experiment on seven trackers and prove that HIT can be seamlessly integrated with other solutions, whether they are motion-based, appearance-based or learning-based. Our codes will be released at https://github.com/dyhBUPT/HIT.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2406.09598.pdf' target='_blank'>https://arxiv.org/pdf/2406.09598.pdf</a></span>   <span><a href='https://facebookresearch.github.io/hot3d/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Fan Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09598">Introducing HOT3D: An Egocentric Dataset for 3D Hand and Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (more than 3.7M images) of multi-view RGB/monochrome image streams showing 19 subjects interacting with 33 diverse rigid objects, multi-modal signals such as eye gaze or scene point clouds, as well as comprehensive ground truth annotations including 3D poses of objects, hands, and cameras, and 3D models of hands and objects. In addition to simple pick-up/observe/put-down actions, HOT3D contains scenarios resembling typical actions in a kitchen, office, and living room environment. The dataset is recorded by two head-mounted devices from Meta: Project Aria, a research prototype of light-weight AR/AI glasses, and Quest 3, a production VR headset sold in millions of units. Ground-truth poses were obtained by a professional motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. We aim to accelerate research on egocentric hand-object interaction by making the HOT3D dataset publicly available and by co-organizing public challenges on the dataset at ECCV 2024. The dataset can be downloaded from the project website: https://facebookresearch.github.io/hot3d/.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2406.08324.pdf' target='_blank'>https://arxiv.org/pdf/2406.08324.pdf</a></span>   <span><a href='https://github.com/Nathan-Li123/LaMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Xiaoqiong Liu, Luke Liu, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08324">LaMOT: Language-Guided Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language MOT is a crucial tracking problem and has drawn increasing attention recently. It aims to track objects based on human language commands, replacing the traditional use of templates or pre-set information from training sets in conventional tracking tasks. Despite various efforts, a key challenge lies in the lack of a clear understanding of why language is used for tracking, which hinders further development in this field. In this paper, we address this challenge by introducing Language-Guided MOT, a unified task framework, along with a corresponding large-scale benchmark, termed LaMOT, which encompasses diverse scenarios and language descriptions. Specially, LaMOT comprises 1,660 sequences from 4 different datasets and aims to unify various Vision-Language MOT tasks while providing a standardized evaluation platform. To ensure high-quality annotations, we manually assign appropriate descriptive texts to each target in every video and conduct careful inspection and correction. To the best of our knowledge, LaMOT is the first benchmark dedicated to Language-Guided MOT. Additionally, we propose a simple yet effective tracker, termed LaMOTer. By establishing a unified task framework, providing challenging benchmarks, and offering insights for future algorithm design and evaluation, we expect to contribute to the advancement of research in Vision-Language MOT. We will release the data at https://github.com/Nathan-Li123/LaMOT.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2406.08037.pdf' target='_blank'>https://arxiv.org/pdf/2406.08037.pdf</a></span>   <span><a href='https://github.com/xyyang317/ABTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyang Yang, Dan Zeng, Xucheng Wang, You Wu, Hengzhou Ye, Qijun Zhao, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08037">Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we introduce a novel ViT pruning method to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: https://github.com/xyyang317/ABTrack.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2406.05039.pdf' target='_blank'>https://arxiv.org/pdf/2406.05039.pdf</a></span>   <span><a href='https://github.com/zyn213/TempRMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yani Zhang, Dongming Wu, Wencheng Han, Xingping Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05039">Bootstrapping Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) aims at detecting and tracking multiple objects following human instruction represented by a natural language expression. Existing RMOT benchmarks are usually formulated through manual annotations, integrated with static regulations. This approach results in a dearth of notable diversity and a constrained scope of implementation. In this work, our key idea is to bootstrap the task of referring multi-object tracking by introducing discriminative language words as much as possible. In specific, we first develop Refer-KITTI into a large-scale dataset, named Refer-KITTI-V2. It starts with 2,719 manual annotations, addressing the issue of class imbalance and introducing more keywords to make it closer to real-world scenarios compared to Refer-KITTI. They are further expanded to a total of 9,758 annotations by prompting large language models, which create 617 different words, surpassing previous RMOT benchmarks. In addition, the end-to-end framework in RMOT is also bootstrapped by a simple yet elegant temporal advancement strategy, which achieves better performance than previous approaches. The source code and dataset is available at https://github.com/zyn213/TempRMOT.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2406.04844.pdf' target='_blank'>https://arxiv.org/pdf/2406.04844.pdf</a></span>   <span><a href='https://github.com/WesLee88524/LG-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Li, Jiale Cao, Muzammal Naseer, Yu Zhu, Jinqiu Sun, Yanning Zhang, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04844">Multi-Granularity Language-Guided Training for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing multi-object tracking methods typically learn visual tracking features via maximizing dis-similarities of different instances and minimizing similarities of the same instance. While such a feature learning scheme achieves promising performance, learning discriminative features solely based on visual information is challenging especially in case of environmental interference such as occlusion, blur and domain variance. In this work, we argue that multi-modal language-driven features provide complementary information to classical visual features, thereby aiding in improving the robustness to such environmental interference. To this end, we propose a new multi-object tracking framework, named LG-MOT, that explicitly leverages language information at different levels of granularity (scene-and instance-level) and combines it with standard visual features to obtain discriminative representations. To develop LG-MOT, we annotate existing MOT datasets with scene-and instance-level language descriptions. We then encode both instance-and scene-level language information into high-dimensional embeddings, which are utilized to guide the visual features during training. At inference, our LG-MOT uses the standard visual features without relying on annotated language descriptions. Extensive experiments on three benchmarks, MOT17, DanceTrack and SportsMOT, reveal the merits of the proposed contributions leading to state-of-the-art performance. On the DanceTrack test set, our LG-MOT achieves an absolute gain of 2.2\% in terms of target object association (IDF1 score), compared to the baseline using only visual features. Further, our LG-MOT exhibits strong cross-domain generalizability. The dataset and code will be available at https://github.com/WesLee88524/LG-MOT.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2406.04221.pdf' target='_blank'>https://arxiv.org/pdf/2406.04221.pdf</a></span>   <span><a href='https://github.com/siyuanliii/masa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Li, Lei Ke, Martin Danelljan, Luigi Piccinelli, Mattia Segu, Luc Van Gool, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04221">Matching Anything by Segmenting Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The robust association of the same objects across video frames in complex scenes is crucial for many applications, especially Multiple Object Tracking (MOT). Current methods predominantly rely on labeled domain-specific video datasets, which limits the cross-domain generalization of learned similarity embeddings. We propose MASA, a novel method for robust instance association learning, capable of matching any objects within videos across diverse domains without tracking labels. Leveraging the rich object segmentation from the Segment Anything Model (SAM), MASA learns instance-level correspondence through exhaustive data transformations. We treat the SAM outputs as dense object region proposals and learn to match those regions from a vast image collection. We further design a universal MASA adapter which can work in tandem with foundational segmentation or detection models and enable them to track any detected objects. Those combinations present strong zero-shot tracking ability in complex domains. Extensive tests on multiple challenging MOT and MOTS benchmarks indicate that the proposed method, using only unlabeled static images, achieves even better performance than state-of-the-art methods trained with fully annotated in-domain video sequences, in zero-shot association. Project Page: https://matchinganything.github.io/
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2406.01765.pdf' target='_blank'>https://arxiv.org/pdf/2406.01765.pdf</a></span>   <span><a href='https://github.com/fatemehN/ReproducibilityStudy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Nourilenjan Nokabadi, Jean-FranÃ§ois Lalonde, Christian GagnÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01765">Reproducibility Study on Adversarial Attacks Against Robust Transformer Trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>New transformer networks have been integrated into object tracking pipelines and have demonstrated strong performance on the latest benchmarks. This paper focuses on understanding how transformer trackers behave under adversarial attacks and how different attacks perform on tracking datasets as their parameters change. We conducted a series of experiments to evaluate the effectiveness of existing adversarial attacks on object trackers with transformer and non-transformer backbones. We experimented on 7 different trackers, including 3 that are transformer-based, and 4 which leverage other architectures. These trackers are tested against 4 recent attack methods to assess their performance and robustness on VOT2022ST, UAV123 and GOT10k datasets. Our empirical study focuses on evaluating adversarial robustness of object trackers based on bounding box versus binary mask predictions, and attack methods at different levels of perturbations. Interestingly, our study found that altering the perturbation level may not significantly affect the overall object tracking results after the attack. Similarly, the sparsity and imperceptibility of the attack perturbations may remain stable against perturbation level shifts. By applying a specific attack on all transformer trackers, we show that new transformer trackers having a stronger cross-attention modeling achieve a greater adversarial robustness on tracking datasets, such as VOT2022ST and GOT10k. Our results also indicate the necessity for new attack methods to effectively tackle the latest types of transformer trackers. The codes necessary to reproduce this study are available at https://github.com/fatemehN/ReproducibilityStudy.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2406.00429.pdf' target='_blank'>https://arxiv.org/pdf/2406.00429.pdf</a></span>   <span><a href='https://github.com/qinzheng2000/GeneralTrack.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Le Wang, Sanping Zhou, Panpan Fu, Gang Hua, Wei Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00429">Towards Generalizable Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking MOT encompasses various tracking scenarios, each characterized by unique traits. Effective trackers should demonstrate a high degree of generalizability across diverse scenarios. However, existing trackers struggle to accommodate all aspects or necessitate hypothesis and experimentation to customize the association information motion and or appearance for a given scenario, leading to narrowly tailored solutions with limited generalizability. In this paper, we investigate the factors that influence trackers generalization to different scenarios and concretize them into a set of tracking scenario attributes to guide the design of more generalizable trackers. Furthermore, we propose a point-wise to instance-wise relation framework for MOT, i.e., GeneralTrack, which can generalize across diverse scenarios while eliminating the need to balance motion and appearance. Thanks to its superior generalizability, our proposed GeneralTrack achieves state-of-the-art performance on multiple benchmarks and demonstrates the potential for domain generalization. https://github.com/qinzheng2000/GeneralTrack.git
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2405.19818.pdf' target='_blank'>https://arxiv.org/pdf/2405.19818.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Guanjie Huang, Hao Wen, Xi Zhou, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19818">WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Underwater object tracking (UOT) is a foundational task for identifying and tracing submerged entities in underwater video sequences. However, current UOT datasets suffer from limitations in scale, diversity of target categories and scenarios covered, hindering the training and evaluation of modern tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, \ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \eg, UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \eg, underwater vision-language tracking. Most existing trackers are tailored for open-air environments, leading to performance degradation when applied to UOT due to domain gaps. Retraining and fine-tuning these trackers are challenging due to sample imbalances and limited real-world underwater datasets. To tackle these challenges, we propose a novel omni-knowledge distillation framework based on WebUOT-1M, incorporating various strategies to guide the learning of the student Transformer. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. Furthermore, we comprehensively evaluate WebUOT-1M using 30 deep trackers, showcasing its value as a benchmark for UOT research by presenting new challenges and opportunities for future studies. The complete dataset, codes and tracking results, will be made publicly available.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2405.19321.pdf' target='_blank'>https://arxiv.org/pdf/2405.19321.pdf</a></span>   <span><a href='https://isaaclabe.github.io/DGD-Website/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Isaac Labe, Noam Issachar, Itai Lang, Sagie Benaim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19321">DGD: Dynamic 3D Gaussians Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We tackle the task of learning dynamic 3D semantic radiance fields given a single monocular video as input. Our learned semantic radiance field captures per-point semantics as well as color and geometric properties for a dynamic 3D scene, enabling the generation of novel views and their corresponding semantics. This enables the segmentation and tracking of a diverse set of 3D semantic entities, specified using a simple and intuitive interface that includes a user click or a text prompt. To this end, we present DGD, a unified 3D representation for both the appearance and semantics of a dynamic 3D scene, building upon the recently proposed dynamic 3D Gaussians representation. Our representation is optimized over time with both color and semantic information. Key to our method is the joint optimization of the appearance and semantic attributes, which jointly affect the geometric properties of the scene. We evaluate our approach in its ability to enable dense semantic 3D object tracking and demonstrate high-quality results that are fast to render, for a diverse set of scenes. Our project webpage is available on https://isaaclabe.github.io/DGD-Website/
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2405.18606.pdf' target='_blank'>https://arxiv.org/pdf/2405.18606.pdf</a></span>   <span><a href='https://github.com/linh-gist/mv-glmb-ab' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Van Ma, Tran Thien Dat Nguyen, Ba-Ngu Vo, Hyunsung Jang, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18606">Track Initialization and Re-Identification for~3D Multi-View Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a 3D multi-object tracking (MOT) solution using only 2D detections from monocular cameras, which automatically initiates/terminates tracks as well as resolves track appearance-reappearance and occlusions. Moreover, this approach does not require detector retraining when cameras are reconfigured but only the camera matrices of reconfigured cameras need to be updated. Our approach is based on a Bayesian multi-object formulation that integrates track initiation/termination, re-identification, occlusion handling, and data association into a single Bayes filtering recursion. However, the exact filter that utilizes all these functionalities is numerically intractable due to the exponentially growing number of terms in the (multi-object) filtering density, while existing approximations trade-off some of these functionalities for speed. To this end, we develop a more efficient approximation suitable for online MOT by incorporating object features and kinematics into the measurement model, which improves data association and subsequently reduces the number of terms. Specifically, we exploit the 2D detections and extracted features from multiple cameras to provide a better approximation of the multi-object filtering density to realize the track initiation/termination and re-identification functionalities. Further, incorporating a tractable geometric occlusion model based on 2D projections of 3D objects on the camera planes realizes the occlusion handling functionality of the filter. Evaluation of the proposed solution on challenging datasets demonstrates significant improvements and robustness when camera configurations change on-the-fly, compared to existing multi-view MOT solutions. The source code is publicly available at https://github.com/linh-gist/mv-glmb-ab.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2405.17773.pdf' target='_blank'>https://arxiv.org/pdf/2405.17773.pdf</a></span>   <span><a href='https://github.com/supertyd/XTrack/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuedong Tan, Zongwei Wu, Yuqian Fu, Zhuyun Zhou, Guolei Sun, Eduard Zamfi, Chao Ma, Danda Pani Paudel, Luc Van Gool, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17773">XTrack: Multimodal Training Boosts RGB-X Video Object Trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal sensing has proven valuable for visual tracking, as different sensor types offer unique strengths in handling one specific challenging scene where object appearance varies. While a generalist model capable of leveraging all modalities would be ideal, development is hindered by data sparsity, typically in practice, only one modality is available at a time. Therefore, it is crucial to ensure and achieve that knowledge gained from multimodal sensing -- such as identifying relevant features and regions -- is effectively shared, even when certain modalities are unavailable at inference. We venture with a simple assumption: similar samples across different modalities have more knowledge to share than otherwise. To implement this, we employ a ``weak" classifier tasked with distinguishing between modalities. More specifically, if the classifier ``fails" to accurately identify the modality of the given sample, this signals an opportunity for cross-modal knowledge sharing. Intuitively, knowledge transfer is facilitated whenever a sample from one modality is sufficiently close and aligned with another. Technically, we achieve this by routing samples from one modality to the expert of the others, within a mixture-of-experts framework designed for multimodal video object tracking. During the inference, the expert of the respective modality is chosen, which we show to benefit from the multimodal knowledge available during training, thanks to the proposed method. Through the exhaustive experiments that use only paired RGB-E, RGB-D, and RGB-T during training, we showcase the benefit of the proposed method for RGB-X tracker during inference, with an average +3\% precision improvement over the current SOTA. Our source code is publicly available at https://github.com/supertyd/XTrack/tree/main.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2405.15700.pdf' target='_blank'>https://arxiv.org/pdf/2405.15700.pdf</a></span>   <span><a href='https://github.com/weigertlab/trackastra' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Gallusser, Martin Weigert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15700">Trackastra: Transformer-based cell tracking for live-cell microscopy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell tracking is a ubiquitous image analysis task in live-cell microscopy. Unlike multiple object tracking (MOT) for natural images, cell tracking typically involves hundreds of similar-looking objects that can divide in each frame, making it a particularly challenging problem. Current state-of-the-art approaches follow the tracking-by-detection paradigm, i.e. first all cells are detected per frame and successively linked in a second step to form biologically consistent cell tracks. Linking is commonly solved via discrete optimization methods, which require manual tuning of hyperparameters for each dataset and are therefore cumbersome to use in practice. Here we propose Trackastra, a general purpose cell tracking approach that uses a simple transformer architecture to directly learn pairwise associations of cells within a temporal window from annotated data. Importantly, unlike existing transformer-based MOT pipelines, our learning architecture also accounts for dividing objects such as cells and allows for accurate tracking even with simple greedy linking, thus making strides towards removing the requirement for a complex linking step. The proposed architecture operates on the full spatio-temporal context of detections within a time window by avoiding the computational burden of processing dense images. We show that our tracking approach performs on par with or better than highly tuned state-of-the-art cell tracking algorithms for various biological datasets, such as bacteria, cell cultures and fluorescent particles. We provide code at https://github.com/weigertlab/trackastra.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2405.14200.pdf' target='_blank'>https://arxiv.org/pdf/2405.14200.pdf</a></span>   <span><a href='https://github.com/983632847/Awesome-Multimodal-Object-Tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhui Zhang, Li Liu, Hao Wen, Xi Zhou, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14200">Awesome Multi-modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal object tracking (MMOT) is an emerging field that combines data from various modalities, \eg vision (RGB), depth, thermal infrared, event, language and audio, to estimate the state of an arbitrary object in a video sequence. It is of great significance for many applications such as autonomous driving and intelligent surveillance. In recent years, MMOT has received more and more attention. However, existing MMOT algorithms mainly focus on two modalities (\eg RGB+depth, RGB+thermal infrared, and RGB+language). To leverage more modalities, some recent efforts have been made to learn a unified visual object tracking model for any modality. Additionally, some large-scale multi-modal tracking benchmarks have been established by simultaneously providing more than two modalities, such as vision-language-audio (\eg WebUAV-3M) and vision-depth-language (\eg UniMod1K). To track the latest progress in MMOT, we conduct a comprehensive investigation in this report. Specifically, we first divide existing MMOT tasks into five main categories, \ie RGBL tracking, RGBE tracking, RGBD tracking, RGBT tracking, and miscellaneous (RGB+X), where X can be any modality, such as language, depth, and event. Then, we analyze and summarize each MMOT task, focusing on widely used datasets and mainstream tracking algorithms based on their technical paradigms (\eg self-supervised learning, prompt learning, knowledge distillation, generative models, and state space models). Finally, we maintain a continuously updated paper list for MMOT at https://github.com/983632847/Awesome-Multimodal-Object-Tracking.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2405.14119.pdf' target='_blank'>https://arxiv.org/pdf/2405.14119.pdf</a></span>   <span><a href='https://github.com/chongweiliu/PuTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chongwei Liu, Haojie Li, Zhihui Wang, Rui Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14119">Is a Pure Transformer Effective for Separated and Online Multi-Object Tracking?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Multi-Object Tracking (MOT) have demonstrated significant success in short-term association within the separated tracking-by-detection online paradigm. However, long-term tracking remains challenging. While graph-based approaches address this by modeling trajectories as global graphs, these methods are unsuitable for real-time applications due to their non-online nature. In this paper, we review the concept of trajectory graphs and propose a novel perspective by representing them as directed acyclic graphs. This representation can be described using frame-ordered object sequences and binary adjacency matrices. We observe that this structure naturally aligns with Transformer attention mechanisms, enabling us to model the association problem using a classic Transformer architecture. Based on this insight, we introduce a concise Pure Transformer (PuTR) to validate the effectiveness of Transformer in unifying short- and long-term tracking for separated online MOT. Extensive experiments on four diverse datasets (SportsMOT, DanceTrack, MOT17, and MOT20) demonstrate that PuTR effectively establishes a solid baseline compared to existing foundational online methods while exhibiting superior domain adaptation capabilities. Furthermore, the separated nature enables efficient training and inference, making it suitable for practical applications. Implementation code and trained models are available at https://github.com/chongweiliu/PuTR .
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2405.05210.pdf' target='_blank'>https://arxiv.org/pdf/2405.05210.pdf</a></span>   <span><a href='https://github.com/mit-acl/tcaff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mason B. Peterson, Parker C. Lusk, Antonio Avila, Jonathan P. How
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05210">TCAFF: Temporal Consistency for Robot Frame Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of collaborative robotics, the ability to communicate spatial information like planned trajectories and shared environment information is crucial. When no global position information is available (e.g., indoor or GPS-denied environments), agents must align their coordinate frames before shared spatial information can be properly expressed and interpreted. Coordinate frame alignment is particularly difficult when robots have no initial alignment and are affected by odometry drift. To this end, we develop a novel multiple hypothesis algorithm, called TCAFF, for aligning the coordinate frames of neighboring robots. TCAFF considers potential alignments from associating sparse open-set object maps and leverages temporal consistency to determine an initial alignment and correct for drift, all without any initial knowledge of neighboring robot poses. We demonstrate TCAFF being used for frame alignment in a collaborative object tracking application on a team of four robots tracking six pedestrians and show that TCAFF enables robots to achieve a tracking accuracy similar to that of a system with ground truth localization. The code and hardware dataset are available at https://github.com/mit-acl/tcaff.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2405.05004.pdf' target='_blank'>https://arxiv.org/pdf/2405.05004.pdf</a></span>   <span><a href='https://github.com/SSSpc333/TENet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Shao, Tianyang Xu, Zhangyong Tang, Linze Li, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05004">TENet: Targetness Entanglement Incorporating with Multi-Scale Pooling and Mutually-Guided Fusion for RGB-E Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There is currently strong interest in improving visual object tracking by augmenting the RGB modality with the output of a visual event camera that is particularly informative about the scene motion. However, existing approaches perform event feature extraction for RGB-E tracking using traditional appearance models, which have been optimised for RGB only tracking, without adapting it for the intrinsic characteristics of the event data. To address this problem, we propose an Event backbone (Pooler), designed to obtain a high-quality feature representation that is cognisant of the innate characteristics of the event data, namely its sparsity. In particular, Multi-Scale Pooling is introduced to capture all the motion feature trends within event data through the utilisation of diverse pooling kernel sizes. The association between the derived RGB and event representations is established by an innovative module performing adaptive Mutually Guided Fusion (MGF). Extensive experimental results show that our method significantly outperforms state-of-the-art trackers on two widely used RGB-E tracking datasets, including VisEvent and COESOT, where the precision and success rates on COESOT are improved by 4.9% and 5.2%, respectively. Our code will be available at https://github.com/SSSpc333/TENet.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2404.13868.pdf' target='_blank'>https://arxiv.org/pdf/2404.13868.pdf</a></span>   <span><a href='https://atomscott.github.io/TeamTrack/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Atom Scott, Ikuma Uchida, Ning Ding, Rikuhei Umemoto, Rory Bunker, Ren Kobayashi, Takeshi Koyama, Masaki Onishi, Yoshinari Kameda, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13868">TeamTrack: A Dataset for Multi-Sport Multi-Object Tracking in Full-pitch Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a critical and challenging task in computer vision, particularly in situations involving objects with similar appearances but diverse movements, as seen in team sports. Current methods, largely reliant on object detection and appearance, often fail to track targets in such complex scenarios accurately. This limitation is further exacerbated by the lack of comprehensive and diverse datasets covering the full view of sports pitches. Addressing these issues, we introduce TeamTrack, a pioneering benchmark dataset specifically designed for MOT in sports. TeamTrack is an extensive collection of full-pitch video data from various sports, including soccer, basketball, and handball. Furthermore, we perform a comprehensive analysis and benchmarking effort to underscore TeamTrack's utility and potential impact. Our work signifies a crucial step forward, promising to elevate the precision and effectiveness of MOT in complex, dynamic settings such as team sports. The dataset, project code and competition is released at: https://atomscott.github.io/TeamTrack/.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2404.07977.pdf' target='_blank'>https://arxiv.org/pdf/2404.07977.pdf</a></span>   <span><a href='https://weijielyu.github.io/Gaga' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07977">Gaga: Group Any Gaussians via 3D-aware Memory Bank</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Gaga, a framework that reconstructs and segments open-world 3D scenes by leveraging inconsistent 2D masks predicted by zero-shot class-agnostic segmentation models. Contrasted to prior 3D scene segmentation approaches that rely on video object tracking or contrastive learning methods, Gaga utilizes spatial information and effectively associates object masks across diverse camera poses through a novel 3D-aware memory bank. By eliminating the assumption of continuous view changes in training images, Gaga demonstrates robustness to variations in camera poses, particularly beneficial for sparsely sampled images, ensuring precise mask label consistency. Furthermore, Gaga accommodates 2D segmentation masks from diverse sources and demonstrates robust performance with different open-world zero-shot class-agnostic segmentation models, significantly enhancing its versatility. Extensive qualitative and quantitative evaluations demonstrate that Gaga performs favorably against state-of-the-art methods, emphasizing its potential for real-world applications such as 3D scene understanding and manipulation.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2404.07553.pdf' target='_blank'>https://arxiv.org/pdf/2404.07553.pdf</a></span>   <span><a href='https://github.com/gitmehrdad/SFSORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>M. M. Morsali, Z. Sharifi, F. Fallah, S. Hashembeiki, H. Mohammadzade, S. Bagheri Shouraki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07553">SFSORT: Scene Features-based Simple Online Real-Time Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces SFSORT, the world's fastest multi-object tracking system based on experiments conducted on MOT Challenge datasets. To achieve an accurate and computationally efficient tracker, this paper employs a tracking-by-detection method, following the online real-time tracking approach established in prior literature. By introducing a novel cost function called the Bounding Box Similarity Index, this work eliminates the Kalman Filter, leading to reduced computational requirements. Additionally, this paper demonstrates the impact of scene features on enhancing object-track association and improving track post-processing. Using a 2.2 GHz Intel Xeon CPU, the proposed method achieves an HOTA of 61.7\% with a processing speed of 2242 Hz on the MOT17 dataset and an HOTA of 60.9\% with a processing speed of 304 Hz on the MOT20 dataset. The tracker's source code, fine-tuned object detection model, and tutorials are available at \url{https://github.com/gitmehrdad/SFSORT}.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2404.05518.pdf' target='_blank'>https://arxiv.org/pdf/2404.05518.pdf</a></span>   <span><a href='https://github.com/JackWoo0831/DepthMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiapeng Wu, Yichen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05518">DepthMOT: Depth Cues Lead to a Strong Multi-Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately distinguishing each object is a fundamental goal of Multi-object tracking (MOT) algorithms. However, achieving this goal still remains challenging, primarily due to: (i) For crowded scenes with occluded objects, the high overlap of object bounding boxes leads to confusion among closely located objects. Nevertheless, humans naturally perceive the depth of elements in a scene when observing 2D videos. Inspired by this, even though the bounding boxes of objects are close on the camera plane, we can differentiate them in the depth dimension, thereby establishing a 3D perception of the objects. (ii) For videos with rapidly irregular camera motion, abrupt changes in object positions can result in ID switches. However, if the camera pose are known, we can compensate for the errors in linear motion models. In this paper, we propose \textit{DepthMOT}, which achieves: (i) detecting and estimating scene depth map \textit{end-to-end}, (ii) compensating the irregular camera motion by camera pose estimation. Extensive experiments demonstrate the superior performance of DepthMOT in VisDrone-MOT and UAVDT datasets. The code will be available at \url{https://github.com/JackWoo0831/DepthMOT}.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2404.03110.pdf' target='_blank'>https://arxiv.org/pdf/2404.03110.pdf</a></span>   <span><a href='https://github.com/noyzzz/EMAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Mahdian, Mohammad Jani, Amir M. Soufi Enayati, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.03110">Ego-Motion Aware Target Prediction Module for Robust Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories. Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target. Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model. These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections. Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories. In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models. Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter. This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model. We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT. In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively. At the same time, it elevates other performance metrics such as HOTA by more than 5%. Our source code is available at https://github.com/noyzzz/EMAP.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2403.17935.pdf' target='_blank'>https://arxiv.org/pdf/2403.17935.pdf</a></span>   <span><a href='https://github.com/wangjk666/OmniVid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17935">OmniVid: A Generative Framework for Universal Video Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution. Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats. In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as GPT-3, with extensive training corpora. Inspired by this, we seek to unify the output space of video understanding tasks by using languages as labels and additionally introducing time and box tokens. In this way, a variety of video tasks could be formulated as video-grounded token generation. This enables us to address various types of video tasks, including classification (such as action recognition), captioning (covering clip captioning, video question answering, and dense video captioning), and localization tasks (such as visual object tracking) within a fully shared encoder-decoder architecture, following a generative framework. Through comprehensive experiments, we demonstrate such a simple and straightforward idea is quite effective and can achieve state-of-the-art or competitive results on seven video benchmarks, providing a novel perspective for more universal video understanding. Code is available at https://github.com/wangjk666/OmniVid.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2403.16848.pdf' target='_blank'>https://arxiv.org/pdf/2403.16848.pdf</a></span>   <span><a href='https://github.com/MCG-NJU/MOTIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruopeng Gao, Ji Qi, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16848">Multiple Object Tracking as ID Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) has been a long-standing challenge in video understanding. A natural and intuitive approach is to split this task into two parts: object detection and association. Most mainstream methods employ meticulously crafted heuristic techniques to maintain trajectory information and compute cost matrices for object matching. Although these methods can achieve notable tracking performance, they often require a series of elaborate handcrafted modifications while facing complicated scenarios. We believe that manually assumed priors limit the method's adaptability and flexibility in learning optimal tracking capabilities from domain-specific data. Therefore, we introduce a new perspective that treats Multiple Object Tracking as an in-context ID Prediction task, transforming the aforementioned object association into an end-to-end trainable task. Based on this, we propose a simple yet effective method termed MOTIP. Given a set of trajectories carried with ID information, MOTIP directly decodes the ID labels for current detections to accomplish the association process. Without using tailored or sophisticated architectures, our method achieves state-of-the-art results across multiple benchmarks by solely leveraging object-level features as tracking cues. The simplicity and impressive results of MOTIP leave substantial room for future advancements, thereby making it a promising baseline for subsequent research. Our code and checkpoints are released at https://github.com/MCG-NJU/MOTIP.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2403.16558.pdf' target='_blank'>https://arxiv.org/pdf/2403.16558.pdf</a></span>   <span><a href='https://github.com/Hon-Wong/Elysium' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, Can Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16558">Elysium: Exploring Object-level Perception in Videos via MLLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal Large Language Models (MLLMs) have demonstrated their ability to perceive objects in still images, but their application in video-related tasks, such as object tracking, remains understudied. This lack of exploration is primarily due to two key challenges. Firstly, extensive pretraining on large-scale video datasets is required to equip MLLMs with the capability to perceive objects across multiple frames and understand inter-frame relationships. Secondly, processing a large number of frames within the context window of Large Language Models (LLMs) can impose a significant computational burden. To address the first challenge, we introduce ElysiumTrack-1M, a large-scale video dataset supported for three tasks: Single Object Tracking (SOT), Referring Single Object Tracking (RSOT), and Video Referring Expression Generation (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video frames with corresponding object boxes and descriptions. Leveraging this dataset, we conduct training of MLLMs and propose a token-compression model T-Selector to tackle the second challenge. Our proposed approach, Elysium: Exploring Object-level Perception in Videos via MLLM, is an end-to-end trainable MLLM that attempts to conduct object-level tasks in videos without requiring any additional plug-in or expert models. All codes and datasets are available at https://github.com/Hon-Wong/Elysium.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2403.16002.pdf' target='_blank'>https://arxiv.org/pdf/2403.16002.pdf</a></span>   <span><a href='https://github.com/hoqolo/SDSTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojun Hou, Jiazheng Xing, Yijie Qian, Yaowei Guo, Shuo Xin, Junhao Chen, Kai Tang, Mengmeng Wang, Zhengkai Jiang, Liang Liu, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16002">SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Visual Object Tracking (VOT) has recently gained significant attention due to its robustness. Early research focused on fully fine-tuning RGB-based trackers, which was inefficient and lacked generalized representation due to the scarcity of multimodal data. Therefore, recent studies have utilized prompt tuning to transfer pre-trained RGB-based trackers to multimodal data. However, the modality gap limits pre-trained knowledge recall, and the dominance of the RGB modality persists, preventing the full utilization of information from other modalities. To address these issues, we propose a novel symmetric multimodal tracking framework called SDSTrack. We introduce lightweight adaptation for efficient fine-tuning, which directly transfers the feature extraction ability from RGB to other domains with a small number of trainable parameters and integrates multimodal features in a balanced, symmetric manner. Furthermore, we design a complementary masked patch distillation strategy to enhance the robustness of trackers in complex environments, such as extreme weather, poor imaging, and sensor failure. Extensive experiments demonstrate that SDSTrack outperforms state-of-the-art methods in various multimodal tracking scenarios, including RGB+Depth, RGB+Thermal, and RGB+Event tracking, and exhibits impressive results in extreme conditions. Our source code is available at https://github.com/hoqolo/SDSTrack.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2403.15712.pdf' target='_blank'>https://arxiv.org/pdf/2403.15712.pdf</a></span>   <span><a href='https://github.com/PholyPeng/PNAS-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chensheng Peng, Zhaoyu Zeng, Jinling Gao, Jundong Zhou, Masayoshi Tomizuka, Xinbing Wang, Chenghu Zhou, Nanyang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15712">PNAS-MOT: Multi-Modal Object Tracking with Pareto Neural Architecture Search</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking is a critical task in autonomous driving. Existing works primarily focus on the heuristic design of neural networks to obtain high accuracy. As tracking accuracy improves, however, neural networks become increasingly complex, posing challenges for their practical application in real driving scenarios due to the high level of latency. In this paper, we explore the use of the neural architecture search (NAS) methods to search for efficient architectures for tracking, aiming for low real-time latency while maintaining relatively high accuracy. Another challenge for object tracking is the unreliability of a single sensor, therefore, we propose a multi-modal framework to improve the robustness. Experiments demonstrate that our algorithm can run on edge devices within lower latency constraints, thus greatly reducing the computational requirements for multi-modal object tracking while keeping lower latency.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2403.15313.pdf' target='_blank'>https://arxiv.org/pdf/2403.15313.pdf</a></span>   <span><a href='https://github.com/ETH-PBL/CR3DT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Baumann, Michael Baumgartner, Edoardo Ghignone, Jonas KÃ¼hne, Tobias Fischer, Yung-Hsu Yang, Marc Pollefeys, Michele Magno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15313">CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To enable self-driving vehicles accurate detection and tracking of surrounding objects is essential. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high-performance systems, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the spatial and velocity information of the RADAR sensor. Experimental results demonstrate an absolute improvement in detection performance of 5.3% in mean Average Precision (mAP) and a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the nuScenes dataset when leveraging both modalities. CR3DT bridges the gap between high-performance and cost-effective perception systems in autonomous driving, by capitalizing on the ubiquitous presence of RADAR in automotive applications. The code is available at: https://github.com/ETH-PBL/CR3DT.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2403.15245.pdf' target='_blank'>https://arxiv.org/pdf/2403.15245.pdf</a></span>   <span><a href='https://github.com/intell-sci-comput/STATM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Li, Pu Ren, Yang Liu, Hao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15245">Reasoning-Enhanced Object-Centric Learning for Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatiotemporal attention computations and fusion. Our experimental results on various datasets indicate that the STATM module can significantly enhance the capabilities of multiple state-of-the-art object-centric learning models for video. Moreover, as a predictive model, the STATM module also performs well in downstream prediction and Visual Question Answering (VQA) tasks. We will release our codes and data at https://github.com/intell-sci-comput/STATM.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2403.13443.pdf' target='_blank'>https://arxiv.org/pdf/2403.13443.pdf</a></span>   <span><a href='https://github.com/lixiaoyu2000/FastPoly' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Li, Dedong Liu, Yitao Wu, Xian Wu, Lijun Zhao, Jinghan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13443">Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT) captures stable and comprehensive motion states of surrounding obstacles, essential for robotic perception. However, current 3D trackers face issues with accuracy and latency consistency. In this paper, we propose Fast-Poly, a fast and effective filter-based method for 3D MOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object rotational anisotropy in 3D space, enhances local computation densification, and leverages parallelization technique, improving inference speed and precision. Fast-Poly is extensively tested on two large-scale tracking benchmarks with Python implementation. On the nuScenes dataset, Fast-Poly achieves new state-of-the-art performance with 75.8% AMOTA among all methods and can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly exhibits competitive accuracy with 63.6% MOTA and impressive inference speed (35.5 FPS). The source code is publicly available at https://github.com/lixiaoyu2000/FastPoly.
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2403.12573.pdf' target='_blank'>https://arxiv.org/pdf/2403.12573.pdf</a></span>   <span><a href='https://github.com/tteepe/TrackTacular' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Torben Teepe, Philipp Wolters, Johannes Gilg, Fabian Herzog, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12573">Lifting Multi-View Detection and Tracking to the Bird's Eye View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Taking advantage of multi-view aggregation presents a promising solution to tackle challenges such as occlusion and missed detection in multi-object tracking and detection. Recent advancements in multi-view detection and 3D object recognition have significantly improved performance by strategically projecting all views onto the ground plane and conducting detection analysis from a Bird's Eye View. In this paper, we compare modern lifting methods, both parameter-free and parameterized, to multi-view aggregation. Additionally, we present an architecture that aggregates the features of multiple times steps to learn robust detection and combines appearance- and motion-based cues for tracking. Most current tracking approaches either focus on pedestrians or vehicles. In our work, we combine both branches and add new challenges to multi-view detection with cross-scene setups. Our method generalizes to three public datasets across two domains: (1) pedestrian: Wildtrack and MultiviewX, and (2) roadside perception: Synthehicle, achieving state-of-the-art performance in detection and tracking. https://github.com/tteepe/TrackTacular
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2403.12504.pdf' target='_blank'>https://arxiv.org/pdf/2403.12504.pdf</a></span>   <span><a href='https://github.com/Franky-X/FVON-TPN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoran Xiong, Guoqing Liu, Qi Wu, Songpengcheng Xia, Tong Hua, Kehui Ma, Zhen Sun, Yan Xiang, Ling Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12504">TON-VIO: Online Time Offset Modeling Networks for Robust Temporal Alignment in High Dynamic Motion VIO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal misalignment (time offset) between sensors is common in low cost visual-inertial odometry (VIO) systems. Such temporal misalignment introduces inconsistent constraints for state estimation, leading to a significant positioning drift especially in high dynamic motion scenarios. In this article, we focus on online temporal calibration to reduce the positioning drift caused by the time offset for high dynamic motion VIO. For the time offset observation model, most existing methods rely on accurate state estimation or stable visual tracking. For the prediction model, current methods oversimplify the time offset as a constant value with white Gaussian noise. However, these ideal conditions are seldom satisfied in real high dynamic scenarios, resulting in the poor performance. In this paper, we introduce online time offset modeling networks (TON) to enhance real-time temporal calibration. TON improves the accuracy of time offset observation and prediction modeling. Specifically, for observation modeling, we propose feature velocity observation networks to enhance velocity computation for features in unstable visual tracking conditions. For prediction modeling, we present time offset prediction networks to learn its evolution pattern. To highlight the effectiveness of our method, we integrate the proposed TON into both optimization-based and filter-based VIO systems. Simulation and real-world experiments are conducted to demonstrate the enhanced performance of our approach. Additionally, to contribute to the VIO community, we will open-source the code of our method on: https://github.com/Franky-X/FVON-TPN.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2403.11186.pdf' target='_blank'>https://arxiv.org/pdf/2403.11186.pdf</a></span>   <span><a href='https://george-zhuang.github.io/nettrack/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangze Zheng, Shijie Lin, Haobo Zuo, Changhong Fu, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11186">NetTrack: Tracking Highly Dynamic Objects with a Net</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The complex dynamicity of open-world objects presents non-negligible challenges for multi-object tracking (MOT), often manifested as severe deformations, fast motion, and occlusions. Most methods that solely depend on coarse-grained object cues, such as boxes and the overall appearance of the object, are susceptible to degradation due to distorted internal relationships of dynamic objects. To address this problem, this work proposes NetTrack, an efficient, generic, and affordable tracking framework to introduce fine-grained learning that is robust to dynamicity. Specifically, NetTrack constructs a dynamicity-aware association with a fine-grained Net, leveraging point-level visual cues. Correspondingly, a fine-grained sampler and matching method have been incorporated. Furthermore, NetTrack learns object-text correspondence for fine-grained localization. To evaluate MOT in extremely dynamic open-world scenarios, a bird flock tracking (BFT) dataset is constructed, which exhibits high dynamicity with diverse species and open-world scenarios. Comprehensive evaluation on BFT validates the effectiveness of fine-grained learning on object dynamicity, and thorough transfer experiments on challenging open-world benchmarks, i.e., TAO, TAO-OW, AnimalTrack, and GMOT-40, validate the strong generalization ability of NetTrack even without finetuning. Project page: https://george-zhuang.github.io/nettrack/.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2403.10425.pdf' target='_blank'>https://arxiv.org/pdf/2403.10425.pdf</a></span>   <span><a href='https://github.com/neufieldrobotics/NeuFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyong Zhang, Huaizu Jiang, Hanumant Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10425">NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots Using Edge Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time high-accuracy optical flow estimation is a crucial component in various applications, including localization and mapping in robotics, object tracking, and activity recognition in computer vision. While recent learning-based optical flow methods have achieved high accuracy, they often come with heavy computation costs. In this paper, we propose a highly efficient optical flow architecture, called NeuFlow, that addresses both high accuracy and computational cost concerns. The architecture follows a global-to-local scheme. Given the features of the input images extracted at different spatial resolutions, global matching is employed to estimate an initial optical flow on the 1/16 resolution, capturing large displacement, which is then refined on the 1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency improvements across different computing platforms. We achieve a notable 10x-80x speedup compared to several state-of-the-art methods, while maintaining comparable accuracy. Our approach achieves around 30 FPS on edge computing platforms, which represents a significant breakthrough in deploying complex computer vision tasks such as SLAM on small robots like drones. The full training and evaluation code is available at https://github.com/neufieldrobotics/NeuFlow.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2403.05839.pdf' target='_blank'>https://arxiv.org/pdf/2403.05839.pdf</a></span>   <span><a href='https://github.com/Event-AHU/FELT_SOT_Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Xufeng Lou, Shiao Wang, Ju Huang, Lan Chen, Bo Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05839">Long-Term Visual Object Tracking with Event Cameras: An Associative Memory Augmented Tracker and A Benchmark Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing event stream based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear. In this paper, we first propose a new long-term, large-scale frame-event visual object tracking dataset, termed FELT. It contains 1,044 long-term videos that involve 1.9 million RGB frames and event stream pairs, 60 different target objects, and 14 challenging attributes. To build a solid benchmark, we retrain and evaluate 21 baseline trackers on our dataset for future work to compare. In addition, we propose a novel Associative Memory Transformer based RGB-Event long-term visual tracker, termed AMTTrack. It follows a one-stream tracking framework and aggregates the multi-scale RGB/event template and search tokens effectively via the Hopfield retrieval layer. The framework also embodies another aspect of associative memory by maintaining dynamic template representations through an associative memory update scheme, which addresses the appearance variation in long-term tracking. Extensive experiments on FELT, FE108, VisEvent, and COESOT datasets fully validated the effectiveness of our proposed tracker. Both the dataset and source code will be released on https://github.com/Event-AHU/FELT_SOT_Benchmark
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2403.05231.pdf' target='_blank'>https://arxiv.org/pdf/2403.05231.pdf</a></span>   <span><a href='https://github.com/LitingLin/LoRAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liting Lin, Heng Fan, Zhipeng Zhang, Yaowei Wang, Yong Xu, Haibin Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05231">Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the Parameter-Efficient Fine-Tuning (PEFT) in large language models, we propose LoRAT, a method that unveils the power of large ViT model for tracking within laboratory-level resources. The essence of our work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. However, unique challenges and potential domain gaps make this transfer not as easy as the first intuition. Firstly, a transformer-based tracker constructs unshared position embedding for template and search image. This poses a challenge for the transfer of LoRA, usually requiring consistency in the design when applied to the pre-trained backbone, to downstream tasks. Secondly, the inductive bias inherent in convolutional heads diminishes the effectiveness of parameter-efficient fine-tuning in tracking models. To overcome these limitations, we first decouple the position embeddings in transformer-based trackers into shared spatial ones and independent type ones. The shared embeddings, which describe the absolute coordinates of multi-resolution images (namely, the template and search images), are inherited from the pre-trained backbones. In contrast, the independent embeddings indicate the sources of each token and are learned from scratch. Furthermore, we design an anchor-free head solely based on MLP to adapt PETR, enabling better performance with less computational overhead. With our design, 1) it becomes practical to train trackers with the ViT-g backbone on GPUs with only memory of 25.8GB (batch size of 16); 2) we reduce the training time of the L-224 variant from 35.0 to 10.8 GPU hours; 3) we improve the LaSOT SUC score from 0.703 to 0.742 with the L-224 variant; 4) we fast the inference speed of the L-224 variant from 52 to 119 FPS. Code and models are available at https://github.com/LitingLin/LoRAT.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2403.05146.pdf' target='_blank'>https://arxiv.org/pdf/2403.05146.pdf</a></span>   <span><a href='https://github.com/PieceZhang/MotionDCTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuelin Zhang, Kim Yan, Chun Ping Lam, Chengyu Fang, Wenxuan Xie, Yufu Qiu, Raymond Shing-Yan Tang, Shing Shin Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05146">Motion-Guided Dual-Camera Tracker for Endoscope Tracking and Motion Analysis in a Mechanical Gastric Simulator</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flexible endoscope motion tracking and analysis in mechanical simulators have proven useful for endoscopy training. Common motion tracking methods based on electromagnetic tracker are however limited by their high cost and material susceptibility. In this work, the motion-guided dual-camera vision tracker is proposed to provide robust and accurate tracking of the endoscope tip's 3D position. The tracker addresses several unique challenges of tracking flexible endoscope tip inside a dynamic, life-sized mechanical simulator. To address the appearance variation and keep dual-camera tracking consistency, the cross-camera mutual template strategy (CMT) is proposed by introducing dynamic transient mutual templates. To alleviate large occlusion and light-induced distortion, the Mamba-based motion-guided prediction head (MMH) is presented to aggregate historical motion with visual tracking. The proposed tracker achieves superior performance against state-of-the-art vision trackers, achieving 42% and 72% improvements against the second-best method in average error and maximum error. Further motion analysis involving novice and expert endoscopists also shows that the tip 3D motion provided by the proposed tracker enables more reliable motion analysis and more substantial differentiation between different expertise levels, compared with other trackers. Project page: https://github.com/PieceZhang/MotionDCTrack
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2403.05021.pdf' target='_blank'>https://arxiv.org/pdf/2403.05021.pdf</a></span>   <span><a href='https://github.com/Nathan-Li123/SMOTer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Qin Li, Hao Wang, Xue Ma, Jiali Yao, Shaohua Dong, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05021">Beyond MOT: Semantic Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multi-object tracking (MOT) aims to predict trajectories of targets (i.e., ''where'') in videos. Yet, knowing merely ''where'' is insufficient in many crucial applications. In comparison, semantic understanding such as fine-grained behaviors, interactions, and overall summarized captions (i.e., ''what'') from videos, associated with ''where'', is highly-desired for comprehensive video analysis. Thus motivated, we introduce Semantic Multi-Object Tracking (SMOT), that aims to estimate object trajectories and meanwhile understand semantic details of associated trajectories including instance captions, instance interactions, and overall video captions, integrating ''where'' and ''what'' for tracking. In order to foster the exploration of SMOT, we propose BenSMOT, a large-scale Benchmark for Semantic MOT. Specifically, BenSMOT comprises 3,292 videos with 151K frames, covering various scenarios for semantic tracking of humans. BenSMOT provides annotations for the trajectories of targets, along with associated instance captions in natural language, instance interactions, and overall caption for each video sequence. To our best knowledge, BenSMOT is the first publicly available benchmark for SMOT. Besides, to encourage future research, we present a novel tracker named SMOTer, which is specially designed and end-to-end trained for SMOT, showing promising performance. By releasing BenSMOT, we expect to go beyond conventional MOT by predicting ''where'' and ''what'' for SMOT, opening up a new direction in tracking for video understanding. We will release BenSMOT and SMOTer at https://github.com/Nathan-Li123/SMOTer.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2403.04700.pdf' target='_blank'>https://arxiv.org/pdf/2403.04700.pdf</a></span>   <span><a href='https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Chen, En Yu, Jinyang Li, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04700">Delving into the Trajectory Long-tail Distribution for Muti-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) is a critical area within computer vision, with a broad spectrum of practical implementations. Current research has primarily focused on the development of tracking algorithms and enhancement of post-processing techniques. Yet, there has been a lack of thorough examination concerning the nature of tracking data it self. In this study, we pioneer an exploration into the distribution patterns of tracking data and identify a pronounced long-tail distribution issue within existing MOT datasets. We note a significant imbalance in the distribution of trajectory lengths across different pedestrians, a phenomenon we refer to as ``pedestrians trajectory long-tail distribution''. Addressing this challenge, we introduce a bespoke strategy designed to mitigate the effects of this skewed distribution. Specifically, we propose two data augmentation strategies, including Stationary Camera View Data Augmentation (SVA) and Dynamic Camera View Data Augmentation (DVA) , designed for viewpoint states and the Group Softmax (GS) module for Re-ID. SVA is to backtrack and predict the pedestrian trajectory of tail classes, and DVA is to use diffusion model to change the background of the scene. GS divides the pedestrians into unrelated groups and performs softmax operation on each group individually. Our proposed strategies can be integrated into numerous existing tracking systems, and extensive experimentation validates the efficacy of our method in reducing the influence of long-tail distribution on multi-object tracking performance. The code is available at https://github.com/chen-si-jia/Trajectory-Long-tail-Distribution-for-MOT.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2403.03493.pdf' target='_blank'>https://arxiv.org/pdf/2403.03493.pdf</a></span>   <span><a href='https://github.com/HengLan/VastTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, Zhipeng Zhang, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03493">VastTrack: Vast Category Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a novel benchmark, dubbed VastTrack, towards facilitating the development of more general visual tracking via encompassing abundant classes and videos. VastTrack possesses several attractive properties: (1) Vast Object Category. In particular, it covers target objects from 2,115 classes, largely surpassing object categories of existing popular benchmarks (e.g., GOT-10k with 563 classes and LaSOT with 70 categories). With such vast object classes, we expect to learn more general object tracking. (2) Larger scale. Compared with current benchmarks, VastTrack offers 50,610 sequences with 4.2 million frames, which makes it to date the largest benchmark regarding the number of videos, and thus could benefit training even more powerful visual trackers in the deep learning era. (3) Rich Annotation. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions for the videos. The rich annotations of VastTrack enables development of both the vision-only and the vision-language tracking. To ensure precise annotation, all videos are manually labeled with multiple rounds of careful inspection and refinement. To understand performance of existing trackers and to provide baselines for future comparison, we extensively assess 25 representative trackers. The results, not surprisingly, show significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are required to improve general tracking. Our VastTrack and all the evaluation results will be made publicly available https://github.com/HengLan/VastTrack.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2402.18302.pdf' target='_blank'>https://arxiv.org/pdf/2402.18302.pdf</a></span>   <span><a href='https://github.com/lab206/EchoTrack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lab206/EchoTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Lin, Jiajun Chen, Kunyu Peng, Xuan He, Zhiyong Li, Rainer Stiefelhagen, Kailun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18302">EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the task of Auditory Referring Multi-Object Tracking (AR-MOT), which dynamically tracks specific objects in a video sequence based on audio expressions and appears as a challenging problem in autonomous driving. Due to the lack of semantic modeling capacity in audio and video, existing works have mainly focused on text-based multi-object tracking, which often comes at the cost of tracking quality, interaction efficiency, and even the safety of assistance systems, limiting the application of such methods in autonomous driving. In this paper, we delve into the problem of AR-MOT from the perspective of audio-video fusion and audio-video tracking. We put forward EchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers. The dual streams are intertwined with our Bidirectional Frequency-domain Cross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and video features from both frequency- and spatiotemporal domains. Moreover, we propose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract homogeneous semantic features between expressions and visual objects by learning homogeneous features between different audio and video objects effectively. Aside from the architectural design, we establish the first set of large-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD. Extensive experiments on the established benchmarks demonstrate the effectiveness of the proposed EchoTrack and its components. The source code and datasets are available at https://github.com/lab206/EchoTrack.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2402.16249.pdf' target='_blank'>https://arxiv.org/pdf/2402.16249.pdf</a></span>   <span><a href='https://github.com/aron-lin/seqtrack3d' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Lin, Zhiheng Li, Yubo Cui, Zheng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16249">SeqTrack3D: Exploring Sequence Information for Robust 3D Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) is an important and challenging task for the autonomous driving and mobile robotics. Most existing methods perform tracking between two consecutive frames while ignoring the motion patterns of the target over a series of frames, which would cause performance degradation in the scenes with sparse points. To break through this limitation, we introduce Sequence-to-Sequence tracking paradigm and a tracker named SeqTrack3D to capture target motion across continuous frames. Unlike previous methods that primarily adopted three strategies: matching two consecutive point clouds, predicting relative motion, or utilizing sequential point clouds to address feature degradation, our SeqTrack3D combines both historical point clouds and bounding box sequences. This novel method ensures robust tracking by leveraging location priors from historical boxes, even in scenes with sparse points. Extensive experiments conducted on large-scale datasets show that SeqTrack3D achieves new state-of-the-art performances, improving by 6.00% on NuScenes and 14.13% on Waymo dataset. The code will be made public at https://github.com/aron-lin/seqtrack3d.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2402.14136.pdf' target='_blank'>https://arxiv.org/pdf/2402.14136.pdf</a></span>   <span><a href='https://github.com/nesl/GDTM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ho Lyun Jeong, Ziqi Wang, Colin Samplawski, Jason Wu, Shiwei Fang, Lance M. Kaplan, Deepak Ganesan, Benjamin Marlin, Mani Srivastava
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14136">GDTM: An Indoor Geospatial Tracking Dataset with Distributed Multimodal Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Constantly locating moving objects, i.e., geospatial tracking, is essential for autonomous building infrastructure. Accurate and robust geospatial tracking often leverages multimodal sensor fusion algorithms, which require large datasets with time-aligned, synchronized data from various sensor types. However, such datasets are not readily available. Hence, we propose GDTM, a nine-hour dataset for multimodal object tracking with distributed multimodal sensors and reconfigurable sensor node placements. Our dataset enables the exploration of several research problems, such as optimizing architectures for processing multimodal data, and investigating models' robustness to adverse sensing conditions and sensor placement variances. A GitHub repository containing the code, sample data, and checkpoints of this work is available at https://github.com/nesl/GDTM.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2402.12303.pdf' target='_blank'>https://arxiv.org/pdf/2402.12303.pdf</a></span>   <span><a href='https://github.com/TRAILab/UncertaintyTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Won Lee, Steven L. Waslander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12303">UncertaintyTrack: Exploiting Detection and Localization Uncertainty in Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) methods have seen a significant boost in performance recently, due to strong interest from the research community and steadily improving object detection methods. The majority of tracking methods follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming detections with no sense of their associated localization uncertainty. This lack of uncertainty awareness poses a problem in safety-critical tasks such as autonomous driving where passengers could be put at risk due to erroneous detections that have propagated to downstream tasks, including MOT. While there are existing works in probabilistic object detection that predict the localization uncertainty around the boxes, no work in 2D MOT for autonomous driving has studied whether these estimates are meaningful enough to be leveraged effectively in object tracking. We introduce UncertaintyTrack, a collection of extensions that can be applied to multiple TBD trackers to account for localization uncertainty estimates from probabilistic object detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the combination of our method and informative uncertainty estimates reduces the number of ID switches by around 19\% and improves mMOTA by 2-3%. The source code is available at https://github.com/TRAILab/UncertaintyTrack
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2402.08437.pdf' target='_blank'>https://arxiv.org/pdf/2402.08437.pdf</a></span>   <span><a href='https://github.com/CVLABLUMS/CVGL-Camera-Calibration' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Waleed, Abdul Rauf, Murtaza Taj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08437">Camera Calibration through Geometric Constraints from Rotation and Projection Matrices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The process of camera calibration involves estimating the intrinsic and extrinsic parameters, which are essential for accurately performing tasks such as 3D reconstruction, object tracking and augmented reality. In this work, we propose a novel constraints-based loss for measuring the intrinsic (focal length: $(f_x, f_y)$ and principal point: $(p_x, p_y)$) and extrinsic (baseline: ($b$), disparity: ($d$), translation: $(t_x, t_y, t_z)$, and rotation specifically pitch: $(Î¸_p)$) camera parameters. Our novel constraints are based on geometric properties inherent in the camera model, including the anatomy of the projection matrix (vanishing points, image of world origin, axis planes) and the orthonormality of the rotation matrix. Thus we proposed a novel Unsupervised Geometric Constraint Loss (UGCL) via a multitask learning framework. Our methodology is a hybrid approach that employs the learning power of a neural network to estimate the desired parameters along with the underlying mathematical properties inherent in the camera projection matrix. This distinctive approach not only enhances the interpretability of the model but also facilitates a more informed learning process. Additionally, we introduce a new CVGL Camera Calibration dataset, featuring over 900 configurations of camera parameters, incorporating 63,600 image pairs that closely mirror real-world conditions. By training and testing on both synthetic and real-world datasets, our proposed approach demonstrates improvements across all parameters when compared to the state-of-the-art (SOTA) benchmarks. The code and the updated dataset can be found here: https://github.com/CVLABLUMS/CVGL-Camera-Calibration
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2402.02574.pdf' target='_blank'>https://arxiv.org/pdf/2402.02574.pdf</a></span>   <span><a href='https://github.com/guanxiongsun/vfe.pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanxiong Sun, Chi Wang, Zhaoyu Zhang, Jiankang Deng, Stefanos Zafeiriou, Yang Hua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.02574">Spatio-temporal Prompting Network for Robust Video Feature Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Frame quality deterioration is one of the main challenges in the field of video understanding. To compensate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information. However, these integration modules are heavy and complex. Furthermore, each integration module is specifically tailored for its target task, making it difficult to generalise to multiple tasks. In this paper, we present a neat and unified framework, called Spatio-Temporal Prompting Network (STPN). It can efficiently extract robust and accurate video features by dynamically adjusting the input features in the backbone network. Specifically, STPN predicts several video prompts containing spatio-temporal information of neighbour frames. Then, these video prompts are prepended to the patch embeddings of the current frame as the updated input for video feature extraction. Moreover, STPN is easy to generalise to various video tasks because it does not contain task-specific modules. Without bells and whistles, STPN achieves state-of-the-art performance on three widely-used datasets for different video understanding tasks, i.e., ImageNetVID for video object detection, YouTubeVIS for video instance segmentation, and GOT-10k for visual object tracking. Code is available at https://github.com/guanxiongsun/vfe.pytorch.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2401.16618.pdf' target='_blank'>https://arxiv.org/pdf/2401.16618.pdf</a></span>   <span><a href='https://github.com/FARAZLOTFI/underwater-object-tracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Faraz Lotfi, Khalil Virji, Nicholas Dudek, Gregory Dudek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16618">A comparison of RL-based and PID controllers for 6-DOF swimming robots: hybrid underwater object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present an exploration and assessment of employing a centralized deep Q-network (DQN) controller as a substitute for the prevalent use of PID controllers in the context of 6DOF swimming robots. Our primary focus centers on illustrating this transition with the specific case of underwater object tracking. DQN offers advantages such as data efficiency and off-policy learning, while remaining simpler to implement than other reinforcement learning methods. Given the absence of a dynamic model for our robot, we propose an RL agent to control this multi-input-multi-output (MIMO) system, where a centralized controller may offer more robust control than distinct PIDs. Our approach involves initially using classical controllers for safe exploration, then gradually shifting to DQN to take full control of the robot.
  We divide the underwater tracking task into vision and control modules. We use established methods for vision-based tracking and introduce a centralized DQN controller. By transmitting bounding box data from the vision module to the control module, we enable adaptation to various objects and effortless vision system replacement. Furthermore, dealing with low-dimensional data facilitates cost-effective online learning for the controller. Our experiments, conducted within a Unity-based simulator, validate the effectiveness of a centralized RL agent over separated PID controllers, showcasing the applicability of our framework for training the underwater RL agent and improved performance compared to traditional control methods. The code for both real and simulation implementations is at https://github.com/FARAZLOTFI/underwater-object-tracking.
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2401.11228.pdf' target='_blank'>https://arxiv.org/pdf/2401.11228.pdf</a></span>   <span><a href='https://github.com/OpenSpaceAI/UVLTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Jinpeng Zhang, Mengxue Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11228">Unifying Visual and Vision-Language Tracking via Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking aims to locate the target object in a video sequence according to the state specified by different modal references, including the initial bounding box (BBOX), natural language (NL), or both (NL+BBOX). Due to the gap between different modalities, most existing trackers are designed for single or partial of these reference settings and overspecialize on the specific modality. Differently, we present a unified tracker called UVLTrack, which can simultaneously handle all three reference settings (BBOX, NL, NL+BBOX) with the same parameters. The proposed UVLTrack enjoys several merits. First, we design a modality-unified feature extractor for joint visual and language feature learning and propose a multi-modal contrastive loss to align the visual and language features into a unified semantic space. Second, a modality-adaptive box head is proposed, which makes full use of the target reference to mine ever-changing scenario features dynamically from video contexts and distinguish the target in a contrastive way, enabling robust performance in different reference settings. Extensive experimental results demonstrate that UVLTrack achieves promising performance on seven visual tracking datasets, three vision-language tracking datasets, and three visual grounding datasets. Codes and models will be open-sourced at https://github.com/OpenSpaceAI/UVLTrack.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2401.06614.pdf' target='_blank'>https://arxiv.org/pdf/2401.06614.pdf</a></span>   <span><a href='https://vveicao.github.io/projects/Motion2VecSets/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Cao, Chang Luo, Biao Zhang, Matthias NieÃner, Jiapeng Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06614">Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Motion2VecSets, a 4D diffusion model for dynamic surface reconstruction from point cloud sequences. While existing state-of-the-art methods have demonstrated success in reconstructing non-rigid objects using neural field representations, conventional feed-forward networks encounter challenges with ambiguous observations from noisy, partial, or sparse point clouds. To address these challenges, we introduce a diffusion model that explicitly learns the shape and motion distribution of non-rigid objects through an iterative denoising process of compressed latent representations. The diffusion-based priors enable more plausible and probabilistic reconstructions when handling ambiguous inputs. We parameterize 4D dynamics with latent sets instead of using global latent codes. This novel 4D representation allows us to learn local shape and deformation patterns, leading to more accurate non-linear motion capture and significantly improving generalizability to unseen motions and identities. For more temporally-coherent object tracking, we synchronously denoise deformation latent sets and exchange information across multiple frames. To avoid computational overhead, we designed an interleaved space and time attention block to alternately aggregate deformation latents along spatial and temporal domains. Extensive comparisons against state-of-the-art methods demonstrate the superiority of our Motion2VecSets in 4D reconstruction from various imperfect observations. More detailed information can be found at https://vveicao.github.io/projects/Motion2VecSets/.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2401.04650.pdf' target='_blank'>https://arxiv.org/pdf/2401.04650.pdf</a></span>   <span><a href='https://parses-lab.github.io/kresling_control' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Immanuel Ampomah Mensah, Jessica Healey, Celina Wu, Andrea Lacunza, Nathaniel Hanson, Kristen L. Dorsey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04650">Hold 'em and Fold 'em: Towards Human-scale, Feedback-Controlled Soft Origami Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>An underdeveloped capability in soft robotics is proprioceptive feedback control, where soft actuators can be sensed and controlled using only sensors on the robot's body. Additionally, soft actuators are often unable to support human-scale loads due to the extremely compliant materials in use. Developing both feedback control and the ability to actuate under large loads (e.g. 500 N) are key capacities required to move soft robotics into everyday applications. In this work, we independently demonstrate these key factors towards controlling and actuating human-scale loads: proprioceptive (embodied) feedback control of a soft, pneumatically-actuated origami robot; and actuation of these origami origami robots under a person's weight in an open-loop configuration. In both demonstrations, the actuators are controlled by internal fluidic pressure. Capacitive sensors patterned onto the robot provide position estimation and serve as input to a feedback controller. We demonstrate position control of a single actuator during stepped setpoints and sinusoidal trajectory following, with root mean square error (RMSE) below 4 mm. We also showcase the actuator's potential towards human-scale robotics as an "origami balance board" by joining three actuators into an open-loop controlled system with a platform that varies its height, roll, and pitch. This work contributes to the field of soft robotics by demonstrating closed-loop feedback position control without visual tracking as an input and lightweight, soft actuators that can support a person's weight. The project repository, including videos, CAD files, and ROS code, is available at https://parses-lab.github.io/kresling_control.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2401.03142.pdf' target='_blank'>https://arxiv.org/pdf/2401.03142.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/EVPTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangtao Shi, Bineng Zhong, Qihua Liang, Ning Li, Shengping Zhang, Xianxian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03142">Explicit Visual Prompts for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>How to effectively exploit spatio-temporal information is crucial to capture target appearance changes in visual tracking. However, most deep learning-based trackers mainly focus on designing a complicated appearance model or template updating strategy, while lacking the exploitation of context between consecutive frames and thus entailing the \textit{when-and-how-to-update} dilemma. To address these issues, we propose a novel explicit visual prompts framework for visual tracking, dubbed \textbf{EVPTrack}. Specifically, we utilize spatio-temporal tokens to propagate information between consecutive frames without focusing on updating templates. As a result, we cannot only alleviate the challenge of \textit{when-to-update}, but also avoid the hyper-parameters associated with updating strategies. Then, we utilize the spatio-temporal tokens to generate explicit visual prompts that facilitate inference in the current frame. The prompts are fed into a transformer encoder together with the image tokens without additional processing. Consequently, the efficiency of our model is improved by avoiding \textit{how-to-update}. In addition, we consider multi-scale information as explicit visual prompts, providing multiscale template features to enhance the EVPTrack's ability to handle target scale changes. Extensive experimental results on six benchmarks (i.e., LaSOT, LaSOT\rm $_{ext}$, GOT-10k, UAV123, TrackingNet, and TNL2K.) validate that our EVPTrack can achieve competitive performance at a real-time speed by effectively exploiting both spatio-temporal and multi-scale information. Code and models are available at https://github.com/GXNU-ZhongLab/EVPTrack.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2401.02826.pdf' target='_blank'>https://arxiv.org/pdf/2401.02826.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Cross_Resolution_SOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yabin Zhu, Xiao Wang, Chenglong Li, Bo Jiang, Lin Zhu, Zhixiang Huang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02826">CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing datasets for RGB-DVS tracking are collected with DVS346 camera and their resolution ($346 \times 260$) is low for practical applications. Actually, only visible cameras are deployed in many practical systems, and the newly designed neuromorphic cameras may have different resolutions. The latest neuromorphic sensors can output high-definition event streams, but it is very difficult to achieve strict alignment between events and frames on both spatial and temporal views. Therefore, how to achieve accurate tracking with unaligned neuromorphic and visible sensors is a valuable but unresearched problem. In this work, we formally propose the task of object tracking using unaligned neuromorphic and visible cameras. We build the first unaligned frame-event dataset CRSOT collected with a specially built data acquisition system, which contains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In addition, we propose a novel unaligned object tracking framework that can realize robust tracking even using the loosely aligned RGB-Event data. Specifically, we extract the template and search regions of RGB and Event data and feed them into a unified ViT backbone for feature embedding. Then, we propose uncertainty perception modules to encode the RGB and Event features, respectively, then, we propose a modality uncertainty fusion module to aggregate the two modalities. These three branches are jointly optimized in the training phase. Extensive experiments demonstrate that our tracker can collaborate the dual modalities for high-performance tracking even without strictly temporal and spatial alignment. The source code, dataset, and pre-trained models will be released at https://github.com/Event-AHU/Cross_Resolution_SOT.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2401.01686.pdf' target='_blank'>https://arxiv.org/pdf/2401.01686.pdf</a></span>   <span><a href='https://github.com/GXNU-ZhongLab/ODTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo, Shengping Zhang, Xianxian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01686">ODTrack: Online Dense Temporal Token Learning for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online contextual reasoning and association across consecutive video frames are critical to perceive instances in visual tracking. However, most current top-performing trackers persistently lean on sparse temporal relationships between reference and search frames via an offline mode. Consequently, they can only interact independently within each image-pair and establish limited temporal correlations. To alleviate the above problem, we propose a simple, flexible and effective video-level tracking pipeline, named \textbf{ODTrack}, which densely associates the contextual relationships of video frames in an online token propagation manner. ODTrack receives video frames of arbitrary length to capture the spatio-temporal trajectory relationships of an instance, and compresses the discrimination features (localization information) of a target into a token sequence to achieve frame-to-frame association. This new solution brings the following benefits: 1) the purified token sequences can serve as prompts for the inference in the next video frame, whereby past information is leveraged to guide future inference; 2) the complex online update strategies are effectively avoided by the iterative propagation of token sequences, and thus we can achieve more efficient model representation and computation. ODTrack achieves a new \textit{SOTA} performance on seven benchmarks, while running at real-time speed. Code and models are available at \url{https://github.com/GXNU-ZhongLab/ODTrack}.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2401.00605.pdf' target='_blank'>https://arxiv.org/pdf/2401.00605.pdf</a></span>   <span><a href='https://github.com/AdelaideAuto-IDLab/Distributed-limitedFoV-CDPMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Chen, Hoa Van Nguyen, Alex S. Leong, Sabita Panicker, Robin Baker, Damith C. Ranasinghe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.00605">Distributed Multi-Object Tracking Under Limited Field of View Heterogeneous Sensors with Density Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the problem of tracking multiple, unknown, and time-varying numbers of objects using a distributed network of heterogeneous sensors. In an effort to derive a formulation for practical settings, we consider limited and unknown sensor field-of-views (FoVs), sensors with limited local computational resources and communication channel capacity. The resulting distributed multi-object tracking algorithm involves solving an NP-hard multidimensional assignment problem either optimally for small-size problems or sub-optimally for general practical problems. For general problems, we propose an efficient distributed multi-object tracking algorithm that performs track-to-track fusion using a clustering-based analysis of the state space transformed into a density space to mitigate the complexity of the assignment problem. The proposed algorithm can more efficiently group local track estimates for fusion than existing approaches. To ensure we achieve globally consistent identities for tracks across a network of nodes as objects move between FoVs, we develop a graph-based algorithm to achieve label consensus and minimise track segmentation. Numerical experiments with synthetic and real-world trajectory datasets demonstrate that our proposed method is significantly more computationally efficient than state-of-the-art solutions, achieving similar tracking accuracy and bandwidth requirements but with improved label consistency.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2312.17448.pdf' target='_blank'>https://arxiv.org/pdf/2312.17448.pdf</a></span>   <span><a href='https://github.com/jiawen-zhu/TrackGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Zhu, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Huchuan Lu, Yifeng Geng, Xuansong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17448">Tracking with Human-Intent Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in perception modeling have significantly improved the performance of object tracking. However, the current methods for specifying the target object in the initial frame are either by 1) using a box or mask template, or by 2) providing an explicit language description. These manners are cumbersome and do not allow the tracker to have self-reasoning ability. Therefore, this work proposes a new tracking task -- Instruction Tracking, which involves providing implicit tracking instructions that require the trackers to perform tracking automatically in video frames. To achieve this, we investigate the integration of knowledge and reasoning capabilities from a Large Vision-Language Model (LVLM) for object tracking. Specifically, we propose a tracker called TrackGPT, which is capable of performing complex reasoning-based tracking. TrackGPT first uses LVLM to understand tracking instructions and condense the cues of what target to track into referring embeddings. The perception component then generates the tracking results based on the embeddings. To evaluate the performance of TrackGPT, we construct an instruction tracking benchmark called InsTrack, which contains over one thousand instruction-video pairs for instruction tuning and evaluation. Experiments show that TrackGPT achieves competitive performance on referring video object segmentation benchmarks, such as getting a new state-of the-art performance of 66.5 $\mathcal{J}\&\mathcal{F}$ on Refer-DAVIS. It also demonstrates a superior performance of instruction tracking under new evaluation protocols. The code and models are available at \href{https://github.com/jiawen-zhu/TrackGPT}{https://github.com/jiawen-zhu/TrackGPT}.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2312.17273.pdf' target='_blank'>https://arxiv.org/pdf/2312.17273.pdf</a></span>   <span><a href='https://github.com/DZSYUNNAN/XNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaisheng Ding, Haiyan Li, Ruichao Hou, Yanyu Liu, Shidong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17273">X Modality Assisting RGBT Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing robust multi-modal feature representations is crucial for enhancing object tracking performance. In pursuit of this objective, a novel X Modality Assisting Network (X-Net) is introduced, which explores the impact of the fusion paradigm by decoupling visual object tracking into three distinct levels, thereby facilitating subsequent processing. Initially, to overcome the challenges associated with feature learning due to significant discrepancies between RGB and thermal modalities, a plug-and-play pixel-level generation module (PGM) based on knowledge distillation learning is proposed. This module effectively generates the X modality, bridging the gap between the two patterns while minimizing noise interference. Subsequently, to optimize sample feature representation and promote cross-modal interactions, a feature-level interaction module (FIM) is introduced, integrating a mixed feature interaction transformer and a spatial dimensional feature translation strategy. Finally, to address random drifting caused by missing instance features, a flexible online optimization strategy called the decision-level refinement module (DRM) is proposed, which incorporates optical flow and refinement mechanisms. The efficacy of X-Net is validated through experiments on three benchmarks, demonstrating its superiority over state-of-the-art trackers. Notably, X-Net achieves performance gains of 0.47%/1.2% in the average of precise rate and success rate, respectively. Additionally, the research content, data, and code are pledged to be made publicly accessible at https://github.com/DZSYUNNAN/XNet.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2312.16245.pdf' target='_blank'>https://arxiv.org/pdf/2312.16245.pdf</a></span>   <span><a href='https://github.com/dyhBUPT/iKUN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Du, Cheng Lei, Zhicheng Zhao, Fei Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16245">iKUN: Speak to Trackers without Retraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) aims to track multiple objects based on input textual descriptions. Previous works realize it by simply integrating an extra textual module into the multi-object tracker. However, they typically need to retrain the entire framework and have difficulties in optimization. In this work, we propose an insertable Knowledge Unification Network, termed iKUN, to enable communication with off-the-shelf trackers in a plug-and-play manner. Concretely, a knowledge unification module (KUM) is designed to adaptively extract visual features based on textual guidance. Meanwhile, to improve the localization accuracy, we present a neural version of Kalman filter (NKF) to dynamically adjust process noise and observation noise based on the current motion status. Moreover, to address the problem of open-set long-tail distribution of textual descriptions, a test-time similarity calibration method is proposed to refine the confidence score with pseudo frequency. Extensive experiments on Refer-KITTI dataset verify the effectiveness of our framework. Finally, to speed up the development of RMOT, we also contribute a more challenging dataset, Refer-Dance, by extending public DanceTrack dataset with motion and dressing descriptions. The codes and dataset are available at https://github.com/dyhBUPT/iKUN.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2312.11051.pdf' target='_blank'>https://arxiv.org/pdf/2312.11051.pdf</a></span>   <span><a href='https://github.com/liangp/MCSTN-3DSOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shihao Feng, Pengpeng Liang, Jin Gao, Erkang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11051">Multi-Correlation Siamese Transformer Network with Dense Connection for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud-based 3D object tracking is an important task in autonomous driving. Though great advances regarding Siamese-based 3D tracking have been made recently, it remains challenging to learn the correlation between the template and search branches effectively with the sparse LIDAR point cloud data. Instead of performing correlation of the two branches at just one point in the network, in this paper, we present a multi-correlation Siamese Transformer network that has multiple stages and carries out feature correlation at the end of each stage based on sparse pillars. More specifically, in each stage, self-attention is first applied to each branch separately to capture the non-local context information. Then, cross-attention is used to inject the template information into the search area. This strategy allows the feature learning of the search area to be aware of the template while keeping the individual characteristics of the template intact. To enable the network to easily preserve the information learned at different stages and ease the optimization, for the search area, we densely connect the initial input sparse pillars and the output of each stage to all subsequent stages and the target localization network, which converts pillars to bird's eye view (BEV) feature maps and predicts the state of the target with a small densely connected convolution network. Deep supervision is added to each stage to further boost the performance as well. The proposed algorithm is evaluated on the popular KITTI, nuScenes, and Waymo datasets, and the experimental results show that our method achieves promising performance compared with the state-of-the-art. Ablation study that shows the effectiveness of each component is provided as well. Code is available at https://github.com/liangp/MCSTN-3DSOT.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2312.10611.pdf' target='_blank'>https://arxiv.org/pdf/2312.10611.pdf</a></span>   <span><a href='https://github.com/SparkTempest/BAT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SparkTempest/BAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bing Cao, Junliang Guo, Pengfei Zhu, Qinghua Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10611">Bi-directional Adapter for Multi-modal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the rapid development of computer vision, single-modal (RGB) object tracking has made significant progress in recent years. Considering the limitation of single imaging sensor, multi-modal images (RGB, Infrared, etc.) are introduced to compensate for this deficiency for all-weather object tracking in complex environments. However, as acquiring sufficient multi-modal tracking data is hard while the dominant modality changes with the open environment, most existing techniques fail to extract multi-modal complementary information dynamically, yielding unsatisfactory tracking performance. To handle this problem, we propose a novel multi-modal visual prompt tracking model based on a universal bi-directional adapter, cross-prompting multiple modalities mutually. Our model consists of a universal bi-directional adapter and multiple modality-specific transformer encoder branches with sharing parameters. The encoders extract features of each modality separately by using a frozen pre-trained foundation model. We develop a simple but effective light feature adapter to transfer modality-specific information from one modality to another, performing visual feature prompt fusion in an adaptive manner. With adding fewer (0.32M) trainable parameters, our model achieves superior tracking performance in comparison with both the full fine-tuning methods and the prompt learning-based methods. Our code is available: https://github.com/SparkTempest/BAT.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2312.08952.pdf' target='_blank'>https://arxiv.org/pdf/2312.08952.pdf</a></span>   <span><a href='https://github.com/corfyi/UCMCTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kefu Yi, Kai Luo, Xiaolei Luo, Jiangui Huang, Hao Wu, Rongdong Hu, Wei Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08952">UCMCTrack: Multi-Object Tracking with Uniform Camera Motion Compensation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in video sequences remains a challenging task, especially in scenarios with significant camera movements. This is because targets can drift considerably on the image plane, leading to erroneous tracking outcomes. Addressing such challenges typically requires supplementary appearance cues or Camera Motion Compensation (CMC). While these strategies are effective, they also introduce a considerable computational burden, posing challenges for real-time MOT. In response to this, we introduce UCMCTrack, a novel motion model-based tracker robust to camera movements. Unlike conventional CMC that computes compensation parameters frame-by-frame, UCMCTrack consistently applies the same compensation parameters throughout a video sequence. It employs a Kalman filter on the ground plane and introduces the Mapped Mahalanobis Distance (MMD) as an alternative to the traditional Intersection over Union (IoU) distance measure. By leveraging projected probability distributions on the ground plane, our approach efficiently captures motion patterns and adeptly manages uncertainties introduced by homography projections. Remarkably, UCMCTrack, relying solely on motion cues, achieves state-of-the-art performance across a variety of challenging datasets, including MOT17, MOT20, DanceTrack and KITTI. More details and code are available at https://github.com/corfyi/UCMCTrack
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2312.08869.pdf' target='_blank'>https://arxiv.org/pdf/2312.08869.pdf</a></span>   <span><a href='https://afterjourney00.github.io/IM-HOI.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengfeng Zhao, Juze Zhang, Jiashen Du, Ziwei Shan, Junye Wang, Jingyi Yu, Jingya Wang, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08869">I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We are living in a world surrounded by diverse and "smart" devices with rich modalities of sensing ability. Conveniently capturing the interactions between us humans and these objects remains far-reaching. In this paper, we present I'm-HOI, a monocular scheme to faithfully capture the 3D motions of both the human and object in a novel setting: using a minimal amount of RGB camera and object-mounted Inertial Measurement Unit (IMU). It combines general motion inference and category-aware refinement. For the former, we introduce a holistic human-object tracking method to fuse the IMU signals and the RGB stream and progressively recover the human motions and subsequently the companion object motions. For the latter, we tailor a category-aware motion diffusion model, which is conditioned on both the raw IMU observations and the results from the previous stage under over-parameterization representation. It significantly refines the initial results and generates vivid body, hand, and object motions. Moreover, we contribute a large dataset with ground truth human and object motions, dense RGB inputs, and rich object-mounted IMU measurements. Extensive experiments demonstrate the effectiveness of I'm-HOI under a hybrid capture setting. Our dataset and code will be released to the community.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2312.06117.pdf' target='_blank'>https://arxiv.org/pdf/2312.06117.pdf</a></span>   <span><a href='https://github.com/ywu0912/TeamCode.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Liu, Yue Wu, Maoguo Gong, Qiguang Miao, Wenping Ma, Can Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06117">M3SOT: Multi-frame, Multi-field, Multi-space 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Single Object Tracking (SOT) stands a forefront task of computer vision, proving essential for applications like autonomous driving. Sparse and occluded data in scene point clouds introduce variations in the appearance of tracked objects, adding complexity to the task. In this research, we unveil M3SOT, a novel 3D SOT framework, which synergizes multiple input frames (template sets), multiple receptive fields (continuous contexts), and multiple solution spaces (distinct tasks) in ONE model. Remarkably, M3SOT pioneers in modeling temporality, contexts, and tasks directly from point clouds, revisiting a perspective on the key factors influencing SOT. To this end, we design a transformer-based network centered on point cloud targets in the search area, aggregating diverse contextual representations and propagating target cues by employing historical frames. As M3SOT spans varied processing perspectives, we've streamlined the network-trimming its depth and optimizing its structure-to ensure a lightweight and efficient deployment for SOT applications. We posit that, backed by practical construction, M3SOT sidesteps the need for complex frameworks and auxiliary components to deliver sterling results. Extensive experiments on benchmarks such as KITTI, nuScenes, and Waymo Open Dataset demonstrate that M3SOT achieves state-of-the-art performance at 38 FPS. Our code and models are available at https://github.com/ywu0912/TeamCode.git.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2312.00937.pdf' target='_blank'>https://arxiv.org/pdf/2312.00937.pdf</a></span>   <span><a href='https://rccchoudhury.github.io/proviq2023' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohan Choudhury, Koichiro Niinuma, Kris M. Kitani, LÃ¡szlÃ³ A. Jeni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00937">Zero-Shot Video Question Answering with Procedural Programs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose to answer zero-shot questions about videos by generating short procedural programs that derive a final answer from solving a sequence of visual subtasks. We present Procedural Video Querying (ProViQ), which uses a large language model to generate such programs from an input question and an API of visual modules in the prompt, then executes them to obtain the output. Recent similar procedural approaches have proven successful for image question answering, but videos remain challenging: we provide ProViQ with modules intended for video understanding, allowing it to generalize to a wide variety of videos. This code generation framework additionally enables ProViQ to perform other video tasks in addition to question answering, such as multi-object tracking or basic video editing. ProViQ achieves state-of-the-art results on a diverse range of benchmarks, with improvements of up to 25% on short, long, open-ended, and multimodal video question-answering datasets. Our project page is at https://rccchoudhury.github.io/proviq2023.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2311.18537.pdf' target='_blank'>https://arxiv.org/pdf/2311.18537.pdf</a></span>   <span><a href='https://github.com/TACJu/Axial-VS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ju He, Qihang Yu, Inkyu Shin, Xueqing Deng, Alan Yuille, Xiaohui Shen, Liang-Chieh Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18537">A Simple Video Segmenter by Tracking Objects Along Axial Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video segmentation requires consistently segmenting and tracking objects over time. Due to the quadratic dependency on input size, directly applying self-attention to video segmentation with high-resolution input features poses significant challenges, often leading to insufficient GPU memory capacity. Consequently, modern video segmenters either extend an image segmenter without incorporating any temporal attention or resort to window space-time attention in a naive manner. In this work, we present Axial-VS, a general and simple framework that enhances video segmenters by tracking objects along axial trajectories. The framework tackles video segmentation through two sub-tasks: short-term within-clip segmentation and long-term cross-clip tracking. In the first step, Axial-VS augments an off-the-shelf clip-level video segmenter with the proposed axial-trajectory attention, sequentially tracking objects along the height- and width-trajectories within a clip, thereby enhancing temporal consistency by capturing motion trajectories. The axial decomposition significantly reduces the computational complexity for dense features, and outperforms the window space-time attention in segmentation quality. In the second step, we further employ axial-trajectory attention to the object queries in clip-level segmenters, which are learned to encode object information, thereby aiding object tracking across different clips and achieving consistent segmentation throughout the video. Without bells and whistles, Axial-VS showcases state-of-the-art results on video segmentation benchmarks, emphasizing its effectiveness in addressing the limitations of modern clip-level video segmenters. Code and models are available at https://github.com/TACJu/Axial-VS.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2311.15851.pdf' target='_blank'>https://arxiv.org/pdf/2311.15851.pdf</a></span>   <span><a href='https://github.com/Zongwei97/UnTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongwei Wu, Jilai Zheng, Xiangxuan Ren, Florin-Alexandru Vasluianu, Chao Ma, Danda Pani Paudel, Luc Van Gool, Radu Timofte
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15851">Single-Model and Any-Modality for Video Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of video object tracking, auxiliary modalities such as depth, thermal, or event data have emerged as valuable assets to complement the RGB trackers. In practice, most existing RGB trackers learn a single set of parameters to use them across datasets and applications. However, a similar single-model unification for multi-modality tracking presents several challenges. These challenges stem from the inherent heterogeneity of inputs -- each with modality-specific representations, the scarcity of multi-modal datasets, and the absence of all the modalities at all times. In this work, we introduce Un-Track, a Unified Tracker of a single set of parameters for any modality. To handle any modality, our method learns their common latent space through low-rank factorization and reconstruction techniques. More importantly, we use only the RGB-X pairs to learn the common latent space. This unique shared representation seamlessly binds all modalities together, enabling effective unification and accommodating any missing modality, all within a single transformer-based architecture. Our Un-Track achieves +8.1 absolute F-score gain, on the DepthTrack dataset, by introducing only +2.14 (over 21.50) GFLOPs with +6.6M (over 93M) parameters, through a simple yet efficient prompting strategy. Extensive comparisons on five benchmark datasets with different modalities show that Un-Track surpasses both SOTA unified trackers and modality-specific counterparts, validating our effectiveness and practicality. The source code is publicly available at https://github.com/Zongwei97/UnTrack.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2311.15674.pdf' target='_blank'>https://arxiv.org/pdf/2311.15674.pdf</a></span>   <span><a href='https://github.com/drapado/mot-detr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Rapado-Rincon, Henk Nap, Katarina Smolenova, Eldert J. van Henten, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15674">MOT-DETR: 3D Single Shot Detection and Tracking with Transformers to build 3D representations for Agro-Food Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the current demand for automation in the agro-food industry, accurately detecting and localizing relevant objects in 3D is essential for successful robotic operations. However, this is a challenge due the presence of occlusions. Multi-view perception approaches allow robots to overcome occlusions, but a tracking component is needed to associate the objects detected by the robot over multiple viewpoints. Most multi-object tracking (MOT) algorithms are designed for high frame rate sequences and struggle with the occlusions generated by robots' motions and 3D environments. In this paper, we introduce MOT-DETR, a novel approach to detect and track objects in 3D over time using a combination of convolutional networks and transformers. Our method processes 2D and 3D data, and employs a transformer architecture to perform data fusion. We show that MOT-DETR outperforms state-of-the-art multi-object tracking methods. Furthermore, we prove that MOT-DETR can leverage 3D data to deal with long-term occlusions and large frame-to-frame distances better than state-of-the-art methods. Finally, we show how our method is resilient to camera pose noise that can affect the accuracy of point clouds. The implementation of MOT-DETR can be found here: https://github.com/drapado/mot-detr
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2311.02072.pdf' target='_blank'>https://arxiv.org/pdf/2311.02072.pdf</a></span>   <span><a href='https://github.com/WenRuiCai/HIPTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenrui Cai, Qingjie Liu, Yunhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02072">HIPTrack: Visual Tracking with Historical Prompts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trackers that follow Siamese paradigm utilize similarity matching between template and search region features for tracking. Many methods have been explored to enhance tracking performance by incorporating tracking history to better handle scenarios involving target appearance variations such as deformation and occlusion. However, the utilization of historical information in existing methods is insufficient and incomprehensive, which typically requires repetitive training and introduces a large amount of computation. In this paper, we show that by providing a tracker that follows Siamese paradigm with precise and updated historical information, a significant performance improvement can be achieved with completely unchanged parameters. Based on this, we propose a historical prompt network that uses refined historical foreground masks and historical visual features of the target to provide comprehensive and precise prompts for the tracker. We build a novel tracker called HIPTrack based on the historical prompt network, which achieves considerable performance improvements without the need to retrain the entire model. We conduct experiments on seven datasets and experimental results demonstrate that our method surpasses the current state-of-the-art trackers on LaSOT, LaSOText, GOT-10k and NfS. Furthermore, the historical prompt network can seamlessly integrate as a plug-and-play module into existing trackers, providing performance enhancements. The source code is available at https://github.com/WenRuiCai/HIPTrack.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2310.19542.pdf' target='_blank'>https://arxiv.org/pdf/2310.19542.pdf</a></span>   <span><a href='https://github.com/Tchuanm/AViTMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanming Tang, Kai Wang, Joost van de Weijer, Jianlin Zhang, Yongmei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19542">AViTMP: A Tracking-Specific Transformer for Single-Branch Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking is a fundamental component of transportation systems, especially for intelligent driving. Despite achieving state-of-the-art performance in visual tracking, recent single-branch trackers tend to overlook the weak prior assumptions associated with the Vision Transformer (ViT) encoder and inference pipeline in visual tracking. Moreover, the effectiveness of discriminative trackers remains constrained due to the adoption of the dual-branch pipeline. To tackle the inferior effectiveness of vanilla ViT, we propose an Adaptive ViT Model Prediction tracker (AViTMP) to design a customised tracking method. This method bridges the single-branch network with discriminative models for the first time. Specifically, in the proposed encoder AViT encoder, we introduce a tracking-tailored Adaptor module for vanilla ViT and a joint target state embedding to enrich the target-prior embedding paradigm. Then, we combine the AViT encoder with a discriminative transformer-specific model predictor to predict the accurate location. Furthermore, to mitigate the limitations of conventional inference practice, we present a novel inference pipeline called CycleTrack, which bolsters the tracking robustness in the presence of distractors via bidirectional cycle tracking verification. In the experiments, we evaluated AViTMP on eight tracking benchmarks for a comprehensive assessment, including LaSOT, LaSOTExtSub, AVisT, etc. The experimental results unequivocally establish that, under fair comparison, AViTMP achieves state-of-the-art performance, especially in terms of long-term tracking and robustness. The source code will be released at https://github.com/Tchuanm/AViTMP.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2310.17875.pdf' target='_blank'>https://arxiv.org/pdf/2310.17875.pdf</a></span>   <span><a href='https://github.com/yumu-173/Siamese-DETR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiankun Liu, Yichen Li, Yuqi Jiang, Ying Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17875">Siamese-DETR for Generic Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to detect and track the dynamic objects in different scenes is fundamental to real-world applications, e.g., autonomous driving and robot navigation. However, traditional Multi-Object Tracking (MOT) is limited to tracking objects belonging to the pre-defined closed-set categories. Recently, Open-Vocabulary MOT (OVMOT) and Generic MOT (GMOT) are proposed to track interested objects beyond pre-defined categories with the given text prompt and template image. However, the expensive well pre-trained (vision-)language model and fine-grained category annotations are required to train OVMOT models. In this paper, we focus on GMOT and propose a simple but effective method, Siamese-DETR, for GMOT. Only the commonly used detection datasets (e.g., COCO) are required for training. Different from existing GMOT methods, which train a Single Object Tracking (SOT) based detector to detect interested objects and then apply a data association based MOT tracker to get the trajectories, we leverage the inherent object queries in DETR variants. Specifically: 1) The multi-scale object queries are designed based on the given template image, which are effective for detecting different scales of objects with the same category as the template image; 2) A dynamic matching training strategy is introduced to train Siamese-DETR on commonly used detection datasets, which takes full advantage of provided annotations; 3) The online tracking pipeline is simplified through a tracking-by-query manner by incorporating the tracked boxes in previous frame as additional query boxes. The complex data association is replaced with the much simpler Non-Maximum Suppression (NMS). Extensive experimental results show that Siamese-DETR surpasses existing MOT methods on GMOT-40 dataset by a large margin. Codes are avaliable at \url{https://github.com/yumu-173/Siamese-DETR}.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2310.17170.pdf' target='_blank'>https://arxiv.org/pdf/2310.17170.pdf</a></span>   <span><a href='https://github.com/liaopan-lp/MO-YOLO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liao Pan, Yang Feng, Zhao Wenhui, Yua Jinwen, Zhang Dingwen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.17170">DecoderTracker: Decoder-Only Method for Multiple-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decoder-only methods, such as GPT, have demonstrated superior performance in many areas compared to traditional encoder-decoder structure transformer methods. Over the years, end-to-end methods based on the traditional transformer structure, like MOTR, have achieved remarkable performance in multi-object tracking. However,the substantial computational resource consumption of these methods, coupled with the optimization challenges posed by dynamic data, results in less favorable inference speeds and training times. To address the aforementioned issues, this paper optimized the network architecture and proposed an effective training strategy to mitigate the problem of prolonged training times, thereby developing DecoderTracker, a novel end-to-end tracking method. Subsequently, to tackle the optimization challenges arising from dynamic data, this paper introduced DecoderTracker+ by incorporating a Fixed-Size Query Memory and refining certain attention layers. Our methods, without any bells and whistles, outperforms MOTR on multiple benchmarks, \textcolor{black}{featuring a 2 to 3 times faster inference than MOTR}, respectively. The proposed method is implemented in open-source code, accessible at https://github.com/liaopan-lp/MO-YOLO.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2310.14294.pdf' target='_blank'>https://arxiv.org/pdf/2310.14294.pdf</a></span>   <span><a href='https://github.com/abhineet123/deep_mdp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhineet Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14294">Deep MDP: A Modular Framework for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a fast and modular framework for Multi-Object Tracking (MOT) based on the Markov descision process (MDP) tracking-by-detection paradigm. It is designed to allow its various functional components to be replaced by custom-designed alternatives to suit a given application. An interactive GUI with integrated object detection, segmentation, MOT and semi-automated labeling is also provided to help make it easier to get started with this framework. Though not breaking new ground in terms of performance, Deep MDP has a large code-base that should be useful for the community to try out new ideas or simply to have an easy-to-use and easy-to-adapt system for any MOT application. Deep MDP is available at https://github.com/abhineet123/deep_mdp.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2310.10071.pdf' target='_blank'>https://arxiv.org/pdf/2310.10071.pdf</a></span>   <span><a href='https://github.com/Kou-99/ZoomTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Kou, Jin Gao, Bing Li, Gang Wang, Weiming Hu, Yizheng Wang, Liang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10071">ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the transformer has enabled the speed-oriented trackers to approach state-of-the-art (SOTA) performance with high-speed thanks to the smaller input size or the lighter feature extraction backbone, though they still substantially lag behind their corresponding performance-oriented versions. In this paper, we demonstrate that it is possible to narrow or even close this gap while achieving high tracking speed based on the smaller input size. To this end, we non-uniformly resize the cropped image to have a smaller input size while the resolution of the area where the target is more likely to appear is higher and vice versa. This enables us to solve the dilemma of attending to a larger visual field while retaining more raw information for the target despite a smaller input size. Our formulation for the non-uniform resizing can be efficiently solved through quadratic programming (QP) and naturally integrated into most of the crop-based local trackers. Comprehensive experiments on five challenging datasets based on two kinds of transformer trackers, \ie, OSTrack and TransT, demonstrate consistent improvements over them. In particular, applying our method to the speed-oriented version of OSTrack even outperforms its performance-oriented counterpart by 0.6% AUC on TNL2K, while running 50% faster and saving over 55% MACs. Codes and models are available at https://github.com/Kou-99/ZoomTrack.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2310.08114.pdf' target='_blank'>https://arxiv.org/pdf/2310.08114.pdf</a></span>   <span><a href='https://github.com/TUMFTM/FusionTracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phillip Karle, Felix Fent, Sebastian Huch, Florian Sauerbeck, Markus Lienkamp
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08114">Multi-Modal Sensor Fusion and Object Tracking for Autonomous Racing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reliable detection and tracking of surrounding objects are indispensable for comprehensive motion prediction and planning of autonomous vehicles. Due to the limitations of individual sensors, the fusion of multiple sensor modalities is required to improve the overall detection capabilities. Additionally, robust motion tracking is essential for reducing the effect of sensor noise and improving state estimation accuracy. The reliability of the autonomous vehicle software becomes even more relevant in complex, adversarial high-speed scenarios at the vehicle handling limits in autonomous racing. In this paper, we present a modular multi-modal sensor fusion and tracking method for high-speed applications. The method is based on the Extended Kalman Filter (EKF) and is capable of fusing heterogeneous detection inputs to track surrounding objects consistently. A novel delay compensation approach enables to reduce the influence of the perception software latency and to output an updated object list. It is the first fusion and tracking method validated in high-speed real-world scenarios at the Indy Autonomous Challenge 2021 and the Autonomous Challenge at CES (AC@CES) 2022, proving its robustness and computational efficiency on embedded systems. It does not require any labeled data and achieves position tracking residuals below 0.1 m. The related code is available as open-source software at https://github.com/TUMFTM/FusionTracking.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2310.06992.pdf' target='_blank'>https://arxiv.org/pdf/2310.06992.pdf</a></span>   <span><a href='https://wenhsuanchu.github.io/ovtracktor/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Hsuan Chu, Adam W. Harley, Pavel Tokmakov, Achal Dave, Leonidas Guibas, Katerina Fragkiadaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06992">Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is central to robot perception and scene understanding. Tracking-by-detection has long been a dominant paradigm for object tracking of specific object categories. Recently, large-scale pre-trained models have shown promising advances in detecting and segmenting objects and parts in 2D static images in the wild. This begs the question: can we re-purpose these large-scale pre-trained static image models for open-vocabulary video tracking? In this paper, we re-purpose an open-vocabulary detector, segmenter, and dense optical flow estimator, into a model that tracks and segments objects of any category in 2D videos. Our method predicts object and part tracks with associated language descriptions in monocular videos, rebuilding the pipeline of Tractor with modern large pre-trained models for static image detection and segmentation: we detect open-vocabulary object instances and propagate their boxes from frame to frame using a flow-based motion model, refine the propagated boxes with the box regression module of the visual detector, and prompt an open-world segmenter with the refined box to segment the objects. We decide the termination of an object track based on the objectness score of the propagated boxes, as well as forward-backward optical flow consistency. We re-identify objects across occlusions using deep feature matching. We show that our model achieves strong performance on multiple established video object segmentation and tracking benchmarks, and can produce reasonable tracks in manipulation data. In particular, our model outperforms previous state-of-the-art in UVO and BURST, benchmarks for open-world object tracking and segmentation, despite never being explicitly trained for tracking. We hope that our approach can serve as a simple and extensible framework for future research.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2310.03006.pdf' target='_blank'>https://arxiv.org/pdf/2310.03006.pdf</a></span>   <span><a href='https://github.com/BoSmallEar/COOLer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizheng Liu, Mattia Segu, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03006">COOLer: Class-Incremental Learning for Appearance-Based Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continual learning allows a model to learn multiple tasks sequentially while retaining the old knowledge without the training data of the preceding tasks. This paper extends the scope of continual learning research to class-incremental learning for multiple object tracking (MOT), which is desirable to accommodate the continuously evolving needs of autonomous systems. Previous solutions for continual learning of object detectors do not address the data association stage of appearance-based trackers, leading to catastrophic forgetting of previous classes' re-identification features. We introduce COOLer, a COntrastive- and cOntinual-Learning-based tracker, which incrementally learns to track new categories while preserving past knowledge by training on a combination of currently available ground truth labels and pseudo-labels generated by the past tracker. To further exacerbate the disentanglement of instance representations, we introduce a novel contrastive class-incremental instance representation learning technique. Finally, we propose a practical evaluation protocol for continual learning for MOT and conduct experiments on the BDD100K and SHIFT datasets. Experimental results demonstrate that COOLer continually learns while effectively addressing catastrophic forgetting of both tracking and detection. The code is available at https://github.com/BoSmallEar/COOLer.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2310.01926.pdf' target='_blank'>https://arxiv.org/pdf/2310.01926.pdf</a></span>   <span><a href='https://github.com/mattiasegu/darth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Segu, Bernt Schiele, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01926">DARTH: Holistic Test-time Adaptation for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) is a fundamental component of perception systems for autonomous driving, and its robustness to unseen conditions is a requirement to avoid life-critical failures. Despite the urge of safety in driving systems, no solution to the MOT adaptation problem to domain shift in test-time conditions has ever been proposed. However, the nature of a MOT system is manifold - requiring object detection and instance association - and adapting all its components is non-trivial. In this paper, we analyze the effect of domain shift on appearance-based trackers, and introduce DARTH, a holistic test-time adaptation framework for MOT. We propose a detection consistency formulation to adapt object detection in a self-supervised fashion, while adapting the instance appearance representations via our novel patch contrastive loss. We evaluate our method on a variety of domain shifts - including sim-to-real, outdoor-to-indoor, indoor-to-outdoor - and substantially improve the source model performance on all metrics. Code: https://github.com/mattiasegu/darth.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2309.14611.pdf' target='_blank'>https://arxiv.org/pdf/2309.14611.pdf</a></span>   <span><a href='https://github.com/Event-AHU/EventVOT_Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Shiao Wang, Chuanming Tang, Lin Zhu, Bo Jiang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.14611">Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking using bio-inspired event cameras has drawn more and more attention in recent years. Existing works either utilize aligned RGB and event data for accurate tracking or directly learn an event-based tracker. The first category needs more cost for inference and the second one may be easily influenced by noisy events or sparse spatial resolution. In this paper, we propose a novel hierarchical knowledge distillation framework that can fully utilize multi-modal / multi-view information during training to facilitate knowledge transfer, enabling us to achieve high-speed and low-latency visual tracking during testing by using only event signals. Specifically, a teacher Transformer-based multi-modal tracking framework is first trained by feeding the RGB frame and event stream simultaneously. Then, we design a new hierarchical knowledge distillation strategy which includes pairwise similarity, feature representation, and response maps-based knowledge distillation to guide the learning of the student Transformer network. Moreover, since existing event-based tracking datasets are all low-resolution ($346 \times 260$), we propose the first large-scale high-resolution ($1280 \times 720$) dataset named EventVOT. It contains 1141 videos and covers a wide range of categories such as pedestrians, vehicles, UAVs, ping pongs, etc. Extensive experiments on both low-resolution (FE240hz, VisEvent, COESOT), and our newly proposed high-resolution EventVOT dataset fully validated the effectiveness of our proposed method. The dataset, evaluation toolkit, and source code are available on \url{https://github.com/Event-AHU/EventVOT_Benchmark}
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2309.13570.pdf' target='_blank'>https://arxiv.org/pdf/2309.13570.pdf</a></span>   <span><a href='https://openark-berkeley.github.io/DTTDNet/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixun Huang, Keling Yao, Seth Z. Zhao, Chuanyu Pan, Allen Y. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13570">Robust 6DoF Pose Estimation Against Depth Noise and a Comprehensive Evaluation on a Mobile Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust 6DoF pose estimation with mobile devices is the foundation for applications in robotics, augmented reality, and digital twin localization. In this paper, we extensively investigate the robustness of existing RGBD-based 6DoF pose estimation methods against varying levels of depth sensor noise. We highlight that existing 6DoF pose estimation methods suffer significant performance discrepancies due to depth measurement inaccuracies. In response to the robustness issue, we present a simple and effective transformer-based 6DoF pose estimation approach called DTTDNet, featuring a novel geometric feature filtering module and a Chamfer distance loss for training. Moreover, we advance the field of robust 6DoF pose estimation and introduce a new dataset -- Digital Twin Tracking Dataset Mobile (DTTD-Mobile), tailored for digital twin object tracking with noisy depth data from the mobile RGBD sensor suite of the Apple iPhone 14 Pro. Extensive experiments demonstrate that DTTDNet significantly outperforms state-of-the-art methods at least 4.32, up to 60.74 points in ADD metrics on the DTTD-Mobile. More importantly, our approach exhibits superior robustness to varying levels of measurement noise, setting a new benchmark for robustness to measurement noise. The project page is publicly available at https://openark-berkeley.github.io/DTTDNet/.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2309.12035.pdf' target='_blank'>https://arxiv.org/pdf/2309.12035.pdf</a></span>   <span><a href='https://github.com/ffi-no' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Vonheim Larsen, Sigmund Rolfsjord, Daniel Gusland, JÃ¶rgen Ahlberg, Kim Mathiassen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12035">BASE: Probably a Better Approach to Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of visual object tracking is dominated by methods that combine simple tracking algorithms and ad hoc schemes. Probabilistic tracking algorithms, which are leading in other fields, are surprisingly absent from the leaderboards. We found that accounting for distance in target kinematics, exploiting detector confidence and modelling non-uniform clutter characteristics is critical for a probabilistic tracker to work in visual tracking. Previous probabilistic methods fail to address most or all these aspects, which we believe is why they fall so far behind current state-of-the-art (SOTA) methods (there are no probabilistic trackers in the MOT17 top 100). To rekindle progress among probabilistic approaches, we propose a set of pragmatic models addressing these challenges, and demonstrate how they can be incorporated into a probabilistic framework. We present BASE (Bayesian Approximation Single-hypothesis Estimator), a simple, performant and easily extendible visual tracker, achieving state-of-the-art (SOTA) on MOT17 and MOT20, without using Re-Id. Code will be made available at https://github.com/ffi-no
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2309.09765.pdf' target='_blank'>https://arxiv.org/pdf/2309.09765.pdf</a></span>   <span><a href='https://github.com/mengting2023/LG-Track' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Meng, Chunyun Fu, Mingguang Huang, Xiyang Wang, Jiawei He, Tao Huang, Wankai Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09765">Localization-Guided Track: A Deep Association Multi-Object Tracking Framework Based on Localization Confidence of Detections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In currently available literature, no tracking-by-detection (TBD) paradigm-based tracking method has considered the localization confidence of detection boxes. In most TBD-based methods, it is considered that objects of low detection confidence are highly occluded and thus it is a normal practice to directly disregard such objects or to reduce their priority in matching. In addition, appearance similarity is not a factor to consider for matching these objects. However, in terms of the detection confidence fusing classification and localization, objects of low detection confidence may have inaccurate localization but clear appearance; similarly, objects of high detection confidence may have inaccurate localization or unclear appearance; yet these objects are not further classified. In view of these issues, we propose Localization-Guided Track (LG-Track). Firstly, localization confidence is applied in MOT for the first time, with appearance clarity and localization accuracy of detection boxes taken into account, and an effective deep association mechanism is designed; secondly, based on the classification confidence and localization confidence, a more appropriate cost matrix can be selected and used; finally, extensive experiments have been conducted on MOT17 and MOT20 datasets. The results show that our proposed method outperforms the compared state-of-art tracking methods. For the benefit of the community, our code has been made publicly at https://github.com/mengting2023/LG-Track.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2309.09737.pdf' target='_blank'>https://arxiv.org/pdf/2309.09737.pdf</a></span>   <span><a href='https://github.com/LJacksonPan/RaTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijun Pan, Fangqiang Ding, Hantao Zhong, Chris Xiaoxuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09737">RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art. We release our code and model at https://github.com/LJacksonPan/RaTrack.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2309.09249.pdf' target='_blank'>https://arxiv.org/pdf/2309.09249.pdf</a></span>   <span><a href='https://github.com/TsingWei/LiteTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingmao Wei, Bi Zeng, Jianqi Liu, Li He, Guotian Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09249">LiteTrack: Layer Pruning with Asynchronous Feature Extraction for Lightweight and Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancements in transformer-based visual trackers have led to significant progress, attributed to their strong modeling capabilities. However, as performance improves, running latency correspondingly increases, presenting a challenge for real-time robotics applications, especially on edge devices with computational constraints. In response to this, we introduce LiteTrack, an efficient transformer-based tracking model optimized for high-speed operations across various devices. It achieves a more favorable trade-off between accuracy and efficiency than the other lightweight trackers. The main innovations of LiteTrack encompass: 1) asynchronous feature extraction and interaction between the template and search region for better feature fushion and cutting redundant computation, and 2) pruning encoder layers from a heavy tracker to refine the balnace between performance and speed. As an example, our fastest variant, LiteTrack-B4, achieves 65.2% AO on the GOT-10k benchmark, surpassing all preceding efficient trackers, while running over 100 fps with ONNX on the Jetson Orin NX edge device. Moreover, our LiteTrack-B9 reaches competitive 72.2% AO on GOT-10k and 82.4% AUC on TrackingNet, and operates at 171 fps on an NVIDIA 2080Ti GPU. The code and demo materials will be available at https://github.com/TsingWei/LiteTrack.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2309.06701.pdf' target='_blank'>https://arxiv.org/pdf/2309.06701.pdf</a></span>   <span><a href='https://github.com/kalyan0510/TOTEM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kalyan Garigapati, Erik Blasch, Jie Wei, Haibin Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06701">Transparent Object Tracking with Enhanced Fusion Module</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate tracking of transparent objects, such as glasses, plays a critical role in many robotic tasks such as robot-assisted living. Due to the adaptive and often reflective texture of such objects, traditional tracking algorithms that rely on general-purpose learned features suffer from reduced performance. Recent research has proposed to instill transparency awareness into existing general object trackers by fusing purpose-built features. However, with the existing fusion techniques, the addition of new features causes a change in the latent space making it impossible to incorporate transparency awareness on trackers with fixed latent spaces. For example, many of the current days transformer-based trackers are fully pre-trained and are sensitive to any latent space perturbations. In this paper, we present a new feature fusion technique that integrates transparency information into a fixed feature space, enabling its use in a broader range of trackers. Our proposed fusion module, composed of a transformer encoder and an MLP module, leverages key query-based transformations to embed the transparency information into the tracking pipeline. We also present a new two-step training strategy for our fusion module to effectively merge transparency features. We propose a new tracker architecture that uses our fusion techniques to achieve superior results for transparent object tracking. Our proposed method achieves competitive results with state-of-the-art trackers on TOTB, which is the largest transparent object tracking benchmark recently released. Our results and the implementation of code will be made publicly available at https://github.com/kalyan0510/TOTEM.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2309.06006.pdf' target='_blank'>https://arxiv.org/pdf/2309.06006.pdf</a></span>   <span><a href='https://github.com/SoccerNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Cioppa, Silvio Giancola, Vladimir Somers, Floriane Magera, Xin Zhou, Hassan Mkhallati, Adrien DeliÃ¨ge, Jan Held, Carlos Hinojosa, Amir M. Mansourian, Pierre Miralles, Olivier Barnich, Christophe De Vleeschouwer, Alexandre Alahi, Bernard Ghanem, Marc Van Droogenbroeck, Abdullah Kamal, Adrien Maglo, Albert ClapÃ©s, Amr Abdelaziz, Artur Xarles, Astrid Orcesi, Atom Scott, Bin Liu, Byoungkwon Lim, Chen Chen, Fabian Deuser, Feng Yan, Fufu Yu, Gal Shitrit, Guanshuo Wang, Gyusik Choi, Hankyul Kim, Hao Guo, Hasby Fahrudin, Hidenari Koguchi, HÃ¥kan ArdÃ¶, Ibrahim Salah, Ido Yerushalmy, Iftikar Muhammad, Ikuma Uchida, Ishay Be'ery, Jaonary Rabarisoa, Jeongae Lee, Jiajun Fu, Jianqin Yin, Jinghang Xu, Jongho Nang, Julien Denize, Junjie Li, Junpei Zhang, Juntae Kim, Kamil Synowiec, Kenji Kobayashi, Kexin Zhang, Konrad Habel, Kota Nakajima, Licheng Jiao, Lin Ma, Lizhi Wang, Luping Wang, Menglong Li, Mengying Zhou, Mohamed Nasr, Mohamed Abdelwahed, Mykola Liashuha, Nikolay Falaleev, Norbert Oswald, Qiong Jia, Quoc-Cuong Pham, Ran Song, Romain HÃ©rault, Rui Peng, Ruilong Chen, Ruixuan Liu, Ruslan Baikulov, Ryuto Fukushima, Sergio Escalera, Seungcheon Lee, Shimin Chen, Shouhong Ding, Taiga Someya, Thomas B. Moeslund, Tianjiao Li, Wei Shen, Wei Zhang, Wei Li, Wei Dai, Weixin Luo, Wending Zhao, Wenjie Zhang, Xinquan Yang, Yanbiao Ma, Yeeun Joo, Yingsen Zeng, Yiyang Gan, Yongqiang Zhu, Yujie Zhong, Zheng Ruan, Zhiheng Li, Zhijian Huang, Ziyu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06006">SoccerNet 2023 Challenges Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The SoccerNet 2023 challenges were the third annual video understanding challenges organized by the SoccerNet team. For this third edition, the challenges were composed of seven vision-based tasks split into three main themes. The first theme, broadcast video understanding, is composed of three high-level tasks related to describing events occurring in the video broadcasts: (1) action spotting, focusing on retrieving all timestamps related to global actions in soccer, (2) ball action spotting, focusing on retrieving all timestamps related to the soccer ball change of state, and (3) dense video captioning, focusing on describing the broadcast with natural language and anchored timestamps. The second theme, field understanding, relates to the single task of (4) camera calibration, focusing on retrieving the intrinsic and extrinsic camera parameters from images. The third and last theme, player understanding, is composed of three low-level tasks related to extracting information about the players: (5) re-identification, focusing on retrieving the same players across multiple views, (6) multiple object tracking, focusing on tracking players and the ball through unedited video streams, and (7) jersey number recognition, focusing on recognizing the jersey number of players from tracklets. Compared to the previous editions of the SoccerNet challenges, tasks (2-3-7) are novel, including new annotations and data, task (4) was enhanced with more data and annotations, and task (6) now focuses on end-to-end approaches. More information on the tasks, challenges, and leaderboards are available on https://www.soccer-net.org. Baselines and development kits can be found on https://github.com/SoccerNet.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2309.05829.pdf' target='_blank'>https://arxiv.org/pdf/2309.05829.pdf</a></span>   <span><a href='https://github.com/goutamyg/MVT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Goutam Yelluru Gopal, Maria A. Amer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05829">Mobile Vision Transformer-based Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2309.03979.pdf' target='_blank'>https://arxiv.org/pdf/2309.03979.pdf</a></span>   <span><a href='https://github.com/goutamyg/SMAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Goutam Yelluru Gopal, Maria A. Amer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03979">Separable Self and Mixed Attention Transformers for Efficient Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The deployment of transformers for visual object tracking has shown state-of-the-art results on several benchmarks. However, the transformer-based models are under-utilized for Siamese lightweight tracking due to the computational complexity of their attention blocks. This paper proposes an efficient self and mixed attention transformer-based architecture for lightweight tracking. The proposed backbone utilizes the separable mixed attention transformers to fuse the template and search regions during feature extraction to generate superior feature encoding. Our prediction head performs global contextual modeling of the encoded features by leveraging efficient self-attention blocks for robust target state estimation. With these contributions, the proposed lightweight tracker deploys a transformer-based backbone and head module concurrently for the first time. Our ablation study testifies to the effectiveness of the proposed combination of backbone and head modules. Simulations show that our Separable Self and Mixed Attention-based Tracker, SMAT, surpasses the performance of related lightweight trackers on GOT10k, TrackingNet, LaSOT, NfS30, UAV123, and AVisT datasets, while running at 37 fps on CPU, 158 fps on GPU, and having 3.8M parameters. For example, it significantly surpasses the closely related trackers E.T.Track and MixFormerV2-S on GOT10k-test by a margin of 7.9% and 5.8%, respectively, in the AO metric. The tracker code and model is available at https://github.com/goutamyg/SMAT
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2309.02975.pdf' target='_blank'>https://arxiv.org/pdf/2309.02975.pdf</a></span>   <span><a href='https://github.com/gakkistar/FishMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Liu, Lulu Han, Xiaoyang Liu, Junli Ren, Fang Wang, YingLiu, Yuanshan Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02975">FishMOT: A Simple and Effective Method for Fish Tracking Based on IoU Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fish tracking plays a vital role in understanding fish behavior and ecology. However, existing tracking methods face challenges in accuracy and robustness dues to morphological change of fish, occlusion and complex environment. This paper proposes FishMOT(Multiple Object Tracking for Fish), a novel fish tracking approach combining object detection and IoU matching, including basic module, interaction module and refind module. Wherein, a basic module performs target association based on IoU of detection boxes between successive frames to deal with morphological change of fish; an interaction module combines IoU of detection boxes and IoU of fish entity to handle occlusions; a refind module use spatio-temporal information uses spatio-temporal information to overcome the tracking failure resulting from the missed detection by the detector under complex environment. FishMOT reduces the computational complexity and memory consumption since it does not require complex feature extraction or identity assignment per fish, and does not need Kalman filter to predict the detection boxes of successive frame. Experimental results demonstrate FishMOT outperforms state-of-the-art multi-object trackers and specialized fish tracking tools in terms of MOTA, accuracy, computation time, memory consumption, etc.. Furthermore, the method exhibits excellent robustness and generalizability for varying environments and fish numbers. The simplified workflow and strong performance make FishMOT as a highly effective fish tracking approach. The source codes and pre-trained models are available at: https://github.com/gakkistar/FishMOT
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2309.02666.pdf' target='_blank'>https://arxiv.org/pdf/2309.02666.pdf</a></span>   <span><a href='https://github.com/git-disl/EMO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanjana Vijay Ganesh, Yanzhao Wu, Gaowen Liu, Ramana Kompella, Ling Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02666">Fast and Resource-Efficient Object Tracking on Edge Devices: A Measurement Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is an important functionality of edge video analytic systems and services. Multi-object tracking (MOT) detects the moving objects and tracks their locations frame by frame as real scenes are being captured into a video. However, it is well known that real time object tracking on the edge poses critical technical challenges, especially with edge devices of heterogeneous computing resources. This paper examines the performance issues and edge-specific optimization opportunities for object tracking. We will show that even the well trained and optimized MOT model may still suffer from random frame dropping problems when edge devices have insufficient computation resources. We present several edge specific performance optimization strategies, collectively coined as EMO, to speed up the real time object tracking, ranging from window-based optimization to similarity based optimization. Extensive experiments on popular MOT benchmarks demonstrate that our EMO approach is competitive with respect to the representative methods for on-device object tracking techniques in terms of run-time performance and tracking accuracy. EMO is released on Github at https://github.com/git-disl/EMO.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2309.02185.pdf' target='_blank'>https://arxiv.org/pdf/2309.02185.pdf</a></span>   <span><a href='https://github.com/xmm-prio/BEVTrack' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/xmm-prio/BEVTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Yang, Yingqi Deng, Jinlong Fan, Jing Zhang, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02185">BEVTrack: A Simple and Strong Baseline for 3D Single Object Tracking in Bird's-Eye View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Single Object Tracking (SOT) is a fundamental task of computer vision, proving essential for applications like autonomous driving. It remains challenging to localize the target from surroundings due to appearance variations, distractors, and the high sparsity of point clouds. To address these issues, prior Siamese and motion-centric trackers both require elaborate designs and solving multiple subtasks. In this paper, we propose BEVTrack, a simple yet effective baseline method. By estimating the target motion in Bird's-Eye View (BEV) to perform tracking, BEVTrack demonstrates surprising simplicity from various aspects, i.e., network designs, training objectives, and tracking pipeline, while achieving superior performance. Besides, to achieve accurate regression for targets with diverse attributes (e.g., sizes and motion patterns), BEVTrack constructs the likelihood function with the learned underlying distributions adapted to different targets, rather than making a fixed Laplacian or Gaussian assumption as in previous works. This provides valuable priors for tracking and thus further boosts performance. While only using a single regression loss with a plain convolutional architecture, BEVTrack achieves state-of-the-art performance on three large-scale datasets, KITTI, NuScenes, and Waymo Open Dataset while maintaining a high inference speed of about 200 FPS. The code will be released at https://github.com/xmm-prio/BEVTrack.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2309.02185.pdf' target='_blank'>https://arxiv.org/pdf/2309.02185.pdf</a></span>   <span><a href='https://github.com/xmm-prio/BEVTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Yang, Yingqi Deng, Mian Pan, Zheng-Jun Zha, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02185">BEVTrack: A Simple and Strong Baseline for 3D Single Object Tracking in Bird's-Eye View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Single Object Tracking (SOT) is a fundamental task in computer vision and plays a critical role in applications like autonomous driving. However, existing algorithms often involve complex designs and multiple loss functions, making model training and deployment challenging. Furthermore, their reliance on fixed probability distribution assumptions (e.g., Laplacian or Gaussian) hinders their ability to adapt to diverse target characteristics such as varying sizes and motion patterns, ultimately affecting tracking precision and robustness. To address these issues, we propose BEVTrack, a simple yet effective motion-based tracking method. BEVTrack directly estimates object motion in Bird's-Eye View (BEV) using a single regression loss. To enhance accuracy for targets with diverse attributes, it learns adaptive likelihood functions tailored to individual targets, avoiding the limitations of fixed distribution assumptions in previous methods. This approach provides valuable priors for tracking and significantly boosts performance. Comprehensive experiments on KITTI, NuScenes, and Waymo Open Dataset demonstrate that BEVTrack achieves state-of-the-art results while operating at 200 FPS, enabling real-time applicability. The code will be released at https://github.com/xmm-prio/BEVTrack.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2309.02185.pdf' target='_blank'>https://arxiv.org/pdf/2309.02185.pdf</a></span>   <span><a href='https://github.com/xmm-prio/BEVTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiang Yang, Yingqi Deng, Mian Pan, Zheng-Jun Zha, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02185">BEVTrack: A Simple and Strong Baseline for 3D Single Object Tracking in Bird's-Eye View</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Single Object Tracking (SOT) is a fundamental task in computer vision and plays a critical role in applications like autonomous driving. However, existing algorithms often involve complex designs and multiple loss functions, making model training and deployment challenging. Furthermore, their reliance on fixed probability distribution assumptions (e.g., Laplacian or Gaussian) hinders their ability to adapt to diverse target characteristics such as varying sizes and motion patterns, ultimately affecting tracking precision and robustness. To address these issues, we propose BEVTrack, a simple yet effective motion-based tracking method. BEVTrack directly estimates object motion in Bird's-Eye View (BEV) using a single regression loss. To enhance accuracy for targets with diverse attributes, it learns adaptive likelihood functions tailored to individual targets, avoiding the limitations of fixed distribution assumptions in previous methods. This approach provides valuable priors for tracking and significantly boosts performance. Comprehensive experiments on KITTI, NuScenes, and Waymo Open Dataset demonstrate that BEVTrack achieves state-of-the-art results while operating at 200 FPS, enabling real-time applicability. The code will be released at https://github.com/xmm-prio/BEVTrack.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2308.15816.pdf' target='_blank'>https://arxiv.org/pdf/2308.15816.pdf</a></span>   <span><a href='https://github.com/BasitAlawode/UWVOT400' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Basit Alawode, Fayaz Ali Dharejo, Mehnaz Ummar, Yuhang Guo, Arif Mahmood, Naoufel Werghi, Fahad Shahbaz Khan, Jiri Matas, Sajid Javed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15816">Improving Underwater Visual Tracking With a Large Scale Dataset and Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a new dataset and general tracker enhancement method for Underwater Visual Object Tracking (UVOT). Despite its significance, underwater tracking has remained unexplored due to data inaccessibility. It poses distinct challenges; the underwater environment exhibits non-uniform lighting conditions, low visibility, lack of sharpness, low contrast, camouflage, and reflections from suspended particles. Performance of traditional tracking methods designed primarily for terrestrial or open-air scenarios drops in such conditions. We address the problem by proposing a novel underwater image enhancement algorithm designed specifically to boost tracking quality. The method has resulted in a significant performance improvement, of up to 5.0% AUC, of state-of-the-art (SOTA) visual trackers. To develop robust and accurate UVOT methods, large-scale datasets are required. To this end, we introduce a large-scale UVOT benchmark dataset consisting of 400 video segments and 275,000 manually annotated frames enabling underwater training and evaluation of deep trackers. The videos are labelled with several underwater-specific tracking attributes including watercolor variation, target distractors, camouflage, target relative size, and low visibility conditions. The UVOT400 dataset, tracking results, and the code are publicly available on: https://github.com/BasitAlawode/UWVOT400.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2308.14652.pdf' target='_blank'>https://arxiv.org/pdf/2308.14652.pdf</a></span>   <span><a href='https://github.com/cbellinger27/bendRL_reacher_tracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Colin Bellinger, Laurence Lamarche-Cliche
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14652">Learning Visual Tracking and Reaching with Deep Reinforcement Learning on a UR10e Robotic Arm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As technology progresses, industrial and scientific robots are increasingly being used in diverse settings. In many cases, however, programming the robot to perform such tasks is technically complex and costly. To maximize the utility of robots in industrial and scientific settings, they require the ability to quickly shift from one task to another. Reinforcement learning algorithms provide the potential to enable robots to learn optimal solutions to complete new tasks without directly reprogramming them. The current state-of-the-art in reinforcement learning, however, generally relies on fast simulations and parallelization to achieve optimal performance. These are often not possible in robotics applications. Thus, a significant amount of research is required to facilitate the efficient and safe, training and deployment of industrial and scientific reinforcement learning robots. This technical report outlines our initial research into the application of deep reinforcement learning on an industrial UR10e robot. The report describes the reinforcement learning environments created to facilitate policy learning with the UR10e, a robotic arm from Universal Robots, and presents our initial results in training deep Q-learning and proximal policy optimization agents on the developed reinforcement learning environments. Our results show that proximal policy optimization learns a better, more stable policy with less data than deep Q-learning. The corresponding code for this work is available at \url{https://github.com/cbellinger27/bendRL_reacher_tracker}
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2308.14392.pdf' target='_blank'>https://arxiv.org/pdf/2308.14392.pdf</a></span>   <span><a href='https://github.com/zhang-tao-whu/DVIS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Zhang, Xingye Tian, Yikang Zhou, Yu Wu, Shunping Ji, Cilin Yan, Xuebo Wang, Xin Tao, Yuan Zhang, Pengfei Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14392">1st Place Solution for the 5th LSVOS Challenge: Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video instance segmentation is a challenging task that serves as the cornerstone of numerous downstream applications, including video editing and autonomous driving. In this report, we present further improvements to the SOTA VIS method, DVIS. First, we introduce a denoising training strategy for the trainable tracker, allowing it to achieve more stable and accurate object tracking in complex and long videos. Additionally, we explore the role of visual foundation models in video instance segmentation. By utilizing a frozen VIT-L model pre-trained by DINO v2, DVIS demonstrates remarkable performance improvements. With these enhancements, our method achieves 57.9 AP and 56.0 AP in the development and test phases, respectively, and ultimately ranked 1st in the VIS track of the 5th LSVOS Challenge. The code will be available at https://github.com/zhang-tao-whu/DVIS.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2308.13266.pdf' target='_blank'>https://arxiv.org/pdf/2308.13266.pdf</a></span>   <span><a href='https://github.com/yoxu515/MITS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyou Xu, Zongxin Yang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13266">Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking any given object(s) spatially and temporally is a common purpose in Visual Object Tracking (VOT) and Video Object Segmentation (VOS). Joint tracking and segmentation have been attempted in some studies but they often lack full compatibility of both box and mask in initialization and prediction, and mainly focus on single-object scenarios. To address these limitations, this paper proposes a Multi-object Mask-box Integrated framework for unified Tracking and Segmentation, dubbed MITS. Firstly, the unified identification module is proposed to support both box and mask reference for initialization, where detailed object information is inferred from boxes or directly retained from masks. Additionally, a novel pinpoint box predictor is proposed for accurate multi-object box prediction, facilitating target-oriented representation learning. All target objects are processed simultaneously from encoding to propagation and decoding, as a unified pipeline for VOT and VOS. Experimental results show MITS achieves state-of-the-art performance on both VOT and VOS benchmarks. Notably, MITS surpasses the best prior VOT competitor by around 6% on the GOT-10k test set, and significantly improves the performance of box initialization on VOS benchmarks. The code is available at https://github.com/yoxu515/MITS.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2308.12035.pdf' target='_blank'>https://arxiv.org/pdf/2308.12035.pdf</a></span>   <span><a href='https://github.com/shuheikurita/RefEgo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhei Kurita, Naoki Katsura, Eri Onami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12035">RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Grounding textual expressions on scene objects from first-person views is a truly demanding capability in developing agents that are aware of their surroundings and behave following intuitive text instructions. Such capability is of necessity for glass-devices or autonomous robots to localize referred objects in the real-world. In the conventional referring expression comprehension tasks of images, however, datasets are mostly constructed based on the web-crawled data and don't reflect diverse real-world structures on the task of grounding textual expressions in diverse objects in the real world. Recently, a massive-scale egocentric video dataset of Ego4D was proposed. Ego4D covers around the world diverse real-world scenes including numerous indoor and outdoor situations such as shopping, cooking, walking, talking, manufacturing, etc. Based on egocentric videos of Ego4D, we constructed a broad coverage of the video-based referring expression comprehension dataset: RefEgo. Our dataset includes more than 12k video clips and 41 hours for video-based referring expression comprehension annotation. In experiments, we combine the state-of-the-art 2D referring expression comprehension models with the object tracking algorithm, achieving the video-wise referred object tracking even in difficult conditions: the referred object becomes out-of-frame in the middle of the video or multiple similar objects are presented in the video. Codes are available at https://github.com/shuheikurita/RefEgo
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2308.11875.pdf' target='_blank'>https://arxiv.org/pdf/2308.11875.pdf</a></span>   <span><a href='https://github.com/LeoZhiheng/MTM-Tracker.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Li, Yu Lin, Yubo Cui, Shuo Li, Zheng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11875">Motion-to-Matching: A Mixed Paradigm for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking with LiDAR points is an important task in the computer vision field. Previous methods usually adopt the matching-based or motion-centric paradigms to estimate the current target status. However, the former is sensitive to the similar distractors and the sparseness of point cloud due to relying on appearance matching, while the latter usually focuses on short-term motion clues (eg. two frames) and ignores the long-term motion pattern of target. To address these issues, we propose a mixed paradigm with two stages, named MTM-Tracker, which combines motion modeling with feature matching into a single network. Specifically, in the first stage, we exploit the continuous historical boxes as motion prior and propose an encoder-decoder structure to locate target coarsely. Then, in the second stage, we introduce a feature interaction module to extract motion-aware features from consecutive point clouds and match them to refine target movement as well as regress other target states. Extensive experiments validate that our paradigm achieves competitive performance on large-scale datasets (70.9% in KITTI and 51.70% in NuScenes). The code will be open soon at https://github.com/LeoZhiheng/MTM-Tracker.git.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2308.11607.pdf' target='_blank'>https://arxiv.org/pdf/2308.11607.pdf</a></span>   <span><a href='https://github.com/kuanchihhuang/MoMA-M3T' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/kuanchihhuang/MoMA-M3T' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuan-Chih Huang, Ming-Hsuan Yang, Yi-Hsuan Tsai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11607">Delving into Motion-Aware Matching for Monocular 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances of monocular 3D object detection facilitate the 3D multi-object tracking task based on low-cost camera sensors. In this paper, we find that the motion cue of objects along different time frames is critical in 3D multi-object tracking, which is less explored in existing monocular-based approaches. In this paper, we propose a motion-aware framework for monocular 3D MOT. To this end, we propose MoMA-M3T, a framework that mainly consists of three motion-aware components. First, we represent the possible movement of an object related to all object tracklets in the feature space as its motion features. Then, we further model the historical object tracklet along the time frame in a spatial-temporal perspective via a motion transformer. Finally, we propose a motion-aware matching module to associate historical object tracklets and current observations as final tracking results. We conduct extensive experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T achieves competitive performance against state-of-the-art methods. Moreover, the proposed tracker is flexible and can be easily plugged into existing image-based 3D object detectors without re-training. Code and models are available at https://github.com/kuanchihhuang/MoMA-M3T.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2308.11157.pdf' target='_blank'>https://arxiv.org/pdf/2308.11157.pdf</a></span>   <span><a href='https://github.com/holmescao/TOPICTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Cao, Yiyao Zheng, Yao Yao, Huapeng Qin, Xiaoyu Cao, Shihui Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11157">TOPIC: A Parallel Association Paradigm for Multi-Object Tracking under Complex Motions and Diverse Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video data and algorithms have been driving advances in multi-object tracking (MOT). While existing MOT datasets focus on occlusion and appearance similarity, complex motion patterns are widespread yet overlooked. To address this issue, we introduce a new dataset called BEE24 to highlight complex motions. Identity association algorithms have long been the focus of MOT research. Existing trackers can be categorized into two association paradigms: single-feature paradigm (based on either motion or appearance feature) and serial paradigm (one feature serves as secondary while the other is primary). However, these paradigms are incapable of fully utilizing different features. In this paper, we propose a parallel paradigm and present the Two rOund Parallel matchIng meChanism (TOPIC) to implement it. The TOPIC leverages both motion and appearance features and can adaptively select the preferable one as the assignment metric based on motion level. Moreover, we provide an Attention-based Appearance Reconstruction Module (AARM) to reconstruct appearance feature embeddings, thus enhancing the representation of appearance features. Comprehensive experiments show that our approach achieves state-of-the-art performance on four public datasets and BEE24. Moreover, BEE24 challenges existing trackers to track multiple similar-appearing small objects with complex motions over long periods, which is critical in real-world applications such as beekeeping and drone swarm surveillance. Notably, our proposed parallel paradigm surpasses the performance of existing association paradigms by a large margin, e.g., reducing false negatives by 6% to 81% compared to the single-feature association paradigm. The introduced dataset and association paradigm in this work offer a fresh perspective for advancing the MOT field. The source code and dataset are available at https://github.com/holmescao/TOPICTrack.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2308.10330.pdf' target='_blank'>https://arxiv.org/pdf/2308.10330.pdf</a></span>   <span><a href='https://github.com/vision4robotics/TCTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziang Cao, Ziyuan Huang, Liang Pan, Shiwei Zhang, Ziwei Liu, Changhong Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10330">Towards Real-World Visual Tracking with Temporal Contexts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual tracking has made significant improvements in the past few decades. Most existing state-of-the-art trackers 1) merely aim for performance in ideal conditions while overlooking the real-world conditions; 2) adopt the tracking-by-detection paradigm, neglecting rich temporal contexts; 3) only integrate the temporal information into the template, where temporal contexts among consecutive frames are far from being fully utilized. To handle those problems, we propose a two-level framework (TCTrack) that can exploit temporal contexts efficiently. Based on it, we propose a stronger version for real-world visual tracking, i.e., TCTrack++. It boils down to two levels: features and similarity maps. Specifically, for feature extraction, we propose an attention-based temporally adaptive convolution to enhance the spatial features using temporal information, which is achieved by dynamically calibrating the convolution weights. For similarity map refinement, we introduce an adaptive temporal transformer to encode the temporal knowledge efficiently and decode it for the accurate refinement of the similarity map. To further improve the performance, we additionally introduce a curriculum learning strategy. Also, we adopt online evaluation to measure performance in real-world conditions. Exhaustive experiments on 8 wellknown benchmarks demonstrate the superiority of TCTrack++. Real-world tests directly verify that TCTrack++ can be readily used in real-world applications.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2308.06974.pdf' target='_blank'>https://arxiv.org/pdf/2308.06974.pdf</a></span>   <span><a href='https://github.com/ganlab/OSTRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiexiong Xu, Weikun Zhao, Zhiyan Tang, Xiangchao Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06974">A One Stop 3D Target Reconstruction and multilevel Segmentation Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D object reconstruction and multilevel segmentation are fundamental to computer vision research. Existing algorithms usually perform 3D scene reconstruction and target objects segmentation independently, and the performance is not fully guaranteed due to the challenge of the 3D segmentation. Here we propose an open-source one stop 3D target reconstruction and multilevel segmentation framework (OSTRA), which performs segmentation on 2D images, tracks multiple instances with segmentation labels in the image sequence, and then reconstructs labelled 3D objects or multiple parts with Multi-View Stereo (MVS) or RGBD-based 3D reconstruction methods. We extend object tracking and 3D reconstruction algorithms to support continuous segmentation labels to leverage the advances in the 2D image segmentation, especially the Segment-Anything Model (SAM) which uses the pretrained neural network without additional training for new scenes, for 3D object segmentation. OSTRA supports most popular 3D object models including point cloud, mesh and voxel, and achieves high performance for semantic segmentation, instance segmentation and part segmentation on several 3D datasets. It even surpasses the manual segmentation in scenes with complex structures and occlusions. Our method opens up a new avenue for reconstructing 3D targets embedded with rich multi-scale segmentation information in complex scenes. OSTRA is available from https://github.com/ganlab/OSTRA.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2308.06635.pdf' target='_blank'>https://arxiv.org/pdf/2308.06635.pdf</a></span>   <span><a href='https://github.com/dsx0511/3DMOTFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuxiao Ding, Eike Rehder, Lukas Schneider, Marius Cordts, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06635">3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking 3D objects accurately and consistently is crucial for autonomous vehicles, enabling more reliable downstream tasks such as trajectory prediction and motion planning. Based on the substantial progress in object detection in recent years, the tracking-by-detection paradigm has become a popular choice due to its simplicity and efficiency. State-of-the-art 3D multi-object tracking (MOT) approaches typically rely on non-learned model-based algorithms such as Kalman Filter but require many manually tuned parameters. On the other hand, learning-based approaches face the problem of adapting the training to the online setting, leading to inevitable distribution mismatch between training and inference as well as suboptimal performance. In this work, we propose 3DMOTFormer, a learned geometry-based 3D MOT framework building upon the transformer architecture. We use an Edge-Augmented Graph Transformer to reason on the track-detection bipartite graph frame-by-frame and conduct data association via edge classification. To reduce the distribution mismatch between training and inference, we propose a novel online training strategy with an autoregressive and recurrent forward pass as well as sequential batch optimization. Using CenterPoint detections, our approach achieves 71.2% and 68.2% AMOTA on the nuScenes validation and test split, respectively. In addition, a trained 3DMOTFormer model generalizes well across different object detectors. Code is available at: https://github.com/dsx0511/3DMOTFormer.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2308.05911.pdf' target='_blank'>https://arxiv.org/pdf/2308.05911.pdf</a></span>   <span><a href='https://github.com/yolomax/ColTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Liu, Junta Wu, Yi Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05911">Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) at low frame rates can reduce computational, storage and power overhead to better meet the constraints of edge devices. Many existing MOT methods suffer from significant performance degradation in low-frame-rate videos due to significant location and appearance changes between adjacent frames. To this end, we propose to explore collaborative tracking learning (ColTrack) for frame-rate-insensitive MOT in a query-based end-to-end manner. Multiple historical queries of the same target jointly track it with richer temporal descriptions. Meanwhile, we insert an information refinement module between every two temporal blocking decoders to better fuse temporal clues and refine features. Moreover, a tracking object consistency loss is proposed to guide the interaction between historical queries. Extensive experimental results demonstrate that in high-frame-rate videos, ColTrack obtains higher performance than state-of-the-art methods on large-scale datasets Dancetrack and BDD100K, and outperforms the existing end-to-end methods on MOT17. More importantly, ColTrack has a significant advantage over state-of-the-art methods in low-frame-rate videos, which allows it to obtain faster processing speeds by reducing frame-rate requirements while maintaining higher performance. Code will be released at https://github.com/yolomax/ColTrack
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2308.05140.pdf' target='_blank'>https://arxiv.org/pdf/2308.05140.pdf</a></span>   <span><a href='https://github.com/dawnyc/ROMTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidong Cai, Jie Liu, Jie Tang, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05140">Robust Object Modeling for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object modeling has become a core part of recent tracking frameworks. Current popular tackers use Transformer attention to extract the template feature separately or interactively with the search region. However, separate template learning lacks communication between the template and search regions, which brings difficulty in extracting discriminative target-oriented features. On the other hand, interactive template learning produces hybrid template features, which may introduce potential distractors to the template via the cluttered search regions. To enjoy the merits of both methods, we propose a robust object modeling framework for visual tracking (ROMTrack), which simultaneously models the inherent template and the hybrid template features. As a result, harmful distractors can be suppressed by combining the inherent features of target objects with search regions' guidance. Target-related features can also be extracted using the hybrid template, thus resulting in a more robust object modeling framework. To further enhance robustness, we present novel variation tokens to depict the ever-changing appearance of target objects. Variation tokens are adaptable to object deformation and appearance variations, which can boost overall performance with negligible computation. Experiments show that our ROMTrack sets a new state-of-the-art on multiple benchmarks.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2308.00783.pdf' target='_blank'>https://arxiv.org/pdf/2308.00783.pdf</a></span>   <span><a href='https://github.com/ymzis69/HybridSORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingzhan Yang, Guangxin Han, Bin Yan, Wenhua Zhang, Jinqing Qi, Huchuan Lu, Dong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00783">Hybrid-SORT: Weak Cues Matter for Online Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) aims to detect and associate all desired objects across frames. Most methods accomplish the task by explicitly or implicitly leveraging strong cues (i.e., spatial and appearance information), which exhibit powerful instance-level discrimination. However, when object occlusion and clustering occur, spatial and appearance information will become ambiguous simultaneously due to the high overlap among objects. In this paper, we demonstrate this long-standing challenge in MOT can be efficiently and effectively resolved by incorporating weak cues to compensate for strong cues. Along with velocity direction, we introduce the confidence and height state as potential weak cues. With superior performance, our method still maintains Simple, Online and Real-Time (SORT) characteristics. Also, our method shows strong generalization for diverse trackers and scenarios in a plug-and-play and training-free manner. Significant and consistent improvements are observed when applying our method to 5 different representative trackers. Further, with both strong and weak cues, our method Hybrid-SORT achieves superior performance on diverse benchmarks, including MOT17, MOT20, and especially DanceTrack where interaction and severe occlusion frequently happen with complex motions. The code and models are available at https://github.com/ymzis69/HybridSORT.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2307.16675.pdf' target='_blank'>https://arxiv.org/pdf/2307.16675.pdf</a></span>   <span><a href='https://github.com/lixiaoyu2000/Poly-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Li, Tao Xie, Dedong Liu, Jinghan Gao, Kun Dai, Zhiqiang Jiang, Lijun Zhao, Ke Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16675">Poly-MOT: A Polyhedral Framework For 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-object tracking (MOT) empowers mobile robots to accomplish well-informed motion planning and navigation tasks by providing motion trajectories of surrounding objects. However, existing 3D MOT methods typically employ a single similarity metric and physical model to perform data association and state estimation for all objects. With large-scale modern datasets and real scenes, there are a variety of object categories that commonly exhibit distinctive geometric properties and motion patterns. In this way, such distinctions would enable various object categories to behave differently under the same standard, resulting in erroneous matches between trajectories and detections, and jeopardizing the reliability of downstream tasks (navigation, etc.). Towards this end, we propose Poly-MOT, an efficient 3D MOT method based on the Tracking-By-Detection framework that enables the tracker to choose the most appropriate tracking criteria for each object category. Specifically, Poly-MOT leverages different motion models for various object categories to characterize distinct types of motion accurately. We also introduce the constraint of the rigid structure of objects into a specific motion model to accurately describe the highly nonlinear motion of the object. Additionally, we introduce a two-stage data association strategy to ensure that objects can find the optimal similarity metric from three custom metrics for their categories and reduce missing matches. On the NuScenes dataset, our proposed method achieves state-of-the-art performance with 75.4\% AMOTA. The code is available at https://github.com/lixiaoyu2000/Poly-MOT
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2307.15700.pdf' target='_blank'>https://arxiv.org/pdf/2307.15700.pdf</a></span>   <span><a href='https://github.com/MCG-NJU/MeMOTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruopeng Gao, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15700">MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a video task, Multiple Object Tracking (MOT) is expected to capture temporal information of targets effectively. Unfortunately, most existing methods only explicitly exploit the object features between adjacent frames, while lacking the capacity to model long-term temporal information. In this paper, we propose MeMOTR, a long-term memory-augmented Transformer for multi-object tracking. Our method is able to make the same object's track embedding more stable and distinguishable by leveraging long-term memory injection with a customized memory-attention layer. This significantly improves the target association ability of our model. Experimental results on DanceTrack show that MeMOTR impressively surpasses the state-of-the-art method by 7.9% and 13.0% on HOTA and AssA metrics, respectively. Furthermore, our model also outperforms other Transformer-based methods on association performance on MOT17 and generalizes well on BDD100K. Code is available at https://github.com/MCG-NJU/MeMOTR.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2307.15409.pdf' target='_blank'>https://arxiv.org/pdf/2307.15409.pdf</a></span>   <span><a href='https://github.com/alibaba/u2mot/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Liu, Sheng Jin, Zhihang Fu, Ze Chen, Rongxin Jiang, Jieping Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15409">Uncertainty-aware Unsupervised Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Without manually annotated identities, unsupervised multi-object trackers are inferior to learning reliable feature embeddings. It causes the similarity-based inter-frame association stage also be error-prone, where an uncertainty problem arises. The frame-by-frame accumulated uncertainty prevents trackers from learning the consistent feature embedding against time variation. To avoid this uncertainty problem, recent self-supervised techniques are adopted, whereas they failed to capture temporal relations. The interframe uncertainty still exists. In fact, this paper argues that though the uncertainty problem is inevitable, it is possible to leverage the uncertainty itself to improve the learned consistency in turn. Specifically, an uncertainty-based metric is developed to verify and rectify the risky associations. The resulting accurate pseudo-tracklets boost learning the feature consistency. And accurate tracklets can incorporate temporal information into spatial transformation. This paper proposes a tracklet-guided augmentation strategy to simulate tracklets' motion, which adopts a hierarchical uncertainty-based sampling mechanism for hard sample mining. The ultimate unsupervised MOT framework, namely U2MOT, is proven effective on MOT-Challenges and VisDrone-MOT benchmark. U2MOT achieves a SOTA performance among the published supervised and unsupervised trackers.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2307.14630.pdf' target='_blank'>https://arxiv.org/pdf/2307.14630.pdf</a></span>   <span><a href='https://github.com/HuajianUP/360VOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huajian Huang, Yinzhe Xu, Yingshu Chen, Sai-Kit Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14630">360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>360Â° images can provide an omnidirectional field of view which is important for stable and long-term scene perception. In this paper, we explore 360Â° images for visual object tracking and perceive new challenges caused by large distortion, stitching artifacts, and other unique attributes of 360Â° images. To alleviate these problems, we take advantage of novel representations of target localization, i.e., bounding field-of-view, and then introduce a general 360 tracking framework that can adopt typical trackers for omnidirectional tracking. More importantly, we propose a new large-scale omnidirectional tracking benchmark dataset, 360VOT, in order to facilitate future research. 360VOT contains 120 sequences with up to 113K high-resolution frames in equirectangular projection. The tracking targets cover 32 categories in diverse scenarios. Moreover, we provide 4 types of unbiased ground truth, including (rotated) bounding boxes and (rotated) bounding field-of-views, as well as new metrics tailored for 360Â° images which allow for the accurate evaluation of omnidirectional tracking performance. Finally, we extensively evaluated 20 state-of-the-art visual trackers and provided a new baseline for future comparisons. Homepage: https://360vot.hkustvgd.com
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2307.13974.pdf' target='_blank'>https://arxiv.org/pdf/2307.13974.pdf</a></span>   <span><a href='https://github.com/jiawen-zhu/HQTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Zhu, Zhenyu Chen, Zeqi Hao, Shijie Chang, Lu Zhang, Dong Wang, Huchuan Lu, Bin Luo, Jun-Yan He, Jin-Peng Lan, Hanyuan Chen, Chenyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13974">Tracking Anything in High Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking is a fundamental video task in computer vision. Recently, the notably increasing power of perception algorithms allows the unification of single/multiobject and box/mask-based tracking. Among them, the Segment Anything Model (SAM) attracts much attention. In this report, we propose HQTrack, a framework for High Quality Tracking anything in videos. HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). Given the object to be tracked in the initial frame of a video, VMOS propagates the object masks to the current frame. The mask results at this stage are not accurate enough since VMOS is trained on several closeset video object segmentation (VOS) datasets, which has limited ability to generalize to complex and corner scenes. To further improve the quality of tracking masks, a pretrained MR model is employed to refine the tracking results. As a compelling testament to the effectiveness of our paradigm, without employing any tricks such as test-time data augmentations and model ensemble, HQTrack ranks the 2nd place in the Visual Object Tracking and Segmentation (VOTS2023) challenge. Code and models are available at https://github.com/jiawen-zhu/HQTrack.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2307.05721.pdf' target='_blank'>https://arxiv.org/pdf/2307.05721.pdf</a></span>   <span><a href='https://iai-hrc.github.io/ha-vid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Zheng, Regina Lee, Yuqian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05721">HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding comprehensive assembly knowledge from videos is critical for futuristic ultra-intelligent industry. To enable technological breakthrough, we present HA-ViD - the first human assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view, multi-modality videos (each video contains one assembly task), 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2306.13518.pdf' target='_blank'>https://arxiv.org/pdf/2306.13518.pdf</a></span>   <span><a href='https://github.com/NanH5837/LettuceMOTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Hu, Daobilige Su, Shuo Wang, Xuechang Wang, Huiyu Zhong, Zimeng Wang, Yongliang Qiao, Yu Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13518">Segmentation and Tracking of Vegetable Plants by Exploiting Vegetable Shape Feature for Precision Spray of Agricultural Robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing deployment of agricultural robots, the traditional manual spray of liquid fertilizer and pesticide is gradually being replaced by agricultural robots. For robotic precision spray application in vegetable farms, accurate plant phenotyping through instance segmentation and robust plant tracking are of great importance and a prerequisite for the following spray action. Regarding the robust tracking of vegetable plants, to solve the challenging problem of associating vegetables with similar color and texture in consecutive images, in this paper, a novel method of Multiple Object Tracking and Segmentation (MOTS) is proposed for instance segmentation and tracking of multiple vegetable plants. In our approach, contour and blob features are extracted to describe unique feature of each individual vegetable, and associate the same vegetables in different images. By assigning a unique ID for each vegetable, it ensures the robot to spray each vegetable exactly once, while traversing along the farm rows. Comprehensive experiments including ablation studies are conducted, which prove its superior performance over two State-Of-The-Art (SOTA) MOTS methods. Compared to the conventional MOTS methods, the proposed method is able to re-identify objects which have gone out of the camera field of view and re-appear again using the proposed data association strategy, which is important to ensure each vegetable be sprayed only once when the robot travels back and forth. Although the method is tested on lettuce farm, it can be applied to other similar vegetables such as broccoli and canola. Both code and the dataset of this paper is publicly released for the benefit of the community: https://github.com/NanH5837/LettuceMOTS.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2306.13074.pdf' target='_blank'>https://arxiv.org/pdf/2306.13074.pdf</a></span>   <span><a href='https://github.com/hsiangwei0903/Deep-EIoU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiang-Wei Huang, Cheng-Yen Yang, Jiacheng Sun, Pyong-Kun Kim, Kwang-Ju Kim, Kyoungoh Lee, Chung-I Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.13074">Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based object detectors have driven notable progress in multi-object tracking algorithms. Yet, current tracking methods mainly focus on simple, regular motion patterns in pedestrians or vehicles. This leaves a gap in tracking algorithms for targets with nonlinear, irregular motion, like athletes. Additionally, relying on the Kalman filter in recent tracking algorithms falls short when object motion defies its linear assumption. To overcome these issues, we propose a novel online and robust multi-object tracking approach named deep ExpansionIoU (Deep-EIoU), which focuses on multi-object tracking for sports scenarios. Unlike conventional methods, we abandon the use of the Kalman filter and leverage the iterative scale-up ExpansionIoU and deep features for robust tracking in sports scenarios. This approach achieves superior tracking performance without adopting a more robust detector, all while keeping the tracking process in an online fashion. Our proposed method demonstrates remarkable effectiveness in tracking irregular motion objects, achieving a score of 77.2% HOTA on the SportsMOT dataset and 85.4% HOTA on the SoccerNet-Tracking dataset. It outperforms all previous state-of-the-art trackers on various large-scale multi-object tracking benchmarks, covering various kinds of sports scenarios. The code and models are available at https://github.com/hsiangwei0903/Deep-EIoU.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2306.05888.pdf' target='_blank'>https://arxiv.org/pdf/2306.05888.pdf</a></span>   <span><a href='https://github.com/poodarchu/EFG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuesong Chen, Shaoshuai Shi, Chao Zhang, Benjin Zhu, Qiang Wang, Ka Chun Cheung, Simon See, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05888">TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking (MOT) is vital for many applications including autonomous driving vehicles and service robots. With the commonly used tracking-by-detection paradigm, 3D MOT has made important progress in recent years. However, these methods only use the detection boxes of the current frame to obtain trajectory-box association results, which makes it impossible for the tracker to recover objects missed by the detector. In this paper, we present TrajectoryFormer, a novel point-cloud-based 3D MOT framework. To recover the missed object by detector, we generates multiple trajectory hypotheses with hybrid candidate boxes, including temporally predicted boxes and current-frame detection boxes, for trajectory-box association. The predicted boxes can propagate object's history trajectory information to the current frame and thus the network can tolerate short-term miss detection of the tracked objects. We combine long-term object motion feature and short-term object appearance feature to create per-hypothesis feature embedding, which reduces the computational overhead for spatial-temporal encoding. Additionally, we introduce a Global-Local Interaction Module to conduct information interaction among all hypotheses and models their spatial relations, leading to accurate estimation of hypotheses. Our TrajectoryFormer achieves state-of-the-art performance on the Waymo 3D MOT benchmarks. Code is available at https://github.com/poodarchu/EFG .
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2306.05238.pdf' target='_blank'>https://arxiv.org/pdf/2306.05238.pdf</a></span>   <span><a href='https://github.com/hustvl/SparseTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zelin Liu, Xinggang Wang, Cheng Wang, Wenyu Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05238">SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exploring robust and efficient association methods has always been an important issue in multiple-object tracking (MOT). Although existing tracking methods have achieved impressive performance, congestion and frequent occlusions still pose challenging problems in multi-object tracking. We reveal that performing sparse decomposition on dense scenes is a crucial step to enhance the performance of associating occluded targets. To this end, we propose a pseudo-depth estimation method for obtaining the relative depth of targets from 2D images. Secondly, we design a depth cascading matching (DCM) algorithm, which can use the obtained depth information to convert a dense target set into multiple sparse target subsets and perform data association on these sparse target subsets in order from near to far. By integrating the pseudo-depth method and the DCM strategy into the data association process, we propose a new tracker, called SparseTrack. SparseTrack provides a new perspective for solving the challenging crowded scene MOT problem. Only using IoU matching, SparseTrack achieves comparable performance with the state-of-the-art (SOTA) methods on the MOT17 and MOT20 benchmarks. Code and models are publicly available at \url{https://github.com/hustvl/SparseTrack}.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2306.04633.pdf' target='_blank'>https://arxiv.org/pdf/2306.04633.pdf</a></span>   <span><a href='https://github.com/yashbhalgat/Contrastive-Lift' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yash Bhalgat, Iro Laina, JoÃ£o F. Henriques, Andrew Zisserman, Andrea Vedaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04633">Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as on our newly created Messy Rooms dataset, demonstrating the effectiveness and scalability of our slow-fast clustering method.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2305.17648.pdf' target='_blank'>https://arxiv.org/pdf/2305.17648.pdf</a></span>   <span><a href='https://fsoft-aic.github.io/Z-GMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kim Hoang Tran, Anh Duy Le Dinh, Tien Phat Nguyen, Thinh Phan, Pha Nguyen, Khoa Luu, Donald Adjeroh, Gianfranco Doretto, Ngan Hoang Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17648">Z-GMOT: Zero-shot Generic Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent significant progress, Multi-Object Tracking (MOT) faces limitations such as reliance on prior knowledge and predefined categories and struggles with unseen objects. To address these issues, Generic Multiple Object Tracking (GMOT) has emerged as an alternative approach, requiring less prior information. However, current GMOT methods often rely on initial bounding boxes and struggle to handle variations in factors such as viewpoint, lighting, occlusion, and scale, among others. Our contributions commence with the introduction of the \textit{Referring GMOT dataset} a collection of videos, each accompanied by detailed textual descriptions of their attributes. Subsequently, we propose $\mathtt{Z-GMOT}$, a cutting-edge tracking solution capable of tracking objects from \textit{never-seen categories} without the need of initial bounding boxes or predefined categories. Within our $\mathtt{Z-GMOT}$ framework, we introduce two novel components: (i) $\mathtt{iGLIP}$, an improved Grounded language-image pretraining, for accurately detecting unseen objects with specific characteristics. (ii) $\mathtt{MA-SORT}$, a novel object association approach that adeptly integrates motion and appearance-based matching strategies to tackle the complex task of tracking objects with high similarity. Our contributions are benchmarked through extensive experiments conducted on the Referring GMOT dataset for GMOT task. Additionally, to assess the generalizability of the proposed $\mathtt{Z-GMOT}$, we conduct ablation studies on the DanceTrack and MOT20 datasets for the MOT task. Our dataset, code, and models are released at: https://fsoft-aic.github.io/Z-GMOT.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2305.13495.pdf' target='_blank'>https://arxiv.org/pdf/2305.13495.pdf</a></span>   <span><a href='https://uark-cviu.github.io/Type-to-Track/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pha Nguyen, Kha Gia Quach, Kris Kitani, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13495">Type-to-Track: Retrieve Any Object via Prompt-based Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One of the recent trends in vision problems is to use natural language captions to describe the objects of interest. This approach can overcome some limitations of traditional methods that rely on bounding boxes or category annotations. This paper introduces a novel paradigm for Multiple Object Tracking called Type-to-Track, which allows users to track objects in videos by typing natural language descriptions. We present a new dataset for that Grounded Multiple Object Tracking task, called GroOT, that contains videos with various types of objects and their corresponding textual captions describing their appearance and action in detail. Additionally, we introduce two new evaluation protocols and formulate evaluation metrics specifically for this task. We develop a new efficient method that models a transformer-based eMbed-ENcoDE-extRact framework (MENDER) using the third-order tensor decomposition. The experiments in five scenarios show that our MENDER approach outperforms another two-stage design in terms of accuracy and efficiency, up to 14.7% accuracy and 4$\times$ speed faster.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2305.07290.pdf' target='_blank'>https://arxiv.org/pdf/2305.07290.pdf</a></span>   <span><a href='https://anti-uav.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Zhao, Jianan Li, Lei Jin, Jiaming Chu, Zhihao Zhang, Jun Wang, Jiangqiang Xia, Kai Wang, Yang Liu, Sadaf Gulshad, Jiaojiao Zhao, Tianyang Xu, Xuefeng Zhu, Shihan Liu, Zheng Zhu, Guibo Zhu, Zechao Li, Zheng Wang, Baigui Sun, Yandong Guo, Shin ichi Satoh, Junliang Xing, Jane Shen Shengmei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.07290">The 3rd Anti-UAV Workshop & Challenge: Methods and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 3rd Anti-UAV Workshop & Challenge aims to encourage research in developing novel and accurate methods for multi-scale object tracking. The Anti-UAV dataset used for the Anti-UAV Challenge has been publicly released. There are two main differences between this year's competition and the previous two. First, we have expanded the existing dataset, and for the first time, released a training set so that participants can focus on improving their models. Second, we set up two tracks for the first time, i.e., Anti-UAV Tracking and Anti-UAV Detection & Tracking. Around 76 participating teams from the globe competed in the 3rd Anti-UAV Challenge. In this paper, we provide a brief summary of the 3rd Anti-UAV Workshop & Challenge including brief introductions to the top three methods in each track. The submission leaderboard will be reopened for researchers that are interested in the Anti-UAV challenge. The benchmark dataset and other information can be found at: https://anti-uav.github.io/.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2305.06558.pdf' target='_blank'>https://arxiv.org/pdf/2305.06558.pdf</a></span>   <span><a href='https://github.com/z-x-yang/Segment-and-Track-Anything' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06558">Segment and Track Anything</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents a framework called Segment And Track Anything (SAMTrack) that allows users to precisely and effectively segment and track any object in a video. Additionally, SAM-Track employs multimodal interaction methods that enable users to select multiple objects in videos for tracking, corresponding to their specific requirements. These interaction methods comprise click, stroke, and text, each possessing unique benefits and capable of being employed in combination. As a result, SAM-Track can be used across an array of fields, ranging from drone technology, autonomous driving, medical imaging, augmented reality, to biological analysis. SAM-Track amalgamates Segment Anything Model (SAM), an interactive key-frame segmentation model, with our proposed AOT-based tracking model (DeAOT), which secured 1st place in four tracks of the VOT 2022 challenge, to facilitate object tracking in video. In addition, SAM-Track incorporates Grounding-DINO, which enables the framework to support text-based interaction. We have demonstrated the remarkable capabilities of SAM-Track on DAVIS-2016 Val (92.0%), DAVIS-2017 Test (79.2%)and its practicability in diverse applications. The project page is available at: https://github.com/z-x-yang/Segment-and-Track-Anything.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2305.04247.pdf' target='_blank'>https://arxiv.org/pdf/2305.04247.pdf</a></span>   <span><a href='https://github.com/Ning-D/Drone_BD_ControlArea' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Ding, Kazuya Takeda, Wenhui Jin, Yingjiu Bei, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.04247">Estimation of control area in badminton doubles with pose information from top and back view drone videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The application of visual tracking to the performance analysis of sports players in dynamic competitions is vital for effective coaching. In doubles matches, coordinated positioning is crucial for maintaining control of the court and minimizing opponents' scoring opportunities. The analysis of such teamwork plays a vital role in understanding the dynamics of the game. However, previous studies have primarily focused on analyzing and assessing singles players without considering occlusion in broadcast videos. These studies have relied on discrete representations, which involve the analysis and representation of specific actions (e.g., strokes) or events that occur during the game while overlooking the meaningful spatial distribution. In this work, we present the first annotated drone dataset from top and back views in badminton doubles and propose a framework to estimate the control area probability map, which can be used to evaluate teamwork performance. We present an efficient framework of deep neural networks that enables the calculation of full probability surfaces. This framework utilizes the embedding of a Gaussian mixture map of players' positions and employs graph convolution on their poses. In the experiment, we verify our approach by comparing various baselines and discovering the correlations between the score and control area. Additionally, we propose a practical application for assessing optimal positioning to provide instructions during a game. Our approach offers both visual and quantitative evaluations of players' movements, thereby providing valuable insights into doubles teamwork. The dataset and related project code is available at https://github.com/Ning-D/Drone_BD_ControlArea
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2305.00406.pdf' target='_blank'>https://arxiv.org/pdf/2305.00406.pdf</a></span>   <span><a href='https://github.com/tiev-tongji/LIMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyang Zhu, Junqiao Zhao, Kai Huang, Xuebo Tian, Jiaye Lin, Chen Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00406">LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous localization and mapping (SLAM) is critical to the implementation of autonomous driving. Most LiDAR-inertial SLAM algorithms assume a static environment, leading to unreliable localization in dynamic environments. Moreover, the accurate tracking of moving objects is of great significance for the control and planning of autonomous vehicles. This study proposes LIMOT, a tightly-coupled multi-object tracking and LiDAR-inertial odometry system that is capable of accurately estimating the poses of both ego-vehicle and objects. We propose a trajectory-based dynamic feature filtering method, which filters out features belonging to moving objects by leveraging tracking results before scan-matching. Factor graph-based optimization is then conducted to optimize the bias of the IMU and the poses of both the ego-vehicle and surrounding objects in a sliding window. Experiments conducted on the KITTI tracking dataset and self-collected dataset show that our method achieves better pose and tracking accuracy than our previous work DL-SLOT and other baseline methods. Our open-source implementation is available at https://github.com/tiev-tongji/LIMOT.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2304.14394.pdf' target='_blank'>https://arxiv.org/pdf/2304.14394.pdf</a></span>   <span><a href='https://github.com/chenxin-dlut/SeqTrackv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Chen, Ben Kang, Jiawen Zhu, Dong Wang, Houwen Peng, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14394">Unified Sequence-to-Sequence Learning for Single- and Multi-Modal Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a new sequence-to-sequence learning framework for RGB-based and multi-modal object tracking. First, we present SeqTrack for RGB-based tracking. It casts visual tracking as a sequence generation task, forecasting object bounding boxes in an autoregressive manner. This differs from previous trackers, which depend on the design of intricate head networks, such as classification and regression heads. SeqTrack employs a basic encoder-decoder transformer architecture. The encoder utilizes a bidirectional transformer for feature extraction, while the decoder generates bounding box sequences autoregressively using a causal transformer. The loss function is a plain cross-entropy. Second, we introduce SeqTrackv2, a unified sequence-to-sequence framework for multi-modal tracking tasks. Expanding upon SeqTrack, SeqTrackv2 integrates a unified interface for auxiliary modalities and a set of task-prompt tokens to specify the task. This enables it to manage multi-modal tracking tasks using a unified model and parameter set. This sequence learning paradigm not only simplifies the tracking framework, but also showcases superior performance across 14 challenging benchmarks spanning five single- and multi-modal tracking tasks. The code and models are available at https://github.com/chenxin-dlut/SeqTrackv2.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2304.11968.pdf' target='_blank'>https://arxiv.org/pdf/2304.11968.pdf</a></span>   <span><a href='https://github.com/gaomingqi/Track-Anything' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, Feng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11968">Track Anything: Segment Anything Meets Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with very little human participation, i.e., several clicks, people can track anything they are interested in, and get satisfactory results in one-pass inference. Without additional training, such an interactive design performs impressively on video object tracking and segmentation. All resources are available on {https://github.com/gaomingqi/Track-Anything}. We hope this work can facilitate related research.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2304.11584.pdf' target='_blank'>https://arxiv.org/pdf/2304.11584.pdf</a></span>   <span><a href='https://github.com/haooozi/OSP2B' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/haooozi/OSP2B' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Nie, Zhiwei He, Yuxiang Yang, Zhengyi Bao, Mingyu Gao, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11584">OSP2B: One-Stage Point-to-Box Network for 3D Siamese Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Two-stage point-to-box network acts as a critical role in the recent popular 3D Siamese tracking paradigm, which first generates proposals and then predicts corresponding proposal-wise scores. However, such a network suffers from tedious hyper-parameter tuning and task misalignment, limiting the tracking performance. Towards these concerns, we propose a simple yet effective one-stage point-to-box network for point cloud-based 3D single object tracking. It synchronizes 3D proposal generation and center-ness score prediction by a parallel predictor without tedious hyper-parameters. To guide a task-aligned score ranking of proposals, a center-aware focal loss is proposed to supervise the training of the center-ness branch, which enhances the network's discriminative ability to distinguish proposals of different quality. Besides, we design a binary target classifier to identify target-relevant points. By integrating the derived classification scores with the center-ness scores, the resulting network can effectively suppress interference proposals and further mitigate task misalignment. Finally, we present a novel one-stage Siamese tracker OSP2B equipped with the designed network. Extensive experiments on challenging benchmarks including KITTI and Waymo SOT Dataset show that our OSP2B achieves leading performance with a considerable real-time speed.Code will be available at https://github.com/haooozi/OSP2B.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2304.08709.pdf' target='_blank'>https://arxiv.org/pdf/2304.08709.pdf</a></span>   <span><a href='https://github.com/wangxiyang2022/YONTD-MOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyang Wang, Chunyun Fu, Jiawei He, Mingguang Huang, Ting Meng, Siyu Zhang, Hangning Zhou, Ziyao Xu, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08709">You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the classical tracking-by-detection (TBD) paradigm, detection and tracking are separately and sequentially conducted, and data association must be properly performed to achieve satisfactory tracking performance. In this paper, a new end-to-end multi-object tracking framework is proposed, which integrates object detection and multi-object tracking into a single model. The proposed tracking framework eliminates the complex data association process in the classical TBD paradigm, and requires no additional training. Secondly, the regression confidence of historical trajectories is investigated, and the possible states of a trajectory (weak object or strong object) in the current frame are predicted. Then, a confidence fusion module is designed to guide non-maximum suppression for trajectories and detections to achieve ordered and robust tracking. Thirdly, by integrating historical trajectory features, the regression performance of the detector is enhanced, which better reflects the occlusion and disappearance patterns of objects in real world. Lastly, extensive experiments are conducted on the commonly used KITTI and Waymo datasets. The results show that the proposed framework can achieve robust tracking by using only a 2D detector and a 3D detector, and it is proven more accurate than many of the state-of-the-art TBD-based multi-modal tracking methods. The source codes of the proposed method are available at https://github.com/wangxiyang2022/YONTD-MOT.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2304.05170.pdf' target='_blank'>https://arxiv.org/pdf/2304.05170.pdf</a></span>   <span><a href='https://deeperaction.github.io/datasets/sportsmot.html' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutao Cui, Chenkai Zeng, Xiaoyu Zhao, Yichun Yang, Gangshan Wu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.05170">SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking in sports scenes plays a critical role in gathering players statistics, supporting further analysis, such as automatic tactical analysis. Yet existing MOT benchmarks cast little attention on the domain, limiting its development. In this work, we present a new large-scale multi-object tracking dataset in diverse sports scenes, coined as \emph{SportsMOT}, where all players on the court are supposed to be tracked. It consists of 240 video sequences, over 150K frames (almost 15\times MOT17) and over 1.6M bounding boxes (3\times MOT17) collected from 3 sports categories, including basketball, volleyball and football. Our dataset is characterized with two key properties: 1) fast and variable-speed motion and 2) similar yet distinguishable appearance. We expect SportsMOT to encourage the MOT trackers to promote in both motion-based association and appearance-based association. We benchmark several state-of-the-art trackers and reveal the key challenge of SportsMOT lies in object association. To alleviate the issue, we further propose a new multi-object tracking framework, termed as \emph{MixSort}, introducing a MixFormer-like structure as an auxiliary association model to prevailing tracking-by-detection trackers. By integrating the customized appearance-based association with the original motion-based association, MixSort achieves state-of-the-art performance on SportsMOT and MOT17. Based on MixSort, we give an in-depth analysis and provide some profound insights into SportsMOT. The dataset and code will be available at https://deeperaction.github.io/datasets/sportsmot.html.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2304.00242.pdf' target='_blank'>https://arxiv.org/pdf/2304.00242.pdf</a></span>   <span><a href='https://github.com/haooozi/GLT-T' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Nie, Zhiwei He, Yuxiang Yang, Xudong Lv, Mingyu Gao, Jing Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00242">GLT-T++: Global-Local Transformer for 3D Siamese Tracking with Ranking Loss</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Siamese trackers based on 3D region proposal network (RPN) have shown remarkable success with deep Hough voting. However, using a single seed point feature as the cue for voting fails to produce high-quality 3D proposals. Additionally, the equal treatment of seed points in the voting process, regardless of their significance, exacerbates this limitation. To address these challenges, we propose a novel transformer-based voting scheme to generate better proposals. Specifically, a global-local transformer (GLT) module is devised to integrate object- and patch-aware geometric priors into seed point features, resulting in robust and accurate cues for offset learning of seed points. To train the GLT module, we introduce an importance prediction branch that learns the potential importance weights of seed points as a training constraint. Incorporating this transformer-based voting scheme into 3D RPN, a novel Siamese method dubbed GLT-T is developed for 3D single object tracking on point clouds. Moreover, we identify that the highest-scored proposal in the Siamese paradigm may not be the most accurate proposal, which limits tracking performance. Towards this concern, we approach the binary score prediction task as a ranking problem, and design a target-aware ranking loss and a localization-aware ranking loss to produce accurate ranking of proposals. With the ranking losses, we further present GLT-T++, an enhanced version of GLT-T. Extensive experiments on multiple benchmarks demonstrate that our GLT-T and GLT-T++ outperform state-of-the-art methods in terms of tracking accuracy while maintaining a real-time inference speed. The source code will be made available at https://github.com/haooozi/GLT-T.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2303.17228.pdf' target='_blank'>https://arxiv.org/pdf/2303.17228.pdf</a></span>   <span><a href='https://github.com/yuzhms/Streaming-Video-Model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Zhao, Chong Luo, Chuanxin Tang, Dongdong Chen, Noel Codella, Zheng-Jun Zha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17228">Streaming Video Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video understanding tasks have traditionally been modeled by two separate architectures, specially tailored for two distinct tasks. Sequence-based video tasks, such as action recognition, use a video backbone to directly extract spatiotemporal features, while frame-based video tasks, such as multiple object tracking (MOT), rely on single fixed-image backbone to extract spatial features. In contrast, we propose to unify video understanding tasks into one novel streaming video architecture, referred to as Streaming Vision Transformer (S-ViT). S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks. Then the frame features are input into a task-related temporal decoder to obtain spatiotemporal features for sequence-based tasks. The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task. We believe that the concept of streaming video model and the implementation of S-ViT are solid steps towards a unified deep learning architecture for video understanding. Code will be available at https://github.com/yuzhms/Streaming-Video-Model.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2303.15414.pdf' target='_blank'>https://arxiv.org/pdf/2303.15414.pdf</a></span>   <span><a href='https://github.com/jiaweihe1996/GMTracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei He, Zehao Huang, Naiyan Wang, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15414">Learnable Graph Matching: A Practical Paradigm for Data Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data association is at the core of many computer vision tasks, e.g., multiple object tracking, image matching, and point cloud registration. however, current data association solutions have some defects: they mostly ignore the intra-view context information; besides, they either train deep association models in an end-to-end way and hardly utilize the advantage of optimization-based assignment methods, or only use an off-the-shelf neural network to extract features. In this paper, we propose a general learnable graph matching method to address these issues. Especially, we model the intra-view relationships as an undirected graph. Then data association turns into a general graph matching problem between graphs. Furthermore, to make optimization end-to-end differentiable, we relax the original graph matching problem into continuous quadratic programming and then incorporate training into a deep graph neural network with KKT conditions and implicit function theorem. In MOT task, our method achieves state-of-the-art performance on several MOT datasets. For image matching, our method outperforms state-of-the-art methods on a popular indoor dataset, ScanNet. For point cloud registration, we also achieve competitive results. Code will be available at https://github.com/jiaweihe1996/GMTracker.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2303.15334.pdf' target='_blank'>https://arxiv.org/pdf/2303.15334.pdf</a></span>   <span><a href='https://github.com/ifzhang/ByteTrack-V2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifu Zhang, Xinggang Wang, Xiaoqing Ye, Wei Zhang, Jincheng Lu, Xiao Tan, Errui Ding, Peize Sun, Jingdong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15334">ByteTrackV2: 2D and 3D Multi-Object Tracking by Associating Every Detection Box</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects across video frames. Detection boxes serve as the basis of both 2D and 3D MOT. The inevitable changing of detection scores leads to object missing after tracking. We propose a hierarchical data association strategy to mine the true objects in low-score detection boxes, which alleviates the problems of object missing and fragmented trajectories. The simple and generic data association strategy shows effectiveness under both 2D and 3D settings. In 3D scenarios, it is much easier for the tracker to predict object velocities in the world coordinate. We propose a complementary motion prediction strategy that incorporates the detected velocities with a Kalman filter to address the problem of abrupt motion and short-term disappearing. ByteTrackV2 leads the nuScenes 3D MOT leaderboard in both camera (56.4% AMOTA) and LiDAR (70.1% AMOTA) modalities. Furthermore, it is nonparametric and can be integrated with various detectors, making it appealing in real applications. The source code is released at https://github.com/ifzhang/ByteTrack-V2.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2303.14346.pdf' target='_blank'>https://arxiv.org/pdf/2303.14346.pdf</a></span>   <span><a href='https://coperception.github.io/MOT-CUP/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanbao Su, Songyang Han, Yiming Li, Zhili Zhang, Chen Feng, Caiwen Ding, Fei Miao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14346">Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection and multiple object tracking (MOT) are essential components of self-driving systems. Accurate detection and uncertainty quantification are both critical for onboard modules, such as perception, prediction, and planning, to improve the safety and robustness of autonomous vehicles. Collaborative object detection (COD) has been proposed to improve detection accuracy and reduce uncertainty by leveraging the viewpoints of multiple agents. However, little attention has been paid to how to leverage the uncertainty quantification from COD to enhance MOT performance. In this paper, as the first attempt to address this challenge, we design an uncertainty propagation framework called MOT-CUP. Our framework first quantifies the uncertainty of COD through direct modeling and conformal prediction, and propagates this uncertainty information into the motion prediction and association steps. MOT-CUP is designed to work with different collaborative object detectors and baseline MOT algorithms. We evaluate MOT-CUP on V2X-Sim, a comprehensive collaborative perception dataset, and demonstrate a 2% improvement in accuracy and a 2.67X reduction in uncertainty compared to the baselines, e.g. SORT and ByteTrack. In scenarios characterized by high occlusion levels, our MOT-CUP demonstrates a noteworthy $4.01\%$ improvement in accuracy. MOT-CUP demonstrates the importance of uncertainty quantification in both COD and MOT, and provides the first attempt to improve the accuracy and reduce the uncertainty in MOT based on COD through uncertainty propagation. Our code is public on https://coperception.github.io/MOT-CUP/.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2303.10951.pdf' target='_blank'>https://arxiv.org/pdf/2303.10951.pdf</a></span>   <span><a href='https://github.com/vision4robotics/SCT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Ye, Changhong Fu, Ziang Cao, Shan An, Guangze Zheng, Bowen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10951">Tracker Meets Night: A Transformer Enhancer for UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most previous progress in object tracking is realized in daytime scenes with favorable illumination. State-of-the-arts can hardly carry on their superiority at night so far, thereby considerably blocking the broadening of visual tracking-related unmanned aerial vehicle (UAV) applications. To realize reliable UAV tracking at night, a spatial-channel Transformer-based low-light enhancer (namely SCT), which is trained in a novel task-inspired manner, is proposed and plugged prior to tracking approaches. To achieve semantic-level low-light enhancement targeting the high-level task, the novel spatial-channel attention module is proposed to model global information while preserving local context. In the enhancement process, SCT denoises and illuminates nighttime images simultaneously through a robust non-linear curve projection. Moreover, to provide a comprehensive evaluation, we construct a challenging nighttime tracking benchmark, namely DarkTrack2021, which contains 110 challenging sequences with over 100 K frames in total. Evaluations on both the public UAVDark135 benchmark and the newly constructed DarkTrack2021 benchmark show that the task-inspired design enables SCT with significant performance gains for nighttime UAV tracking compared with other top-ranked low-light enhancers. Real-world tests on a typical UAV platform further verify the practicability of the proposed approach. The DarkTrack2021 benchmark and the code of the proposed approach are publicly available at https://github.com/vision4robotics/SCT.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2303.10826.pdf' target='_blank'>https://arxiv.org/pdf/2303.10826.pdf</a></span>   <span><a href='https://github.com/jiawen-zhu/ViPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Zhu, Simiao Lai, Xin Chen, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10826">Visual Prompt Multi-Modal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visible-modal object tracking gives rise to a series of downstream multi-modal tracking tributaries. To inherit the powerful representations of the foundation model, a natural modus operandi for multi-modal tracking is full fine-tuning on the RGB-based parameters. Albeit effective, this manner is not optimal due to the scarcity of downstream data and poor transferability, etc. In this paper, inspired by the recent success of the prompt learning in language models, we develop Visual Prompt multi-modal Tracking (ViPT), which learns the modal-relevant prompts to adapt the frozen pre-trained foundation model to various downstream multimodal tracking tasks. ViPT finds a better way to stimulate the knowledge of the RGB-based model that is pre-trained at scale, meanwhile only introducing a few trainable parameters (less than 1% of model parameters). ViPT outperforms the full fine-tuning paradigm on multiple downstream tracking tasks including RGB+Depth, RGB+Thermal, and RGB+Event tracking. Extensive experiments show the potential of visual prompt learning for multi-modal tracking, and ViPT can achieve state-of-the-art performance while satisfying parameter efficiency. Code and models are available at https://github.com/jiawen-zhu/ViPT.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2303.09219.pdf' target='_blank'>https://arxiv.org/pdf/2303.09219.pdf</a></span>   <span><a href='https://github.com/Mumuqiao/MixCycle' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Wu, Jiaqi Yang, Kun Sun, Chu'ai Zhang, Yanning Zhang, Mathieu Salzmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09219">MixCycle: Mixup Assisted Semi-Supervised 3D Single Object Tracking with Cycle Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) is an indispensable part of automated driving. Existing approaches rely heavily on large, densely labeled datasets. However, annotating point clouds is both costly and time-consuming. Inspired by the great success of cycle tracking in unsupervised 2D SOT, we introduce the first semi-supervised approach to 3D SOT. Specifically, we introduce two cycle-consistency strategies for supervision: 1) Self tracking cycles, which leverage labels to help the model converge better in the early stages of training; 2) forward-backward cycles, which strengthen the tracker's robustness to motion variations and the template noise caused by the template update strategy. Furthermore, we propose a data augmentation strategy named SOTMixup to improve the tracker's robustness to point cloud diversity. SOTMixup generates training samples by sampling points in two point clouds with a mixing rate and assigns a reasonable loss weight for training according to the mixing rate. The resulting MixCycle approach generalizes to appearance matching-based trackers. On the KITTI benchmark, based on the P2B tracker, MixCycle trained with $\textbf{10\%}$ labels outperforms P2B trained with $\textbf{100\%}$ labels, and achieves a $\textbf{28.4\%}$ precision improvement when using $\textbf{1\%}$ labels. Our code will be released at \url{https://github.com/Mumuqiao/MixCycle}.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2303.06674.pdf' target='_blank'>https://arxiv.org/pdf/2303.06674.pdf</a></span>   <span><a href='https://github.com/MasterBin-IIAU/UNINEXT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.06674">Universal Instance Perception as Object Discovery and Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>All instance perception tasks aim at finding certain objects specified by some queries such as category names, language expressions, and target annotations, but this complete field has been split into multiple independent subtasks. In this work, we present a universal instance perception model of the next generation, termed UNINEXT. UNINEXT reformulates diverse instance perception tasks into a unified object discovery and retrieval paradigm and can flexibly perceive different types of objects by simply changing the input prompts. This unified formulation brings the following benefits: (1) enormous data from different tasks and label vocabularies can be exploited for jointly training general instance-level representations, which is especially beneficial for tasks lacking in training data. (2) the unified model is parameter-efficient and can save redundant computation when handling multiple tasks simultaneously. UNINEXT shows superior performance on 20 challenging benchmarks from 10 instance-level tasks including classical image-level tasks (object detection and instance segmentation), vision-and-language tasks (referring expression comprehension and segmentation), and six video-level object tracking tasks. Code is available at https://github.com/MasterBin-IIAU/UNINEXT.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2303.04378.pdf' target='_blank'>https://arxiv.org/pdf/2303.04378.pdf</a></span>   <span><a href='https://github.com/vision4robotics/SGDViT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangliang Yao, Changhong Fu, Sihang Li, Guangze Zheng, Junjie Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04378">SGDViT: Saliency-Guided Dynamic Vision Transformer for UAV Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based object tracking has boosted extensive autonomous applications for unmanned aerial vehicles (UAVs). However, the dynamic changes in flight maneuver and viewpoint encountered in UAV tracking pose significant difficulties, e.g. , aspect ratio change, and scale variation. The conventional cross-correlation operation, while commonly used, has limitations in effectively capturing perceptual similarity and incorporates extraneous background information. To mitigate these limitations, this work presents a novel saliency-guided dynamic vision Transformer (SGDViT) for UAV tracking. The proposed method designs a new task-specific object saliency mining network to refine the cross-correlation operation and effectively discriminate foreground and background information. Additionally, a saliency adaptation embedding operation dynamically generates tokens based on initial saliency, thereby reducing the computational complexity of the Transformer architecture. Finally, a lightweight saliency filtering Transformer further refines saliency information and increases the focus on appearance information. The efficacy and robustness of the proposed approach have been thoroughly assessed through experiments on three widely-used UAV tracking benchmarks and real-world scenarios, with results demonstrating its superiority. The source code and demo videos are available at https://github.com/vision4robotics/SGDViT.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2303.03366.pdf' target='_blank'>https://arxiv.org/pdf/2303.03366.pdf</a></span>   <span><a href='https://github.com/wudongming97/RMOT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wudongming97/RMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, Jianbing Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03366">Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing referring understanding tasks tend to involve the detection of a single text-referred object. In this paper, we propose a new and general referring understanding task, termed referring multi-object tracking (RMOT). Its core idea is to employ a language expression as a semantic cue to guide the prediction of multi-object tracking. To the best of our knowledge, it is the first work to achieve an arbitrary number of referent object predictions in videos. To push forward RMOT, we construct one benchmark with scalable expressions based on KITTI, named Refer-KITTI. Specifically, it provides 18 videos with 818 expressions, and each expression in a video is annotated with an average of 10.7 objects. Further, we develop a transformer-based architecture TransRMOT to tackle the new task in an online manner, which achieves impressive detection performance and outperforms other counterparts. The dataset and code will be available at https://github.com/wudongming97/RMOT.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2303.01786.pdf' target='_blank'>https://arxiv.org/pdf/2303.01786.pdf</a></span>   <span><a href='https://github.com/hejiawei2023/UG3DMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei He, Chunyun Fu, Xiyang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.01786">3D Multi-Object Tracking Based on Uncertainty-Guided Data Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the existing literature, most 3D multi-object tracking algorithms based on the tracking-by-detection framework employed deterministic tracks and detections for similarity calculation in the data association stage. Namely, the inherent uncertainties existing in tracks and detections are overlooked. In this work, we discard the commonly used deterministic tracks and deterministic detections for data association, instead, we propose to model tracks and detections as random vectors in which uncertainties are taken into account. Then, based on the Jensen-Shannon divergence, the similarity between two multidimensional distributions, i.e. track and detection, is evaluated for data association purposes. Lastly, the level of track uncertainty is incorporated in our cost function design to guide the data association process. Comparative experiments have been conducted on two typical datasets, KITTI and nuScenes, and the results indicated that our proposed method outperformed the compared state-of-the-art 3D tracking algorithms. For the benefit of the community, our code has been made available at https://github.com/hejiawei2023/UG3DMOT.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2302.14807.pdf' target='_blank'>https://arxiv.org/pdf/2302.14807.pdf</a></span>   <span><a href='https://github.com/MohamedNagyMostafa/DFR-FastMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Nagy, Majid Khonji, Jorge Dias, Sajid Javed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14807">DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Persistent multi-object tracking (MOT) allows autonomous vehicles to navigate safely in highly dynamic environments. One of the well-known challenges in MOT is object occlusion when an object becomes unobservant for subsequent frames. The current MOT methods store objects information, like objects' trajectory, in internal memory to recover the objects after occlusions. However, they retain short-term memory to save computational time and avoid slowing down the MOT method. As a result, they lose track of objects in some occlusion scenarios, particularly long ones. In this paper, we propose DFR-FastMOT, a light MOT method that uses data from a camera and LiDAR sensors and relies on an algebraic formulation for object association and fusion. The formulation boosts the computational time and permits long-term memory that tackles more occlusion scenarios. Our method shows outstanding tracking performance over recent learning and non-learning benchmarks with about 3% and 4% margin in MOTA, respectively. Also, we conduct extensive experiments that simulate occlusion phenomena by employing detectors with various distortion levels. The proposed solution enables superior performance under various distortion levels in detection over current state-of-art methods. Our framework processes about 7,763 frames in 1.48 seconds, which is seven times faster than recent benchmarks. The framework will be available at https://github.com/MohamedNagyMostafa/DFR-FastMOT.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2302.11813.pdf' target='_blank'>https://arxiv.org/pdf/2302.11813.pdf</a></span>   <span><a href='https://github.com/GerardMaggiolino/Deep-OC-SORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerard Maggiolino, Adnan Ahmad, Jinkun Cao, Kris Kitani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11813">Deep OC-SORT: Multi-Pedestrian Tracking by Adaptive Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motion-based association for Multi-Object Tracking (MOT) has recently re-achieved prominence with the rise of powerful object detectors. Despite this, little work has been done to incorporate appearance cues beyond simple heuristic models that lack robustness to feature degradation. In this paper, we propose a novel way to leverage objects' appearances to adaptively integrate appearance matching into existing high-performance motion-based methods. Building upon the pure motion-based method OC-SORT, we achieve 1st place on MOT20 and 2nd place on MOT17 with 63.9 and 64.9 HOTA, respectively. We also achieve 61.3 HOTA on the challenging DanceTrack benchmark as a new state-of-the-art even compared to more heavily-designed methods. The code and models are available at \url{https://github.com/GerardMaggiolino/Deep-OC-SORT}.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2302.07676.pdf' target='_blank'>https://arxiv.org/pdf/2302.07676.pdf</a></span>   <span><a href='https://github.com/shengyuhao/DIVOTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenghao Hao, Peiyuan Liu, Yibing Zhan, Kaixun Jin, Zuozhu Liu, Mingli Song, Jenq-Neng Hwang, Gaoang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07676">DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-view multi-object tracking aims to link objects between frames and camera views with substantial overlaps. Although cross-view multi-object tracking has received increased attention in recent years, existing datasets still have several issues, including 1) missing real-world scenarios, 2) lacking diverse scenes, 3) owning a limited number of tracks, 4) comprising only static cameras, and 5) lacking standard benchmarks, which hinder the investigation and comparison of cross-view tracking methods. To solve the aforementioned issues, we introduce DIVOTrack: a new cross-view multi-object tracking dataset for DIVerse Open scenes with dense tracking pedestrians in realistic and non-experimental environments. Our DIVOTrack has fifteen distinct scenarios and 953 cross-view tracks, surpassing all cross-view multi-object tracking datasets currently available. Furthermore, we provide a novel baseline cross-view tracking method with a unified joint detection and cross-view tracking framework named CrossMOT, which learns object detection, single-view association, and cross-view matching with an all-in-one embedding model. Finally, we present a summary of current methodologies and a set of standard benchmarks with our DIVOTrack to provide a fair comparison and conduct a comprehensive analysis of current approaches and our proposed CrossMOT. The dataset and code are available at https://github.com/shengyuhao/DIVOTrack.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2302.05991.pdf' target='_blank'>https://arxiv.org/pdf/2302.05991.pdf</a></span>   <span><a href='https://github.com/augcog/DTTDv1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiyu Feng, Seth Z. Zhao, Chuanyu Pan, Adam Chang, Yichen Chen, Zekun Wang, Allen Y. Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05991">Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for Longer-Range Object Tracking Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital twin is a problem of augmenting real objects with their digital counterparts. It can underpin a wide range of applications in augmented reality (AR), autonomy, and UI/UX. A critical component in a good digital-twin system is real-time, accurate 3D object tracking. Most existing works solve 3D object tracking through the lens of robotic grasping, employ older generations of depth sensors, and measure performance metrics that may not apply to other digital-twin applications such as in AR. In this work, we create a novel RGB-D dataset, called Digital Twin Tracking Dataset (DTTD), to enable further research of the problem and extend potential solutions towards longer ranges and mm localization accuracy. To reduce point cloud noise from the input source, we select the latest Microsoft Azure Kinect as the state-of-the-art time-of-flight (ToF) camera. In total, 103 scenes of 10 common off-the-shelf objects with rich textures are recorded, with each frame annotated with a per-pixel semantic segmentation and ground-truth object poses provided by a commercial motion capturing system. Through extensive experiments with model-level and dataset-level analysis, we demonstrate that DTTD can help researchers develop future object tracking methods and analyze new challenges. The dataset, data generation, annotation, and model evaluation pipeline are made publicly available as open source code at: https://github.com/augcog/DTTDv1.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2302.03802.pdf' target='_blank'>https://arxiv.org/pdf/2302.03802.pdf</a></span>   <span><a href='https://github.com/TRI-ML/PF-Track' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqi Pang, Jie Li, Pavel Tokmakov, Dian Chen, Sergey Zagoruyko, Yu-Xiong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03802">Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes an end-to-end multi-camera 3D multi-object tracking (MOT) framework. It emphasizes spatio-temporal continuity and integrates both past and future reasoning for tracked objects. Thus, we name it "Past-and-Future reasoning for Tracking" (PF-Track). Specifically, our method adapts the "tracking by attention" framework and represents tracked instances coherently over time with object queries. To explicitly use historical cues, our "Past Reasoning" module learns to refine the tracks and enhance the object features by cross-attending to queries from previous frames and other objects. The "Future Reasoning" module digests historical information and predicts robust future trajectories. In the case of long-term occlusions, our method maintains the object positions and enables re-association by integrating motion predictions. On the nuScenes dataset, our method improves AMOTA by a large margin and remarkably reduces ID-Switches by 90% compared to prior approaches, which is an order of magnitude less. The code and models are made available at https://github.com/TRI-ML/PF-Track.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2302.02814.pdf' target='_blank'>https://arxiv.org/pdf/2302.02814.pdf</a></span>   <span><a href='https://github.com/MCG-NJU/MixFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutao Cui, Cheng Jiang, Gangshan Wu, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02814">MixFormer: End-to-End Tracking with Iterative Mixed Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking often employs a multi-stage pipeline of feature extraction, target information integration, and bounding box estimation. To simplify this pipeline and unify the process of feature extraction and target information integration, in this paper, we present a compact tracking framework, termed as MixFormer, built upon transformers. Our core design is to utilize the flexibility of attention operations, and propose a Mixed Attention Module (MAM) for simultaneous feature extraction and target information integration. This synchronous modeling scheme allows to extract target-specific discriminative features and perform extensive communication between target and search area. Based on MAM, we build our MixFormer trackers simply by stacking multiple MAMs and placing a localization head on top. Specifically, we instantiate two types of MixFormer trackers, a hierarchical tracker MixCvT, and a non-hierarchical tracker MixViT. For these two trackers, we investigate a series of pre-training methods and uncover the different behaviors between supervised pre-training and self-supervised pre-training in our MixFormer trackers. We also extend the masked pre-training to our MixFormer trackers and design the competitive TrackMAE pre-training technique. Finally, to handle multiple target templates during online tracking, we devise an asymmetric attention scheme in MAM to reduce computational cost, and propose an effective score prediction module to select high-quality templates. Our MixFormer trackers set a new state-of-the-art performance on seven tracking benchmarks, including LaSOT, TrackingNet, VOT2020, GOT-10k, OTB100 and UAV123. In particular, our MixViT-L achieves AUC score of 73.3% on LaSOT, 86.1% on TrackingNet, EAO of 0.584 on VOT2020, and AO of 75.7% on GOT-10k. Code and trained models are publicly available at https://github.com/MCG-NJU/MixFormer.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2301.10938.pdf' target='_blank'>https://arxiv.org/pdf/2301.10938.pdf</a></span>   <span><a href='https://github.com/HUSTDML/CTTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zikai Song, Run Luo, Junqing Yu, Yi-Ping Phoebe Chen, Wei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10938">Compact Transformer Tracker with Correlative Masked Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer framework has been showing superior performances in visual object tracking for its great strength in information aggregation across the template and search image with the well-known attention mechanism. Most recent advances focus on exploring attention mechanism variants for better information aggregation. We find these schemes are equivalent to or even just a subset of the basic self-attention mechanism. In this paper, we prove that the vanilla self-attention structure is sufficient for information aggregation, and structural adaption is unnecessary. The key is not the attention structure, but how to extract the discriminative feature for tracking and enhance the communication between the target and search image. Based on this finding, we adopt the basic vision transformer (ViT) architecture as our main tracker and concatenate the template and search image for feature embedding. To guide the encoder to capture the invariant feature for tracking, we attach a lightweight correlative masked decoder which reconstructs the original template and search image from the corresponding masked tokens. The correlative masked decoder serves as a plugin for the compact transform tracker and is skipped in inference. Our compact tracker uses the most simple structure which only consists of a ViT backbone and a box head, and can run at 40 fps. Extensive experiments show the proposed compact transform tracker outperforms existing approaches, including advanced attention variants, and demonstrates the sufficiency of self-attention in tracking tasks. Our method achieves state-of-the-art performance on five challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet, and GOT-10k benchmarks. Our project is available at https://github.com/HUSTDML/CTTrack.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2301.10559.pdf' target='_blank'>https://arxiv.org/pdf/2301.10559.pdf</a></span>   <span><a href='https://github.com/chamathabeysinghe/da-tracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chamath Abeysinghe, Chris Reid, Hamid Rezatofighi, Bernd Meyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10559">Tracking Different Ant Species: An Unsupervised Domain Adaptation Framework and a Dataset for Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking individuals is a vital part of many experiments conducted to understand collective behaviour. Ants are the paradigmatic model system for such experiments but their lack of individually distinguishing visual features and their high colony densities make it extremely difficult to perform reliable tracking automatically. Additionally, the wide diversity of their species' appearances makes a generalized approach even harder. In this paper, we propose a data-driven multi-object tracker that, for the first time, employs domain adaptation to achieve the required generalisation. This approach is built upon a joint-detection-and-tracking framework that is extended by a set of domain discriminator modules integrating an adversarial training strategy in addition to the tracking loss. In addition to this novel domain-adaptive tracking framework, we present a new dataset and a benchmark for the ant tracking problem. The dataset contains 57 video sequences with full trajectory annotation, including 30k frames captured from two different ant species moving on different background patterns. It comprises 33 and 24 sequences for source and target domains, respectively. We compare our proposed framework against other domain-adaptive and non-domain-adaptive multi-object tracking baselines using this dataset and show that incorporating domain adaptation at multiple levels of the tracking pipeline yields significant improvements. The code and the dataset are available at https://github.com/chamathabeysinghe/da-tracker.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2212.14304.pdf' target='_blank'>https://arxiv.org/pdf/2212.14304.pdf</a></span>   <span><a href='https://github.com/Dongzhou-1996/RAMAVT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Zhou, Guanghui Sun, Zhao Zhang, Ligang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14304">On Deep Recurrent Reinforcement Learning for Active Visual Tracking of Space Noncooperative Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active tracking of space noncooperative object that merely relies on vision camera is greatly significant for autonomous rendezvous and debris removal. Considering its Partial Observable Markov Decision Process (POMDP) property, this paper proposes a novel tracker based on deep recurrent reinforcement learning, named as RAMAVT which drives the chasing spacecraft to follow arbitrary space noncooperative object with high-frequency and near-optimal velocity control commands. To further improve the active tracking performance, we introduce Multi-Head Attention (MHA) module and Squeeze-and-Excitation (SE) layer into RAMAVT, which remarkably improve the representative ability of neural network with almost no extra computational cost. Extensive experiments and ablation study implemented on SNCOAT benchmark show the effectiveness and robustness of our method compared with other state-of-the-art algorithm. The source codes are available on https://github.com/Dongzhou-1996/RAMAVT.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2211.11010.pdf' target='_blank'>https://arxiv.org/pdf/2211.11010.pdf</a></span>   <span><a href='https://github.com/Event-AHU/COESOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanming Tang, Xiao Wang, Ju Huang, Bo Jiang, Lin Zhu, Jianlin Zhang, Yaowei Wang, Yonghong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11010">Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Combining the Color and Event cameras (also called Dynamic Vision Sensors, DVS) for robust object tracking is a newly emerging research topic in recent years. Existing color-event tracking framework usually contains multiple scattered modules which may lead to low efficiency and high computational complexity, including feature extraction, fusion, matching, interactive learning, etc. In this paper, we propose a single-stage backbone network for Color-Event Unified Tracking (CEUTrack), which achieves the above functions simultaneously. Given the event points and RGB frames, we first transform the points into voxels and crop the template and search regions for both modalities, respectively. Then, these regions are projected into tokens and parallelly fed into the unified Transformer backbone network. The output features will be fed into a tracking head for target object localization. Our proposed CEUTrack is simple, effective, and efficient, which achieves over 75 FPS and new SOTA performance. To better validate the effectiveness of our model and address the data deficiency of this task, we also propose a generic and large-scale benchmark dataset for color-event tracking, termed COESOT, which contains 90 categories and 1354 video sequences. Additionally, a new evaluation metric named BOC is proposed in our evaluation toolkit to evaluate the prominence with respect to the baseline methods. We hope the newly proposed method, dataset, and evaluation metric provide a better platform for color-event-based tracking. The dataset, toolkit, and source code will be released on: \url{https://github.com/Event-AHU/COESOT}.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2211.09791.pdf' target='_blank'>https://arxiv.org/pdf/2211.09791.pdf</a></span>   <span><a href='https://github.com/megvii-research/MOTRv2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuang Zhang, Tiancai Wang, Xiangyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.09791">MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose MOTRv2, a simple yet effective pipeline to bootstrap end-to-end multi-object tracking with a pretrained object detector. Existing end-to-end methods, MOTR and TrackFormer are inferior to their tracking-by-detection counterparts mainly due to their poor detection performance. We aim to improve MOTR by elegantly incorporating an extra object detector. We first adopt the anchor formulation of queries and then use an extra object detector to generate proposals as anchors, providing detection prior to MOTR. The simple modification greatly eases the conflict between joint learning detection and association tasks in MOTR. MOTRv2 keeps the query propogation feature and scales well on large-scale benchmarks. MOTRv2 ranks the 1st place (73.4% HOTA on DanceTrack) in the 1st Multiple People Tracking in Group Dance Challenge. Moreover, MOTRv2 reaches state-of-the-art performance on the BDD100K dataset. We hope this simple and effective pipeline can provide some new insights to the end-to-end MOT community. Code is available at \url{https://github.com/megvii-research/MOTRv2}.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2211.08824.pdf' target='_blank'>https://arxiv.org/pdf/2211.08824.pdf</a></span>   <span><a href='https://github.com/pingyang1117/SMILEtrack_Official' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Hsiang Wang, Jun-Wei Hsieh, Ping-Yang Chen, Ming-Ching Chang, Hung Hin So, Xin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.08824">SMILEtrack: SiMIlarity LEarning for Occlusion-Aware Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite recent progress in Multiple Object Tracking (MOT), several obstacles such as occlusions, similar objects, and complex scenes remain an open challenge. Meanwhile, a systematic study of the cost-performance tradeoff for the popular tracking-by-detection paradigm is still lacking. This paper introduces SMILEtrack, an innovative object tracker that effectively addresses these challenges by integrating an efficient object detector with a Siamese network-based Similarity Learning Module (SLM). The technical contributions of SMILETrack are twofold. First, we propose an SLM that calculates the appearance similarity between two objects, overcoming the limitations of feature descriptors in Separate Detection and Embedding (SDE) models. The SLM incorporates a Patch Self-Attention (PSA) block inspired by the vision Transformer, which generates reliable features for accurate similarity matching. Second, we develop a Similarity Matching Cascade (SMC) module with a novel GATE function for robust object matching across consecutive video frames, further enhancing MOT performance. Together, these innovations help SMILETrack achieve an improved trade-off between the cost ({\em e.g.}, running speed) and performance (e.g., tracking accuracy) over several existing state-of-the-art benchmarks, including the popular BYTETrack method. SMILETrack outperforms BYTETrack by 0.4-0.8 MOTA and 2.1-2.2 HOTA points on MOT17 and MOT20 datasets. Code is available at https://github.com/pingyang1117/SMILEtrack_Official
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2211.06663.pdf' target='_blank'>https://arxiv.org/pdf/2211.06663.pdf</a></span>   <span><a href='https://github.com/franktpmvu/NeighborTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Hsi Chen, Chien-Yao Wang, Cheng-Yun Yang, Hung-Shuo Chang, Youn-Long Lin, Yung-Yu Chuang, Hong-Yuan Mark Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.06663">NeighborTrack: Improving Single Object Tracking by Bipartite Matching with Neighbor Tracklets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a post-processor, called NeighborTrack, that leverages neighbor information of the tracking target to validate and improve single-object tracking (SOT) results. It requires no additional data or retraining. Instead, it uses the confidence score predicted by the backbone SOT network to automatically derive neighbor information and then uses this information to improve the tracking results. When tracking an occluded target, its appearance features are untrustworthy. However, a general siamese network often cannot tell whether the tracked object is occluded by reading the confidence score alone, because it could be misled by neighbors with high confidence scores. Our proposed NeighborTrack takes advantage of unoccluded neighbors' information to reconfirm the tracking target and reduces false tracking when the target is occluded. It not only reduces the impact caused by occlusion, but also fixes tracking problems caused by object appearance changes. NeighborTrack is agnostic to SOT networks and post-processing methods. For the VOT challenge dataset commonly used in short-term object tracking, we improve three famous SOT networks, Ocean, TransT, and OSTrack, by an average of ${1.92\%}$ EAO and ${2.11\%}$ robustness. For the mid- and long-term tracking experiments based on OSTrack, we achieve state-of-the-art ${72.25\%}$ AUC on LaSOT and ${75.7\%}$ AO on GOT-10K. Code duplication can be found in https://github.com/franktpmvu/NeighborTrack.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2210.15511.pdf' target='_blank'>https://arxiv.org/pdf/2210.15511.pdf</a></span>   <span><a href='https://github.com/zhiqic/ProContEXT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin-Peng Lan, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Xu Bao, Wangmeng Xiang, Yifeng Geng, Xuansong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.15511">ProContEXT: Exploring Progressive Context Transformer for Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing Visual Object Tracking (VOT) only takes the target area in the first frame as a template. This causes tracking to inevitably fail in fast-changing and crowded scenes, as it cannot account for changes in object appearance between frames. To this end, we revamped the tracking framework with Progressive Context Encoding Transformer Tracker (ProContEXT), which coherently exploits spatial and temporal contexts to predict object motion trajectories. Specifically, ProContEXT leverages a context-aware self-attention module to encode the spatial and temporal context, refining and updating the multi-scale static and dynamic templates to progressively perform accurately tracking. It explores the complementary between spatial and temporal context, raising a new pathway to multi-context modeling for transformer-based trackers. In addition, ProContEXT revised the token pruning technique to reduce computational complexity. Extensive experiments on popular benchmark datasets such as GOT-10k and TrackingNet demonstrate that the proposed ProContEXT achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2210.13570.pdf' target='_blank'>https://arxiv.org/pdf/2210.13570.pdf</a></span>   <span><a href='https://github.com/amitgalor18/STC_Tracker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amit Galor, Roy Orfaig, Ben-Zion Bobrovsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13570">Strong-TransCenter: Improved Multi-Object Tracking based on Transformers with Dense Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer networks have been a focus of research in many fields in recent years, being able to surpass the state-of-the-art performance in different computer vision tasks. However, in the task of Multiple Object Tracking (MOT), leveraging the power of Transformers remains relatively unexplored. Among the pioneering efforts in this domain, TransCenter, a Transformer-based MOT architecture with dense object queries, demonstrated exceptional tracking capabilities while maintaining reasonable runtime. Nonetheless, one critical aspect in MOT, track displacement estimation, presents room for enhancement to further reduce association errors. In response to this challenge, our paper introduces a novel improvement to TransCenter. We propose a post-processing mechanism grounded in the Track-by-Detection paradigm, aiming to refine the track displacement estimation. Our approach involves the integration of a carefully designed Kalman filter, which incorporates Transformer outputs into measurement error estimation, and the use of an embedding network for target re-identification. This combined strategy yields substantial improvement in the accuracy and robustness of the tracking process. We validate our contributions through comprehensive experiments on the MOTChallenge datasets MOT17 and MOT20, where our proposed approach outperforms other Transformer-based trackers. The code is publicly available at: https://github.com/amitgalor18/STC_Tracker
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2210.05278.pdf' target='_blank'>https://arxiv.org/pdf/2210.05278.pdf</a></span>   <span><a href='https://github.com/dyhBUPT/EnsembleMOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Du, Zihang Liu, Fei Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.05278">EnsembleMOT: A Step towards Ensemble Learning of Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) has rapidly progressed in recent years. Existing works tend to design a single tracking algorithm to perform both detection and association. Though ensemble learning has been exploited in many tasks, i.e, classification and object detection, it hasn't been studied in the MOT task, which is mainly caused by its complexity and evaluation metrics. In this paper, we propose a simple but effective ensemble method for MOT, called EnsembleMOT, which merges multiple tracking results from various trackers with spatio-temporal constraints. Meanwhile, several post-processing procedures are applied to filter out abnormal results. Our method is model-independent and doesn't need the learning procedure. What's more, it can easily work in conjunction with other algorithms, e.g., tracklets interpolation. Experiments on the MOT17 dataset demonstrate the effectiveness of the proposed method. Codes are available at https://github.com/dyhBUPT/EnsembleMOT.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2209.03910.pdf' target='_blank'>https://arxiv.org/pdf/2209.03910.pdf</a></span>   <span><a href='https://github.com/GiantAI/pixtrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prajwal Chidananda, Saurabh Nair, Douglas Lee, Adrian Kaehler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.03910">PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and Feature-metric Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PixTrack, a vision based object pose tracking framework using novel view synthesis and deep feature-metric alignment. We follow an SfM-based relocalization paradigm where we use a Neural Radiance Field to canonically represent the tracked object. Our evaluations demonstrate that our method produces highly accurate, robust, and jitter-free 6DoF pose estimates of objects in both monocular RGB images and RGB-D images without the need of any data annotation or trajectory smoothing. Our method is also computationally efficient making it easy to have multi-object tracking with no alteration to our algorithm through simple CPU multiprocessing. Our code is available at: https://github.com/GiantAI/pixtrack
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2208.09787.pdf' target='_blank'>https://arxiv.org/pdf/2208.09787.pdf</a></span>   <span><a href='https://github.com/xuefeng-zhu5/RGBD1K' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue-Feng Zhu, Tianyang Xu, Zhangyong Tang, Zucheng Wu, Haodong Liu, Xiao Yang, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.09787">RGBD1K: A Large-scale Dataset and Benchmark for RGB-D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-D object tracking has attracted considerable attention recently, achieving promising performance thanks to the symbiosis between visual and depth channels. However, given a limited amount of annotated RGB-D tracking data, most state-of-the-art RGB-D trackers are simple extensions of high-performance RGB-only trackers, without fully exploiting the underlying potential of the depth channel in the offline training stage. To address the dataset deficiency issue, a new RGB-D dataset named RGBD1K is released in this paper. The RGBD1K contains 1,050 sequences with about 2.5M frames in total. To demonstrate the benefits of training on a larger RGB-D data set in general, and RGBD1K in particular, we develop a transformer-based RGB-D tracker, named SPT, as a baseline for future visual object tracking studies using the new dataset. The results, of extensive experiments using the SPT tracker emonstrate the potential of the RGBD1K dataset to improve the performance of RGB-D tracking, inspiring future developments of effective tracker designs. The dataset and codes will be available on the project homepage: https://github.com/xuefeng-zhu5/RGBD1K.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2208.03571.pdf' target='_blank'>https://arxiv.org/pdf/2208.03571.pdf</a></span>   <span><a href='https://github.com/psaltaath/tadn-mot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Athena Psalta, Vasileios Tsironis, Konstantinos Karantzalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.03571">Transformer-based assignment decision network for multiple object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data association is a crucial component for any multiple object tracking (MOT) method that follows the tracking-by-detection paradigm. To generate complete trajectories such methods employ a data association process to establish assignments between detections and existing targets during each timestep. Recent data association approaches try to solve either a multi-dimensional linear assignment task or a network flow minimization problem or tackle it via multiple hypotheses tracking. However, during inference an optimization step that computes optimal assignments is required for every sequence frame inducing additional complexity to any given solution. To this end, in the context of this work we introduce Transformer-based Assignment Decision Network (TADN) that tackles data association without the need of any explicit optimization during inference. In particular, TADN can directly infer assignment pairs between detections and active targets in a single forward pass of the network. We have integrated TADN in a rather simple MOT framework, designed a novel training strategy for efficient end-to-end training and demonstrated the high potential of our approach for online visual tracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and UA-DETRAC. Our proposed approach demonstrates strong performance in most evaluation metrics despite its simple nature as a tracker lacking significant auxiliary components such as occlusion handling or re-identification. The implementation of our method is publicly available at https://github.com/psaltaath/tadn-mot.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2206.14451.pdf' target='_blank'>https://arxiv.org/pdf/2206.14451.pdf</a></span>   <span><a href='https://github.com/synsin0/SRCN3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yining Shi, Jingyan Shen, Yifan Sun, Yunlong Wang, Jiaxin Li, Shiqi Sun, Kun Jiang, Diange Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.14451">SRCN3D: Sparse R-CNN 3D for Compact Convolutional Multi-View 3D Object Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detection and tracking of moving objects is an essential component in environmental perception for autonomous driving. In the flourishing field of multi-view 3D camera-based detectors, different transformer-based pipelines are designed to learn queries in 3D space from 2D feature maps of perspective views, but the dominant dense BEV query mechanism is computationally inefficient. This paper proposes Sparse R-CNN 3D (SRCN3D), a novel two-stage fully-sparse detector that incorporates sparse queries, sparse attention with box-wise sampling, and sparse prediction. SRCN3D adopts a cascade structure with the twin-track update of both a fixed number of query boxes and latent query features. Our novel sparse feature sampling module only utilizes local 2D region of interest (RoI) features calculated by the projection of 3D query boxes for further box refinement, leading to a fully-convolutional and deployment-friendly pipeline. For multi-object tracking, motion features, query features and RoI features are comprehensively utilized in multi-hypotheses data association. Extensive experiments on nuScenes dataset demonstrate that SRCN3D achieves competitive performance in both 3D object detection and multi-object tracking tasks, while also exhibiting superior efficiency compared to transformer-based methods. Code and models are available at https://github.com/synsin0/SRCN3D.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2206.09900.pdf' target='_blank'>https://arxiv.org/pdf/2206.09900.pdf</a></span>   <span><a href='https://github.com/chaytonmin/Occupancy-MAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Min, Xinli Xu, Dawei Zhao, Liang Xiao, Yiming Nie, Bin Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.09900">Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point Clouds with Masked Occupancy Autoencoders</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current perception models in autonomous driving heavily rely on large-scale labelled 3D data, which is both costly and time-consuming to annotate. This work proposes a solution to reduce the dependence on labelled 3D training data by leveraging pre-training on large-scale unlabeled outdoor LiDAR point clouds using masked autoencoders (MAE). While existing masked point autoencoding methods mainly focus on small-scale indoor point clouds or pillar-based large-scale outdoor LiDAR data, our approach introduces a new self-supervised masked occupancy pre-training method called Occupancy-MAE, specifically designed for voxel-based large-scale outdoor LiDAR point clouds. Occupancy-MAE takes advantage of the gradually sparse voxel occupancy structure of outdoor LiDAR point clouds and incorporates a range-aware random masking strategy and a pretext task of occupancy prediction. By randomly masking voxels based on their distance to the LiDAR and predicting the masked occupancy structure of the entire 3D surrounding scene, Occupancy-MAE encourages the extraction of high-level semantic information to reconstruct the masked voxel using only a small number of visible voxels. Extensive experiments demonstrate the effectiveness of Occupancy-MAE across several downstream tasks. For 3D object detection, Occupancy-MAE reduces the labelled data required for car detection on the KITTI dataset by half and improves small object detection by approximately 2% in AP on the Waymo dataset. For 3D semantic segmentation, Occupancy-MAE outperforms training from scratch by around 2% in mIoU. For multi-object tracking, Occupancy-MAE enhances training from scratch by approximately 1% in terms of AMOTA and AMOTP. Codes are publicly available at https://github.com/chaytonmin/Occupancy-MAE.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2206.09372.pdf' target='_blank'>https://arxiv.org/pdf/2206.09372.pdf</a></span>   <span><a href='https://github.com/Cardio-AI/mvhota' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lalith Sharan, Halvar Kelm, Gabriele Romano, Matthias Karck, Raffaele De Simone, Sandy Engelhardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.09372">mvHOTA: A multi-view higher order tracking accuracy metric to measure spatial and temporal associations in multi-point detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-point tracking is a challenging task that involves detecting points in the scene and tracking them across a sequence of frames. Computing detection-based measures like the F-measure on a frame-by-frame basis is not sufficient to assess the overall performance, as it does not interpret performance in the temporal domain. The main evaluation metric available comes from Multi-object tracking (MOT) methods to benchmark performance on datasets such as KITTI with the recently proposed higher order tracking accuracy (HOTA) metric, which is capable of providing a better description of the performance over metrics such as MOTA, DetA, and IDF1. While the HOTA metric takes into account temporal associations, it does not provide a tailored means to analyse the spatial associations of a dataset in a multi-camera setup. Moreover, there are differences in evaluating the detection task for points when compared to objects (point distances vs. bounding box overlap). Therefore in this work, we propose a multi-view higher order tracking metric (mvHOTA) to determine the accuracy of multi-point (multi-instance and multi-class) tracking methods, while taking into account temporal and spatial associations.mvHOTA can be interpreted as the geometric mean of detection, temporal, and spatial associations, thereby providing equal weighting to each of the factors. We demonstrate the use of this metric to evaluate the tracking performance on an endoscopic point detection dataset from a previously organised surgical data science challenge. Furthermore, we compare with other adjusted MOT metrics for this use-case, discuss the properties of mvHOTA, and show how the proposed multi-view Association and the Occlusion index (OI) facilitate analysis of methods with respect to handling of occlusions. The code is available at https://github.com/Cardio-AI/mvhota.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2206.04656.pdf' target='_blank'>https://arxiv.org/pdf/2206.04656.pdf</a></span>   <span><a href='https://github.com/dvl-tum/GHOST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jenny Seidenschwarz, Guillem BrasÃ³, Victor Castro Serrano, Ismail Elezi, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.04656">Simple Cues Lead to a Strong Multi-Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For a long time, the most common paradigm in Multi-Object Tracking was tracking-by-detection (TbD), where objects are first detected and then associated over video frames. For association, most models resourced to motion and appearance cues, e.g., re-identification networks. Recent approaches based on attention propose to learn the cues in a data-driven manner, showing impressive results. In this paper, we ask ourselves whether simple good old TbD methods are also capable of achieving the performance of end-to-end models. To this end, we propose two key ingredients that allow a standard re-identification network to excel at appearance-based tracking. We extensively analyse its failure cases, and show that a combination of our appearance features with a simple motion model leads to strong tracking results. Our tracker generalizes to four public datasets, namely MOT17, MOT20, BDD100k, and DanceTrack, achieving state-of-the-art performance. https://github.com/dvl-tum/GHOST.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2205.00968.pdf' target='_blank'>https://arxiv.org/pdf/2205.00968.pdf</a></span>   <span><a href='https://github.com/HYUNJS/SGT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongseok Hyun, Myunggu Kang, Dongyoon Wee, Dit-Yan Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.00968">Detection Recovery in Online Multi-Object Tracking with Sparse Graph Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In existing joint detection and tracking methods, pairwise relational features are used to match previous tracklets to current detections. However, the features may not be discriminative enough for a tracker to identify a target from a large number of detections. Selecting only high-scored detections for tracking may lead to missed detections whose confidence score is low. Consequently, in the online setting, this results in disconnections of tracklets which cannot be recovered. In this regard, we present Sparse Graph Tracker (SGT), a novel online graph tracker using higher-order relational features which are more discriminative by aggregating the features of neighboring detections and their relations. SGT converts video data into a graph where detections, their connections, and the relational features of two connected nodes are represented by nodes, edges, and edge features, respectively. The strong edge features allow SGT to track targets with tracking candidates selected by top-K scored detections with large K. As a result, even low-scored detections can be tracked, and the missed detections are also recovered. The robustness of K value is shown through the extensive experiments. In the MOT16/17/20 and HiEve Challenge, SGT outperforms the state-of-the-art trackers with real-time inference speed. Especially, a large improvement in MOTA is shown in the MOT20 and HiEve Challenge. Code is available at https://github.com/HYUNJS/SGT.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2204.12489.pdf' target='_blank'>https://arxiv.org/pdf/2204.12489.pdf</a></span>   <span><a href='https://ificl.github.io/stereocrw/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyang Chen, David F. Fouhey, Andrew Owens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.12489">Sound Localization by Self-Supervised Time Delay Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Sounds reach one microphone in a stereo pair sooner than the other, resulting in an interaural time delay that conveys their directions. Estimating a sound's time delay requires finding correspondences between the signals recorded by each microphone. We propose to learn these correspondences through self-supervision, drawing on recent techniques from visual tracking. We adapt the contrastive random walk of Jabri et al. to learn a cycle-consistent representation from unlabeled stereo sounds, resulting in a model that performs on par with supervised methods on "in the wild" internet recordings. We also propose a multimodal contrastive learning model that solves a visually-guided localization task: estimating the time delay for a particular person in a multi-speaker mixture, given a visual representation of their face. Project site: https://ificl.github.io/stereocrw/
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2203.14360.pdf' target='_blank'>https://arxiv.org/pdf/2203.14360.pdf</a></span>   <span><a href='https://github.com/noahcao/OC_SORT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinkun Cao, Jiangmiao Pang, Xinshuo Weng, Rawal Khirodkar, Kris Kitani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.14360">Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kalman filter (KF) based methods for multi-object tracking (MOT) make an assumption that objects move linearly. While this assumption is acceptable for very short periods of occlusion, linear estimates of motion for prolonged time can be highly inaccurate. Moreover, when there is no measurement available to update Kalman filter parameters, the standard convention is to trust the priori state estimations for posteriori update. This leads to the accumulation of errors during a period of occlusion. The error causes significant motion direction variance in practice. In this work, we show that a basic Kalman filter can still obtain state-of-the-art tracking performance if proper care is taken to fix the noise accumulated during occlusion. Instead of relying only on the linear state estimate (i.e., estimation-centric approach), we use object observations (i.e., the measurements by object detector) to compute a virtual trajectory over the occlusion period to fix the error accumulation of filter parameters during the occlusion period. This allows more time steps to correct errors accumulated during occlusion. We name our method Observation-Centric SORT (OC-SORT). It remains Simple, Online, and Real-Time but improves robustness during occlusion and non-linear motion. Given off-the-shelf detections as input, OC-SORT runs at 700+ FPS on a single CPU. It achieves state-of-the-art on multiple datasets, including MOT17, MOT20, KITTI, head tracking, and especially DanceTrack where the object motion is highly non-linear. The code and models are available at \url{https://github.com/noahcao/OC_SORT}.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2203.04232.pdf' target='_blank'>https://arxiv.org/pdf/2203.04232.pdf</a></span>   <span><a href='https://github.com/jimmy-dq/DMT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Xia, Qiangqiang Wu, Wei Li, Antoni B. Chan, Uwe Stilla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.04232">A Lightweight and Detector-free 3D Single Object Tracker on Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works on 3D single object tracking treat the task as a target-specific 3D detection task, where an off-the-shelf 3D detector is commonly employed for the tracking. However, it is non-trivial to perform accurate target-specific detection since the point cloud of objects in raw LiDAR scans is usually sparse and incomplete. In this paper, we address this issue by explicitly leveraging temporal motion cues and propose DMT, a Detector-free Motion-prediction-based 3D Tracking network that completely removes the usage of complicated 3D detectors and is lighter, faster, and more accurate than previous trackers. Specifically, the motion prediction module is first introduced to estimate a potential target center of the current frame in a point-cloud-free manner. Then, an explicit voting module is proposed to directly regress the 3D box from the estimated target center. Extensive experiments on KITTI and NuScenes datasets demonstrate that our DMT can still achieve better performance (~10% improvement over the NuScenes dataset) and a faster tracking speed (i.e., 72 FPS) than state-of-the-art approaches without applying any complicated 3D detectors. Our code is released at \url{https://github.com/jimmy-dq/DMT}
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2202.13514.pdf' target='_blank'>https://arxiv.org/pdf/2202.13514.pdf</a></span>   <span><a href='https://github.com/dyhBUPT/StrongSORT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/open-mmlab/mmtracking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Du, Zhicheng Zhao, Yang Song, Yanyun Zhao, Fei Su, Tao Gong, Hongying Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.13514">StrongSORT: Make DeepSORT Great Again</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Multi-Object Tracking (MOT) has attracted rising attention, and accordingly, remarkable progresses have been achieved. However, the existing methods tend to use various basic models (e.g, detector and embedding model), and different training or inference tricks, etc. As a result, the construction of a good baseline for a fair comparison is essential. In this paper, a classic tracker, i.e., DeepSORT, is first revisited, and then is significantly improved from multiple perspectives such as object detection, feature embedding, and trajectory association. The proposed tracker, named StrongSORT, contributes a strong and fair baseline for the MOT community. Moreover, two lightweight and plug-and-play algorithms are proposed to address two inherent "missing" problems of MOT: missing association and missing detection. Specifically, unlike most methods, which associate short tracklets into complete trajectories at high computation complexity, we propose an appearance-free link model (AFLink) to perform global association without appearance information, and achieve a good balance between speed and accuracy. Furthermore, we propose a Gaussian-smoothed interpolation (GSI) based on Gaussian process regression to relieve the missing detection. AFLink and GSI can be easily plugged into various trackers with a negligible extra computational cost (1.7 ms and 7.1 ms per image, respectively, on MOT17). Finally, by fusing StrongSORT with AFLink and GSI, the final tracker (StrongSORT++) achieves state-of-the-art results on multiple public benchmarks, i.e., MOT17, MOT20, DanceTrack and KITTI. Codes are available at https://github.com/dyhBUPT/StrongSORT and https://github.com/open-mmlab/mmtracking.
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2111.08954.pdf' target='_blank'>https://arxiv.org/pdf/2111.08954.pdf</a></span>   <span><a href='https://github.com/JHL-HUST/TraSw' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Delv Lin, Qi Chen, Chengyu Zhou, Kun He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.08954">Tracklet-Switch Adversarial Attack against Pedestrian Multi-Object Tracking Trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) has achieved aggressive progress and derived many excellent deep learning trackers. Meanwhile, most deep learning models are known to be vulnerable to adversarial examples that are crafted with small perturbations but could mislead the model prediction. In this work, we observe that the robustness on the MOT trackers is rarely studied, and it is challenging to attack the MOT system since its mature association algorithms are designed to be robust against errors during the tracking. To this end, we analyze the vulnerability of popular MOT trackers and propose a novel adversarial attack method called Tracklet-Switch (TraSw) against the complete tracking pipeline of MOT. The proposed TraSw can fool the advanced deep pedestrian trackers (i.e., FairMOT and ByteTrack), causing them fail to track the targets in the subsequent frames by perturbing very few frames. Experiments on the MOT-Challenge datasets (i.e., 2DMOT15, MOT17, and MOT20) show that TraSw can achieve an extraordinarily high success attack rate of over 95% by attacking only four frames on average. To our knowledge, this is the first work on the adversarial attack against the pedestrian MOT trackers. Code is available at https://github.com/JHL-HUST/TraSw .
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2108.05015.pdf' target='_blank'>https://arxiv.org/pdf/2108.05015.pdf</a></span>   <span><a href='https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jianing Li, Lin Zhu, Zhipeng Zhang, Zhe Chen, Xin Li, Yaowei Wang, Yonghong Tian, Feng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.05015">VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Different from visible cameras which record intensity images frame by frame, the biologically inspired event camera produces a stream of asynchronous and sparse events with much lower latency. In practice, visible cameras can better perceive texture details and slow motion, while event cameras can be free from motion blurs and have a larger dynamic range which enables them to work well under fast motion and low illumination. Therefore, the two sensors can cooperate with each other to achieve more reliable object tracking. In this work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to the lack of a realistic and scaled dataset for this task. Our dataset consists of 820 video pairs captured under low illumination, high speed, and background clutter scenarios, and it is divided into a training and a testing subset, each of which contains 500 and 320 videos, respectively. Based on VisEvent, we transform the event flows into event images and construct more than 30 baseline methods by extending current single-modality trackers into dual-modality versions. More importantly, we further build a simple but effective tracking algorithm by proposing a cross-modality transformer, to achieve more effective feature fusion between visible and event data. Extensive experiments on the proposed VisEvent dataset, FE108, COESOT, and two simulated datasets (i.e., OTB-DVS and VOT-DVS), validated the effectiveness of our model. The dataset and source code have been released on: \url{https://github.com/wangxiao5791509/VisEvent_SOT_Benchmark}.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/1912.02175.pdf' target='_blank'>https://arxiv.org/pdf/1912.02175.pdf</a></span>   <span><a href='https://github.com/martius-lab/blackbox-backprop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marin Vlastelica, Anselm Paulus, VÃ­t Musil, Georg Martius, Michal RolÃ­nek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1912.02175">Differentiation of Blackbox Combinatorial Solvers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem. The code is available at https://github.com/martius-lab/blackbox-backprop.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2410.01678.pdf' target='_blank'>https://arxiv.org/pdf/2410.01678.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayesha Ishaq, Mohamed El Amine Boudjoghra, Jean Lahoud, Fahad Shahbaz Khan, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01678">Open3DTrack: Towards Open-Vocabulary 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking plays a critical role in autonomous driving by enabling the real-time monitoring and prediction of multiple objects' movements. Traditional 3D tracking systems are typically constrained by predefined object categories, limiting their adaptability to novel, unseen objects in dynamic environments. To address this limitation, we introduce open-vocabulary 3D tracking, which extends the scope of 3D tracking to include objects beyond predefined categories. We formulate the problem of open-vocabulary 3D tracking and introduce dataset splits designed to represent various open-vocabulary scenarios. We propose a novel approach that integrates open-vocabulary capabilities into a 3D tracking framework, allowing for generalization to unseen object classes. Our method effectively reduces the performance gap between tracking known and novel objects through strategic adaptation. Experimental results demonstrate the robustness and adaptability of our method in diverse outdoor driving scenarios. To the best of our knowledge, this work is the first to address open-vocabulary 3D tracking, presenting a significant advancement for autonomous systems in real-world settings. Code, trained models, and dataset splits are available publicly.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2209.11404.pdf' target='_blank'>https://arxiv.org/pdf/2209.11404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitao Feng, Lei Bai, Yongqiang Yao, Fengwei Yu, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.11404">Towards Frame Rate Agnostic Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme (PTS) to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2505.12340.pdf' target='_blank'>https://arxiv.org/pdf/2505.12340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jirong Zha, Yuxuan Fan, Kai Li, Han Li, Chen Gao, Xinlei Chen, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12340">DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State estimation is challenging for 3D object tracking with high maneuverability, as the target's state transition function changes rapidly, irregularly, and is unknown to the estimator. Existing work based on interacting multiple model (IMM) achieves more accurate estimation than single-filter approaches through model combination, aligning appropriate models for different motion modes of the target object over time. However, two limitations of conventional IMM remain unsolved. First, the solution space of the model combination is constrained as the target's diverse kinematic properties in different directions are ignored. Second, the model combination weights calculated by the observation likelihood are not accurate enough due to the measurement uncertainty. In this paper, we propose a novel framework, DIMM, to effectively combine estimates from different motion models in each direction, thus increasing the 3D object tracking accuracy. First, DIMM extends the model combination solution space of conventional IMM from a hyperplane to a hypercube by designing a 3D-decoupled multi-hierarchy filter bank, which describes the target's motion with various-order linear models. Second, DIMM generates more reliable combination weight matrices through a differentiable adaptive fusion network for importance allocation rather than solely relying on the observation likelihood; it contains an attention-based twin delayed deep deterministic policy gradient (TD3) method with a hierarchical reward. Experiments demonstrate that DIMM significantly improves the tracking accuracy of existing state estimation methods by 31.61%~99.23%.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2303.12079.pdf' target='_blank'>https://arxiv.org/pdf/2303.12079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Xiyang Dai, Lu Yuan, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12079">OmniTracker: Unifying Object Tracking by Tracking-with-Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking (OT) aims to estimate the positions of target objects in a video sequence. Depending on whether the initial states of target objects are specified by provided annotations in the first frame or the categories, OT could be classified as instance tracking (e.g., SOT and VOS) and category tracking (e.g., MOT, MOTS, and VIS) tasks. Combing the advantages of the best practices developed in both communities, we propose a novel tracking-with-detection paradigm, where tracking supplements appearance priors for detection and detection provides tracking with candidate bounding boxes for association. Equipped with such a design, a unified tracking model, OmniTracker, is further presented to resolve all the tracking tasks with a fully shared network architecture, model weights, and inference pipeline. Extensive experiments on 7 tracking datasets, including LaSOT, TrackingNet, DAVIS16-17, MOT17, MOTS20, and YTVIS19, demonstrate that OmniTracker achieves on-par or even better results than both task-specific and unified tracking models.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2312.08951.pdf' target='_blank'>https://arxiv.org/pdf/2312.08951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Gao, Haojun Xu, Nannan Wang, Jie Li, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08951">Multi-Scene Generalized Trajectory Global Graph Solver with Composite Nodes for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The global multi-object tracking (MOT) system can consider interaction, occlusion, and other ``visual blur'' scenarios to ensure effective object tracking in long videos. Among them, graph-based tracking-by-detection paradigms achieve surprising performance. However, their fully-connected nature poses storage space requirements that challenge algorithm handling long videos. Currently, commonly used methods are still generated trajectories by building one-forward associations across frames. Such matches produced under the guidance of first-order similarity information may not be optimal from a longer-time perspective. Moreover, they often lack an end-to-end scheme for correcting mismatches. This paper proposes the Composite Node Message Passing Network (CoNo-Link), a multi-scene generalized framework for modeling ultra-long frames information for association. CoNo-Link's solution is a low-storage overhead method for building constrained connected graphs. In addition to the previous method of treating objects as nodes, the network innovatively treats object trajectories as nodes for information interaction, improving the graph neural network's feature representation capability. Specifically, we formulate the graph-building problem as a top-k selection task for some reliable objects or trajectories. Our model can learn better predictions on longer-time scales by adding composite nodes. As a result, our method outperforms the state-of-the-art in several commonly used datasets.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2305.10254.pdf' target='_blank'>https://arxiv.org/pdf/2305.10254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yang, Haixing Dai, Zihao Wu, Ramesh Bist, Sachin Subedi, Jin Sun, Guoyu Lu, Changying Li, Tianming Liu, Lilong Chai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10254">SAM for Poultry Science</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the agricultural industry has witnessed significant advancements in artificial intelligence (AI), particularly with the development of large-scale foundational models. Among these foundation models, the Segment Anything Model (SAM), introduced by Meta AI Research, stands out as a groundbreaking solution for object segmentation tasks. While SAM has shown success in various agricultural applications, its potential in the poultry industry, specifically in the context of cage-free hens, remains relatively unexplored. This study aims to assess the zero-shot segmentation performance of SAM on representative chicken segmentation tasks, including part-based segmentation and the use of infrared thermal images, and to explore chicken-tracking tasks by using SAM as a segmentation tool. The results demonstrate SAM's superior performance compared to SegFormer and SETR in both whole and part-based chicken segmentation. SAM-based object tracking also provides valuable data on the behavior and movement patterns of broiler birds. The findings of this study contribute to a better understanding of SAM's potential in poultry science and lay the foundation for future advancements in chicken segmentation and tracking.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2507.05718.pdf' target='_blank'>https://arxiv.org/pdf/2507.05718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Que, Jie Yang, Tao Du, Shuqiang Xia, Chao-Kai Wen, Shi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05718">Cooperative Mapping, Localization, and Beam Management via Multi-Modal SLAM in ISAC Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous localization and mapping (SLAM) plays a critical role in integrated sensing and communication (ISAC) systems for sixth-generation (6G) millimeter-wave (mmWave) networks, enabling environmental awareness and precise user equipment (UE) positioning. While cooperative multi-user SLAM has demonstrated potential in leveraging distributed sensing, its application within multi-modal ISAC systems remains limited, particularly in terms of theoretical modeling and communication-layer integration. This paper proposes a novel multi-modal SLAM framework that addresses these limitations through three key contributions. First, a Bayesian estimation framework is developed for cooperative multi-user SLAM, along with a two-stage algorithm for robust radio map construction under dynamic and heterogeneous sensing conditions. Second, a multi-modal localization strategy is introduced, fusing SLAM results with camera-based multi-object tracking and inertial measurement unit (IMU) data via an error-aware model, significantly improving UE localization in multi-user scenarios. Third, a sensing-aided beam management scheme is proposed, utilizing global radio maps and localization data to generate UE-specific prior information for beam selection, thereby reducing inter-user interference and enhancing downlink spectral efficiency. Simulation results demonstrate that the proposed system improves radio map accuracy by up to 60%, enhances localization accuracy by 37.5%, and significantly outperforms traditional methods in both indoor and outdoor environments.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2406.11303.pdf' target='_blank'>https://arxiv.org/pdf/2406.11303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11303">VideoVista: A Versatile Benchmark for Video Understanding and Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant breakthroughs in video analysis driven by the rapid development of large multimodal models (LMMs), there remains a lack of a versatile evaluation benchmark to comprehensively assess these models' performance in video understanding and reasoning. To address this, we present VideoVista, a video QA benchmark that integrates challenges across diverse content categories, durations, and abilities. Specifically, VideoVista comprises 25,000 questions derived from 3,400 videos spanning 14 categories (e.g., Howto, Film, and Entertainment) with durations ranging from a few seconds to over 10 minutes. Besides, it encompasses 19 types of understanding tasks (e.g., anomaly detection, interaction understanding) and 8 reasoning tasks (e.g., logical reasoning, causal reasoning). To achieve this, we present an automatic data construction framework, leveraging powerful GPT-4o alongside advanced analysis tools (e.g., video splitting, object segmenting, and tracking). We also utilize this framework to construct training data to enhance the capabilities of video-related LMMs (Video-LMMs). Through a comprehensive and quantitative evaluation of cutting-edge models, we reveal that: 1) Video-LMMs face difficulties in fine-grained video tasks involving temporal location, object tracking, and anomaly detection; 2) Video-LMMs present inferior logical and relation reasoning abilities; 3) Open-source Video-LMMs' performance is significantly lower than GPT-4o and Gemini-1.5, lagging by 20 points. This highlights the crucial role VideoVista will play in advancing LMMs that can accurately understand videos and perform precise reasoning.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2408.00606.pdf' target='_blank'>https://arxiv.org/pdf/2408.00606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongtong Feng, Xin Wang, Feilin Han, Leping Zhang, Wenwu Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00606">U2UData: A Large-scale Cooperative Perception Dataset for Swarm UAVs Autonomous Flight</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern perception systems for autonomous flight are sensitive to occlusion and have limited long-range capability, which is a key bottleneck in improving low-altitude economic task performance. Recent research has shown that the UAV-to-UAV (U2U) cooperative perception system has great potential to revolutionize the autonomous flight industry. However, the lack of a large-scale dataset is hindering progress in this area. This paper presents U2UData, the first large-scale cooperative perception dataset for swarm UAVs autonomous flight. The dataset was collected by three UAVs flying autonomously in the U2USim, covering a 9 km$^2$ flight area. It comprises 315K LiDAR frames, 945K RGB and depth frames, and 2.41M annotated 3D bounding boxes for 3 classes. It also includes brightness, temperature, humidity, smoke, and airflow values covering all flight routes. U2USim is the first real-world mapping swarm UAVs simulation environment. It takes Yunnan Province as the prototype and includes 4 terrains, 7 weather conditions, and 8 sensor types. U2UData introduces two perception tasks: cooperative 3D object detection and cooperative 3D object tracking. This paper provides comprehensive benchmarks of recent cooperative perception algorithms on these tasks.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2504.06958.pdf' target='_blank'>https://arxiv.org/pdf/2504.06958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06958">VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2504.06958.pdf' target='_blank'>https://arxiv.org/pdf/2504.06958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06958">VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2504.06958.pdf' target='_blank'>https://arxiv.org/pdf/2504.06958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, Limin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06958">VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2309.04682.pdf' target='_blank'>https://arxiv.org/pdf/2309.04682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Fu, Xiaocong Wang, Haiyang Yu, Ke Niu, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04682">DeNoising-MOT: Towards Multiple Object Tracking with Severe Occlusions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) tends to become more challenging when severe occlusions occur. In this paper, we analyze the limitations of traditional Convolutional Neural Network-based methods and Transformer-based methods in handling occlusions and propose DNMOT, an end-to-end trainable DeNoising Transformer for MOT. To address the challenge of occlusions, we explicitly simulate the scenarios when occlusions occur. Specifically, we augment the trajectory with noises during training and make our model learn the denoising process in an encoder-decoder architecture, so that our model can exhibit strong robustness and perform well under crowded scenes. Additionally, we propose a Cascaded Mask strategy to better coordinate the interaction between different types of queries in the decoder to prevent the mutual suppression between neighboring trajectories under crowded scenes. Notably, the proposed method requires no additional modules like matching strategy and motion state estimation in inference. We conduct extensive experiments on the MOT17, MOT20, and DanceTrack datasets, and the experimental results show that our method outperforms previous state-of-the-art methods by a clear margin.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2303.12535.pdf' target='_blank'>https://arxiv.org/pdf/2303.12535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui Cheng, Shuguang Cui, Zhen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12535">An Effective Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking in LiDAR point clouds (LiDAR SOT) plays a crucial role in autonomous driving. Current approaches all follow the Siamese paradigm based on appearance matching. However, LiDAR point clouds are usually textureless and incomplete, which hinders effective appearance matching. Besides, previous methods greatly overlook the critical motion clues among targets. In this work, beyond 3D Siamese tracking, we introduce a motion-centric paradigm to handle LiDAR SOT from a new perspective. Following this paradigm, we propose a matching-free two-stage tracker M^2-Track. At the 1st-stage, M^2-Track localizes the target within successive frames via motion transformation. Then it refines the target box through motion-assisted shape completion at the 2nd-stage. Due to the motion-centric nature, our method shows its impressive generalizability with limited training labels and provides good differentiability for end-to-end cycle training. This inspires us to explore semi-supervised LiDAR SOT by incorporating a pseudo-label-based motion augmentation and a self-supervised loss term. Under the fully-supervised setting, extensive experiments confirm that M^2-Track significantly outperforms previous state-of-the-arts on three large-scale datasets while running at 57FPS (~3%, ~11% and ~22% precision gains on KITTI, NuScenes, and Waymo Open Dataset respectively). While under the semi-supervised setting, our method performs on par with or even surpasses its fully-supervised counterpart using fewer than half of the labels from KITTI. Further analysis verifies each component's effectiveness and shows the motion-centric paradigm's promising potential for auto-labeling and unsupervised domain adaptation.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2504.12709.pdf' target='_blank'>https://arxiv.org/pdf/2504.12709.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shumin Wang, Zhuoran Yang, Lidian Wang, Zhipeng Tang, Heng Li, Lehan Pan, Sha Zhang, Jie Peng, Jianmin Ji, Yanyong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12709">Self-Supervised Pre-training with Combined Datasets for 3D Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The significant achievements of pre-trained models leveraging large volumes of data in the field of NLP and 2D vision inspire us to explore the potential of extensive data pre-training for 3D perception in autonomous driving. Toward this goal, this paper proposes to utilize massive unlabeled data from heterogeneous datasets to pre-train 3D perception models. We introduce a self-supervised pre-training framework that learns effective 3D representations from scratch on unlabeled data, combined with a prompt adapter based domain adaptation strategy to reduce dataset bias. The approach significantly improves model performance on downstream tasks such as 3D object detection, BEV segmentation, 3D object tracking, and occupancy prediction, and shows steady performance increase as the training data volume scales up, demonstrating the potential of continually benefit 3D perception models for autonomous driving. We will release the source code to inspire further investigations in the community.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2411.13317.pdf' target='_blank'>https://arxiv.org/pdf/2411.13317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sivan Doveh, Nimrod Shabtay, Wei Lin, Eli Schwartz, Hilde Kuehne, Raja Giryes, Rogerio Feris, Leonid Karlinsky, James Glass, Assaf Arbelle, Shimon Ullman, M. Jehanzeb Mirza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13317">Teaching VLMs to Localize Specific Objects from In-context Examples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) have shown remarkable capabilities across diverse visual tasks, including image recognition, video understanding, and Visual Question Answering (VQA) when explicitly trained for these tasks. Despite these advances, we find that present-day VLMs (including the proprietary GPT-4o) lack a fundamental cognitive ability: learning to localize specific objects in a scene by taking into account the context. In this work, we focus on the task of few-shot personalized localization, where a model is given a small set of annotated images (in-context examples) -- each with a category label and bounding box -- and is tasked with localizing the same object type in a query image. Personalized localization can be particularly important in cases of ambiguity of several related objects that can respond to a text or an object that is hard to describe with words. To provoke personalized localization abilities in models, we present a data-centric solution that fine-tunes them using carefully curated data from video object tracking datasets. By leveraging sequences of frames tracking the same object across multiple shots, we simulate instruction-tuning dialogues that promote context awareness. To reinforce this, we introduce a novel regularization technique that replaces object labels with pseudo-names, ensuring the model relies on visual context rather than prior knowledge. Our method significantly enhances the few-shot localization performance of recent VLMs ranging from 7B to 72B in size, without sacrificing generalization, as demonstrated on several benchmarks tailored towards evaluating personalized localization abilities. This work is the first to explore and benchmark personalized few-shot localization for VLMs -- exposing critical weaknesses in present-day VLMs, and laying a foundation for future research in context-driven vision-language applications.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2503.11218.pdf' target='_blank'>https://arxiv.org/pdf/2503.11218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andong Lu, Mai Wen, Jinhu Wang, Yuanzhi Guo, Chenglong Li, Jin Tang, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11218">Towards General Multimodal Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multimodal tracking studies focus on bi-modal scenarios such as RGB-Thermal, RGB-Event, and RGB-Language. Although promising tracking performance is achieved through leveraging complementary cues from different sources, it remains challenging in complex scenes due to the limitations of bi-modal scenarios. In this work, we introduce a general multimodal visual tracking task that fully exploits the advantages of four modalities, including RGB, thermal infrared, event, and language, for robust tracking under challenging conditions. To provide a comprehensive evaluation platform for general multimodal visual tracking, we construct QuadTrack600, a large-scale, high-quality benchmark comprising 600 video sequences (totaling 384.7K high-resolution (640x480) frame groups). In each frame group, all four modalities are spatially aligned and meticulously annotated with bounding boxes, while 21 sequence-level challenge attributes are provided for detailed performance analysis. Despite quad-modal data provides richer information, the differences in information quantity among modalities and the computational burden from four modalities are two challenging issues in fusing four modalities. To handle these issues, we propose a novel approach called QuadFusion, which incorporates an efficient Multiscale Fusion Mamba with four different scanning scales to achieve sufficient interactions of the four modalities while overcoming the exponential computational burden, for general multimodal visual tracking. Extensive experiments on the QuadTrack600 dataset and three bi-modal tracking datasets, including LasHeR, VisEvent, and TNL2K, validate the effectiveness of our QuadFusion.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2312.14471.pdf' target='_blank'>https://arxiv.org/pdf/2312.14471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Liu, Chenglong Li, Futian Wang, Longfeng Shen, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14471">Prototype-based Cross-Modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cross-modal object tracking is an important research topic in the field of information fusion, and it aims to address imaging limitations in challenging scenarios by integrating switchable visible and near-infrared modalities. However, existing tracking methods face some difficulties in adapting to significant target appearance variations in the presence of modality switch. For instance, model update based tracking methods struggle to maintain stable tracking results during modality switching, leading to error accumulation and model drift. Template based tracking methods solely rely on the template information from first frame and/or last frame, which lacks sufficient representation ability and poses challenges in handling significant target appearance changes. To address this problem, we propose a prototype-based cross-modal object tracker called ProtoTrack, which introduces a novel prototype learning scheme to adapt to significant target appearance variations, for cross-modal object tracking. In particular, we design a multi-modal prototype to represent target information by multi-kind samples, including a fixed sample from the first frame and two representative samples from different modalities. Moreover, we develop a prototype generation algorithm based on two new modules to ensure the prototype representative in different challenges......
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2312.14446.pdf' target='_blank'>https://arxiv.org/pdf/2312.14446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Liu, Mengya Zhang, Cheng Li, Chenglong Li, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14446">Cross-Modal Object Tracking via Modality-Aware Fusion Network and A Large-Scale Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual tracking often faces challenges such as invalid targets and decreased performance in low-light conditions when relying solely on RGB image sequences. While incorporating additional modalities like depth and infrared data has proven effective, existing multi-modal imaging platforms are complex and lack real-world applicability. In contrast, near-infrared (NIR) imaging, commonly used in surveillance cameras, can switch between RGB and NIR based on light intensity. However, tracking objects across these heterogeneous modalities poses significant challenges, particularly due to the absence of modality switch signals during tracking. To address these challenges, we propose an adaptive cross-modal object tracking algorithm called Modality-Aware Fusion Network (MAFNet). MAFNet efficiently integrates information from both RGB and NIR modalities using an adaptive weighting mechanism, effectively bridging the appearance gap and enabling a modality-aware target representation. It consists of two key components: an adaptive weighting module and a modality-specific representation module......
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2507.03441.pdf' target='_blank'>https://arxiv.org/pdf/2507.03441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthias Zeller, Daniel Casado Herraez, Jens Behley, Michael Heidingsfeld, Cyrill Stachniss
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03441">Radar Tracker: Moving Instance Tracking in Sparse and Noisy Radar Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots and autonomous vehicles should be aware of what happens in their surroundings. The segmentation and tracking of moving objects are essential for reliable path planning, including collision avoidance. We investigate this estimation task for vehicles using radar sensing. We address moving instance tracking in sparse radar point clouds to enhance scene interpretation. We propose a learning-based radar tracker incorporating temporal offset predictions to enable direct center-based association and enhance segmentation performance by including additional motion cues. We implement attention-based tracking for sparse radar scans to include appearance features and enhance performance. The final association combines geometric and appearance features to overcome the limitations of center-based tracking to associate instances reliably. Our approach shows an improved performance on the moving instance tracking benchmark of the RadarScenes dataset compared to the current state of the art.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2307.02508.pdf' target='_blank'>https://arxiv.org/pdf/2307.02508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanyou Xu, Jiahao Li, Zongxin Yang, Yi Yang, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02508">ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Associating Objects with Transformers (AOT) framework has exhibited exceptional performance in a wide range of complex scenarios for video object tracking and segmentation. In this study, we convert the bounding boxes to masks in reference frames with the help of the Segment Anything Model (SAM) and Alpha-Refine, and then propagate the masks to the current frame, transforming the task from Video Object Tracking (VOT) to video object segmentation (VOS). Furthermore, we introduce MSDeAOT, a variant of the AOT series that incorporates transformers at multiple feature scales. MSDeAOT efficiently propagates object masks from previous frames to the current frame using two feature scales of 16 and 8. As a testament to the effectiveness of our design, we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object Tracking Challenge.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2505.03184.pdf' target='_blank'>https://arxiv.org/pdf/2505.03184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Xu, Ruotong Li, Mengjun Yi, Baile XU, Furao Shen, Jian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03184">Interactive Instance Annotation with Siamese Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Annotating instance masks is time-consuming and labor-intensive. A promising solution is to predict contours using a deep learning model and then allow users to refine them. However, most existing methods focus on in-domain scenarios, limiting their effectiveness for cross-domain annotation tasks. In this paper, we propose SiamAnno, a framework inspired by the use of Siamese networks in object tracking. SiamAnno leverages one-shot learning to annotate previously unseen objects by taking a bounding box as input and predicting object boundaries, which can then be adjusted by annotators. Trained on one dataset and tested on another without fine-tuning, SiamAnno achieves state-of-the-art (SOTA) performance across multiple datasets, demonstrating its ability to handle domain and environment shifts in cross-domain tasks. We also provide more comprehensive results compared to previous work, establishing a strong baseline for future research. To our knowledge, SiamAnno is the first model to explore Siamese architecture for instance annotation.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2310.11957.pdf' target='_blank'>https://arxiv.org/pdf/2310.11957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malte JanÃen, Tobias Pfandzelter, Minghe Wang, David Bermbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11957">Supporting UAVs with Edge Computing: A Review of Opportunities and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the last years, Unmanned Aerial Vehicles (UAVs) have seen significant advancements in sensor capabilities and computational abilities, allowing for efficient autonomous navigation and visual tracking applications. However, the demand for computationally complex tasks has increased faster than advances in battery technology. This opens up possibilities for improvements using edge computing. In edge computing, edge servers can achieve lower latency responses compared to traditional cloud servers through strategic geographic deployments. Furthermore, these servers can maintain superior computational performance compared to UAVs, as they are not limited by battery constraints. Combining these technologies by aiding UAVs with edge servers, research finds measurable improvements in task completion speed, energy efficiency, and reliability across multiple applications and industries. This systematic literature review aims to analyze the current state of research and collect, select, and extract the key areas where UAV activities can be supported and improved through edge computing.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2505.04917.pdf' target='_blank'>https://arxiv.org/pdf/2505.04917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Peng, Chenxu Wang, Minrui Zou, Danyang Li, Zhengpeng Yang, Yimian Dai, Ming-Ming Cheng, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04917">A Simple Detector with Frame Dynamics is a Strong Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2502.18748.pdf' target='_blank'>https://arxiv.org/pdf/2502.18748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaheer Mohamed, Tharindu Fernando, Sridha Sridharan, Peyman Moghadam, Clinton Fookes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18748">Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking using snapshot mosaic cameras is emerging as it provides enhanced spectral information alongside spatial data, contributing to a more comprehensive understanding of material properties. Using transformers, which have consistently outperformed convolutional neural networks (CNNs) in learning better feature representations, would be expected to be effective for Hyperspectral object tracking. However, training large transformers necessitates extensive datasets and prolonged training periods. This is particularly critical for complex tasks like object tracking, and the scarcity of large datasets in the hyperspectral domain acts as a bottleneck in achieving the full potential of powerful transformer models. This paper proposes an effective methodology that adapts large pretrained transformer-based foundation models for hyperspectral object tracking. We propose an adaptive, learnable spatial-spectral token fusion module that can be extended to any transformer-based backbone for learning inherent spatial-spectral features in hyperspectral data. Furthermore, our model incorporates a cross-modality training pipeline that facilitates effective learning across hyperspectral datasets collected with different sensor modalities. This enables the extraction of complementary knowledge from additional modalities, whether or not they are present during testing. Our proposed model also achieves superior performance with minimal training iterations.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2503.07516.pdf' target='_blank'>https://arxiv.org/pdf/2503.07516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weize Li, Yunhao Du, Qixiang Yin, Zhicheng Zhao, Fei Su, Daqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07516">Just Functioning as a Hook for Two-Stage Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) aims to localize target trajectories in videos specified by natural language expressions. Despite recent progress, the intrinsic relationship between the two subtasks of tracking and referring in RMOT has not been fully studied. In this paper, we present a systematic analysis of their interdependence, revealing that current two-stage Referring-by-Tracking (RBT) frameworks remain fundamentally limited by insufficient modeling of subtask interactions and inflexible reliance on semantic alignment modules like CLIP. To this end, we propose JustHook, a novel two-stage RBT framework where a Hook module is firstly designed to redefine the linkage between subtasks. The Hook is built centered on grid sampling at the feature-level and is used for context-aware target feature extraction. Moreover, we propose a Parallel Combined Decoder (PCD) that learns in a unified joint feature space rather than relying on pre-defined cross-modal embeddings. Our design not only enhances the interpretability and modularity but also significantly improves the generalization. Extensive experiments on Refer-KITTI, Refer-KITTI-V2, and Refer-Dance demonstrate that JustHook achieves state-of-the-art performance, improving the HOTA by +6.9\% on Refer-KITTI-V2 with superior efficiency. Code will be available soon.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2502.06583.pdf' target='_blank'>https://arxiv.org/pdf/2502.06583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiantao Hu, Bineng Zhong, Qihua Liang, Zhiyi Mo, Liangtao Shi, Ying Tai, Jian Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06583">Adaptive Perception for Unified Visual Multi-modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, many multi-modal trackers prioritize RGB as the dominant modality, treating other modalities as auxiliary, and fine-tuning separately various multi-modal tasks. This imbalance in modality dependence limits the ability of methods to dynamically utilize complementary information from each modality in complex scenarios, making it challenging to fully perceive the advantages of multi-modal. As a result, a unified parameter model often underperforms in various multi-modal tracking tasks. To address this issue, we propose APTrack, a novel unified tracker designed for multi-modal adaptive perception. Unlike previous methods, APTrack explores a unified representation through an equal modeling strategy. This strategy allows the model to dynamically adapt to various modalities and tasks without requiring additional fine-tuning between different tasks. Moreover, our tracker integrates an adaptive modality interaction (AMI) module that efficiently bridges cross-modality interactions by generating learnable tokens. Experiments conducted on five diverse multi-modal datasets (RGBT234, LasHeR, VisEvent, DepthTrack, and VOT-RGBD2022) demonstrate that APTrack not only surpasses existing state-of-the-art unified multi-modal trackers but also outperforms trackers designed for specific multi-modal tasks.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2411.16934.pdf' target='_blank'>https://arxiv.org/pdf/2411.16934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaira Manigrasso, Matteo Dunnhofer, Antonino Furnari, Moritz Nottebaum, Antonio Finocchiaro, Davide Marana, Rosario Forte, Giovanni Maria Farinella, Christian Micheloni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16934">Online Episodic Memory Visual Query Localization with Egocentric Streaming Object Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Episodic memory retrieval enables wearable cameras to recall objects or events previously observed in video. However, existing formulations assume an "offline" setting with full video access at query time, limiting their applicability in real-world scenarios with power and storage-constrained wearable devices. Towards more application-ready episodic memory systems, we introduce Online Visual Query 2D (OVQ2D), a task where models process video streams online, observing each frame only once, and retrieve object localizations using a compact memory instead of full video history. We address OVQ2D with ESOM (Egocentric Streaming Object Memory), a novel framework integrating an object discovery module, an object tracking module, and a memory module that find, track, and store spatio-temporal object information for efficient querying. Experiments on Ego4D demonstrate ESOM's superiority over other online approaches, though OVQ2D remains challenging, with top performance at only ~4% success. ESOM's accuracy increases markedly with perfect object tracking (31.91%), discovery (40.55%), or both (81.92%), underscoring the need of applied research on these components.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2406.05810.pdf' target='_blank'>https://arxiv.org/pdf/2406.05810.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Ma, Ningfei Wang, Zhengyu Zhao, Qian Wang, Qi Alfred Chen, Chao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05810">ControlLoc: Physical-World Hijacking Attack on Visual Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent research in adversarial machine learning has focused on visual perception in Autonomous Driving (AD) and has shown that printed adversarial patches can attack object detectors. However, it is important to note that AD visual perception encompasses more than just object detection; it also includes Multiple Object Tracking (MOT). MOT enhances the robustness by compensating for object detection errors and requiring consistent object detection results across multiple frames before influencing tracking results and driving decisions. Thus, MOT makes attacks on object detection alone less effective. To attack such robust AD visual perception, a digital hijacking attack has been proposed to cause dangerous driving scenarios. However, this attack has limited effectiveness.
  In this paper, we introduce a novel physical-world adversarial patch attack, ControlLoc, designed to exploit hijacking vulnerabilities in entire AD visual perception. ControlLoc utilizes a two-stage process: initially identifying the optimal location for the adversarial patch, and subsequently generating the patch that can modify the perceived location and shape of objects with the optimal location. Extensive evaluations demonstrate the superior performance of ControlLoc, achieving an impressive average attack success rate of around 98.1% across various AD visual perceptions and datasets, which is four times greater effectiveness than the existing hijacking attack. The effectiveness of ControlLoc is further validated in physical-world conditions, including real vehicle tests under different conditions such as outdoor light conditions with an average attack success rate of 77.5%. AD system-level impact assessments are also included, such as vehicle collision, using industry-grade AD systems and production-grade AD simulators with an average vehicle collision rate and unnecessary emergency stop rate of 81.3%.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2406.05800.pdf' target='_blank'>https://arxiv.org/pdf/2406.05800.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Ma, Ningfei Wang, Zhengyu Zhao, Qi Alfred Chen, Chao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.05800">SlowPerception: Physical-World Latency Attack against Visual Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous Driving (AD) systems critically depend on visual perception for real-time object detection and multiple object tracking (MOT) to ensure safe driving. However, high latency in these visual perception components can lead to significant safety risks, such as vehicle collisions. While previous research has extensively explored latency attacks within the digital realm, translating these methods effectively to the physical world presents challenges. For instance, existing attacks rely on perturbations that are unrealistic or impractical for AD, such as adversarial perturbations affecting areas like the sky, or requiring large patches that obscure most of a camera's view, thus making them impossible to be conducted effectively in the real world.
  In this paper, we introduce SlowPerception, the first physical-world latency attack against AD perception, via generating projector-based universal perturbations. SlowPerception strategically creates numerous phantom objects on various surfaces in the environment, significantly increasing the computational load of Non-Maximum Suppression (NMS) and MOT, thereby inducing substantial latency. Our SlowPerception achieves second-level latency in physical-world settings, with an average latency of 2.5 seconds across different AD perception systems, scenarios, and hardware configurations. This performance significantly outperforms existing state-of-the-art latency attacks. Additionally, we conduct AD system-level impact assessments, such as vehicle collisions, using industry-grade AD systems with production-grade AD simulators with a 97% average rate. We hope that our analyses can inspire further research in this critical domain, enhancing the robustness of AD systems against emerging vulnerabilities.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2404.12031.pdf' target='_blank'>https://arxiv.org/pdf/2404.12031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeliang Ma, Song Yang, Zhe Cui, Zhicheng Zhao, Fei Su, Delong Liu, Jingyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12031">MLS-Track: Multilevel Semantic Interaction in RMOT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The new trend in multi-object tracking task is to track objects of interest using natural language. However, the scarcity of paired prompt-instance data hinders its progress. To address this challenge, we propose a high-quality yet low-cost data generation method base on Unreal Engine 5 and construct a brand-new benchmark dataset, named Refer-UE-City, which primarily includes scenes from intersection surveillance videos, detailing the appearance and actions of people and vehicles. Specifically, it provides 14 videos with a total of 714 expressions, and is comparable in scale to the Refer-KITTI dataset. Additionally, we propose a multi-level semantic-guided multi-object framework called MLS-Track, where the interaction between the model and text is enhanced layer by layer through the introduction of Semantic Guidance Module (SGM) and Semantic Correlation Branch (SCB). Extensive experiments on Refer-UE-City and Refer-KITTI datasets demonstrate the effectiveness of our proposed framework and it achieves state-of-the-art performance. Code and datatsets will be available.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2503.22199.pdf' target='_blank'>https://arxiv.org/pdf/2503.22199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Gao, Yunhe Zhang, Langkun Chen, Yan Jiang, Weiying Xie, Yunsong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22199">Hyperspectral Adapter for Object Tracking based on Hyperspectral Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking based on hyperspectral video attracts increasing attention to the rich material and motion information in the hyperspectral videos. The prevailing hyperspectral methods adapt pretrained RGB-based object tracking networks for hyperspectral tasks by fine-tuning the entire network on hyperspectral datasets, which achieves impressive results in challenging scenarios. However, the performance of hyperspectral trackers is limited by the loss of spectral information during the transformation, and fine-tuning the entire pretrained network is inefficient for practical applications. To address the issues, a new hyperspectral object tracking method, hyperspectral adapter for tracking (HyA-T), is proposed in this work. The hyperspectral adapter for the self-attention (HAS) and the hyperspectral adapter for the multilayer perceptron (HAM) are proposed to generate the adaption information and to transfer the multi-head self-attention (MSA) module and the multilayer perceptron (MLP) in pretrained network for the hyperspectral object tracking task by augmenting the adaption information into the calculation of the MSA and MLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed to augment the original spectral information into the input of the tracking network. The proposed methods extract spectral information directly from the hyperspectral images, which prevent the loss of the spectral information. Moreover, only the parameters in the proposed methods are fine-tuned, which is more efficient than the existing methods. Extensive experiments were conducted on four datasets with various spectral bands, verifing the effectiveness of the proposed methods. The HyA-T achieves state-of-the-art performance on all the datasets.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2509.24741.pdf' target='_blank'>https://arxiv.org/pdf/2509.24741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue-Feng Zhu, Tianyang Xu, Yifan Pan, Jinjie Gu, Xi Li, Jiwen Lu, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24741">Collaborating Vision, Depth, and Thermal Signals for Multi-Modal Tracking: Dataset and Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multi-modal object tracking approaches primarily focus on dual-modal paradigms, such as RGB-Depth or RGB-Thermal, yet remain challenged in complex scenarios due to limited input modalities. To address this gap, this work introduces a novel multi-modal tracking task that leverages three complementary modalities, including visible RGB, Depth (D), and Thermal Infrared (TIR), aiming to enhance robustness in complex scenarios. To support this task, we construct a new multi-modal tracking dataset, coined RGBDT500, which consists of 500 videos with synchronised frames across the three modalities. Each frame provides spatially aligned RGB, depth, and thermal infrared images with precise object bounding box annotations. Furthermore, we propose a novel multi-modal tracker, dubbed RDTTrack. RDTTrack integrates tri-modal information for robust tracking by leveraging a pretrained RGB-only tracking model and prompt learning techniques. In specific, RDTTrack fuses thermal infrared and depth modalities under a proposed orthogonal projection constraint, then integrates them with RGB signals as prompts for the pre-trained foundation tracking model, effectively harmonising tri-modal complementary cues. The experimental results demonstrate the effectiveness and advantages of the proposed method, showing significant improvements over existing dual-modal approaches in terms of tracking accuracy and robustness in complex scenarios.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2509.24741.pdf' target='_blank'>https://arxiv.org/pdf/2509.24741.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue-Feng Zhu, Tianyang Xu, Yifan Pan, Jinjie Gu, Xi Li, Jiwen Lu, Xiao-Jun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24741">Collaborating Vision, Depth, and Thermal Signals for Multi-Modal Tracking: Dataset and Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing multi-modal object tracking approaches primarily focus on dual-modal paradigms, such as RGB-Depth or RGB-Thermal, yet remain challenged in complex scenarios due to limited input modalities. To address this gap, this work introduces a novel multi-modal tracking task that leverages three complementary modalities, including visible RGB, Depth (D), and Thermal Infrared (TIR), aiming to enhance robustness in complex scenarios. To support this task, we construct a new multi-modal tracking dataset, coined RGBDT500, which consists of 500 videos with synchronised frames across the three modalities. Each frame provides spatially aligned RGB, depth, and thermal infrared images with precise object bounding box annotations. Furthermore, we propose a novel multi-modal tracker, dubbed RDTTrack. RDTTrack integrates tri-modal information for robust tracking by leveraging a pretrained RGB-only tracking model and prompt learning techniques. In specific, RDTTrack fuses thermal infrared and depth modalities under a proposed orthogonal projection constraint, then integrates them with RGB signals as prompts for the pre-trained foundation tracking model, effectively harmonising tri-modal complementary cues. The experimental results demonstrate the effectiveness and advantages of the proposed method, showing significant improvements over existing dual-modal approaches in terms of tracking accuracy and robustness in complex scenarios.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2508.13000.pdf' target='_blank'>https://arxiv.org/pdf/2508.13000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Hui Li, Shaochuan Zhao, Tao Zhou, Chunyang Cheng, Xiaojun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13000">Omni Survey for Multimodality Analysis in Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of smart cities has led to the generation of massive amounts of multi-modal data in the context of a range of tasks that enable a comprehensive monitoring of the smart city infrastructure and services. This paper surveys one of the most critical tasks, multi-modal visual object tracking (MMVOT), from the perspective of multimodality analysis. Generally, MMVOT differs from single-modal tracking in four key aspects, data collection, modality alignment and annotation, model designing, and evaluation. Accordingly, we begin with an introduction to the relevant data modalities, laying the groundwork for their integration. This naturally leads to a discussion of challenges of multi-modal data collection, alignment, and annotation. Subsequently, existing MMVOT methods are categorised, based on different ways to deal with visible (RGB) and X modalities: programming the auxiliary X branch with replicated or non-replicated experimental configurations from the RGB branch. Here X can be thermal infrared (T), depth (D), event (E), near infrared (NIR), language (L), or sonar (S). The final part of the paper addresses evaluation and benchmarking. In summary, we undertake an omni survey of all aspects of multi-modal visual object tracking (VOT), covering six MMVOT tasks and featuring 338 references in total. In addition, we discuss the fundamental rhetorical question: Is multi-modal tracking always guaranteed to provide a superior solution to unimodal tracking with the help of information fusion, and if not, in what circumstances its application is beneficial. Furthermore, for the first time in this field, we analyse the distributions of the object categories in the existing MMVOT datasets, revealing their pronounced long-tail nature and a noticeable lack of animal categories when compared with RGB datasets.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2506.20381.pdf' target='_blank'>https://arxiv.org/pdf/2506.20381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Kang, Xin Chen, Jie Zhao, Chunjuan Bo, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20381">Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based visual trackers have demonstrated significant advancements due to their powerful modeling capabilities. However, their practicality is limited on resource-constrained devices because of their slow processing speeds. To address this challenge, we present HiT, a novel family of efficient tracking models that achieve high performance while maintaining fast operation across various devices. The core innovation of HiT lies in its Bridge Module, which connects lightweight transformers to the tracking framework, enhancing feature representation quality. Additionally, we introduce a dual-image position encoding approach to effectively encode spatial information. HiT achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark, outperforming all previous efficient trackers.Building on HiT, we propose DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by selecting routes with varying computational requirements. DyHiT uses search area features extracted by the backbone network and inputs them into an efficient dynamic router to classify tracking scenarios. Based on the classification, DyHiT applies a divide-and-conquer strategy, selecting appropriate routes to achieve a superior trade-off between accuracy and speed. The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free acceleration method based on the dynamic routing architecture of DyHiT. This method significantly improves the execution speed of various high-performance trackers without sacrificing accuracy. For instance, our acceleration method enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of 69.9% on the LaSOT.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2412.19138.pdf' target='_blank'>https://arxiv.org/pdf/2412.19138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Chen, Ben Kang, Wanting Geng, Jiawen Zhu, Yi Liu, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19138">SUTrack: Towards Simple and Unified Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a simple yet unified single object tracking (SOT) framework, dubbed SUTrack. It consolidates five SOT tasks (RGB-based, RGB-Depth, RGB-Thermal, RGB-Event, RGB-Language Tracking) into a unified model trained in a single session. Due to the distinct nature of the data, current methods typically design individual architectures and train separate models for each task. This fragmentation results in redundant training processes, repetitive technological innovations, and limited cross-modal knowledge sharing. In contrast, SUTrack demonstrates that a single model with a unified input representation can effectively handle various common SOT tasks, eliminating the need for task-specific designs and separate training sessions. Additionally, we introduce a task-recognition auxiliary training strategy and a soft token type embedding to further enhance SUTrack's performance with minimal overhead. Experiments show that SUTrack outperforms previous task-specific counterparts across 11 datasets spanning five SOT tasks. Moreover, we provide a range of models catering edge devices as well as high-performance GPUs, striking a good trade-off between speed and accuracy. We hope SUTrack could serve as a strong foundation for further compelling research into unified tracking models. Code and models are available at github.com/chenxin-dlut/SUTrack.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2403.17651.pdf' target='_blank'>https://arxiv.org/pdf/2403.17651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Zhu, Xin Chen, Haiwen Diao, Shuai Li, Jun-Yan He, Chenyang Li, Bin Luo, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17651">Exploring Dynamic Transformer for Efficient Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The speed-precision trade-off is a critical problem for visual object tracking which usually requires low latency and deployment on constrained resources. Existing solutions for efficient tracking mainly focus on adopting light-weight backbones or modules, which nevertheless come at the cost of a sacrifice in precision. In this paper, inspired by dynamic network routing, we propose DyTrack, a dynamic transformer framework for efficient tracking. Real-world tracking scenarios exhibit diverse levels of complexity. We argue that a simple network is sufficient for easy frames in video sequences, while more computation could be assigned to difficult ones. DyTrack automatically learns to configure proper reasoning routes for various inputs, gaining better utilization of the available computational budget. Thus, it can achieve higher performance with the same running speed. We formulate instance-specific tracking as a sequential decision problem and attach terminating branches to intermediate layers of the entire model. Especially, to fully utilize the computations, we introduce the feature recycling mechanism to reuse the outputs of predecessors. Furthermore, a target-aware self-distillation strategy is designed to enhance the discriminating capabilities of early predictions by effectively mimicking the representation pattern of the deep model. Extensive experiments on multiple benchmarks demonstrate that DyTrack achieves promising speed-precision trade-offs with only a single model. For instance, DyTrack obtains 64.9% AUC on LaSOT with a speed of 256 fps.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2403.09634.pdf' target='_blank'>https://arxiv.org/pdf/2403.09634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Chen, Jinglun Li, Zhaoyu Chen, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09634">OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking aims to localize the target object of each frame based on its initial appearance in the first frame. Depending on the input modility, tracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and RGB+D) tracking. Despite the different input modalities, the core aspect of tracking is the temporal matching. Based on this common ground, we present a general framework to unify various tracking tasks, termed as OneTracker. OneTracker first performs a large-scale pre-training on a RGB tracker called Foundation Tracker. This pretraining phase equips the Foundation Tracker with a stable ability to estimate the location of the target object. Then we regard other modality information as prompt and build Prompt Tracker upon Foundation Tracker. Through freezing the Foundation Tracker and only adjusting some additional trainable parameters, Prompt Tracker inhibits the strong localization ability from Foundation Tracker and achieves parameter-efficient finetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of our general framework OneTracker, which is consisted of Foundation Tracker and Prompt Tracker, we conduct extensive experiments on 6 popular tracking tasks across 11 benchmarks and our OneTracker outperforms other models and achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2310.18895.pdf' target='_blank'>https://arxiv.org/pdf/2310.18895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingzhou Sun, Lehan Wang, Zhaojun Nan, Yuxuan Sun, Sheng Zhou, Zhisheng Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18895">Optimizing Task-Specific Timeliness With Edge-Assisted Scheduling for Status Update</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent real-time applications, such as video surveillance, demand intensive computation to extract status information from raw sensing data. This poses a substantial challenge in orchestrating computation and communication resources to provide fresh status information. In this paper, we consider a scenario where multiple energy-constrained devices served by an edge server. To extract status information, each device can either do the computation locally or offload it to the edge server. A scheduling policy is needed to determine when and where to compute for each device, taking into account communication and computation capabilities, as well as task-specific timeliness requirements. To that end, we first model the timeliness requirements as general penalty functions of Age of Information (AoI). A convex optimization problem is formulated to provide a lower bound of the minimum AoI penalty given system parameters. Using KKT conditions, we proposed a novel scheduling policy which evaluates status update priorities based on communication and computation delays and task-specific timeliness requirements. The proposed policy is applied to an object tracking application and carried out on a large video dataset. Simulation results show that our policy improves tracking accuracy compared with scheduling policies based on video content information.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2309.08264.pdf' target='_blank'>https://arxiv.org/pdf/2309.08264.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Zhao, Johan Edstedt, Michael Felsberg, Dong Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.08264">Leveraging the Power of Data Augmentation for Transformer-based Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to long-distance correlation and powerful pretrained models, transformer-based methods have initiated a breakthrough in visual object tracking performance. Previous works focus on designing effective architectures suited for tracking, but ignore that data augmentation is equally crucial for training a well-performing model. In this paper, we first explore the impact of general data augmentations on transformer-based trackers via systematic experiments, and reveal the limited effectiveness of these common strategies. Motivated by experimental observations, we then propose two data augmentation methods customized for tracking. First, we optimize existing random cropping via a dynamic search radius mechanism and simulation for boundary samples. Second, we propose a token-level feature mixing augmentation strategy, which enables the model against challenges like background interference. Extensive experiments on two transformer-based trackers and six benchmarks demonstrate the effectiveness and data efficiency of our methods, especially under challenging settings, like one-shot tracking and small image resolutions.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2308.06904.pdf' target='_blank'>https://arxiv.org/pdf/2308.06904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ben Kang, Xin Chen, Dong Wang, Houwen Peng, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.06904">Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based visual trackers have demonstrated significant progress owing to their superior modeling capabilities. However, existing trackers are hampered by low speed, limiting their applicability on devices with limited computational power. To alleviate this problem, we propose HiT, a new family of efficient tracking models that can run at high speed on different devices while retaining high performance. The central idea of HiT is the Bridge Module, which bridges the gap between modern lightweight transformers and the tracking framework. The Bridge Module incorporates the high-level information of deep features into the shallow large-resolution features. In this way, it produces better features for the tracking head. We also propose a novel dual-image position encoding technique that simultaneously encodes the position information of both the search region and template images. The HiT model achieves promising speed with competitive performance. For instance, it runs at 61 frames per second (fps) on the Nvidia Jetson AGX edge device. Furthermore, HiT attains 64.6% AUC on the LaSOT benchmark, surpassing all previous efficient trackers.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2305.16835.pdf' target='_blank'>https://arxiv.org/pdf/2305.16835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinxue Guo, Tony Huang, Peiyang He, Xuefeng Liu, Tianjun Xiao, Zhaoyu Chen, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16835">OpenVIS: Open-vocabulary Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary Video Instance Segmentation (OpenVIS) can simultaneously detect, segment, and track arbitrary object categories in a video, without being constrained to categories seen during training. In this work, we propose InstFormer, a carefully designed framework for the OpenVIS task that achieves powerful open-vocabulary capabilities through lightweight fine-tuning with limited-category data. InstFormer begins with the open-world mask proposal network, encouraged to propose all potential instance class-agnostic masks by the contrastive instance margin loss. Next, we introduce InstCLIP, adapted from pre-trained CLIP with Instance Guidance Attention, which encodes open-vocabulary instance tokens efficiently. These instance tokens not only enable open-vocabulary classification but also offer strong universal tracking capabilities. Furthermore, to prevent the tracking module from being constrained by the training data with limited categories, we propose the universal rollout association, which transforms the tracking problem into predicting the next frame's instance tracking token. The experimental results demonstrate the proposed InstFormer achieve state-of-the-art capabilities on a comprehensive OpenVIS evaluation benchmark, while also achieves competitive performance in fully supervised VIS task.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2207.04438.pdf' target='_blank'>https://arxiv.org/pdf/2207.04438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Zhu, Xin Chen, Pengyu Zhang, Xinying Wang, Dong Wang, Wenda Zhao, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.04438">SRRT: Exploring Search Region Regulation for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The dominant trackers generate a fixed-size rectangular region based on the previous prediction or initial bounding box as the model input, i.e., search region. While this manner obtains promising tracking efficiency, a fixed-size search region lacks flexibility and is likely to fail in some cases, e.g., fast motion and distractor interference. Trackers tend to lose the target object due to the limited search region or experience interference from distractors due to the excessive search region. Drawing inspiration from the pattern humans track an object, we propose a novel tracking paradigm, called Search Region Regulation Tracking (SRRT) that applies a small eyereach when the target is captured and zooms out the search field when the target is about to be lost. SRRT applies a proposed search region regulator to estimate an optimal search region dynamically for each frame, by which the tracker can flexibly respond to transient changes in the location of object occurrences. To adapt the object's appearance variation during online tracking, we further propose a lockingstate determined updating strategy for reference frame updating. The proposed SRRT is concise without bells and whistles, yet achieves evident improvements and competitive results with other state-of-the-art trackers on eight benchmarks. On the large-scale LaSOT benchmark, SRRT improves SiamRPN++ and TransT with absolute gains of 4.6% and 3.1% in terms of AUC. The code and models will be released.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2204.06918.pdf' target='_blank'>https://arxiv.org/pdf/2204.06918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Cioppa, Silvio Giancola, Adrien Deliege, Le Kang, Xin Zhou, Zhiyu Cheng, Bernard Ghanem, Marc Van Droogenbroeck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.06918">SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking objects in soccer videos is extremely important to gather both player and team statistics, whether it is to estimate the total distance run, the ball possession or the team formation. Video processing can help automating the extraction of those information, without the need of any invasive sensor, hence applicable to any team on any stadium. Yet, the availability of datasets to train learnable models and benchmarks to evaluate methods on a common testbed is very limited. In this work, we propose a novel dataset for multiple object tracking composed of 200 sequences of 30s each, representative of challenging soccer scenarios, and a complete 45-minutes half-time for long-term tracking. The dataset is fully annotated with bounding boxes and tracklet IDs, enabling the training of MOT baselines in the soccer domain and a full benchmarking of those methods on our segregated challenge sets. Our analysis shows that multiple player, referee and ball tracking in soccer videos is far from being solved, with several improvement required in case of fast motion or in scenarios of severe occlusion.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2503.08145.pdf' target='_blank'>https://arxiv.org/pdf/2503.08145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Yifan Jiao, Dan Meng, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08145">Attention to Trajectory: Trajectory-Aware Open-Vocabulary Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-Vocabulary Multi-Object Tracking (OV-MOT) aims to enable approaches to track objects without being limited to a predefined set of categories. Current OV-MOT methods typically rely primarily on instance-level detection and association, often overlooking trajectory information that is unique and essential for object tracking tasks. Utilizing trajectory information can enhance association stability and classification accuracy, especially in cases of occlusion and category ambiguity, thereby improving adaptability to novel classes. Thus motivated, in this paper we propose \textbf{TRACT}, an open-vocabulary tracker that leverages trajectory information to improve both object association and classification in OV-MOT. Specifically, we introduce a \textit{Trajectory Consistency Reinforcement} (\textbf{TCR}) strategy, that benefits tracking performance by improving target identity and category consistency. In addition, we present \textbf{TraCLIP}, a plug-and-play trajectory classification module. It integrates \textit{Trajectory Feature Aggregation} (\textbf{TFA}) and \textit{Trajectory Semantic Enrichment} (\textbf{TSE}) strategies to fully leverage trajectory information from visual and language perspectives for enhancing the classification results. Extensive experiments on OV-TAO show that our TRACT significantly improves tracking performance, highlighting trajectory information as a valuable asset for OV-MOT. Code will be released.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2411.00553.pdf' target='_blank'>https://arxiv.org/pdf/2411.00553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Mancusi, Mattia Bernardi, Aniello Panariello, Angelo Porrello, Rita Cucchiara, Simone Calderara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00553">Is Multiple Object Tracking a Matter of Specialization?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end transformer-based trackers have achieved remarkable performance on most human-related datasets. However, training these trackers in heterogeneous scenarios poses significant challenges, including negative interference - where the model learns conflicting scene-specific parameters - and limited domain generalization, which often necessitates expensive fine-tuning to adapt the models to new domains. In response to these challenges, we introduce Parameter-efficient Scenario-specific Tracking Architecture (PASTA), a novel framework that combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL). Specifically, we define key scenario attributes (e.g, camera-viewpoint, lighting condition) and train specialized PEFT modules for each attribute. These expert modules are combined in parameter space, enabling systematic generalization to new domains without increasing inference time. Extensive experiments on MOTSynth, along with zero-shot evaluations on MOT17 and PersonPath22 demonstrate that a neural tracker built from carefully selected modules surpasses its monolithic counterpart. We release models and code.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2311.10382.pdf' target='_blank'>https://arxiv.org/pdf/2311.10382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhe Li, Sanping Zhou, Zheng Qin, Le Wang, Jinjun Wang, Nanning Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10382">Single-Shot and Multi-Shot Feature Learning for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) remains a vital component of intelligent video analysis, which aims to locate targets and maintain a consistent identity for each target throughout a video sequence. Existing works usually learn a discriminative feature representation, such as motion and appearance, to associate the detections across frames, which are easily affected by mutual occlusion and background clutter in practice. In this paper, we propose a simple yet effective two-stage feature learning paradigm to jointly learn single-shot and multi-shot features for different targets, so as to achieve robust data association in the tracking process. For the detections without being associated, we design a novel single-shot feature learning module to extract discriminative features of each detection, which can efficiently associate targets between adjacent frames. For the tracklets being lost several frames, we design a novel multi-shot feature learning module to extract discriminative features of each tracklet, which can accurately refind these lost targets after a long period. Once equipped with a simple data association logic, the resulting VisualTracker can perform robust MOT based on the single-shot and multi-shot feature representations. Extensive experimental results demonstrate that our method has achieved significant improvements on MOT17 and MOT20 datasets while reaching state-of-the-art performance on DanceTrack dataset.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2308.11513.pdf' target='_blank'>https://arxiv.org/pdf/2308.11513.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gianluca Mancusi, Aniello Panariello, Angelo Porrello, Matteo Fabbri, Simone Calderara, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11513">TrackFlow: Multi-Object Tracking with Normalizing Flows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a candidate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations. Our experiments, conducted on both simulated and real benchmarks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2308.07537.pdf' target='_blank'>https://arxiv.org/pdf/2308.07537.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Li, Zhen Xiao, Lin Yang, Dan Meng, Xin Zhou, Heng Fan, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07537">AttMOT: Improving Multiple-Object Tracking by Introducing Auxiliary Pedestrian Attributes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a fundamental problem in computer vision with numerous applications, such as intelligent surveillance and automated driving. Despite the significant progress made in MOT, pedestrian attributes, such as gender, hairstyle, body shape, and clothing features, which contain rich and high-level information, have been less explored. To address this gap, we propose a simple, effective, and generic method to predict pedestrian attributes to support general Re-ID embedding. We first introduce AttMOT, a large, highly enriched synthetic dataset for pedestrian tracking, containing over 80k frames and 6 million pedestrian IDs with different time, weather conditions, and scenarios. To the best of our knowledge, AttMOT is the first MOT dataset with semantic attributes. Subsequently, we explore different approaches to fuse Re-ID embedding and pedestrian attributes, including attention mechanisms, which we hope will stimulate the development of attribute-assisted MOT. The proposed method AAM demonstrates its effectiveness and generality on several representative pedestrian multi-object tracking benchmarks, including MOT17 and MOT20, through experiments on the AttMOT dataset. When applied to state-of-the-art trackers, AAM achieves consistent improvements in MOTA, HOTA, AssA, IDs, and IDF1 scores. For instance, on MOT17, the proposed method yields a +1.1 MOTA, +1.7 HOTA, and +1.8 IDF1 improvement when used with FairMOT. To encourage further research on attribute-assisted MOT, we will release the AttMOT dataset.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2303.10404.pdf' target='_blank'>https://arxiv.org/pdf/2303.10404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Qin, Sanping Zhou, Le Wang, Jinghai Duan, Gang Hua, Wei Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10404">MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The main challenge of Multi-Object Tracking~(MOT) lies in maintaining a continuous trajectory for each target. Existing methods often learn reliable motion patterns to match the same target between adjacent frames and discriminative appearance features to re-identify the lost targets after a long period. However, the reliability of motion prediction and the discriminability of appearances can be easily hurt by dense crowds and extreme occlusions in the tracking process. In this paper, we propose a simple yet effective multi-object tracker, i.e., MotionTrack, which learns robust short-term and long-term motions in a unified framework to associate trajectories from a short to long range. For dense crowds, we design a novel Interaction Module to learn interaction-aware motions from short-term trajectories, which can estimate the complex movement of each target. For extreme occlusions, we build a novel Refind Module to learn reliable long-term motions from the target's history trajectory, which can link the interrupted trajectory with its corresponding detection. Our Interaction Module and Refind Module are embedded in the well-known tracking-by-detection paradigm, which can work in tandem to maintain superior performance. Extensive experimental results on MOT17 and MOT20 datasets demonstrate the superiority of our approach in challenging scenarios, and it achieves state-of-the-art performances at various MOT metrics.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2303.07625.pdf' target='_blank'>https://arxiv.org/pdf/2303.07625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Liu, Xiaoqiong Liu, Ziruo Yi, Xin Zhou, Thanh Le, Libo Zhang, Yan Huang, Qing Yang, Heng Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07625">PlanarTrack: A Large-scale Challenging Benchmark for Planar Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Planar object tracking is a critical computer vision problem and has drawn increasing interest owing to its key roles in robotics, augmented reality, etc. Despite rapid progress, its further development, especially in the deep learning era, is largely hindered due to the lack of large-scale challenging benchmarks. Addressing this, we introduce PlanarTrack, a large-scale challenging planar tracking benchmark. Specifically, PlanarTrack consists of 1,000 videos with more than 490K images. All these videos are collected in complex unconstrained scenarios from the wild, which makes PlanarTrack, compared with existing benchmarks, more challenging but realistic for real-world applications. To ensure the high-quality annotation, each frame in PlanarTrack is manually labeled using four corners with multiple-round careful inspection and refinement. To our best knowledge, PlanarTrack, to date, is the largest and most challenging dataset dedicated to planar object tracking. In order to analyze the proposed PlanarTrack, we evaluate 10 planar trackers and conduct comprehensive comparisons and in-depth analysis. Our results, not surprisingly, demonstrate that current top-performing planar trackers degenerate significantly on the challenging PlanarTrack and more efforts are needed to improve planar tracking in the future. In addition, we further derive a variant named PlanarTrack$_{\mathbf{BB}}$ for generic object tracking from PlanarTrack. Our evaluation of 10 excellent generic trackers on PlanarTrack$_{\mathrm{BB}}$ manifests that, surprisingly, PlanarTrack$_{\mathrm{BB}}$ is even more challenging than several popular generic tracking benchmarks and more attention should be paid to handle such planar objects, though they are rigid. All benchmarks and evaluations will be released at the project webpage.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2505.18111.pdf' target='_blank'>https://arxiv.org/pdf/2505.18111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Yen Yang, Hsiang-Wei Huang, Pyong-Kun Kim, Chien-Kai Kuo, Jui-Wei Chang, Kwang-Ju Kim, Chung-I Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18111">Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present an effective approach for adapting the Segment Anything Model 2 (SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the powerful pre-trained capabilities of SAM2 and incorporates several key techniques to enhance its performance in VOT applications. By combining SAM2 with our proposed optimizations, we achieved a first place AUC score of 89.4 on the 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the effectiveness of our approach. This paper details our methodology, the specific enhancements made to SAM2, and a comprehensive analysis of our results in the context of VOT solutions along with the multi-modality aspect of the dataset.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2503.01907.pdf' target='_blank'>https://arxiv.org/pdf/2503.01907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunjun Li, Cheng-Yen Yang, Hsiang-Wei Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01907">Technical Report for ReID-SAM on SkiTB Visual Tracking Challenge 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report introduces ReID-SAM, a novel model developed for the SkiTB Challenge that addresses the complexities of tracking skier appearance. Our approach integrates the SAMURAI tracker with a person re-identification (Re-ID) module and advanced post-processing techniques to enhance accuracy in challenging skiing scenarios. We employ an OSNet-based Re-ID model to minimize identity switches and utilize YOLOv11 with Kalman filtering or STARK-based object detection for precise equipment tracking. When evaluated on the SkiTB dataset, ReID-SAM achieved a state-of-the-art F1-score of 0.870, surpassing existing methods across alpine, ski jumping, and freestyle skiing disciplines. These results demonstrate significant advancements in skier tracking accuracy and provide valuable insights for computer vision applications in winter sports.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2411.06702.pdf' target='_blank'>https://arxiv.org/pdf/2411.06702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Syuen Lim, Yadan Luo, Zhi Chen, Tianqi Wei, Scott Chapman, Zi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06702">Track Any Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Detection and Multi-Object Tracking of Sweet Peppers Challenge, we present Track Any Peppers (TAP) - a weakly supervised ensemble technique for sweet peppers tracking. TAP leverages the zero-shot detection capabilities of vision-language foundation models like Grounding DINO to automatically generate pseudo-labels for sweet peppers in video sequences with minimal human intervention. These pseudo-labels, refined when necessary, are used to train a YOLOv8 segmentation network. To enhance detection accuracy under challenging conditions, we incorporate pre-processing techniques such as relighting adjustments and apply depth-based filtering during post-inference. For object tracking, we integrate the Matching by Segment Anything (MASA) adapter with the BoT-SORT algorithm. Our approach achieves a HOTA score of 80.4%, MOTA of 66.1%, Recall of 74.0%, and Precision of 90.7%, demonstrating effective tracking of sweet peppers without extensive manual effort. This work highlights the potential of foundation models for efficient and accurate object detection and tracking in agricultural settings.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2410.23907.pdf' target='_blank'>https://arxiv.org/pdf/2410.23907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Run Luo, Zikai Song, Longze Chen, Yunshui Li, Min Yang, Wei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23907">IP-MOT: Instance Prompt Learning for Cross-Domain Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) aims to associate multiple objects across video frames and is a challenging vision task due to inherent complexities in the tracking environment. Most existing approaches train and track within a single domain, resulting in a lack of cross-domain generalizability to data from other domains. While several works have introduced natural language representation to bridge the domain gap in visual tracking, these textual descriptions often provide too high-level a view and fail to distinguish various instances within the same class. In this paper, we address this limitation by developing IP-MOT, an end-to-end transformer model for MOT that operates without concrete textual descriptions. Our approach is underpinned by two key innovations: Firstly, leveraging a pre-trained vision-language model, we obtain instance-level pseudo textual descriptions via prompt-tuning, which are invariant across different tracking scenes; Secondly, we introduce a query-balanced strategy, augmented by knowledge distillation, to further boost the generalization capabilities of our model. Extensive experiments conducted on three widely used MOT benchmarks, including MOT17, MOT20, and DanceTrack, demonstrate that our approach not only achieves competitive performance on same-domain data compared to state-of-the-art models but also significantly improves the performance of query-based trackers by large margins for cross-domain inputs.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2407.13937.pdf' target='_blank'>https://arxiv.org/pdf/2407.13937.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sheng-Yao Kuan, Jen-Hao Cheng, Hsiang-Wei Huang, Wenhao Chai, Cheng-Yen Yang, Hugo Latapie, Gaowen Liu, Bing-Fei Wu, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13937">Boosting Online 3D Multi-Object Tracking through Camera-Radar Cross Check</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the domain of autonomous driving, the integration of multi-modal perception techniques based on data from diverse sensors has demonstrated substantial progress. Effectively surpassing the capabilities of state-of-the-art single-modality detectors through sensor fusion remains an active challenge. This work leverages the respective advantages of cameras in perspective view and radars in Bird's Eye View (BEV) to greatly enhance overall detection and tracking performance. Our approach, Camera-Radar Associated Fusion Tracking Booster (CRAFTBooster), represents a pioneering effort to enhance radar-camera fusion in the tracking stage, contributing to improved 3D MOT accuracy. The superior experimental results on the K-Radaar dataset, which exhibit 5-6% on IDF1 tracking performance gain, validate the potential of effective sensor fusion in advancing autonomous driving.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2404.05351.pdf' target='_blank'>https://arxiv.org/pdf/2404.05351.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Umberto Albertin, Alessandro Navone, Mauro Martini, Marcello Chiaberge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05351">Semi-Supervised Novelty Detection for Precise Ultra-Wideband Error Signal Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ultra-Wideband (UWB) technology is an emerging low-cost solution for localization in a generic environment. However, UWB signal can be affected by signal reflections and non-line-of-sight (NLoS) conditions between anchors; hence, in a broader sense, the specific geometry of the environment and the disposition of obstructing elements in the map may drastically hinder the reliability of UWB for precise robot localization. This work aims to mitigate this problem by learning a map-specific characterization of the UWB quality signal with a fingerprint semi-supervised novelty detection methodology. An unsupervised autoencoder neural network is trained on nominal UWB map conditions, and then it is used to predict errors derived from the introduction of perturbing novelties in the environment. This work poses a step change in the understanding of UWB localization and its reliability in evolving environmental conditions. The resulting performance of the proposed method is proved by fine-grained experiments obtained with a visual tracking ground truth.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2403.10826.pdf' target='_blank'>https://arxiv.org/pdf/2403.10826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiang-Wei Huang, Cheng-Yen Yang, Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10826">MambaMOT: State-Space Model as Motion Predictor for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of multi-object tracking (MOT), traditional methods often rely on the Kalman filter for motion prediction, leveraging its strengths in linear motion scenarios. However, the inherent limitations of these methods become evident when confronted with complex, nonlinear motions and occlusions prevalent in dynamic environments like sports and dance. This paper explores the possibilities of replacing the Kalman filter with a learning-based motion model that effectively enhances tracking accuracy and adaptability beyond the constraints of Kalman filter-based tracker. In this paper, our proposed method MambaMOT and MambaMOT+, demonstrate advanced performance on challenging MOT datasets such as DanceTrack and SportsMOT, showcasing their ability to handle intricate, non-linear motion patterns and frequent occlusions more effectively than traditional methods.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2402.12763.pdf' target='_blank'>https://arxiv.org/pdf/2402.12763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Jinlin Wu, Jian Chen, Lujie Li, Hongbin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12763">BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localizing the bronchoscope in real time is essential for ensuring intervention quality. However, most existing methods struggle to balance between speed and generalization. To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway association.To achieve real-time performance, we employ a benchmark lightweight detector for efficient lumen detection. We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures. To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway graph that encodes the hierarchy of bronchial tree structures.Experiments on nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64 \%, while accessing up to the 4th generation of airways.Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.Experimental evaluation underscores BronchoTrack's real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2311.16477.pdf' target='_blank'>https://arxiv.org/pdf/2311.16477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyu Jiang, Wenhao Chai, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.16477">UniHPE: Towards Unified Human Pose Estimation via Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent times, there has been a growing interest in developing effective perception techniques for combining information from multiple modalities. This involves aligning features obtained from diverse sources to enable more efficient training with larger datasets and constraints, as well as leveraging the wealth of information contained in each modality. 2D and 3D Human Pose Estimation (HPE) are two critical perceptual tasks in computer vision, which have numerous downstream applications, such as Action Recognition, Human-Computer Interaction, Object tracking, etc. Yet, there are limited instances where the correlation between Image and 2D/3D human pose has been clearly researched using a contrastive paradigm. In this paper, we propose UniHPE, a unified Human Pose Estimation pipeline, which aligns features from all three modalities, i.e., 2D human pose estimation, lifting-based and image-based 3D human pose estimation, in the same pipeline. To align more than two modalities at the same time, we propose a novel singular value based contrastive learning loss, which better aligns different modalities and further boosts the performance. In our evaluation, UniHPE achieves remarkable performance metrics: MPJPE $50.5$mm on the Human3.6M dataset and PAMPJPE $51.6$mm on the 3DPW dataset. Our proposed method holds immense potential to advance the field of computer vision and contribute to various applications.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2311.14762.pdf' target='_blank'>https://arxiv.org/pdf/2311.14762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Kiefer, Lojze Å½ust, Matej Kristan, Janez PerÅ¡, Matija TerÅ¡ek, Arnold Wiliem, Martin Messmer, Cheng-Yen Yang, Hsiang-Wei Huang, Zhongyu Jiang, Heng-Cheng Kuo, Jie Mei, Jenq-Neng Hwang, Daniel Stadler, Lars Sommer, Kaer Huang, Aiguo Zheng, Weitu Chong, Kanokphan Lertniphonphan, Jun Xie, Feng Chen, Jian Li, Zhepeng Wang, Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Tuan-Anh Vu, Hai Nguyen-Truong, Tan-Sang Ha, Quan-Dung Pham, Sai-Kit Yeung, Yuan Feng, Nguyen Thanh Thien, Lixin Tian, Sheng-Yao Kuan, Yuan-Hao Ho, Angel Bueno Rodriguez, Borja Carrillo-Perez, Alexander Klein, Antje Alex, Yannik Steiniger, Felix Sattler, Edgardo Solano-Carrillo, Matej FabijaniÄ, Magdalena Å umunec, Nadir KapetanoviÄ, Andreas Michel, Wolfgang Gross, Martin Weinmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.14762">The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024 addresses maritime computer vision for Unmanned Aerial Vehicles (UAV) and Unmanned Surface Vehicles (USV). Three challenges categories are considered: (i) UAV-based Maritime Object Tracking with Re-identification, (ii) USV-based Maritime Obstacle Segmentation and Detection, (iii) USV-based Maritime Boat Tracking. The USV-based Maritime Obstacle Segmentation and Detection features three sub-challenges, including a new embedded challenge addressing efficicent inference on real-world embedded devices. This report offers a comprehensive overview of the findings from the challenges. We provide both statistical and qualitative analyses, evaluating trends from over 195 submissions. All datasets, evaluation code, and the leaderboard are available to the public at https://macvi.org/workshop/macvi24.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2311.03561.pdf' target='_blank'>https://arxiv.org/pdf/2311.03561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Yen Yang, Hsiang-Wei Huang, Zhongyu Jiang, Heng-Cheng Kuo, Jie Mei, Chung-I Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03561">Sea You Later: Metadata-Guided Long-Term Re-Identification for UAV-Based Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Re-identification (ReID) in multi-object tracking (MOT) for UAVs in maritime computer vision has been challenging for several reasons. More specifically, short-term re-identification (ReID) is difficult due to the nature of the characteristics of small targets and the sudden movement of the drone's gimbal. Long-term ReID suffers from the lack of useful appearance diversity. In response to these challenges, we present an adaptable motion-based MOT algorithm, called Metadata Guided MOT (MG-MOT). This algorithm effectively merges short-term tracking data into coherent long-term tracks, harnessing crucial metadata from UAVs, including GPS position, drone altitude, and camera orientations. Extensive experiments are conducted to validate the efficacy of our MOT algorithm. Utilizing the challenging SeaDroneSee tracking dataset, which encompasses the aforementioned scenarios, we achieve a much-improved performance in the latest edition of the UAV-based Maritime Object Tracking Challenge with a state-of-the-art HOTA of 69.5% and an IDF1 of 85.9% on the testing split.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2308.09905.pdf' target='_blank'>https://arxiv.org/pdf/2308.09905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Run Luo, Zikai Song, Lintao Ma, Jinlin Wei, Wei Yang, Min Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09905">DiffusionTrack: Diffusion Model For Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a challenging vision task that aims to detect individual objects within a single frame and associate them across multiple frames. Recent MOT approaches can be categorized into two-stage tracking-by-detection (TBD) methods and one-stage joint detection and tracking (JDT) methods. Despite the success of these approaches, they also suffer from common problems, such as harmful global or local inconsistency, poor trade-off between robustness and model complexity, and lack of flexibility in different scenes within the same video. In this paper we propose a simple but robust framework that formulates object detection and association jointly as a consistent denoising diffusion process from paired noise boxes to paired ground-truth boxes. This novel progressive denoising diffusion strategy substantially augments the tracker's effectiveness, enabling it to discriminate between various objects. During the training stage, paired object boxes diffuse from paired ground-truth boxes to random distribution, and the model learns detection and tracking simultaneously by reversing this noising process. In inference, the model refines a set of paired randomly generated boxes to the detection and tracking results in a flexible one-step or multi-step denoising diffusion process. Extensive experiments on three widely used MOT benchmarks, including MOT17, MOT20, and Dancetrack, demonstrate that our approach achieves competitive performance compared to the current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2306.05416.pdf' target='_blank'>https://arxiv.org/pdf/2306.05416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei He, Lue Fan, Yuqi Wang, Yuntao Chen, Zehao Huang, Naiyan Wang, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.05416">Tracking Objects with 3D Representation from Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data association is a knotty problem for 2D Multiple Object Tracking due to the object occlusion. However, in 3D space, data association is not so hard. Only with a 3D Kalman Filter, the online object tracker can associate the detections from LiDAR. In this paper, we rethink the data association in 2D MOT and utilize the 3D object representation to separate each object in the feature space. Unlike the existing depth-based MOT methods, the 3D object representation can be jointly learned with the object association module. Besides, the object's 3D representation is learned from the video and supervised by the 2D tracking labels without additional manual annotations from LiDAR or pretrained depth estimator. With 3D object representation learning from Pseudo 3D object labels in monocular videos, we propose a new 2D MOT paradigm, called P3DTrack. Extensive experiments show the effectiveness of our method. We achieve new state-of-the-art performance on the large-scale Waymo Open Dataset.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2305.14298.pdf' target='_blank'>https://arxiv.org/pdf/2305.14298.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>En Yu, Tiancai Wang, Zhuoling Li, Yuang Zhang, Xiangyu Zhang, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.14298">MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although end-to-end multi-object trackers like MOTR enjoy the merits of simplicity, they suffer from the conflict between detection and association seriously, resulting in unsatisfactory convergence dynamics. While MOTRv2 partly addresses this problem, it demands an additional detection network for assistance. In this work, we serve as the first to reveal that this conflict arises from the unfair label assignment between detect queries and track queries during training, where these detect queries recognize targets and track queries associate them. Based on this observation, we propose MOTRv3, which balances the label assignment process using the developed release-fetch supervision strategy. In this strategy, labels are first released for detection and gradually fetched back for association. Besides, another two strategies named pseudo label distillation and track group denoising are designed to further improve the supervision for detection and association. Without the assistance of an extra detection network during inference, MOTRv3 achieves impressive performance across diverse benchmarks, e.g., MOT17, DanceTrack.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2304.04816.pdf' target='_blank'>https://arxiv.org/pdf/2304.04816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Yen Yang, Alan Yu Shyang Tan, Melanie J. Underwood, Charlotte Bodie, Zhongyu Jiang, Steve George, Karl Warr, Jenq-Neng Hwang, Emma Jones
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.04816">Multi-Object Tracking by Iteratively Associating Detections with Uniform Appearance for Trawl-Based Fishing Bycatch Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The aim of in-trawl catch monitoring for use in fishing operations is to detect, track and classify fish targets in real-time from video footage. Information gathered could be used to release unwanted bycatch in real-time. However, traditional multi-object tracking (MOT) methods have limitations, as they are developed for tracking vehicles or pedestrians with linear motions and diverse appearances, which are different from the scenarios such as livestock monitoring. Therefore, we propose a novel MOT method, built upon an existing observation-centric tracking algorithm, by adopting a new iterative association step to significantly boost the performance of tracking targets with a uniform appearance. The iterative association module is designed as an extendable component that can be merged into most existing tracking methods. Our method offers improved performance in tracking targets with uniform appearance and outperforms state-of-the-art techniques on our underwater fish datasets as well as the MOT17 dataset, without increasing latency nor sacrificing accuracy as measured by HOTA, MOTA, and IDF1 performance metrics.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2211.07977.pdf' target='_blank'>https://arxiv.org/pdf/2211.07977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Marchionna, Giulio Pugliese, Mauro Martini, Simone Angarano, Francesco Salvetti, Marcello Chiaberge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.07977">Deep Instance Segmentation and Visual Servoing to Play Jenga with a Cost-Effective Robotic System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The game of Jenga represents an inspiring benchmark for developing innovative manipulation solutions for complex tasks. Indeed, it encouraged the study of novel robotics methods to successfully extract blocks from the tower. A Jenga game round undoubtedly embeds many traits of complex industrial or surgical manipulation tasks, requiring a multi-step strategy, the combination of visual and tactile data, and the highly precise motion of the robotic arm to perform a single block extraction. In this work, we propose a novel, cost-effective architecture for playing Jenga with e.Do, a 6-DOF anthropomorphic manipulator manufactured by Comau, a standard depth camera, and an inexpensive monodirectional force sensor. Our solution focuses on a visual-based control strategy to accurately align the end-effector with the desired block, enabling block extraction by pushing. To this aim, we train an instance segmentation deep learning model on a synthetic custom dataset to segment each piece of the Jenga tower, allowing visual tracking of the desired block's pose during the motion of the manipulator. We integrate the visual-based strategy with a 1D force sensor to detect whether the block can be safely removed by identifying a force threshold value. Our experimentation shows that our low-cost solution allows e.DO to precisely reach removable blocks and perform up to 14 consecutive extractions in a row.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2201.09207.pdf' target='_blank'>https://arxiv.org/pdf/2201.09207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xue-Feng Zhu, Tianyang Xu, Xiao-Jun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2201.09207">Visual Object Tracking on Multi-modal RGB-D Videos: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of visual object tracking has continued for decades. Recent years, as the wide accessibility of the low-cost RGBD sensors, the task of visual object tracking on RGB-D videos has drawn much attention. Compared to conventional RGB-only tracking, the RGB-D videos can provide more information that facilitates objecting tracking in some complicated scenarios. The goal of this review is to summarize the relative knowledge of the research filed of RGB-D tracking. To be specific, we will generalize the related RGB-D tracking benchmarking datasets as well as the corresponding performance measurements. Besides, the existing RGB-D tracking methods are summarized in the paper. Moreover, we discuss the possible future direction in the field of RGB-D tracking.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2412.09991.pdf' target='_blank'>https://arxiv.org/pdf/2412.09991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengmeng Wang, Teli Ma, Shuo Xin, Xiaojun Hou, Jiazheng Xing, Guang Dai, Jingdong Wang, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09991">Visual Object Tracking across Diverse Data Modalities: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Object Tracking (VOT) is an attractive and significant research area in computer vision, which aims to recognize and track specific targets in video sequences where the target objects are arbitrary and class-agnostic. The VOT technology could be applied in various scenarios, processing data of diverse modalities such as RGB, thermal infrared and point cloud. Besides, since no one sensor could handle all the dynamic and varying environments, multi-modal VOT is also investigated. This paper presents a comprehensive survey of the recent progress of both single-modal and multi-modal VOT, especially the deep learning methods. Specifically, we first review three types of mainstream single-modal VOT, including RGB, thermal infrared and point cloud tracking. In particular, we conclude four widely-used single-modal frameworks, abstracting their schemas and categorizing the existing inheritors. Then we summarize four kinds of multi-modal VOT, including RGB-Depth, RGB-Thermal, RGB-LiDAR and RGB-Language. Moreover, the comparison results in plenty of VOT benchmarks of the discussed modalities are presented. Finally, we provide recommendations and insightful observations, inspiring the future development of this fast-growing literature.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2311.02572.pdf' target='_blank'>https://arxiv.org/pdf/2311.02572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoqi Hu, Axi Niu, Yu Zhu, Qingsen Yan, Jinqiu Sun, Yanning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02572">Multiple Object Tracking based on Occlusion-Aware Embedding Consistency Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Joint Detection and Embedding (JDE) framework has achieved remarkable progress for multiple object tracking. Existing methods often employ extracted embeddings to re-establish associations between new detections and previously disrupted tracks. However, the reliability of embeddings diminishes when the region of the occluded object frequently contains adjacent objects or clutters, especially in scenarios with severe occlusion. To alleviate this problem, we propose a novel multiple object tracking method based on visual embedding consistency, mainly including: 1) Occlusion Prediction Module (OPM) and 2) Occlusion-Aware Association Module (OAAM). The OPM predicts occlusion information for each true detection, facilitating the selection of valid samples for consistency learning of the track's visual embedding. The OAAM leverages occlusion cues and visual embeddings to generate two separate embeddings for each track, guaranteeing consistency in both unoccluded and occluded detections. By integrating these two modules, our method is capable of addressing track interruptions caused by occlusion in online tracking scenarios. Extensive experimental results demonstrate that our approach achieves promising performance levels in both unoccluded and occluded tracking scenarios.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2503.11496.pdf' target='_blank'>https://arxiv.org/pdf/2503.11496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaofeng Liang, Runwei Guan, Wangwang Lian, Daizong Liu, Xiaolou Sun, Dongming Wu, Yutao Yue, Weiping Ding, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11496">Cognitive Disentanglement for Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a significant application of multi-source information fusion in intelligent transportation perception systems, Referring Multi-Object Tracking (RMOT) involves localizing and tracking specific objects in video sequences based on language references. However, existing RMOT approaches often treat language descriptions as holistic embeddings and struggle to effectively integrate the rich semantic information contained in language expressions with visual features. This limitation is especially apparent in complex scenes requiring comprehensive understanding of both static object attributes and spatial motion information. In this paper, we propose a Cognitive Disentanglement for Referring Multi-Object Tracking (CDRMT) framework that addresses these challenges. It adapts the "what" and "where" pathways from the human visual processing system to RMOT tasks. Specifically, our framework first establishes cross-modal connections while preserving modality-specific characteristics. It then disentangles language descriptions and hierarchically injects them into object queries, refining object understanding from coarse to fine-grained semantic levels. Finally, we reconstruct language representations based on visual features, ensuring that tracked objects faithfully reflect the referring expression. Extensive experiments on different benchmark datasets demonstrate that CDRMT achieves substantial improvements over state-of-the-art methods, with average gains of 6.0% in HOTA score on Refer-KITTI and 3.2% on Refer-KITTI-V2. Our approach advances the state-of-the-art in RMOT while simultaneously providing new insights into multi-source information fusion.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2409.04979.pdf' target='_blank'>https://arxiv.org/pdf/2409.04979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Lin, Zhe Liu, Yongtao Wang, Le Zhang, Ce Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04979">RCBEVDet++: Toward High-accuracy Radar-Camera Fusion 3D Perception Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving the surrounding environment is a fundamental task in autonomous driving. To obtain highly accurate perception results, modern autonomous driving systems typically employ multi-modal sensors to collect comprehensive environmental data. Among these, the radar-camera multi-modal perception system is especially favored for its excellent sensing capabilities and cost-effectiveness. However, the substantial modality differences between radar and camera sensors pose challenges in fusing information. To address this problem, this paper presents RCBEVDet, a radar-camera fusion 3D object detection framework. Specifically, RCBEVDet is developed from an existing camera-based 3D object detector, supplemented by a specially designed radar feature extractor, RadarBEVNet, and a Cross-Attention Multi-layer Fusion (CAMF) module. Firstly, RadarBEVNet encodes sparse radar points into a dense bird's-eye-view (BEV) feature using a dual-stream radar backbone and a Radar Cross Section aware BEV encoder. Secondly, the CAMF module utilizes a deformable attention mechanism to align radar and camera BEV features and adopts channel and spatial fusion layers to fuse them. To further enhance RCBEVDet's capabilities, we introduce RCBEVDet++, which advances the CAMF through sparse fusion, supports query-based multi-view camera perception models, and adapts to a broader range of perception tasks. Extensive experiments on the nuScenes show that our method integrates seamlessly with existing camera-based 3D perception models and improves their performance across various perception tasks. Furthermore, our method achieves state-of-the-art radar-camera fusion results in 3D object detection, BEV semantic segmentation, and 3D multi-object tracking tasks. Notably, with ViT-L as the image backbone, RCBEVDet++ achieves 72.73 NDS and 67.34 mAP in 3D object detection without test-time augmentation or model ensembling.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2406.01559.pdf' target='_blank'>https://arxiv.org/pdf/2406.01559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Han, Yawen Lu, Guohao Sun, James C. Liang, Zhiwen Cao, Qifan Wang, Qiang Guan, Sohail A. Dianat, Raghuveer M. Rao, Tong Geng, Zhiqiang Tao, Dongfang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01559">Prototypical Transformer as Unified Motion Learners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce the Prototypical Transformer (ProtoFormer), a general and unified framework that approaches various motion tasks from a prototype perspective. ProtoFormer seamlessly integrates prototype learning with Transformer by thoughtfully considering motion dynamics, introducing two innovative designs. First, Cross-Attention Prototyping discovers prototypes based on signature motion patterns, providing transparency in understanding motion scenes. Second, Latent Synchronization guides feature representation learning via prototypes, effectively mitigating the problem of motion uncertainty. Empirical results demonstrate that our approach achieves competitive performance on popular motion tasks such as optical flow and scene depth. Furthermore, it exhibits generality across various downstream tasks, including object tracking and video stabilization.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2304.11523.pdf' target='_blank'>https://arxiv.org/pdf/2304.11523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yawen Lu, Qifan Wang, Siqi Ma, Tong Geng, Yingjie Victor Chen, Huaijin Chen, Dongfang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11523">TransFlow: Transformer as Flow Learner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical flow is an indispensable building block for various important computer vision tasks, including motion estimation, object tracking, and disparity measurement. In this work, we propose TransFlow, a pure transformer architecture for optical flow estimation. Compared to dominant CNN-based methods, TransFlow demonstrates three advantages. First, it provides more accurate correlation and trustworthy matching in flow estimation by utilizing spatial self-attention and cross-attention mechanisms between adjacent frames to effectively capture global dependencies; Second, it recovers more compromised information (e.g., occlusion and motion blur) in flow estimation through long-range temporal association in dynamic scenes; Third, it enables a concise self-learning paradigm and effectively eliminate the complex and laborious multi-stage pre-training procedures. We achieve the state-of-the-art results on the Sintel, KITTI-15, as well as several downstream tasks, including video object detection, interpolation and stabilization. For its efficacy, we hope TransFlow could serve as a flexible baseline for optical flow estimation.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2412.10749.pdf' target='_blank'>https://arxiv.org/pdf/2412.10749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangbin Li, Jinxing Zhou, Jing Zhang, Shengeng Tang, Kun Li, Dan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10749">Patch-level Sounding Object Tracking for Audio-Visual Question Answering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Answering questions related to audio-visual scenes, i.e., the AVQA task, is becoming increasingly popular. A critical challenge is accurately identifying and tracking sounding objects related to the question along the timeline. In this paper, we present a new Patch-level Sounding Object Tracking (PSOT) method. It begins with a Motion-driven Key Patch Tracking (M-KPT) module, which relies on visual motion information to identify salient visual patches with significant movements that are more likely to relate to sounding objects and questions. We measure the patch-wise motion intensity map between neighboring video frames and utilize it to construct and guide a motion-driven graph network. Meanwhile, we design a Sound-driven KPT (S-KPT) module to explicitly track sounding patches. This module also involves a graph network, with the adjacency matrix regularized by the audio-visual correspondence map. The M-KPT and S-KPT modules are performed in parallel for each temporal segment, allowing balanced tracking of salient and sounding objects. Based on the tracked patches, we further propose a Question-driven KPT (Q-KPT) module to retain patches highly relevant to the question, ensuring the model focuses on the most informative clues. The audio-visual-question features are updated during the processing of these modules, which are then aggregated for final answer prediction. Extensive experiments on standard datasets demonstrate the effectiveness of our method, achieving competitive performance even compared to recent large-scale pretraining-based approaches.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2412.04915.pdf' target='_blank'>https://arxiv.org/pdf/2412.04915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khurram Azeem Hashmi, Talha Uddin Sheikh, Didier Stricker, Muhammad Zeshan Afzal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04915">Beyond Boxes: Mask-Guided Spatio-Temporal Feature Aggregation for Video Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The primary challenge in Video Object Detection (VOD) is effectively exploiting temporal information to enhance object representations. Traditional strategies, such as aggregating region proposals, often suffer from feature variance due to the inclusion of background information. We introduce a novel instance mask-based feature aggregation approach, significantly refining this process and deepening the understanding of object dynamics across video frames. We present FAIM, a new VOD method that enhances temporal Feature Aggregation by leveraging Instance Mask features. In particular, we propose the lightweight Instance Feature Extraction Module (IFEM) to learn instance mask features and the Temporal Instance Classification Aggregation Module (TICAM) to aggregate instance mask and classification features across video frames. Using YOLOX as a base detector, FAIM achieves 87.9% mAP on the ImageNet VID dataset at 33 FPS on a single 2080Ti GPU, setting a new benchmark for the speed-accuracy trade-off. Additional experiments on multiple datasets validate that our approach is robust, method-agnostic, and effective in multi-object tracking, demonstrating its broader applicability to video understanding tasks.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2303.16235.pdf' target='_blank'>https://arxiv.org/pdf/2303.16235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanhao Wu, Tong Zhang, Wei Ke, Sabine SÃ¼sstrunk, Mathieu Salzmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.16235">Spatiotemporal Self-supervised Learning for Point Clouds in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL) has the potential to benefit many applications, particularly those where manually annotating data is cumbersome. One such situation is the semantic segmentation of point clouds. In this context, existing methods employ contrastive learning strategies and define positive pairs by performing various augmentation of point clusters in a single frame. As such, these methods do not exploit the temporal nature of LiDAR data. In this paper, we introduce an SSL strategy that leverages positive pairs in both the spatial and temporal domain. To this end, we design (i) a point-to-cluster learning strategy that aggregates spatial information to distinguish objects; and (ii) a cluster-to-cluster learning strategy based on unsupervised object tracking that exploits temporal correspondences. We demonstrate the benefits of our approach via extensive experiments performed by self-supervised training on two large-scale LiDAR datasets and transferring the resulting models to other point cloud segmentation benchmarks. Our results evidence that our method outperforms the state-of-the-art point cloud SSL methods.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2502.17822.pdf' target='_blank'>https://arxiv.org/pdf/2502.17822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Zhang, Xin Li, Xin Lin, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17822">Easy-Poly: A Easy Polyhedral Framework For 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in 3D multi-object tracking (3D MOT) have predominantly relied on tracking-by-detection pipelines. However, these approaches often neglect potential enhancements in 3D detection processes, leading to high false positives (FP), missed detections (FN), and identity switches (IDS), particularly in challenging scenarios such as crowded scenes, small-object configurations, and adverse weather conditions. Furthermore, limitations in data preprocessing, association mechanisms, motion modeling, and life-cycle management hinder overall tracking robustness. To address these issues, we present Easy-Poly, a real-time, filter-based 3D MOT framework for multiple object categories. Our contributions include: (1) An Augmented Proposal Generator utilizing multi-modal data augmentation and refined SpConv operations, significantly improving mAP and NDS on nuScenes; (2) A Dynamic Track-Oriented (DTO) data association algorithm that effectively manages uncertainties and occlusions through optimal assignment and multiple hypothesis handling; (3) A Dynamic Motion Modeling (DMM) incorporating a confidence-weighted Kalman filter and adaptive noise covariances, enhancing MOTA and AMOTA in challenging conditions; and (4) An extended life-cycle management system with adjustive thresholds to reduce ID switches and false terminations. Experimental results show that Easy-Poly outperforms state-of-the-art methods such as Poly-MOT and Fast-Poly, achieving notable gains in mAP (e.g., from 63.30% to 64.96% with LargeKernel3D) and AMOTA (e.g., from 73.1% to 74.5%), while also running in real-time. These findings highlight Easy-Poly's adaptability and robustness in diverse scenarios, making it a compelling choice for autonomous driving and related 3D MOT applications. The source code of this paper will be published upon acceptance.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2408.09191.pdf' target='_blank'>https://arxiv.org/pdf/2408.09191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Wang, Yongcai Wang, Zhimin Xu, Yongyu Guo, Wanting Li, Zhe Huang, Xuewei Bai, Deying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09191">GSLAMOT: A Tracklet and Query Graph-based Simultaneous Locating, Mapping, and Multiple Object Tracking System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>For interacting with mobile objects in unfamiliar environments, simultaneously locating, mapping, and tracking the 3D poses of multiple objects are crucially required. This paper proposes a Tracklet Graph and Query Graph-based framework, i.e., GSLAMOT, to address this challenge. GSLAMOT utilizes camera and LiDAR multimodal information as inputs and divides the representation of the dynamic scene into a semantic map for representing the static environment, a trajectory of the ego-agent, and an online maintained Tracklet Graph (TG) for tracking and predicting the 3D poses of the detected mobile objects. A Query Graph (QG) is constructed in each frame by object detection to query and update TG. For accurate object association, a Multi-criteria Star Graph Association (MSGA) method is proposed to find matched objects between the detections in QG and the predicted tracklets in TG. Then, an Object-centric Graph Optimization (OGO) method is proposed to simultaneously optimize the TG, the semantic map, and the agent trajectory. It triangulates the detected objects into the map to enrich the map's semantic information. We address the efficiency issues to handle the three tightly coupled tasks in parallel. Experiments are conducted on KITTI, Waymo, and an emulated Traffic Congestion dataset that highlights challenging scenarios. Experiments show that GSLAMOT enables accurate crowded object tracking while conducting SLAM accurately in challenging scenarios, demonstrating more excellent performances than the state-of-the-art methods. The code and dataset are at https://gslamot.github.io.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2407.10485.pdf' target='_blank'>https://arxiv.org/pdf/2407.10485.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufeng Yao, Jinlong Peng, Qingdong He, Bo Peng, Hao Chen, Mingmin Chi, Chao Liu, Jon Atli Benediktsson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10485">MM-Tracker: Motion Mamba with Margin Loss for UAV-platform Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) from unmanned aerial vehicle (UAV) platforms requires efficient motion modeling. This is because UAV-MOT faces both local object motion and global camera motion. Motion blur also increases the difficulty of detecting large moving objects. Previous UAV motion modeling approaches either focus only on local motion or ignore motion blurring effects, thus limiting their tracking performance and speed. To address these issues, we propose the Motion Mamba Module, which explores both local and global motion features through cross-correlation and bi-directional Mamba Modules for better motion modeling. To address the detection difficulties caused by motion blur, we also design motion margin loss to effectively improve the detection accuracy of motion blurred objects. Based on the Motion Mamba module and motion margin loss, our proposed MM-Tracker surpasses the state-of-the-art in two widely open-source UAV-MOT datasets. Code will be available.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2407.09051.pdf' target='_blank'>https://arxiv.org/pdf/2407.09051.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Wang, Yongcai Wang, Deying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09051">DroneMOT: Drone-based Multi-Object Tracking Considering Detection Difficulties and Simultaneous Moving of Drones and Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) on static platforms, such as by surveillance cameras, has achieved significant progress, with various paradigms providing attractive performances. However, the effectiveness of traditional MOT methods is significantly reduced when it comes to dynamic platforms like drones. This decrease is attributed to the distinctive challenges in the MOT-on-drone scenario: (1) objects are generally small in the image plane, blurred, and frequently occluded, making them challenging to detect and recognize; (2) drones move and see objects from different angles, causing the unreliability of the predicted positions and feature embeddings of the objects. This paper proposes DroneMOT, which firstly proposes a Dual-domain Integrated Attention (DIA) module that considers the fast movements of drones to enhance the drone-based object detection and feature embedding for small-sized, blurred, and occluded objects. Then, an innovative Motion-Driven Association (MDA) scheme is introduced, considering the concurrent movements of both the drone and the objects. Within MDA, an Adaptive Feature Synchronization (AFS) technique is presented to update the object features seen from different angles. Additionally, a Dual Motion-based Prediction (DMP) method is employed to forecast the object positions. Finally, both the refined feature embeddings and the predicted positions are integrated to enhance the object association. Comprehensive evaluations on VisDrone2019-MOT and UAVDT datasets show that DroneMOT provides substantial performance improvements over the state-of-the-art in the domain of MOT on drones.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2407.08394.pdf' target='_blank'>https://arxiv.org/pdf/2407.08394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengbo Zhang, Li Xu, Duo Peng, Hossein Rahmani, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08394">Diff-Tracker: Text-to-Image Diffusion Models are Unsupervised Trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce Diff-Tracker, a novel approach for the challenging unsupervised visual tracking task leveraging the pre-trained text-to-image diffusion model. Our main idea is to leverage the rich knowledge encapsulated within the pre-trained diffusion model, such as the understanding of image semantics and structural information, to address unsupervised visual tracking. To this end, we design an initial prompt learner to enable the diffusion model to recognize the tracking target by learning a prompt representing the target. Furthermore, to facilitate dynamic adaptation of the prompt to the target's movements, we propose an online prompt updater. Extensive experiments on five benchmark datasets demonstrate the effectiveness of our proposed method, which also achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2406.02147.pdf' target='_blank'>https://arxiv.org/pdf/2406.02147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Tang, Lijun Zhou, Pengkun Hao, Zihang He, Kalok Ho, Shuo Gu, Zhihui Hao, Haiyang Sun, Kun Zhan, Peng Jia, XianPeng Lang, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02147">S2-Track: A Simple yet Strong Approach for End-to-End 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multiple object tracking (MOT) plays a crucial role in autonomous driving perception. Recent end-to-end query-based trackers simultaneously detect and track objects, which have shown promising potential for the 3D MOT task. However, existing methods are still in the early stages of development and lack systematic improvements, failing to track objects in certain complex scenarios, like occlusions and the small size of target object's situations. In this paper, we first summarize the current end-to-end 3D MOT framework by decomposing it into three constituent parts: query initialization, query propagation, and query matching. Then we propose corresponding improvements, which lead to a strong yet simple tracker: S2-Track. Specifically, for query initialization, we present 2D-Prompted Query Initialization, which leverages predicted 2D object and depth information to prompt an initial estimate of the object's 3D location. For query propagation, we introduce an Uncertainty-aware Probabilistic Decoder to capture the uncertainty of complex environment in object prediction with probabilistic attention. For query matching, we propose a Hierarchical Query Denoising strategy to enhance training robustness and convergence. As a result, our S2-Track achieves state-of-the-art performance on nuScenes benchmark, i.e., 66.3% AMOTA on test split, surpassing the previous best end-to-end solution by a significant margin of 8.9% AMOTA. We achieve 1st place on the nuScenes tracking task leaderboard.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2403.01781.pdf' target='_blank'>https://arxiv.org/pdf/2403.01781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tung Le, Khai Nguyen, Shanlin Sun, Nhat Ho, Xiaohui Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01781">Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis. Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT). Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost. Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an unsupervised shape matching framework. This unsupervised framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability measures. We also introduce an adaptive refinement process utilizing entropy regularized OT, further refining feature alignments for accurate point-to-point correspondences. Our method demonstrates superior performance in non-rigid shape matching, including near-isometric and non-isometric scenarios, and excels in downstream tasks like segmentation transfer. The empirical results on diverse datasets highlight our framework's effectiveness and generalization capabilities, setting new standards in non-rigid shape matching with efficient OT metrics and an adaptive refinement module.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2309.15411.pdf' target='_blank'>https://arxiv.org/pdf/2309.15411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Zhang, Xin Li, Liang He, Xin Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15411">3D Multiple Object Tracking on Autonomous Driving: A Literature Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking (3D MOT) stands as a pivotal domain within autonomous driving, experiencing a surge in scholarly interest and commercial promise over recent years. Despite its paramount significance, 3D MOT confronts a myriad of formidable challenges, encompassing abrupt alterations in object appearances, pervasive occlusion, the presence of diminutive targets, data sparsity, missed detections, and the unpredictable initiation and termination of object motion trajectories. Countless methodologies have emerged to grapple with these issues, yet 3D MOT endures as a formidable problem that warrants further exploration. This paper undertakes a comprehensive examination, assessment, and synthesis of the research landscape in this domain, remaining attuned to the latest developments in 3D MOT while suggesting prospective avenues for future investigation. Our exploration commences with a systematic exposition of key facets of 3D MOT and its associated domains, including problem delineation, classification, methodological approaches, fundamental principles, and empirical investigations. Subsequently, we categorize these methodologies into distinct groups, dissecting each group meticulously with regard to its challenges, underlying rationale, progress, merits, and demerits. Furthermore, we present a concise recapitulation of experimental metrics and offer an overview of prevalent datasets, facilitating a quantitative comparison for a more intuitive assessment. Lastly, our deliberations culminate in a discussion of the prevailing research landscape, highlighting extant challenges and charting possible directions for 3D MOT research. We present a structured and lucid road-map to guide forthcoming endeavors in this field.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2308.12549.pdf' target='_blank'>https://arxiv.org/pdf/2308.12549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Teli Ma, Mengmeng Wang, Jimin Xiao, Huifeng Wu, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12549">Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Siamese network has been a de facto benchmark framework for 3D LiDAR object tracking with a shared-parametric encoder extracting features from template and search region, respectively. This paradigm relies heavily on an additional matching network to model the cross-correlation/similarity of the template and search region. In this paper, we forsake the conventional Siamese paradigm and propose a novel single-branch framework, SyncTrack, synchronizing the feature extracting and matching to avoid forwarding encoder twice for template and search region as well as introducing extra parameters of matching network. The synchronization mechanism is based on the dynamic affinity of the Transformer, and an in-depth analysis of the relevance is provided theoretically. Moreover, based on the synchronization, we introduce a novel Attentive Points-Sampling strategy into the Transformer layers (APST), replacing the random/Farthest Points Sampling (FPS) method with sampling under the supervision of attentive relations between the template and search region. It implies connecting point-wise sampling with the feature learning, beneficial to aggregating more distinctive and geometric features for tracking with sparse points. Extensive experiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrack achieves state-of-the-art performance in real-time tracking.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2305.09195.pdf' target='_blank'>https://arxiv.org/pdf/2305.09195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengmeng Wang, Teli Ma, Xingxing Zuo, Jiajun Lv, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09195">Correlation Pyramid Network for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D LiDAR-based single object tracking (SOT) has gained increasing attention as it plays a crucial role in 3D applications such as autonomous driving. The central problem is how to learn a target-aware representation from the sparse and incomplete point clouds. In this paper, we propose a novel Correlation Pyramid Network (CorpNet) with a unified encoder and a motion-factorized decoder. Specifically, the encoder introduces multi-level self attentions and cross attentions in its main branch to enrich the template and search region features and realize their fusion and interaction, respectively. Additionally, considering the sparsity characteristics of the point clouds, we design a lateral correlation pyramid structure for the encoder to keep as many points as possible by integrating hierarchical correlated features. The output features of the search region from the encoder can be directly fed into the decoder for predicting target locations without any extra matcher. Moreover, in the decoder of CorpNet, we design a motion-factorized head to explicitly learn the different movement patterns of the up axis and the x-y plane together. Extensive experiments on two commonly-used datasets show our CorpNet achieves state-of-the-art results while running in real-time.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2509.19851.pdf' target='_blank'>https://arxiv.org/pdf/2509.19851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Bogenberger, Oliver Harrison, Orrin Dahanaggamaarachchi, Lukas Brunke, Jingxing Qian, Siqi Zhou, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19851">Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots deployed in real-world environments, such as homes, must not only navigate safely but also understand their surroundings and adapt to environment changes. To perform tasks efficiently, they must build and maintain a semantic map that accurately reflects the current state of the environment. Existing research on semantic exploration largely focuses on static scenes without persistent object-level instance tracking. A consistent map is, however, crucial for real-world robotic applications where objects in the environment can be removed, reintroduced, or shifted over time. In this work, to close this gap, we propose an open-vocabulary, semantic exploration system for semi-static environments. Our system maintains a consistent map by building a probabilistic model of object instance stationarity, systematically tracking semi-static changes, and actively exploring areas that have not been visited for a prolonged period of time. In addition to active map maintenance, our approach leverages the map's semantic richness with LLM-based reasoning for open-vocabulary object-goal navigation. This enables the robot to search more efficiently by prioritizing contextually relevant areas. We evaluate our approach across multiple real-world semi-static environments. Our system detects 95% of map changes on average, improving efficiency by more than 29% as compared to random and patrol baselines. Overall, our approach achieves a mapping precision within 2% of a fully rebuilt map while requiring substantially less exploration and further completes object goal navigation tasks about 14% faster than the next-best tested strategy (coverage patrolling). A video of our work can be found at http://tiny.cc/sem-explor-semi-static .
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2509.19851.pdf' target='_blank'>https://arxiv.org/pdf/2509.19851.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Bogenberger, Oliver Harrison, Orrin Dahanaggamaarachchi, Lukas Brunke, Jingxing Qian, Siqi Zhou, Angela P. Schoellig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19851">Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots deployed in real-world environments, such as homes, must not only navigate safely but also understand their surroundings and adapt to environment changes. To perform tasks efficiently, they must build and maintain a semantic map that accurately reflects the current state of the environment. Existing research on semantic exploration largely focuses on static scenes without persistent object-level instance tracking. A consistent map is, however, crucial for real-world robotic applications where objects in the environment can be removed, reintroduced, or shifted over time. In this work, to close this gap, we propose an open-vocabulary, semantic exploration system for semi-static environments. Our system maintains a consistent map by building a probabilistic model of object instance stationarity, systematically tracking semi-static changes, and actively exploring areas that have not been visited for a prolonged period of time. In addition to active map maintenance, our approach leverages the map's semantic richness with LLM-based reasoning for open-vocabulary object-goal navigation. This enables the robot to search more efficiently by prioritizing contextually relevant areas. We evaluate our approach across multiple real-world semi-static environments. Our system detects 95% of map changes on average, improving efficiency by more than 29% as compared to random and patrol baselines. Overall, our approach achieves a mapping precision within 2% of a fully rebuilt map while requiring substantially less exploration and further completes object goal navigation tasks about 14% faster than the next-best tested strategy (coverage patrolling). A video of our work can be found at http://tiny.cc/sem-explor-semi-static .
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2508.01599.pdf' target='_blank'>https://arxiv.org/pdf/2508.01599.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minhaj Uddin Ahmad, Sagar Dasgupta, Mizanur Rahman, Sakib Khan, Md Wasiul Haque, Suhala Rabab Saba, David Bodoh, Nathan Huynh, Li Zhao, Eren Erman Ozguven
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01599">Lessons Learned from the Real-World Deployment of Multi-Sensor Fusion for Proactive Work Zone Safety Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proactive safety systems that anticipate and mitigate traffic risks before incidents occur are increasingly recognized as essential for improving work zone safety. Unlike traditional reactive methods, these systems rely on real-time sensing, trajectory prediction, and intelligent infrastructure to detect potential hazards. Existing simulation-based studies often overlook, and real-world deployment studies rarely discuss the practical challenges associated with deploying such systems in operational settings, particularly those involving roadside infrastructure and multi-sensor integration and fusion. This study addresses that gap by presenting deployment insights and technical lessons learned from a real-world implementation of a multi-sensor safety system at an active bridge repair work zone along the N-2/US-75 corridor in Lincoln, Nebraska. The deployed system combines LiDAR, radar, and camera sensors with an edge computing platform to support multi-modal object tracking, trajectory fusion, and real-time analytics. Specifically, this study presents key lessons learned across three critical stages of deployment: (1) sensor selection and placement, (2) sensor calibration, system integration, and validation, and (3) sensor fusion. Additionally, we propose a predictive digital twin framework that leverages fused trajectory data for early conflict detection and real-time warning generation, enabling proactive safety interventions.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2507.05663.pdf' target='_blank'>https://arxiv.org/pdf/2507.05663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Neelay Joglekar, Fei Liu, Florian Richter, Michael C. Yip
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05663">Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Remote Center of Motion (RCM) robotic manipulators have revolutionized Minimally Invasive Surgery, enabling precise, dexterous surgical manipulation within the patient's body cavity without disturbing the insertion point on the patient. Accurate RCM tool control is vital for incorporating autonomous subtasks like suturing, blood suction, and tumor resection into robotic surgical procedures, reducing surgeon fatigue and improving patient outcomes. However, these cable-driven systems are subject to significant joint reading errors, corrupting the kinematics computation necessary to perform control. Although visual tracking with endoscopic cameras can correct errors on in-view joints, errors in the kinematic chain prior to the insertion point are irreparable because they remain out of view. No prior work has characterized the stability of control under these conditions. We fill this gap by designing a provably stable tracking-in-the-loop controller for the out-of-view portion of the RCM manipulator kinematic chain. We additionally incorporate this controller into a bilevel control scheme for the full kinematic chain. We rigorously benchmark our method in simulated and real world settings to verify our theoretical findings. Our work provides key insights into the next steps required for the transition from teleoperated to autonomous surgery.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2504.09361.pdf' target='_blank'>https://arxiv.org/pdf/2504.09361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahuan Long, Tingsong Jiang, Wen Yao, Shuai Jia, Weijia Zhang, Weien Zhou, Chao Ma, Xiaoqian Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09361">PapMOT: Exploring Adversarial Patch Attack against Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking multiple objects in a continuous video stream is crucial for many computer vision tasks. It involves detecting and associating objects with their respective identities across successive frames. Despite significant progress made in multiple object tracking (MOT), recent studies have revealed the vulnerability of existing MOT methods to adversarial attacks. Nevertheless, all of these attacks belong to digital attacks that inject pixel-level noise into input images, and are therefore ineffective in physical scenarios. To fill this gap, we propose PapMOT, which can generate physical adversarial patches against MOT for both digital and physical scenarios. Besides attacking the detection mechanism, PapMOT also optimizes a printable patch that can be detected as new targets to mislead the identity association process. Moreover, we introduce a patch enhancement strategy to further degrade the temporal consistency of tracking results across video frames, resulting in more aggressive attacks. We further develop new evaluation metrics to assess the robustness of MOT against such attacks. Extensive evaluations on multiple datasets demonstrate that our PapMOT can successfully attack various architectures of MOT trackers in digital scenarios. We also validate the effectiveness of PapMOT for physical attacks by deploying printed adversarial patches in the real world.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2501.02467.pdf' target='_blank'>https://arxiv.org/pdf/2501.02467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhou, Jinglun Li, Lingyi Hong, Kaixun Jiang, Pinxue Guo, Weifeng Ge, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02467">DeTrack: In-model Latent Denoising Learning for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous visual object tracking methods employ image-feature regression models or coordinate autoregression models for bounding box prediction. Image-feature regression methods heavily depend on matching results and do not utilize positional prior, while the autoregressive approach can only be trained using bounding boxes available in the training set, potentially resulting in suboptimal performance during testing with unseen data. Inspired by the diffusion model, denoising learning enhances the model's robustness to unseen data. Therefore, We introduce noise to bounding boxes, generating noisy boxes for training, thus enhancing model robustness on testing data. We propose a new paradigm to formulate the visual object tracking problem as a denoising learning process. However, tracking algorithms are usually asked to run in real-time, directly applying the diffusion model to object tracking would severely impair tracking speed. Therefore, we decompose the denoising learning process into every denoising block within a model, not by running the model multiple times, and thus we summarize the proposed paradigm as an in-model latent denoising learning process. Specifically, we propose a denoising Vision Transformer (ViT), which is composed of multiple denoising blocks. In the denoising block, template and search embeddings are projected into every denoising block as conditions. A denoising block is responsible for removing the noise in a predicted bounding box, and multiple stacked denoising blocks cooperate to accomplish the whole denoising process. Subsequently, we utilize image features and trajectory information to refine the denoised bounding box. Besides, we also utilize trajectory memory and visual memory to improve tracking stability. Experimental results validate the effectiveness of our approach, achieving competitive performance on several challenging datasets.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2412.12561.pdf' target='_blank'>https://arxiv.org/pdf/2412.12561.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjun Huang, Yang Ni, Hanning Chen, Yirui He, Ian Bryant, Yezi Liu, Mohsen Imani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12561">Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to localize an arbitrary number of targets based on a language expression and continuously track them in a video. This intricate task involves reasoning on multi-modal data and precise target localization with temporal association. However, prior studies overlook the imbalanced data distribution between newborn targets and existing targets due to the nature of the task. In addition, they only indirectly fuse multi-modal features, struggling to deliver clear guidance on newborn target detection. To solve the above issues, we conduct a collaborative matching strategy to alleviate the impact of the imbalance, boosting the ability to detect newborn targets while maintaining tracking performance. In the encoder, we integrate and enhance the cross-modal and multi-scale fusion, overcoming the bottlenecks in previous work, where limited multi-modal information is shared and interacted between feature maps. In the decoder, we also develop a referring-infused adaptation that provides explicit referring guidance through the query tokens. The experiments showcase the superior performance of our model (+3.42%) compared to prior works, demonstrating the effectiveness of our designs.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2404.06247.pdf' target='_blank'>https://arxiv.org/pdf/2404.06247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianlang Chen, Xuhong Ren, Qing Guo, Felix Juefei-Xu, Di Lin, Wei Feng, Lei Ma, Jianjun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.06247">LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames. This can lead to significant robustness and security issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal continuous representation using the semantic text guidance of the object of interest. This novel continuous representation enables us to reconstruct incoming frames to maintain semantic and appearance consistency with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is even higher than the accuracy on clean data.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2402.14392.pdf' target='_blank'>https://arxiv.org/pdf/2402.14392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhou, Pinxue Guo, Lingyi Hong, Jinglun Li, Wei Zhang, Weifeng Ge, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14392">Reading Relevant Feature from Global Representation Memory for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reference features from a template or historical frames are crucial for visual object tracking. Prior works utilize all features from a fixed template or memory for visual object tracking. However, due to the dynamic nature of videos, the required reference historical information for different search regions at different time steps is also inconsistent. Therefore, using all features in the template and memory can lead to redundancy and impair tracking performance. To alleviate this issue, we propose a novel tracking paradigm, consisting of a relevance attention mechanism and a global representation memory, which can adaptively assist the search region in selecting the most relevant historical information from reference features. Specifically, the proposed relevance attention mechanism in this work differs from previous approaches in that it can dynamically choose and build the optimal global representation memory for the current frame by accessing cross-frame information globally. Moreover, it can flexibly read the relevant historical information from the constructed memory to reduce redundancy and counteract the negative effects of harmful information. Extensive experiments validate the effectiveness of the proposed method, achieving competitive performance on five challenging datasets with 71 FPS.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2308.09908.pdf' target='_blank'>https://arxiv.org/pdf/2308.09908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenrong Zhang, Jianan Liu, Yuxuan Xia, Tao Huang, Qing-Long Han, Hongbin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09908">LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranking board and remains 2nd at the time of submitting this paper, among all online trackers in the KITTI MOT benchmark for cars1
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2308.09908.pdf' target='_blank'>https://arxiv.org/pdf/2308.09908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenrong Zhang, Jianan Liu, Yuxuan Xia, Tao Huang, Qing-Long Han, Hongbin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09908">LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranking board and remains 2nd at the time of submitting this paper, among all online trackers in the KITTI MOT benchmark for cars1
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2308.03061.pdf' target='_blank'>https://arxiv.org/pdf/2308.03061.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanyan Shao, Qi Ye, Wenhan Luo, Kaihao Zhang, Jiming Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03061">InterTracker: Discovering and Tracking General Objects Interacting with Hands in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding human interaction with objects is an important research topic for embodied Artificial Intelligence and identifying the objects that humans are interacting with is a primary problem for interaction understanding. Existing methods rely on frame-based detectors to locate interacting objects. However, this approach is subjected to heavy occlusions, background clutter, and distracting objects. To address the limitations, in this paper, we propose to leverage spatio-temporal information of hand-object interaction to track interactive objects under these challenging cases. Without prior knowledge of the general objects to be tracked like object tracking problems, we first utilize the spatial relation between hands and objects to adaptively discover the interacting objects from the scene. Second, the consistency and continuity of the appearance of objects between successive frames are exploited to track the objects. With this tracking formulation, our method also benefits from training on large-scale general object-tracking datasets. We further curate a video-level hand-object interaction dataset for testing and evaluation from 100DOH. The quantitative results demonstrate that our proposed method outperforms the state-of-the-art methods. Specifically, in scenes with continuous interaction with different objects, we achieve an impressive improvement of about 10% as evaluated using the Average Precision (AP) metric. Our qualitative findings also illustrate that our method can produce more continuous trajectories for interacting objects.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2305.12724.pdf' target='_blank'>https://arxiv.org/pdf/2305.12724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Yan, Weixin Luo, Yujie Zhong, Yiyang Gan, Lin Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12724">Bridging the Gap Between End-to-end and Non-End-to-end Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing end-to-end Multi-Object Tracking (e2e-MOT) methods have not surpassed non-end-to-end tracking-by-detection methods. One potential reason is its label assignment strategy during training that consistently binds the tracked objects with tracking queries and then assigns the few newborns to detection queries. With one-to-one bipartite matching, such an assignment will yield unbalanced training, i.e., scarce positive samples for detection queries, especially for an enclosed scene, as the majority of the newborns come on stage at the beginning of videos. Thus, e2e-MOT will be easier to yield a tracking terminal without renewal or re-initialization, compared to other tracking-by-detection methods. To alleviate this problem, we present Co-MOT, a simple and effective method to facilitate e2e-MOT by a novel coopetition label assignment with a shadow concept. Specifically, we add tracked objects to the matching targets for detection queries when performing the label assignment for training the intermediate decoders. For query initialization, we expand each query by a set of shadow counterparts with limited disturbance to itself. With extensive ablations, Co-MOT achieves superior performance without extra costs, e.g., 69.4% HOTA on DanceTrack and 52.8% TETA on BDD100K. Impressively, Co-MOT only requires 38\% FLOPs of MOTRv2 to attain a similar performance, resulting in the 1.4$\times$ faster inference speed.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2206.10255.pdf' target='_blank'>https://arxiv.org/pdf/2206.10255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianan Liu, Liping Bai, Yuxuan Xia, Tao Huang, Bing Zhu, Qing-Long Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.10255">GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is among crucial applications in modern advanced driver assistance systems (ADAS) and autonomous driving (AD) systems. The global nearest neighbor (GNN) filter, as the earliest random vector-based Bayesian tracking framework, has been adopted in most of state-of-the-arts trackers in the automotive industry. The development of random finite set (RFS) theory facilitates a mathematically rigorous treatment of the MOT problem, and different variants of RFS-based Bayesian filters have then been proposed. However, their effectiveness in the real ADAS and AD application is still an open problem. In this paper, it is demonstrated that the latest RFS-based Bayesian tracking framework could be superior to typical random vector-based Bayesian tracking framework via a systematic comparative study of both traditional random vector-based Bayesian filters with rule-based heuristic track maintenance and RFS-based Bayesian filters on the nuScenes validation dataset. An RFS-based tracker, namely Poisson multi-Bernoulli filter using the global nearest neighbor (GNN-PMB), is proposed to LiDAR-based MOT tasks. This GNN-PMB tracker is simple to use, and it achieves competitive results on the nuScenes dataset. Specifically, the proposed GNN-PMB tracker outperforms most state-of-the-art LiDAR-only trackers and LiDAR and camera fusion-based trackers, ranking the $3^{rd}$ among all LiDAR-only trackers on nuScenes 3D tracking challenge leader board at the time of submission.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2510.00181.pdf' target='_blank'>https://arxiv.org/pdf/2510.00181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis Burbano, Diego Ortiz, Qi Sun, Siwei Yang, Haoqin Tu, Cihang Xie, Yinzhi Cao, Alvaro A Cardenas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00181">CHAI: Command Hijacking against embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms state-of-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2510.00181.pdf' target='_blank'>https://arxiv.org/pdf/2510.00181.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luis Burbano, Diego Ortiz, Qi Sun, Siwei Yang, Haoqin Tu, Cihang Xie, Yinzhi Cao, Alvaro A Cardenas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00181">CHAI: Command Hijacking against embodied AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied Artificial Intelligence (AI) promises to handle edge cases in robotic vehicle systems where data is scarce by using common-sense reasoning grounded in perception and action to generalize beyond training distributions and adapt to novel real-world situations. These capabilities, however, also create new security risks. In this paper, we introduce CHAI (Command Hijacking against embodied AI), a new class of prompt-based attacks that exploit the multimodal language interpretation abilities of Large Visual-Language Models (LVLMs). CHAI embeds deceptive natural language instructions, such as misleading signs, in visual input, systematically searches the token space, builds a dictionary of prompts, and guides an attacker model to generate Visual Attack Prompts. We evaluate CHAI on four LVLM agents; drone emergency landing, autonomous driving, and aerial object tracking, and on a real robotic vehicle. Our experiments show that CHAI consistently outperforms state-of-the-art attacks. By exploiting the semantic and multimodal reasoning strengths of next-generation embodied AI systems, CHAI underscores the urgent need for defenses that extend beyond traditional adversarial robustness.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2505.19990.pdf' target='_blank'>https://arxiv.org/pdf/2505.19990.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Hong, Shilin Yan, Zehao Xiao, Jiayin Cai, Xiaolong Jiang, Yao Hu, Henghui Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19990">Progressive Scaling Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a progressive scaling training strategy for visual object tracking, systematically analyzing the influence of training data volume, model size, and input resolution on tracking performance. Our empirical study reveals that while scaling each factor leads to significant improvements in tracking accuracy, naive training suffers from suboptimal optimization and limited iterative refinement. To address this issue, we introduce DT-Training, a progressive scaling framework that integrates small teacher transfer and dual-branch alignment to maximize model potential. The resulting scaled tracker consistently outperforms state-of-the-art methods across multiple benchmarks, demonstrating strong generalization and transferability of the proposed method. Furthermore, we validate the broader applicability of our approach to additional tasks, underscoring its versatility beyond tracking.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2311.00987.pdf' target='_blank'>https://arxiv.org/pdf/2311.00987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Cui, Cheng Han, Dongfang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.00987">CML-MOTS: Collaborative Multi-task Learning for Multi-Object Tracking and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of computer vision has pushed visual analysis tasks from still images to the video domain. In recent years, video instance segmentation, which aims to track and segment multiple objects in video frames, has drawn much attention for its potential applications in various emerging areas such as autonomous driving, intelligent transportation, and smart retail. In this paper, we propose an effective framework for instance-level visual analysis on video frames, which can simultaneously conduct object detection, instance segmentation, and multi-object tracking. The core idea of our method is collaborative multi-task learning which is achieved by a novel structure, named associative connections among detection, segmentation, and tracking task heads in an end-to-end learnable CNN. These additional connections allow information propagation across multiple related tasks, so as to benefit these tasks simultaneously. We evaluate the proposed method extensively on KITTI MOTS and MOTS Challenge datasets and obtain quite encouraging results.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2309.01078.pdf' target='_blank'>https://arxiv.org/pdf/2309.01078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Son Tran, Cong Tran, Anh Tran, Cuong Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01078">UnsMOT: Unified Framework for Unsupervised Multi-Object Tracking with Geometric Topology Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection has long been a topic of high interest in computer vision literature. Motivated by the fact that annotating data for the multi-object tracking (MOT) problem is immensely expensive, recent studies have turned their attention to the unsupervised learning setting. In this paper, we push forward the state-of-the-art performance of unsupervised MOT methods by proposing UnsMOT, a novel framework that explicitly combines the appearance and motion features of objects with geometric information to provide more accurate tracking. Specifically, we first extract the appearance and motion features using CNN and RNN models, respectively. Then, we construct a graph of objects based on their relative distances in a frame, which is fed into a GNN model together with CNN features to output geometric embedding of objects optimized using an unsupervised loss function. Finally, associations between objects are found by matching not only similar extracted features but also geometric embedding of detections and tracklets. Experimental results show remarkable performance in terms of HOTA, IDF1, and MOTA metrics in comparison with state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2301.10732.pdf' target='_blank'>https://arxiv.org/pdf/2301.10732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aotian Wu, Pan He, Xiao Li, Ke Chen, Sanjay Ranka, Anand Rangarajan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10732">An Efficient Semi-Automated Scheme for Infrastructure LiDAR Annotation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing perception systems rely on sensory data acquired from cameras, which perform poorly in low light and adverse weather conditions. To resolve this limitation, we have witnessed advanced LiDAR sensors become popular in perception tasks in autonomous driving applications. Nevertheless, their usage in traffic monitoring systems is less ubiquitous. We identify two significant obstacles in cost-effectively and efficiently developing such a LiDAR-based traffic monitoring system: (i) public LiDAR datasets are insufficient for supporting perception tasks in infrastructure systems, and (ii) 3D annotations on LiDAR point clouds are time-consuming and expensive. To fill this gap, we present an efficient semi-automated annotation tool that automatically annotates LiDAR sequences with tracking algorithms while offering a fully annotated infrastructure LiDAR dataset -- FLORIDA (Florida LiDAR-based Object Recognition and Intelligent Data Annotation) -- which will be made publicly available. Our advanced annotation tool seamlessly integrates multi-object tracking (MOT), single-object tracking (SOT), and suitable trajectory post-processing techniques. Specifically, we introduce a human-in-the-loop schema in which annotators recursively fix and refine annotations imperfectly predicted by our tool and incrementally add them to the training dataset to obtain better SOT and MOT models. By repeating the process, we significantly increase the overall annotation speed by three to four times and obtain better qualitative annotations than a state-of-the-art annotation tool. The human annotation experiments verify the effectiveness of our annotation tool. In addition, we provide detailed statistics and object detection evaluation results for our dataset in serving as a benchmark for perception tasks at traffic intersections.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2209.12849.pdf' target='_blank'>https://arxiv.org/pdf/2209.12849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sourish Ghosh, Jay Patrikar, Brady Moon, Milad Moghassem Hamidi, Sebastian Scherer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.12849">AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detect-and-Avoid (DAA) capabilities are critical for safe operations of unmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time vision-only detect and tracking framework that respects the size, weight, and power (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios (SNR) of far away aircraft, we propose using full resolution images in a deep learning framework that aligns successive images to remove ego-motion. The aligned images are then used downstream in cascaded primary and secondary classifiers to improve detection and tracking performance on multiple metrics. We show that AirTrack outperforms state-of-the art baselines on the Amazon Airborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a Cessna 182 interacting with general aviation traffic and additional near-collision flight tests with a Bell helicopter flying towards a UAS in a controlled setting showcase that the proposed approach satisfies the newly introduced ASTM F3442/F3442M standard for DAA. Empirical evaluations show that our system has a probability of track of more than 95% up to a range of 700m. Video available at https://youtu.be/H3lL_Wjxjpw .
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2507.09095.pdf' target='_blank'>https://arxiv.org/pdf/2507.09095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Hasan Shahriar, Md Mohaimin Al Barat, Harshavardhan Sundar, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09095">On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal fusion (MMF) plays a critical role in the perception of autonomous driving, which primarily fuses camera and LiDAR streams for a comprehensive and efficient scene understanding. However, its strict reliance on precise temporal synchronization exposes it to new vulnerabilities. In this paper, we introduce DejaVu, a novel attack that exploits network-induced delays to create subtle temporal misalignments across sensor streams, severely degrading downstream MMF-based perception tasks. Our comprehensive attack analysis across different models and datasets reveals these sensors' task-specific imbalanced sensitivities: object detection is overly dependent on LiDAR inputs while object tracking is highly reliant on the camera inputs. Consequently, with a single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to 88.5%, while with a three-frame camera delay, multiple object tracking accuracy (MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense patch that can work alongside the existing perception model to monitor temporal alignment through cross-modal temporal consistency. AION leverages multimodal shared representation learning and dynamic time warping to determine the path of temporal alignment and calculate anomaly scores based on the alignment. Our thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with low false positives across datasets and model architectures, demonstrating it as a robust and generalized defense against the temporal misalignment attacks.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2507.09095.pdf' target='_blank'>https://arxiv.org/pdf/2507.09095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Hasan Shahriar, Md Mohaimin Al Barat, Harshavardhan Sundar, Ning Zhang, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09095">Temporal Misalignment Attacks against Multimodal Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal fusion (MMF) plays a critical role in the perception of autonomous driving, which primarily fuses camera and LiDAR streams for a comprehensive and efficient scene understanding. However, its strict reliance on precise temporal synchronization exposes it to new vulnerabilities. In this paper, we introduce DejaVu, an attack that exploits the in-vehicular network and induces delays across sensor streams to create subtle temporal misalignments, severely degrading downstream MMF-based perception tasks. Our comprehensive attack analysis across different models and datasets reveals the sensors' task-specific imbalanced sensitivities: object detection is overly dependent on LiDAR inputs, while object tracking is highly reliant on the camera inputs. Consequently, with a single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to 88.5%, while with a three-frame camera delay, multiple object tracking accuracy (MOTA) for car drops by 73%. We further demonstrated two attack scenarios using an automotive Ethernet testbed for hardware-in-the-loop validation and the Autoware stack for end-to-end AD simulation, demonstrating the feasibility of the DejaVu attack and its severe impact, such as collisions and phantom braking.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2507.09095.pdf' target='_blank'>https://arxiv.org/pdf/2507.09095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Hasan Shahriar, Md Mohaimin Al Barat, Harshavardhan Sundar, Ning Zhang, Naren Ramakrishnan, Y. Thomas Hou, Wenjing Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09095">Temporal Misalignment Attacks against Multimodal Perception in Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal fusion (MMF) plays a critical role in the perception of autonomous driving, which primarily fuses camera and LiDAR streams for a comprehensive and efficient scene understanding. However, its strict reliance on precise temporal synchronization exposes it to new vulnerabilities. In this paper, we introduce DejaVu, an attack that exploits the in-vehicular network and induces delays across sensor streams to create subtle temporal misalignments, severely degrading downstream MMF-based perception tasks. Our comprehensive attack analysis across different models and datasets reveals the sensors' task-specific imbalanced sensitivities: object detection is overly dependent on LiDAR inputs, while object tracking is highly reliant on the camera inputs. Consequently, with a single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to 88.5%, while with a three-frame camera delay, multiple object tracking accuracy (MOTA) for car drops by 73%. We further demonstrated two attack scenarios using an automotive Ethernet testbed for hardware-in-the-loop validation and the Autoware stack for end-to-end AD simulation, demonstrating the feasibility of the DejaVu attack and its severe impact, such as collisions and phantom braking.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2505.20718.pdf' target='_blank'>https://arxiv.org/pdf/2505.20718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Wu, Shuhang Xu, Hao Chen, Churan Wang, Zhoujun Li, Yizhou Wang, Fangwei Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20718">VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel self-improving framework that enhances Embodied Visual Tracking (EVT) with Vision-Language Models (VLMs) to address the limitations of current active visual tracking systems in recovering from tracking failure. Our approach combines the off-the-shelf active tracking methods with VLMs' reasoning capabilities, deploying a fast visual policy for normal tracking and activating VLM reasoning only upon failure detection. The framework features a memory-augmented self-reflection mechanism that enables the VLM to progressively improve by learning from past experiences, effectively addressing VLMs' limitations in 3D spatial reasoning. Experimental results demonstrate significant performance improvements, with our framework boosting success rates by $72\%$ with state-of-the-art RL-based approaches and $220\%$ with PID-based methods in challenging environments. This work represents the first integration of VLM-based reasoning to assist EVT agents in proactive failure recovery, offering substantial advances for real-world robotic applications that require continuous target monitoring in dynamic, unstructured environments. Project website: https://sites.google.com/view/evt-recovery-assistant.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2505.20710.pdf' target='_blank'>https://arxiv.org/pdf/2505.20710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Wu, Hao Chen, Churan Wang, Fakhri Karray, Zhoujun Li, Yizhou Wang, Fangwei Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20710">Hierarchical Instruction-aware Embodied Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for reinforcement learning-based models due to the substantial gap between high-level user instructions and low-level agent actions. While recent advancements in language models (e.g., LLMs, VLMs, VLAs) have improved instruction comprehension, these models face critical limitations in either inference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To address these challenges, we propose \textbf{Hierarchical Instruction-aware Embodied Visual Tracking (HIEVT)} agent, which bridges instruction comprehension and action generation using \textit{spatial goals} as intermediaries. HIEVT first introduces \textit{LLM-based Semantic-Spatial Goal Aligner} to translate diverse human instructions into spatial goals that directly annotate the desired spatial position. Then the \textit{RL-based Adaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to position the target as specified by the spatial goal. To benchmark UC-EVT tasks, we collect over ten million trajectories for training and evaluate across one seen environment and nine unseen challenging environments. Extensive experiments and real-world deployments demonstrate the robustness and generalizability of HIEVT across diverse environments, varying target dynamics, and complex instruction combinations. The complete project is available at https://sites.google.com/view/hievt.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2502.01896.pdf' target='_blank'>https://arxiv.org/pdf/2502.01896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nastaran Darabi, Divake Kumar, Sina Tayebati, Amit Ranjan Trivedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01896">INTACT: Inducing Noise Tolerance through Adversarial Curriculum Training for LiDAR-based Safety-Critical Perception and Autonomy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present INTACT, a novel two-phase framework designed to enhance the robustness of deep neural networks (DNNs) against noisy LiDAR data in safety-critical perception tasks. INTACT combines meta-learning with adversarial curriculum training (ACT) to systematically address challenges posed by data corruption and sparsity in 3D point clouds. The meta-learning phase equips a teacher network with task-agnostic priors, enabling it to generate robust saliency maps that identify critical data regions. The ACT phase leverages these saliency maps to progressively expose a student network to increasingly complex noise patterns, ensuring targeted perturbation and improved noise resilience. INTACT's effectiveness is demonstrated through comprehensive evaluations on object detection, tracking, and classification benchmarks using diverse datasets, including KITTI, Argoverse, and ModelNet40. Results indicate that INTACT improves model robustness by up to 20% across all tasks, outperforming standard adversarial and curriculum training methods. This framework not only addresses the limitations of conventional training strategies but also offers a scalable and efficient solution for real-world deployment in resource-constrained safety-critical systems. INTACT's principled integration of meta-learning and adversarial training establishes a new paradigm for noise-tolerant 3D perception in safety-critical applications. INTACT improved KITTI Multiple Object Tracking Accuracy (MOTA) by 9.6% (64.1% -> 75.1%) and by 12.4% under Gaussian noise (52.5% -> 73.7%). Similarly, KITTI mean Average Precision (mAP) rose from 59.8% to 69.8% (50% point drop) and 49.3% to 70.9% (Gaussian noise), highlighting the framework's ability to enhance deep learning model resilience in safety-critical object tracking scenarios.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2411.18850.pdf' target='_blank'>https://arxiv.org/pdf/2411.18850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lipeng Gu, Xuefeng Yan, Weiming Wang, Honghua Chen, Dingkun Zhu, Liangliang Nan, Mingqiang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18850">CrossTracker: Robust Multi-modal 3D Multi-Object Tracking via Cross Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The fusion of camera- and LiDAR-based detections offers a promising solution to mitigate tracking failures in 3D multi-object tracking (MOT). However, existing methods predominantly exploit camera detections to correct tracking failures caused by potential LiDAR detection problems, neglecting the reciprocal benefit of refining camera detections using LiDAR data. This limitation is rooted in their single-stage architecture, akin to single-stage object detectors, lacking a dedicated trajectory refinement module to fully exploit the complementary multi-modal information. To this end, we introduce CrossTracker, a novel two-stage paradigm for online multi-modal 3D MOT. CrossTracker operates in a coarse-to-fine manner, initially generating coarse trajectories and subsequently refining them through an independent refinement process. Specifically, CrossTracker incorporates three essential modules: i) a multi-modal modeling (M^3) module that, by fusing multi-modal information (images, point clouds, and even plane geometry extracted from images), provides a robust metric for subsequent trajectory generation. ii) a coarse trajectory generation (C-TG) module that generates initial coarse dual-stream trajectories, and iii) a trajectory refinement (TR) module that refines coarse trajectories through cross correction between camera and LiDAR streams. Comprehensive experiments demonstrate the superior performance of our CrossTracker over its eighteen competitors, underscoring its effectiveness in harnessing the synergistic benefits of camera and LiDAR sensors for robust multi-modal 3D MOT.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2410.08781.pdf' target='_blank'>https://arxiv.org/pdf/2410.08781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinxue Guo, Zixu Zhao, Jianxiong Gao, Chongruo Wu, Tong He, Zheng Zhang, Tianjun Xiao, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08781">VideoSAM: Open-World Video Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video segmentation is essential for advancing robotics and autonomous driving, particularly in open-world settings where continuous perception and object association across video frames are critical. While the Segment Anything Model (SAM) has excelled in static image segmentation, extending its capabilities to video segmentation poses significant challenges. We tackle two major hurdles: a) SAM's embedding limitations in associating objects across frames, and b) granularity inconsistencies in object segmentation. To this end, we introduce VideoSAM, an end-to-end framework designed to address these challenges by improving object tracking and segmentation consistency in dynamic environments. VideoSAM integrates an agglomerated backbone, RADIO, enabling object association through similarity metrics and introduces Cycle-ack-Pairs Propagation with a memory mechanism for stable object tracking. Additionally, we incorporate an autoregressive object-token mechanism within the SAM decoder to maintain consistent granularity across frames. Our method is extensively evaluated on the UVO and BURST benchmarks, and robotic videos from RoboTAP, demonstrating its effectiveness and robustness in real-world scenarios. All codes will be available.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2404.10534.pdf' target='_blank'>https://arxiv.org/pdf/2404.10534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nadezda Kirillova, M. Jehanzeb Mirza, Horst Bischof, Horst Possegger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10534">Into the Fog: Evaluating Robustness of Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art Multiple Object Tracking (MOT) approaches have shown remarkable performance when trained and evaluated on current benchmarks. However, these benchmarks primarily consist of clear weather scenarios, overlooking adverse atmospheric conditions such as fog, haze, smoke and dust. As a result, the robustness of trackers against these challenging conditions remains underexplored. To address this gap, we introduce physics-based volumetric fog simulation method for arbitrary MOT datasets, utilizing frame-by-frame monocular depth estimation and a fog formation optical model. We enhance our simulation by rendering both homogeneous and heterogeneous fog and propose to use the dark channel prior method to estimate atmospheric light, showing promising results even in night and indoor scenes. We present the leading benchmark MOTChallenge (third release) augmented with fog (smoke for indoor scenes) of various intensities and conduct a comprehensive evaluation of MOT methods, revealing their limitations under fog and fog-like challenges.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2403.10830.pdf' target='_blank'>https://arxiv.org/pdf/2403.10830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deyi Ji, Siqi Gao, Lanyun Zhu, Qi Zhu, Yiru Zhao, Peng Xu, Hongtao Lu, Feng Zhao, Jieping Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10830">View-Centric Multi-Object Tracking with Homographic Matching in Moving UAV</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we address the challenge of multi-object tracking (MOT) in moving Unmanned Aerial Vehicle (UAV) scenarios, where irregular flight trajectories, such as hovering, turning left/right, and moving up/down, lead to significantly greater complexity compared to fixed-camera MOT. Specifically, changes in the scene background not only render traditional frame-to-frame object IOU association methods ineffective but also introduce significant view shifts in the objects, which complicates tracking. To overcome these issues, we propose a novel universal HomView-MOT framework, which for the first time, harnesses the view Homography inherent in changing scenes to solve MOT challenges in moving environments, incorporating Homographic Matching and View-Centric concepts. We introduce a Fast Homography Estimation (FHE) algorithm for rapid computation of Homography matrices between video frames, enabling object View-Centric ID Learning (VCIL) and leveraging multi-view Homography to learn cross-view ID features. Concurrently, our Homographic Matching Filter (HMF) maps object bounding boxes from different frames onto a common view plane for a more realistic physical IOU association. Extensive experiments have proven that these innovations allow HomView-MOT to achieve state-of-the-art performance on prominent UAV MOT datasets VisDrone and UAVDT.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2308.00330.pdf' target='_blank'>https://arxiv.org/pdf/2308.00330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matti Henning, Michael Buchholz, Klaus Dietmayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00330">Advancing Frame-Dropping in Multi-Object Tracking-by-Detection Systems Through Event-Based Detection Triggering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With rising computational requirements modern automated vehicles (AVs) often consider trade-offs between energy consumption and perception performance, potentially jeopardizing their safe operation. Frame-dropping in tracking-by-detection perception systems presents a promising approach, although late traffic participant detection might be induced.
  In this paper, we extend our previous work on frame-dropping in tracking-by-detection perception systems. We introduce an additional event-based triggering mechanism using camera object detections to increase both the system's efficiency, as well as its safety. Evaluating both single and multi-modal tracking methods we show that late object detections are mitigated while the potential for reduced energy consumption is significantly increased, reaching nearly 60 Watt per reduced point in HOTA score.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2304.08152.pdf' target='_blank'>https://arxiv.org/pdf/2304.08152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matti Henning, Michael Buchholz, Klaus Dietmayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08152">The Impact of Frame-Dropping on Performance and Energy Consumption for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The safety of automated vehicles (AVs) relies on the representation of their environment. Consequently, state-of-the-art AVs employ potent sensor systems to achieve the best possible environment representation at all times. Although these high-performing systems achieve impressive results, they induce significant requirements for the processing capabilities of an AV's computational hardware components and their energy consumption.
  To enable a dynamic adaptation of such perception systems based on the situational perception requirements, we introduce a model-agnostic method for the scalable employment of single-frame object detection models using frame-dropping in tracking-by-detection systems. We evaluate our approach on the KITTI 3D Tracking Benchmark, showing that significant energy savings can be achieved at acceptable performance degradation, reaching up to 28% reduction of energy consumption at a performance decline of 6.6% in HOTA score.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2304.03623.pdf' target='_blank'>https://arxiv.org/pdf/2304.03623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang, Yizhou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03623">RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active Object Tracking (AOT) aims to maintain a specific relation between the tracker and object(s) by autonomously controlling the motion system of a tracker given observations. AOT has wide-ranging applications, such as in mobile robots and autonomous driving. However, building a generalizable active tracker that works robustly across different scenarios remains a challenge, especially in unstructured environments with cluttered obstacles and diverse layouts. We argue that constructing a state representation capable of modeling the geometry structure of the surroundings and the dynamics of the target is crucial for achieving this goal. To address this challenge, we present RSPT, a framework that forms a structure-aware motion representation by Reconstructing the Surroundings and Predicting the target Trajectory. Additionally, we enhance the generalization of the policy network by training in an asymmetric dueling mechanism. We evaluate RSPT on various simulated scenarios and show that it outperforms existing methods in unseen environments, particularly those with complex obstacles and layouts. We also demonstrate the successful transfer of RSPT to real-world settings. Project Website: https://sites.google.com/view/aot-rspt.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2508.00358.pdf' target='_blank'>https://arxiv.org/pdf/2508.00358.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Gong, Mengjun Chen, Hao Liu, Gao Yongsheng, Lei Yang, Naibang Wang, Ziying Song, Haoqun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00358">Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) enables autonomous vehicles to continuously perceive dynamic objects, supplying essential temporal cues for prediction, behavior understanding, and safe planning. However, conventional tracking-by-detection methods typically rely on static coordinate transformations based on ego-vehicle poses, disregarding ego-vehicle speed-induced variations in observation noise and reference frame changes, which degrades tracking stability and accuracy in dynamic, high-speed scenarios. In this paper, we investigate the critical role of ego-vehicle speed in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that dynamically adapts uncertainty modeling to ego-vehicle speed, significantly improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that adaptively predicts key parameters of SG-LKF. To enhance inter-frame association and trajectory continuity, we introduce a self-supervised trajectory consistency loss jointly optimized with semantic and positional constraints. Extensive experiments show that SG-LKF ranks first among all vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on nuScenes 3D MOT.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2412.09097.pdf' target='_blank'>https://arxiv.org/pdf/2412.09097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengcai Zhou, Halvin Yang, Luping Xiang, Kun Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09097">Temporal-Assisted Beamforming and Trajectory Prediction in Sensing-Enabled UAV Communications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the evolving landscape of high-speed communication, the shift from traditional pilot-based methods to a Sensing-Oriented Approach (SOA) is anticipated to gain momentum. This paper delves into the development of an innovative Integrated Sensing and Communication (ISAC) framework, specifically tailored for beamforming and trajectory prediction processes. Central to this research is the exploration of an Unmanned Aerial Vehicle (UAV)-enabled communication system, which seamlessly integrates ISAC technology. This integration underscores the synergistic interplay between sensing and communication capabilities. The proposed system initially deploys omnidirectional beams for the sensing-focused phase, subsequently transitioning to directional beams for precise object tracking. This process incorporates an Extended Kalman Filtering (EKF) methodology for the accurate estimation and prediction of object states. A novel frame structure is introduced, employing historical sensing data to optimize beamforming in real-time for subsequent time slots, a strategy we refer to as 'temporal-assisted' beamforming. To refine the temporal-assisted beamforming technique, we employ Successive Convex Approximation (SCA) in tandem with Iterative Rank Minimization (IRM), yielding high-quality suboptimal solutions. Comparative analysis with conventional pilot-based systems reveals that our approach yields a substantial improvement of 156\% in multi-object scenarios and 136\% in single-object scenarios.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2412.02734.pdf' target='_blank'>https://arxiv.org/pdf/2412.02734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaofeng Hu, Sifan Zhou, Zhihang Yuan, Dawei Yang, Shibo Zhao, Ci-Jyun Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02734">MVCTrack: Boosting 3D Point Cloud Tracking via Multimodal-Guided Virtual Cues</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking is essential in autonomous driving and robotics. Existing methods often struggle with sparse and incomplete point cloud scenarios. To address these limitations, we propose a Multimodal-guided Virtual Cues Projection (MVCP) scheme that generates virtual cues to enrich sparse point clouds. Additionally, we introduce an enhanced tracker MVCTrack based on the generated virtual cues. Specifically, the MVCP scheme seamlessly integrates RGB sensors into LiDAR-based systems, leveraging a set of 2D detections to create dense 3D virtual cues that significantly improve the sparsity of point clouds. These virtual cues can naturally integrate with existing LiDAR-based 3D trackers, yielding substantial performance gains. Extensive experiments demonstrate that our method achieves competitive performance on the NuScenes dataset.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2403.16410.pdf' target='_blank'>https://arxiv.org/pdf/2403.16410.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijia Guo, Yuanxi Bai, Liwen Hu, Mianzhi Liu, Ziyi Guo, Lei Ma, Tiejun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16410">Spike-NeRF: Neural Radiance Field Based On Spike Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a neuromorphic sensor with high temporal resolution, spike cameras offer notable advantages over traditional cameras in high-speed vision applications such as high-speed optical estimation, depth estimation, and object tracking. Inspired by the success of the spike camera, we proposed Spike-NeRF, the first Neural Radiance Field derived from spike data, to achieve 3D reconstruction and novel viewpoint synthesis of high-speed scenes. Instead of the multi-view images at the same time of NeRF, the inputs of Spike-NeRF are continuous spike streams captured by a moving spike camera in a very short time. To reconstruct a correct and stable 3D scene from high-frequency but unstable spike data, we devised spike masks along with a distinctive loss function. We evaluate our method qualitatively and numerically on several challenging synthetic scenes generated by blender with the spike camera simulator. Our results demonstrate that Spike-NeRF produces more visually appealing results than the existing methods and the baseline we proposed in high-speed scenes. Our code and data will be released soon.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2311.11580.pdf' target='_blank'>https://arxiv.org/pdf/2311.11580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Trinh, Ali Anwar, Siegfried Mercelis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.11580">SeaDSC: A video-based unsupervised method for dynamic scene change detection in unmanned surface vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, there has been an upsurge in the research on maritime vision, where a lot of works are influenced by the application of computer vision for Unmanned Surface Vehicles (USVs). Various sensor modalities such as camera, radar, and lidar have been used to perform tasks such as object detection, segmentation, object tracking, and motion planning. A large subset of this research is focused on the video analysis, since most of the current vessel fleets contain the camera's onboard for various surveillance tasks. Due to the vast abundance of the video data, video scene change detection is an initial and crucial stage for scene understanding of USVs. This paper outlines our approach to detect dynamic scene changes in USVs. To the best of our understanding, this work represents the first investigation of scene change detection in the maritime vision application. Our objective is to identify significant changes in the dynamic scenes of maritime video data, particularly those scenes that exhibit a high degree of resemblance. In our system for dynamic scene change detection, we propose completely unsupervised learning method. In contrast to earlier studies, we utilize a modified cutting-edge generative picture model called VQ-VAE-2 to train on multiple marine datasets, aiming to enhance the feature extraction. Next, we introduce our innovative similarity scoring technique for directly calculating the level of similarity in a sequence of consecutive frames by utilizing grid calculation on retrieved features. The experiments were conducted using a nautical video dataset called RoboWhaler to showcase the efficient performance of our technique.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2305.08808.pdf' target='_blank'>https://arxiv.org/pdf/2305.08808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Tian, Haoxi Ran, Yue Wang, Hang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08808">GeoMAE: Masked Geometric Target Prediction for Self-supervised Point Cloud Pre-Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tries to address a fundamental question in point cloud self-supervised learning: what is a good signal we should leverage to learn features from point clouds without annotations? To answer that, we introduce a point cloud representation learning framework, based on geometric feature reconstruction. In contrast to recent papers that directly adopt masked autoencoder (MAE) and only predict original coordinates or occupancy from masked point clouds, our method revisits differences between images and point clouds and identifies three self-supervised learning objectives peculiar to point clouds, namely centroid prediction, normal estimation, and curvature prediction. Combined with occupancy prediction, these four objectives yield an nontrivial self-supervised learning task and mutually facilitate models to better reason fine-grained geometry of point clouds. Our pipeline is conceptually simple and it consists of two major steps: first, it randomly masks out groups of points, followed by a Transformer-based point cloud encoder; second, a lightweight Transformer decoder predicts centroid, normal, and curvature for points in each voxel. We transfer the pre-trained Transformer encoder to a downstream peception model. On the nuScene Datset, our model achieves 3.38 mAP improvment for object detection, 2.1 mIoU gain for segmentation, and 1.7 AMOTA gain for multi-object tracking. We also conduct experiments on the Waymo Open Dataset and achieve significant performance improvements over baselines as well.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2303.07601.pdf' target='_blank'>https://arxiv.org/pdf/2303.07601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, Hongkai Yu, Bolei Zhou, Jiaqi Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07601">V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern perception systems of autonomous vehicles are known to be sensitive to occlusions and lack the capability of long perceiving range. It has been one of the key bottlenecks that prevents Level 5 autonomy. Recent research has demonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system has great potential to revolutionize the autonomous driving industry. However, the lack of a real-world dataset hinders the progress of this field. To facilitate the development of cooperative perception, we present V2V4Real, the first large-scale real-world multi-modal dataset for V2V perception. The data is collected by two vehicles equipped with multi-modal sensors driving together through diverse scenarios. Our V2V4Real dataset covers a driving area of 410 km, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding boxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real introduces three perception tasks, including cooperative 3D object detection, cooperative 3D object tracking, and Sim2Real domain adaptation for cooperative perception. We provide comprehensive benchmarks of recent cooperative perception algorithms on three tasks. The V2V4Real dataset can be found at https://research.seas.ucla.edu/mobility-lab/v2v4real/.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2503.22943.pdf' target='_blank'>https://arxiv.org/pdf/2503.22943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Wang, Ruishan Guo, Pengtao Ma, Ciyu Ruan, Xinyu Luo, Wenhua Ding, Tianyang Zhong, Jingao Xu, Yunhao Liu, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22943">Event Camera Meets Resource-Aware Mobile Computing: Abstraction, Algorithm, Acceleration, Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in achieving high-accuracy and low-latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution and low latency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, lack of stable, persistent semantic information, and large data volume pose challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature from 2014 to 2025 and presents a comprehensive overview of event-based mobile sensing, encompassing its fundamental principles, event \textit{abstraction} methods, \textit{algorithm} advancements, and both hardware and software \textit{acceleration} strategies. We discuss key \textit{applications} of event cameras in mobile sensing, including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving the event camera with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms. To support ongoing research, we provide an open-source \textit{Online Sheet} with recent developments. We hope this survey serves as a reference, facilitating the adoption of event-based vision across diverse applications.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2503.22943.pdf' target='_blank'>https://arxiv.org/pdf/2503.22943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Wang, Ruishan Guo, Pengtao Ma, Ciyu Ruan, Xinyu Luo, Wenhua Ding, Tianyang Zhong, Jingao Xu, Yunhao Liu, Xinlei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22943">Event Camera Meets Resource-Aware Mobile Computing: Abstraction, Algorithm, Acceleration, Application</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in achieving high-accuracy and low-latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution and low latency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, lack of stable, persistent semantic information, and large data volume pose challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature from 2014 to 2025 and presents a comprehensive overview of event-based mobile sensing, encompassing its fundamental principles, event \textit{abstraction} methods, \textit{algorithm} advancements, and both hardware and software \textit{acceleration} strategies. We discuss key \textit{applications} of event cameras in mobile sensing, including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving the event camera with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms. To support ongoing research, we provide an open-source \textit{Online Sheet} with recent developments. We hope this survey serves as a reference, facilitating the adoption of event-based vision across diverse applications.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2412.07392.pdf' target='_blank'>https://arxiv.org/pdf/2412.07392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhayy Ud Din, Ahsan B. Bakht, Waseem Akram, Yihao Dong, Lakmal Seneviratne, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07392">Benchmarking Vision-Based Object Tracking for USVs in Complex Maritime Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based target tracking is crucial for unmanned surface vehicles (USVs) to perform tasks such as inspection, monitoring, and surveillance. However, real-time tracking in complex maritime environments is challenging due to dynamic camera movement, low visibility, and scale variation. Typically, object detection methods combined with filtering techniques are commonly used for tracking, but they often lack robustness, particularly in the presence of camera motion and missed detections. Although advanced tracking methods have been proposed recently, their application in maritime scenarios is limited. To address this gap, this study proposes a vision-guided object-tracking framework for USVs, integrating state-of-the-art tracking algorithms with low-level control systems to enable precise tracking in dynamic maritime environments. We benchmarked the performance of seven distinct trackers, developed using advanced deep learning techniques such as Siamese Networks and Transformers, by evaluating them on both simulated and real-world maritime datasets. In addition, we evaluated the robustness of various control algorithms in conjunction with these tracking systems. The proposed framework was validated through simulations and real-world sea experiments, demonstrating its effectiveness in handling dynamic maritime conditions. The results show that SeqTrack, a Transformer-based tracker, performed best in adverse conditions, such as dust storms. Among the control algorithms evaluated, the linear quadratic regulator controller (LQR) demonstrated the most robust and smooth control, allowing for stable tracking of the USV.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2412.06258.pdf' target='_blank'>https://arxiv.org/pdf/2412.06258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Yin, Calvin Yeung, Qingrui Hu, Jun Ichikawa, Hirotsugu Azechi, Susumu Takahashi, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.06258">Enhanced Multi-Object Tracking Using Pose-based Virtual Markers in 3x3 Basketball</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is crucial for various multi-agent analyses such as evaluating team sports tactics and player movements and performance. While pedestrian tracking has advanced with Tracking-by-Detection MOT, team sports like basketball pose unique challenges. These challenges include players' unpredictable movements, frequent close interactions, and visual similarities that complicate pose labeling and lead to significant occlusions, frequent ID switches, and high manual annotation costs. To address these challenges, we propose a novel pose-based virtual marker (VM) MOT method for team sports, named Sports-vmTracking. This method builds on the vmTracking approach developed for multi-animal tracking with active learning. First, we constructed a 3x3 basketball pose dataset for VMs and applied active learning to enhance model performance in generating VMs. Then, we overlaid the VMs on video to identify players, extract their poses with unique IDs, and convert these into bounding boxes for comparison with automated MOT methods. Using our 3x3 basketball dataset, we demonstrated that our VM configuration has been highly effective, and reduced the need for manual corrections and labeling during pose model training while maintaining high accuracy. Our approach achieved an average HOTA score of 72.3%, over 10 points higher than other state-of-the-art methods without VM, and resulted in 0 ID switches. Beyond improving performance in handling occlusions and minimizing ID switches, our framework could substantially increase the time and cost efficiency compared to traditional manual annotation.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2411.01816.pdf' target='_blank'>https://arxiv.org/pdf/2411.01816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thanh Nguyen Canh, Huy-Hoang Ngo, Xiem HoangVan, Nak Young Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01816">Toward Integrating Semantic-aware Path Planning and Reliable Localization for UAV Operations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Localization is one of the most crucial tasks for Unmanned Aerial Vehicle systems (UAVs) directly impacting overall performance, which can be achieved with various sensors and applied to numerous tasks related to search and rescue operations, object tracking, construction, etc. However, due to the negative effects of challenging environments, UAVs may lose signals for localization. In this paper, we present an effective path-planning system leveraging semantic segmentation information to navigate around texture-less and problematic areas like lakes, oceans, and high-rise buildings using a monocular camera. We introduce a real-time semantic segmentation architecture and a novel keyframe decision pipeline to optimize image inputs based on pixel distribution, reducing processing time. A hierarchical planner based on the Dynamic Window Approach (DWA) algorithm, integrated with a cost map, is designed to facilitate efficient path planning. The system is implemented in a photo-realistic simulation environment using Unity, aligning with segmentation model parameters. Comprehensive qualitative and quantitative evaluations validate the effectiveness of our approach, showing significant improvements in the reliability and efficiency of UAV localization in challenging environments.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2409.17025.pdf' target='_blank'>https://arxiv.org/pdf/2409.17025.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrito Das, Bilal Sidiqi, Laurent Mennillo, Zhehua Mao, Mikael Brudfors, Miguel Xochicale, Danyal Z. Khan, Nicola Newall, John G. Hanrahan, Matthew J. Clarkson, Danail Stoyanov, Hani J. Marcus, Sophia Bano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17025">Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improved surgical skill is generally associated with improved patient outcomes, although assessment is subjective; labour-intensive; and requires domain specific expertise. Automated data driven metrics can alleviate these difficulties, as demonstrated by existing machine learning instrument tracking models in minimally invasive surgery. However, these models have been tested on limited datasets of laparoscopic surgery, with a focus on isolated tasks and robotic surgery. In this paper, a new public dataset is introduced, focusing on simulated surgery, using the nasal phase of endoscopic pituitary surgery as an exemplar. Simulated surgery allows for a realistic yet repeatable environment, meaning the insights gained from automated assessment can be used by novice surgeons to hone their skills on the simulator before moving to real surgery. PRINTNet (Pituitary Real-time INstrument Tracking Network) has been created as a baseline model for this automated assessment. Consisting of DeepLabV3 for classification and segmentation; StrongSORT for tracking; and the NVIDIA Holoscan SDK for real-time performance, PRINTNet achieved 71.9% Multiple Object Tracking Precision running at 22 Frames Per Second. Using this tracking output, a Multilayer Perceptron achieved 87% accuracy in predicting surgical skill level (novice or expert), with the "ratio of total procedure time to instrument visible time" correlated with higher surgical skill. This therefore demonstrates the feasibility of automated surgical skill assessment in simulated endoscopic pituitary surgery. The new publicly available dataset can be found here: https://doi.org/10.5522/04/26511049.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2408.16445.pdf' target='_blank'>https://arxiv.org/pdf/2408.16445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sierra Bonilla, Chiara Di Vece, Rema Daher, Xinwei Ju, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16445">Mismatched: Evaluating the Limits of Image Matching Approaches and Benchmarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Three-dimensional (3D) reconstruction from two-dimensional images is an active research field in computer vision, with applications ranging from navigation and object tracking to segmentation and three-dimensional modeling. Traditionally, parametric techniques have been employed for this task. However, recent advancements have seen a shift towards learning-based methods. Given the rapid pace of research and the frequent introduction of new image matching methods, it is essential to evaluate them. In this paper, we present a comprehensive evaluation of various image matching methods using a structure-from-motion pipeline. We assess the performance of these methods on both in-domain and out-of-domain datasets, identifying key limitations in both the methods and benchmarks. We also investigate the impact of edge detection as a pre-processing step. Our analysis reveals that image matching for 3D reconstruction remains an open challenge, necessitating careful selection and tuning of models for specific scenarios, while also highlighting mismatches in how metrics currently represent method performance.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2406.19655.pdf' target='_blank'>https://arxiv.org/pdf/2406.19655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingrui Hu, Atom Scott, Calvin Yeung, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19655">Basketball-SORT: An Association Method for Complex Multi-object Occlusion Problems in Basketball Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent deep learning-based object detection approaches have led to significant progress in multi-object tracking (MOT) algorithms. The current MOT methods mainly focus on pedestrian or vehicle scenes, but basketball sports scenes are usually accompanied by three or more object occlusion problems with similar appearances and high-intensity complex motions, which we call complex multi-object occlusion (CMOO). Here, we propose an online and robust MOT approach, named Basketball-SORT, which focuses on the CMOO problems in basketball videos. To overcome the CMOO problem, instead of using the intersection-over-union-based (IoU-based) approach, we use the trajectories of neighboring frames based on the projected positions of the players. Our method designs the basketball game restriction (BGR) and reacquiring Long-Lost IDs (RLLI) based on the characteristics of basketball scenes, and we also solve the occlusion problem based on the player trajectories and appearance features. Experimental results show that our method achieves a Higher Order Tracking Accuracy (HOTA) score of 63.48$\%$ on the basketball fixed video dataset and outperforms other recent popular approaches. Overall, our approach solved the CMOO problem more effectively than recent MOT algorithms.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2406.09982.pdf' target='_blank'>https://arxiv.org/pdf/2406.09982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacinto Colan, Ana Davila, Yasuhisa Hasegawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09982">Constrained Motion Planning for a Robotic Endoscope Holder based on Hierarchical Quadratic Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Minimally Invasive Surgeries (MIS) are challenging for surgeons due to the limited field of view and constrained range of motion imposed by narrow access ports. These challenges can be addressed by robot-assisted endoscope systems which provide precise and stabilized positioning, as well as constrained and smooth motion control of the endoscope. In this work, we propose an online hierarchical optimization framework for visual servoing control of the endoscope in MIS. The framework prioritizes maintaining a remote-center-of-motion (RCM) constraint to prevent tissue damage, while a visual tracking task is defined as a secondary task to enable autonomous tracking of visual features of interest. We validated our approach using a 6-DOF Denso VS050 manipulator and achieved optimization solving times under 0.4 ms and maximum RCM deviation of approximately 0.4 mm. Our results demonstrate the effectiveness of the proposed approach in addressing the constrained motion planning challenges of MIS, enabling precise and autonomous endoscope positioning and visual tracking.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2403.00976.pdf' target='_blank'>https://arxiv.org/pdf/2403.00976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlin Song, Antoine Richard, Miguel Olivares-Mendez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.00976">Joint Spatial-Temporal Calibration for Camera and Global Pose Sensor</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In robotics, motion capture systems have been widely used to measure the accuracy of localization algorithms. Moreover, this infrastructure can also be used for other computer vision tasks, such as the evaluation of Visual (-Inertial) SLAM dynamic initialization, multi-object tracking, or automatic annotation. Yet, to work optimally, these functionalities require having accurate and reliable spatial-temporal calibration parameters between the camera and the global pose sensor. In this study, we provide two novel solutions to estimate these calibration parameters. Firstly, we design an offline target-based method with high accuracy and consistency. Spatial-temporal parameters, camera intrinsic, and trajectory are optimized simultaneously. Then, we propose an online target-less method, eliminating the need for a calibration target and enabling the estimation of time-varying spatial-temporal parameters. Additionally, we perform detailed observability analysis for the target-less method. Our theoretical findings regarding observability are validated by simulation experiments and provide explainable guidelines for calibration. Finally, the accuracy and consistency of two proposed methods are evaluated with hand-held real-world datasets where traditional hand-eye calibration method do not work.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2312.10608.pdf' target='_blank'>https://arxiv.org/pdf/2312.10608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingwen Zhang, Zikun Zhou, Guangming Lu, Jiandong Tian, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10608">Robust 3D Tracking with Quality-Aware Shape Completion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking remains a challenging problem due to the sparsity and incompleteness of the point clouds. Existing algorithms attempt to address the challenges in two strategies. The first strategy is to learn dense geometric features based on the captured sparse point cloud. Nevertheless, it is quite a formidable task since the learned dense geometric features are with high uncertainty for depicting the shape of the target object. The other strategy is to aggregate the sparse geometric features of multiple templates to enrich the shape information, which is a routine solution in 2D tracking. However, aggregating the coarse shape representations can hardly yield a precise shape representation. Different from 2D pixels, 3D points of different frames can be directly fused by coordinate transform, i.e., shape completion. Considering that, we propose to construct a synthetic target representation composed of dense and complete point clouds depicting the target shape precisely by shape completion for robust 3D tracking. Specifically, we design a voxelized 3D tracking framework with shape completion, in which we propose a quality-aware shape completion mechanism to alleviate the adverse effect of noisy historical predictions. It enables us to effectively construct and leverage the synthetic target representation. Besides, we also develop a voxelized relation modeling module and box refinement module to improve tracking performance. Favorable performance against state-of-the-art algorithms on three benchmarks demonstrates the effectiveness and generalization ability of our method.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2311.17197.pdf' target='_blank'>https://arxiv.org/pdf/2311.17197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhayy Ud Din, Ahmed Humais, Waseem Akram, Mohamed Alblooshi, Lyes Saad Saoud, Abdelrahman Alblooshi, Lakmal Seneviratne, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17197">Marine$\mathcal{X}$: Design and Implementation of Unmanned Surface Vessel for Vision Guided Navigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Marine robots, particularly Unmanned Surface Vessels (USVs), have gained considerable attention for their diverse applications in maritime tasks, including search and rescue, environmental monitoring, and maritime security. This paper presents the design and implementation of a USV named marine$\mathcal{X}$. The hardware components of marine$\mathcal{X}$ are meticulously developed to ensure robustness, efficiency, and adaptability to varying environmental conditions. Furthermore, the integration of a vision-based object tracking algorithm empowers marine$\mathcal{X}$ to autonomously track and monitor specific objects on the water surface. The control system utilizes PID control, enabling precise navigation of marine$\mathcal{X}$ while maintaining a desired course and distance to the target object. To assess the performance of marine$\mathcal{X}$, comprehensive testing is conducted, encompassing simulation, trials in the marine pool, and real-world tests in the open sea. The successful outcomes of these tests demonstrate the USV's capabilities in achieving real-time object tracking, showcasing its potential for various applications in maritime operations.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2309.00233.pdf' target='_blank'>https://arxiv.org/pdf/2309.00233.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixu Zhao, Jiaze Wang, Max Horn, Yizhuo Ding, Tong He, Zechen Bai, Dominik Zietlow, Carl-Johann Simon-Gabriel, Bing Shuai, Zhuowen Tu, Thomas Brox, Bernt Schiele, Yanwei Fu, Francesco Locatello, Zheng Zhang, Tianjun Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00233">Object-Centric Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised object-centric learning methods allow the partitioning of scenes into entities without additional localization information and are excellent candidates for reducing the annotation burden of multiple-object tracking (MOT) pipelines. Unfortunately, they lack two key properties: objects are often split into parts and are not consistently tracked over time. In fact, state-of-the-art models achieve pixel-level accuracy and temporal consistency by relying on supervised object detection with additional ID labels for the association through time. This paper proposes a video object-centric model for MOT. It consists of an index-merge module that adapts the object-centric slots into detection outputs and an object memory module that builds complete object prototypes to handle occlusions. Benefited from object-centric learning, we only require sparse detection labels (0%-6.25%) for object localization and feature binding. Relying on our self-supervised Expectation-Maximization-inspired loss for object association, our approach requires no ID labels. Our experiments significantly narrow the gap between the existing object-centric model and the fully supervised state-of-the-art and outperform several unsupervised trackers.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2308.11322.pdf' target='_blank'>https://arxiv.org/pdf/2308.11322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Li, Yuqing Huang, Zhenyu He, Yaowei Wang, Huchuan Lu, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11322">CiteTracker: Correlating Image and Text for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing visual tracking methods typically take an image patch as the reference of the target to perform tracking. However, a single image patch cannot provide a complete and precise concept of the target object as images are limited in their ability to abstract and can be ambiguous, which makes it difficult to track targets with drastic variations. In this paper, we propose the CiteTracker to enhance target modeling and inference in visual tracking by connecting images and text. Specifically, we develop a text generation module to convert the target image patch into a descriptive text containing its class and attribute information, providing a comprehensive reference point for the target. In addition, a dynamic description module is designed to adapt to target variations for more effective target representation. We then associate the target description and the search image using an attention-based correlation module to generate the correlated features for target state reference. Extensive experiments on five diverse datasets are conducted to evaluate the proposed algorithm and the favorable performance against the state-of-the-art methods demonstrates the effectiveness of the proposed tracking method.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2308.00763.pdf' target='_blank'>https://arxiv.org/pdf/2308.00763.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabin Schieffer, Nattawat Pornthisan, Daniel AraÃºjo de Medeiros, Stefano Markidis, Jacob Wahlgren, Ivy Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00763">Boosting the Performance of Object Tracking with a Half-Precision Particle Filter on GPU</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-performance GPU-accelerated particle filter methods are critical for object detection applications, ranging from autonomous driving, robot localization, to time-series prediction. In this work, we investigate the design, development and optimization of particle-filter using half-precision on CUDA cores and compare their performance and accuracy with single- and double-precision baselines on Nvidia V100, A100, A40 and T4 GPUs. To mitigate numerical instability and precision losses, we introduce algorithmic changes in the particle filters. Using half-precision leads to a performance improvement of 1.5-2x and 2.5-4.6x with respect to single- and double-precision baselines respectively, at the cost of a relatively small loss of accuracy.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2106.10900.pdf' target='_blank'>https://arxiv.org/pdf/2106.10900.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Li, Wenjie Pei, Yaowei Wang, Zhenyu He, Huchuan Lu, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.10900">Self-Supervised Tracking via Target-Aware Data Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep-learning based tracking methods have achieved substantial progress, they entail large-scale and high-quality annotated data for sufficient training. To eliminate expensive and exhaustive annotation, we study self-supervised learning for visual tracking. In this work, we develop the Crop-Transform-Paste operation, which is able to synthesize sufficient training data by simulating various appearance variations during tracking, including appearance variations of objects and background interference. Since the target state is known in all synthesized data, existing deep trackers can be trained in routine ways using the synthesized data without human annotation. The proposed target-aware data-synthesis method adapts existing tracking approaches within a self-supervised learning framework without algorithmic changes. Thus, the proposed self-supervised learning mechanism can be seamlessly integrated into existing tracking frameworks to perform training. Extensive experiments show that our method 1) achieves favorable performance against supervised learning schemes under the cases with limited annotations; 2) helps deal with various tracking challenges such as object deformation, occlusion, or background clutter due to its manipulability; 3) performs favorably against state-of-the-art unsupervised tracking methods; 4) boosts the performance of various state-of-the-art supervised learning frameworks, including SiamRPN++, DiMP, and TransT.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2509.02182.pdf' target='_blank'>https://arxiv.org/pdf/2509.02182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shyma Alhuwaider, Motasem Alfarra, Juan C. Perez, Merey Ramazanova, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02182">ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel tracklet-based dataset for benchmarking test-time adaptation (TTA) methods. The aim of this dataset is to mimic the intricate challenges encountered in real-world environments such as images captured by hand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus on how models face distribution shifts, when deployed, and on violations to the customary independent-and-identically-distributed (i.i.d.) assumption in machine learning. Yet, these benchmarks fail to faithfully represent realistic scenarios that naturally display temporal dependencies, such as how consecutive frames from a video stream likely show the same object across time. We address this shortcoming of current datasets by proposing a novel TTA benchmark we call the "Inherent Temporal Dependencies" (ITD) dataset. We ensure the instances in ITD naturally embody temporal dependencies by collecting them from tracklets-sequences of object-centric images we compile from the bounding boxes of an object-tracking dataset. We use ITD to conduct a thorough experimental analysis of current TTA methods, and shed light on the limitations of these methods when faced with the challenges of temporal dependencies. Moreover, we build upon these insights and propose a novel adversarial memory initialization strategy to improve memory-based TTA methods. We find this strategy substantially boosts the performance of various methods on our challenging benchmark.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2509.02182.pdf' target='_blank'>https://arxiv.org/pdf/2509.02182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shyma Alhuwaider, Motasem Alfarra, Juan C. Perez, Merey Ramazanova, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02182">ADVMEM: Adversarial Memory Initialization for Realistic Test-Time Adaptation via Tracklet-Based Benchmarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel tracklet-based dataset for benchmarking test-time adaptation (TTA) methods. The aim of this dataset is to mimic the intricate challenges encountered in real-world environments such as images captured by hand-held cameras, self-driving cars, etc. The current benchmarks for TTA focus on how models face distribution shifts, when deployed, and on violations to the customary independent-and-identically-distributed (i.i.d.) assumption in machine learning. Yet, these benchmarks fail to faithfully represent realistic scenarios that naturally display temporal dependencies, such as how consecutive frames from a video stream likely show the same object across time. We address this shortcoming of current datasets by proposing a novel TTA benchmark we call the "Inherent Temporal Dependencies" (ITD) dataset. We ensure the instances in ITD naturally embody temporal dependencies by collecting them from tracklets-sequences of object-centric images we compile from the bounding boxes of an object-tracking dataset. We use ITD to conduct a thorough experimental analysis of current TTA methods, and shed light on the limitations of these methods when faced with the challenges of temporal dependencies. Moreover, we build upon these insights and propose a novel adversarial memory initialization strategy to improve memory-based TTA methods. We find this strategy substantially boosts the performance of various methods on our challenging benchmark.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2505.11905.pdf' target='_blank'>https://arxiv.org/pdf/2505.11905.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Takuya Ikeda, Sergey Zakharov, Muhammad Zubair Irshad, Istvan Balazs Opra, Shun Iwase, Dian Chen, Mark Tjersland, Robert Lee, Alexandre Dilly, Rares Ambrus, Koichi Nishiwaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11905">GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel method for 6-DoF object tracking and high-quality 3D reconstruction from monocular RGBD video. Existing methods, while achieving impressive results, often struggle with complex objects, particularly those exhibiting symmetry, intricate geometry or complex appearance. To bridge these gaps, we introduce an adaptive method that combines 3D Gaussian Splatting, hybrid geometry/appearance tracking, and key frame selection to achieve robust tracking and accurate reconstructions across a diverse range of objects. Additionally, we present a benchmark covering these challenging object classes, providing high-quality annotations for evaluating both tracking and reconstruction performance. Our approach demonstrates strong capabilities in recovering high-fidelity object meshes, setting a new standard for single-sensor 3D reconstruction in open-world environments.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2411.01756.pdf' target='_blank'>https://arxiv.org/pdf/2411.01756.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Sun, Fan Yu, Shaoxiang Chen, Yu Zhang, Junwei Huang, Chenhui Li, Yang Li, Changbo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01756">ChatTracker: Enhancing Visual Tracking Performance via Chatting with Multimodal Large Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking aims to locate a targeted object in a video sequence based on an initial bounding box. Recently, Vision-Language~(VL) trackers have proposed to utilize additional natural language descriptions to enhance versatility in various applications. However, VL trackers are still inferior to State-of-The-Art (SoTA) visual trackers in terms of tracking performance. We found that this inferiority primarily results from their heavy reliance on manual textual annotations, which include the frequent provision of ambiguous language descriptions. In this paper, we propose ChatTracker to leverage the wealth of world knowledge in the Multimodal Large Language Model (MLLM) to generate high-quality language descriptions and enhance tracking performance. To this end, we propose a novel reflection-based prompt optimization module to iteratively refine the ambiguous and inaccurate descriptions of the target with tracking feedback. To further utilize semantic information produced by MLLM, a simple yet effective VL tracking framework is proposed and can be easily integrated as a plug-and-play module to boost the performance of both VL and visual trackers. Experimental results show that our proposed ChatTracker achieves a performance comparable to existing methods.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2407.05017.pdf' target='_blank'>https://arxiv.org/pdf/2407.05017.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Jiang, Fangyuan Wang, Rongzhang Zheng, Han Liu, Yixiong Huo, Jinzhang Peng, Lu Tian, Emad Barsoum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05017">VIPS-Odom: Visual-Inertial Odometry Tightly-coupled with Parking Slots for Autonomous Parking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precise localization is of great importance for autonomous parking task since it provides service for the downstream planning and control modules, which significantly affects the system performance. For parking scenarios, dynamic lighting, sparse textures, and the instability of global positioning system (GPS) signals pose challenges for most traditional localization methods. To address these difficulties, we propose VIPS-Odom, a novel semantic visual-inertial odometry framework for underground autonomous parking, which adopts tightly-coupled optimization to fuse measurements from multi-modal sensors and solves odometry. Our VIPS-Odom integrates parking slots detected from the synthesized bird-eye-view (BEV) image with traditional feature points in the frontend, and conducts tightly-coupled optimization with joint constraints introduced by measurements from the inertial measurement unit, wheel speed sensor and parking slots in the backend. We develop a multi-object tracking framework to robustly track parking slots' states. To prove the superiority of our method, we equip an electronic vehicle with related sensors and build an experimental platform based on ROS2 system. Extensive experiments demonstrate the efficacy and advantages of our method compared with other baselines for parking scenarios.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2405.02425.pdf' target='_blank'>https://arxiv.org/pdf/2405.02425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02425">Learning Robot Soccer from Egocentric Vision with Deep Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We apply multi-agent deep reinforcement learning (RL) to train end-to-end robot soccer policies with fully onboard computation and sensing via egocentric RGB vision. This setting reflects many challenges of real-world robotics, including active perception, agile full-body control, and long-horizon planning in a dynamic, partially-observable, multi-agent domain. We rely on large-scale, simulation-based data generation to obtain complex behaviors from egocentric vision which can be successfully transferred to physical robots using low-cost sensors. To achieve adequate visual realism, our simulation combines rigid-body physics with learned, realistic rendering via multiple Neural Radiance Fields (NeRFs). We combine teacher-based multi-agent RL and cross-experiment data reuse to enable the discovery of sophisticated soccer strategies. We analyze active-perception behaviors including object tracking and ball seeking that emerge when simply optimizing perception-agnostic soccer play. The agents display equivalent levels of performance and agility as policies with access to privileged, ground-truth state. To our knowledge, this paper constitutes a first demonstration of end-to-end training for multi-agent robot soccer, mapping raw pixel observations to joint-level actions, that can be deployed in the real world. Videos of the game-play and analyses can be seen on our website https://sites.google.com/view/vision-soccer .
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2402.12644.pdf' target='_blank'>https://arxiv.org/pdf/2402.12644.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Lin, Xiang Zhang, Lei Yang, Lei Yu, Bin Zhou, Xiaowei Luo, Wenping Wang, Jia Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12644">Neuromorphic Synergy for Video Binarization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bimodal objects, such as the checkerboard pattern used in camera calibration, markers for object tracking, and text on road signs, to name a few, are prevalent in our daily lives and serve as a visual form to embed information that can be easily recognized by vision systems. While binarization from intensity images is crucial for extracting the embedded information in the bimodal objects, few previous works consider the task of binarization of blurry images due to the relative motion between the vision sensor and the environment. The blurry images can result in a loss in the binarization quality and thus degrade the downstream applications where the vision system is in motion. Recently, neuromorphic cameras offer new capabilities for alleviating motion blur, but it is non-trivial to first deblur and then binarize the images in a real-time manner. In this work, we propose an event-based binary reconstruction method that leverages the prior knowledge of the bimodal target's properties to perform inference independently in both event space and image space and merge the results from both domains to generate a sharp binary image. We also develop an efficient integration method to propagate this binary image to high frame rate binary video. Finally, we develop a novel method to naturally fuse events and images for unsupervised threshold identification. The proposed method is evaluated in publicly available and our collected data sequence, and shows the proposed method can outperform the SOTA methods to generate high frame rate binary video in real-time on CPU-only devices.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2310.19670.pdf' target='_blank'>https://arxiv.org/pdf/2310.19670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jorge de Heuvel, Xiangyu Zeng, Weixian Shi, Tharun Sethuraman, Maren Bennewitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19670">Spatiotemporal Attention Enhances Lidar-Based Robot Navigation in Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Foresighted robot navigation in dynamic indoor environments with cost-efficient hardware necessitates the use of a lightweight yet dependable controller. So inferring the scene dynamics from sensor readings without explicit object tracking is a pivotal aspect of foresighted navigation among pedestrians. In this paper, we introduce a spatiotemporal attention pipeline for enhanced navigation based on 2D~lidar sensor readings. This pipeline is complemented by a novel lidar-state representation that emphasizes dynamic obstacles over static ones. Subsequently, the attention mechanism enables selective scene perception across both space and time, resulting in improved overall navigation performance within dynamic scenarios. We thoroughly evaluated the approach in different scenarios and simulators, finding excellent generalization to unseen environments. The results demonstrate outstanding performance compared to state-of-the-art methods, thereby enabling the seamless deployment of the learned controller on a real robot.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2307.04129.pdf' target='_blank'>https://arxiv.org/pdf/2307.04129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyu Zhu, Junhui Hou, Dapeng Oliver Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04129">Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of cross-modal object tracking from RGB videos and event data. Rather than constructing a complex cross-modal fusion network, we explore the great potential of a pre-trained vision Transformer (ViT). Particularly, we delicately investigate plug-and-play training augmentations that encourage the ViT to bridge the vast distribution gap between the two modalities, enabling comprehensive cross-modal information interaction and thus enhancing its ability. Specifically, we propose a mask modeling strategy that randomly masks a specific modality of some tokens to enforce the interaction between tokens from different modalities interacting proactively. To mitigate network oscillations resulting from the masking strategy and further amplify its positive effect, we then theoretically propose an orthogonal high-rank loss to regularize the attention matrix. Extensive experiments demonstrate that our plug-and-play training augmentation techniques can significantly boost state-of-the-art one-stream and twostream trackers to a large extent in terms of both tracking precision and success rate. Our new perspective and findings will potentially bring insights to the field of leveraging powerful pre-trained ViTs to model cross-modal data. The code will be publicly available.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2306.03454.pdf' target='_blank'>https://arxiv.org/pdf/2306.03454.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Gao, Zhijie Wang, Yang Feng, Lei Ma, Zhenyu Chen, Baowen Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03454">Benchmarking Robustness of AI-Enabled Multi-sensor Fusion Systems: Challenges and Opportunities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Sensor Fusion (MSF) based perception systems have been the foundation in supporting many industrial applications and domains, such as self-driving cars, robotic arms, and unmanned aerial vehicles. Over the past few years, the fast progress in data-driven artificial intelligence (AI) has brought a fast-increasing trend to empower MSF systems by deep learning techniques to further improve performance, especially on intelligent systems and their perception systems. Although quite a few AI-enabled MSF perception systems and techniques have been proposed, up to the present, limited benchmarks that focus on MSF perception are publicly available. Given that many intelligent systems such as self-driving cars are operated in safety-critical contexts where perception systems play an important role, there comes an urgent need for a more in-depth understanding of the performance and reliability of these MSF systems. To bridge this gap, we initiate an early step in this direction and construct a public benchmark of AI-enabled MSF-based perception systems including three commonly adopted tasks (i.e., object detection, object tracking, and depth completion). Based on this, to comprehensively understand MSF systems' robustness and reliability, we design 14 common and realistic corruption patterns to synthesize large-scale corrupted datasets. We further perform a systematic evaluation of these systems through our large-scale evaluation. Our results reveal the vulnerability of the current AI-enabled MSF perception systems, calling for researchers and practitioners to take robustness and reliability into account when designing AI-enabled MSF.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2304.06419.pdf' target='_blank'>https://arxiv.org/pdf/2304.06419.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denys Rozumnyi, Jiri Matas, Marc Pollefeys, Vittorio Ferrari, Martin R. Oswald
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06419">Tracking by 3D Model Estimation of Unknown Objects in Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most model-free visual object tracking methods formulate the tracking task as object location estimation given by a 2D segmentation or a bounding box in each video frame. We argue that this representation is limited and instead propose to guide and improve 2D tracking with an explicit object representation, namely the textured 3D shape and 6DoF pose in each video frame. Our representation tackles a complex long-term dense correspondence problem between all 3D points on the object for all video frames, including frames where some points are invisible. To achieve that, the estimation is driven by re-rendering the input video frames as well as possible through differentiable rendering, which has not been used for tracking before. The proposed optimization minimizes a novel loss function to estimate the best 3D shape, texture, and 6DoF pose. We improve the state-of-the-art in 2D segmentation tracking on three different datasets with mostly rigid objects.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2303.13885.pdf' target='_blank'>https://arxiv.org/pdf/2303.13885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Zhao, Junsong Chen, Lijun Wang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13885">ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compared with traditional RGB-only visual tracking, few datasets have been constructed for RGB-D tracking. In this paper, we propose ARKitTrack, a new RGB-D tracking dataset for both static and dynamic scenes captured by consumer-grade LiDAR scanners equipped on Apple's iPhone and iPad. ARKitTrack contains 300 RGB-D sequences, 455 targets, and 229.7K video frames in total. Along with the bounding box annotations and frame-level attributes, we also annotate this dataset with 123.9K pixel-level target masks. Besides, the camera intrinsic and camera pose of each frame are provided for future developments. To demonstrate the potential usefulness of this dataset, we further present a unified baseline for both box-level and pixel-level tracking, which integrates RGB features with bird's-eye-view representations to better explore cross-modality 3D geometry. In-depth empirical analysis has verified that the ARKitTrack dataset can significantly facilitate RGB-D tracking and that the proposed baseline method compares favorably against the state of the arts. The code and dataset is available at https://arkittrack.github.io.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2508.08219.pdf' target='_blank'>https://arxiv.org/pdf/2508.08219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Sun, Quanyun Wu, Hanqing Xu, Kyle Gao, Zhengsen Xu, Yiping Chen, Dedong Zhang, Lingfei Ma, John S. Zelek, Jonathan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08219">SAGOnline: Segment Any Gaussians Online</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit 3D scene representation, yet achieving efficient and consistent 3D segmentation remains challenging. Current methods suffer from prohibitive computational costs, limited 3D spatial reasoning, and an inability to track multiple objects simultaneously. We present Segment Any Gaussians Online (SAGOnline), a lightweight and zero-shot framework for real-time 3D segmentation in Gaussian scenes that addresses these limitations through two key innovations: (1) a decoupled strategy that integrates video foundation models (e.g., SAM2) for view-consistent 2D mask propagation across synthesized views; and (2) a GPU-accelerated 3D mask generation and Gaussian-level instance labeling algorithm that assigns unique identifiers to 3D primitives, enabling lossless multi-object tracking and segmentation across views. SAGOnline achieves state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU) benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times in inference speed (27 ms/frame). Qualitative results demonstrate robust multi-object segmentation and tracking in complex scenes. Our contributions include: (i) a lightweight and zero-shot framework for 3D segmentation in Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling simultaneous segmentation and tracking, and (iii) the effective adaptation of 2D video foundation models to the 3D domain. This work allows real-time rendering and 3D scene understanding, paving the way for practical AR/VR and robotic applications.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2507.07483.pdf' target='_blank'>https://arxiv.org/pdf/2507.07483.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiangqiang Wu, Yi Yu, Chenqi Kong, Ziquan Liu, Jia Wan, Haoliang Li, Alex C. Kot, Antoni B. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07483">Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of social media, vast amounts of user-uploaded videos (e.g., YouTube) are utilized as training data for Visual Object Tracking (VOT). However, the VOT community has largely overlooked video data-privacy issues, as many private videos have been collected and used for training commercial models without authorization. To alleviate these issues, this paper presents the first investigation on preventing personal video data from unauthorized exploitation by deep trackers. Existing methods for preventing unauthorized data use primarily focus on image-based tasks (e.g., image classification), directly applying them to videos reveals several limitations, including inefficiency, limited effectiveness, and poor generalizability. To address these issues, we propose a novel generative framework for generating Temporal Unlearnable Examples (TUEs), and whose efficient computation makes it scalable for usage on large-scale video datasets. The trackers trained w/ TUEs heavily rely on unlearnable noises for temporal matching, ignoring the original data structure and thus ensuring training video data-privacy. To enhance the effectiveness of TUEs, we introduce a temporal contrastive loss, which further corrupts the learning of existing trackers when using our TUEs for training. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in video data-privacy protection, with strong transferability across VOT models, datasets, and temporal matching tasks.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2503.09191.pdf' target='_blank'>https://arxiv.org/pdf/2503.09191.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juana Valeria Hurtado, Sajad Marvi, Rohit Mohan, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09191">Learning Appearance and Motion Cues for Panoptic Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Panoptic tracking enables pixel-level scene interpretation of videos by integrating instance tracking in panoptic segmentation. This provides robots with a spatio-temporal understanding of the environment, an essential attribute for their operation in dynamic environments. In this paper, we propose a novel approach for panoptic tracking that simultaneously captures general semantic information and instance-specific appearance and motion features. Unlike existing methods that overlook dynamic scene attributes, our approach leverages both appearance and motion cues through dedicated network heads. These interconnected heads employ multi-scale deformable convolutions that reason about scene motion offsets with semantic context and motion-enhanced appearance features to learn tracking embeddings. Furthermore, we introduce a novel two-step fusion module that integrates the outputs from both heads by first matching instances from the current time step with propagated instances from previous time steps and subsequently refines associations using motion-enhanced appearance embeddings, improving robustness in challenging scenarios. Extensive evaluations of our proposed \netname model on two benchmark datasets demonstrate that it achieves state-of-the-art performance in panoptic tracking accuracy, surpassing prior methods in maintaining object identities over time. To facilitate future research, we make the code available at http://panoptictracking.cs.uni-freiburg.de
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2501.00758.pdf' target='_blank'>https://arxiv.org/pdf/2501.00758.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenlong Xu, Bineng Zhong, Qihua Liang, Yaozong Zheng, Guorong Li, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00758">Less is More: Token Context-aware Learning for Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, several studies have shown that utilizing contextual information to perceive target states is crucial for object tracking. They typically capture context by incorporating multiple video frames. However, these naive frame-context methods fail to consider the importance of each patch within a reference frame, making them susceptible to noise and redundant tokens, which deteriorates tracking performance. To address this challenge, we propose a new token context-aware tracking pipeline named LMTrack, designed to automatically learn high-quality reference tokens for efficient visual tracking. Embracing the principle of Less is More, the core idea of LMTrack is to analyze the importance distribution of all reference tokens, where important tokens are collected, continually attended to, and updated. Specifically, a novel Token Context Memory module is designed to dynamically collect high-quality spatio-temporal information of a target in an autoregressive manner, eliminating redundant background tokens from the reference frames. Furthermore, an effective Unidirectional Token Attention mechanism is designed to establish dependencies between reference tokens and search frame, enabling robust cross-frame association and target localization. Extensive experiments demonstrate the superiority of our tracker, achieving state-of-the-art results on tracking benchmarks such as GOT-10K, TrackingNet, and LaSOT.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2412.13615.pdf' target='_blank'>https://arxiv.org/pdf/2412.13615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohai Li, Bineng Zhong, Qihua Liang, Guorong Li, Zhiyi Mo, Shuxiang Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13615">MambaLCT: Boosting Tracking via Long-term Context State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effectively constructing context information with long-term dependencies from video sequences is crucial for object tracking. However, the context length constructed by existing work is limited, only considering object information from adjacent frames or video clips, leading to insufficient utilization of contextual information. To address this issue, we propose MambaLCT, which constructs and utilizes target variation cues from the first frame to the current frame for robust tracking. First, a novel unidirectional Context Mamba module is designed to scan frame features along the temporal dimension, gathering target change cues throughout the entire sequence. Specifically, target-related information in frame features is compressed into a hidden state space through selective scanning mechanism. The target information across the entire video is continuously aggregated into target variation cues. Next, we inject the target change cues into the attention mechanism, providing temporal information for modeling the relationship between the template and search frames. The advantage of MambaLCT is its ability to continuously extend the length of the context, capturing complete target change cues, which enhances the stability and robustness of the tracker. Extensive experiments show that long-term context information enhances the model's ability to perceive targets in complex scenarios. MambaLCT achieves new SOTA performance on six benchmarks while maintaining real-time running speeds.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2410.16695.pdf' target='_blank'>https://arxiv.org/pdf/2410.16695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Yu, Yuezun Li, Xin Sun, Junyu Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16695">MPT: A Large-scale Multi-Phytoplankton Tracking Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Phytoplankton are a crucial component of aquatic ecosystems, and effective monitoring of them can provide valuable insights into ocean environments and ecosystem changes. Traditional phytoplankton monitoring methods are often complex and lack timely analysis. Therefore, deep learning algorithms offer a promising approach for automated phytoplankton monitoring. However, the lack of large-scale, high-quality training samples has become a major bottleneck in advancing phytoplankton tracking. In this paper, we propose a challenging benchmark dataset, Multiple Phytoplankton Tracking (MPT), which covers diverse background information and variations in motion during observation. The dataset includes 27 species of phytoplankton and zooplankton, 14 different backgrounds to simulate diverse and complex underwater environments, and a total of 140 videos. To enable accurate real-time observation of phytoplankton, we introduce a multi-object tracking method, Deviation-Corrected Multi-Scale Feature Fusion Tracker(DSFT), which addresses issues such as focus shifts during tracking and the loss of small target information when computing frame-to-frame similarity. Specifically, we introduce an additional feature extractor to predict the residuals of the standard feature extractor's output, and compute multi-scale frame-to-frame similarity based on features from different layers of the extractor. Extensive experiments on the MPT have demonstrated the validity of the dataset and the superiority of DSFT in tracking phytoplankton, providing an effective solution for phytoplankton monitoring.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2410.10053.pdf' target='_blank'>https://arxiv.org/pdf/2410.10053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pha Nguyen, Ngan Le, Jackson Cothren, Alper Yilmaz, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10053">DINTR: Tracking via Diffusion-based Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a fundamental task in computer vision, requiring the localization of objects of interest across video frames. Diffusion models have shown remarkable capabilities in visual generation, making them well-suited for addressing several requirements of the tracking problem. This work proposes a novel diffusion-based methodology to formulate the tracking task. Firstly, their conditional process allows for injecting indications of the target object into the generation process. Secondly, diffusion mechanics can be developed to inherently model temporal correspondences, enabling the reconstruction of actual frames in video. However, existing diffusion models rely on extensive and unnecessary mapping to a Gaussian noise domain, which can be replaced by a more efficient and stable interpolation process. Our proposed interpolation mechanism draws inspiration from classic image-processing techniques, offering a more interpretable, stable, and faster approach tailored specifically for the object tracking task. By leveraging the strengths of diffusion models while circumventing their limitations, our Diffusion-based INterpolation TrackeR (DINTR) presents a promising new paradigm and achieves a superior multiplicity on seven benchmarks across five indicator representations.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2405.04390.pdf' target='_blank'>https://arxiv.org/pdf/2405.04390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Min, Dawei Zhao, Liang Xiao, Jian Zhao, Xinli Xu, Zheng Zhu, Lei Jin, Jianshu Li, Yulan Guo, Junliang Xing, Liping Jing, Yiming Nie, Bin Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04390">DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-centric autonomous driving has recently raised wide attention due to its lower cost. Pre-training is essential for extracting a universal representation. However, current vision-centric pre-training typically relies on either 2D or 3D pre-text tasks, overlooking the temporal characteristics of autonomous driving as a 4D scene understanding task. In this paper, we address this challenge by introducing a world model-based autonomous driving 4D representation learning framework, dubbed \emph{DriveWorld}, which is capable of pre-training from multi-camera driving videos in a spatio-temporal fashion. Specifically, we propose a Memory State-Space Model for spatio-temporal modelling, which consists of a Dynamic Memory Bank module for learning temporal-aware latent dynamics to predict future changes and a Static Scene Propagation module for learning spatial-aware latent statics to offer comprehensive scene contexts. We additionally introduce a Task Prompt to decouple task-aware features for various downstream tasks. The experiments demonstrate that DriveWorld delivers promising results on various autonomous driving tasks. When pre-trained with the OpenScene dataset, DriveWorld achieves a 7.5% increase in mAP for 3D object detection, a 3.0% increase in IoU for online mapping, a 5.0% increase in AMOTA for multi-object tracking, a 0.1m decrease in minADE for motion forecasting, a 3.0% increase in IoU for occupancy prediction, and a 0.34m reduction in average L2 error for planning.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2404.12359.pdf' target='_blank'>https://arxiv.org/pdf/2404.12359.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julian Ost, Tanushree Banerjee, Mario Bijelic, Felix Heide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12359">Inverse Neural Rendering for Explainable Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Today, most methods for image understanding tasks rely on feed-forward neural networks. While this approach has allowed for empirical accuracy, efficiency, and task adaptation via fine-tuning, it also comes with fundamental disadvantages. Existing networks often struggle to generalize across different datasets, even on the same task. By design, these networks ultimately reason about high-dimensional scene features, which are challenging to analyze. This is true especially when attempting to predict 3D information based on 2D images. We propose to recast 3D multi-object tracking from RGB cameras as an \emph{Inverse Rendering (IR)} problem, by optimizing via a differentiable rendering pipeline over the latent space of pre-trained 3D object representations and retrieve the latents that best represent object instances in a given input image. To this end, we optimize an image loss over generative latent spaces that inherently disentangle shape and appearance properties. We investigate not only an alternate take on tracking but our method also enables examining the generated objects, reasoning about failure situations, and resolving ambiguous cases. We validate the generalization and scaling capabilities of our method by learning the generative prior exclusively from synthetic data and assessing camera-based 3D tracking on the nuScenes and Waymo datasets. Both these datasets are completely unseen to our method and do not require fine-tuning. Videos and code are available at https://light.princeton.edu/inverse-rendering-tracking/.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2403.18238.pdf' target='_blank'>https://arxiv.org/pdf/2403.18238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangyu Xu, Wanxuan Lu, Hongfeng Yu, Yongqiang Mao, Hanbo Bi, Chenglong Liu, Xian Sun, Kun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18238">TAFormer: A Unified Target-Aware Transformer for Video and Motion Joint Prediction in Aerial Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As drone technology advances, using unmanned aerial vehicles for aerial surveys has become the dominant trend in modern low-altitude remote sensing. The surge in aerial video data necessitates accurate prediction for future scenarios and motion states of the interested target, particularly in applications like traffic management and disaster response. Existing video prediction methods focus solely on predicting future scenes (video frames), suffering from the neglect of explicitly modeling target's motion states, which is crucial for aerial video interpretation. To address this issue, we introduce a novel task called Target-Aware Aerial Video Prediction, aiming to simultaneously predict future scenes and motion states of the target. Further, we design a model specifically for this task, named TAFormer, which provides a unified modeling approach for both video and target motion states. Specifically, we introduce Spatiotemporal Attention (STA), which decouples the learning of video dynamics into spatial static attention and temporal dynamic attention, effectively modeling the scene appearance and motion. Additionally, we design an Information Sharing Mechanism (ISM), which elegantly unifies the modeling of video and target motion by facilitating information interaction through two sets of messenger tokens. Moreover, to alleviate the difficulty of distinguishing targets in blurry predictions, we introduce Target-Sensitive Gaussian Loss (TSGL), enhancing the model's sensitivity to both target's position and content. Extensive experiments on UAV123VP and VisDroneVP (derived from single-object tracking datasets) demonstrate the exceptional performance of TAFormer in target-aware video prediction, showcasing its adaptability to the additional requirements of aerial video interpretation for target awareness.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2311.18839.pdf' target='_blank'>https://arxiv.org/pdf/2311.18839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lihao Liu, Yanqi Cheng, Zhongying Deng, Shujun Wang, Dongdong Chen, Xiaowei Hu, Pietro LiÃ², Carola-Bibiane SchÃ¶nlieb, Angelica Aviles-Rivero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18839">TrafficMOT: A Challenging Dataset for Multi-Object Tracking in Complex Traffic Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking in traffic videos is a crucial research area, offering immense potential for enhancing traffic monitoring accuracy and promoting road safety measures through the utilisation of advanced machine learning algorithms. However, existing datasets for multi-object tracking in traffic videos often feature limited instances or focus on single classes, which cannot well simulate the challenges encountered in complex traffic scenarios. To address this gap, we introduce TrafficMOT, an extensive dataset designed to encompass diverse traffic situations with complex scenarios. To validate the complexity and challenges presented by TrafficMOT, we conducted comprehensive empirical studies using three different settings: fully-supervised, semi-supervised, and a recent powerful zero-shot foundation model Tracking Anything Model (TAM). The experimental results highlight the inherent complexity of this dataset, emphasising its value in driving advancements in the field of traffic monitoring and multi-object tracking.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2308.05504.pdf' target='_blank'>https://arxiv.org/pdf/2308.05504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maarten de Backer, Wouter Jansen, Dennis Laurijssen, Ralph Simon, Walter Daems, Jan Steckel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05504">Detecting and Classifying Bio-Inspired Artificial Landmarks Using In-Air 3D Sonar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various autonomous applications rely on recognizing specific known landmarks in their environment. For example, Simultaneous Localization And Mapping (SLAM) is an important technique that lays the foundation for many common tasks, such as navigation and long-term object tracking. This entails building a map on the go based on sensory inputs which are prone to accumulating errors. Recognizing landmarks in the environment plays a vital role in correcting these errors and further improving the accuracy of SLAM. The most popular choice of sensors for conducting SLAM today is optical sensors such as cameras or LiDAR sensors. These can use landmarks such as QR codes as a prerequisite. However, such sensors become unreliable in certain conditions, e.g., foggy, dusty, reflective, or glass-rich environments. Sonar has proven to be a viable alternative to manage such situations better. However, acoustic sensors also require a different type of landmark. In this paper, we put forward a method to detect the presence of bio-mimetic acoustic landmarks using support vector machines trained on the frequency bands of the reflecting acoustic echoes using an embedded real-time imaging sonar.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2308.01622.pdf' target='_blank'>https://arxiv.org/pdf/2308.01622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaer Huang, Bingchuan Sun, Feng Chen, Tao Zhang, Jun Xie, Jian Li, Christopher Walter Twombly, Zhepeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01622">ReIDTrack: Multi-Object Track and Segmentation Without Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, dominant Multi-object tracking (MOT) and segmentation (MOTS) methods mainly follow the tracking-by-detection paradigm. Transformer-based end-to-end (E2E) solutions bring some ideas to MOT and MOTS, but they cannot achieve a new state-of-the-art (SOTA) performance in major MOT and MOTS benchmarks. Detection and association are two main modules of the tracking-by-detection paradigm. Association techniques mainly depend on the combination of motion and appearance information. As deep learning has been recently developed, the performance of the detection and appearance model is rapidly improved. These trends made us consider whether we can achieve SOTA based on only high-performance detection and appearance model. Our paper mainly focuses on exploring this direction based on CBNetV2 with Swin-B as a detection model and MoCo-v2 as a self-supervised appearance model. Motion information and IoU mapping were removed during the association. Our method wins 1st place on the MOTS track and wins 2nd on the MOT track in the CVPR2023 WAD workshop. We hope our simple and effective method can give some insights to the MOT and MOTS research community. Source code will be released under this git repository
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2306.09613.pdf' target='_blank'>https://arxiv.org/pdf/2306.09613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pha Nguyen, Kha Gia Quach, John Gauch, Samee U. Khan, Bhiksha Raj, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09613">UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2303.12304.pdf' target='_blank'>https://arxiv.org/pdf/2303.12304.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Bao, Kaiqiang Chen, Xian Sun, Liangjin Zhao, Wenhui Diao, Menglong Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12304">SiamTHN: Siamese Target Highlight Network for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Siamese network based trackers develop rapidly in the field of visual object tracking in recent years. The majority of siamese network based trackers now in use treat each channel in the feature maps generated by the backbone network equally, making the similarity response map sensitive to background influence and hence challenging to focus on the target region. Additionally, there are no structural links between the classification and regression branches in these trackers, and the two branches are optimized separately during training. Therefore, there is a misalignment between the classification and regression branches, which results in less accurate tracking results. In this paper, a Target Highlight Module is proposed to help the generated similarity response maps to be more focused on the target region. To reduce the misalignment and produce more precise tracking results, we propose a corrective loss to train the model. The two branches of the model are jointly tuned with the use of corrective loss to produce more reliable prediction results. Experiments on 5 challenging benchmark datasets reveal that the method outperforms current models in terms of performance, and runs at 38 fps, proving its effectiveness and efficiency.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2302.05914.pdf' target='_blank'>https://arxiv.org/pdf/2302.05914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Illia Oleksiienko, Paraskevi Nousi, Nikolaos Passalis, Anastasios Tefas, Alexandros Iosifidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05914">Variational Voxel Pseudo Image Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Uncertainty estimation is an important task for critical problems, such as robotics and autonomous driving, because it allows creating statistically better perception models and signaling the model's certainty in its predictions to the decision method or a human supervisor. In this paper, we propose a Variational Neural Network-based version of a Voxel Pseudo Image Tracking (VPIT) method for 3D Single Object Tracking. The Variational Feature Generation Network of the proposed Variational VPIT computes features for target and search regions and the corresponding uncertainties, which are later combined using an uncertainty-aware cross-correlation module in one of two ways: by computing similarity between the corresponding uncertainties and adding it to the regular cross-correlation values, or by penalizing the uncertain feature channels to increase influence of the certain features. In experiments, we show that both methods improve tracking performance, while penalization of uncertain features provides the best uncertainty quality.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2212.02077.pdf' target='_blank'>https://arxiv.org/pdf/2212.02077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuebo Tian, Zhongyang Zhu, Junqiao Zhao, Gengxuan Tian, Chen Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02077">DL-SLOT: Dynamic LiDAR SLAM and object tracking based on collaborative graph optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ego-pose estimation and dynamic object tracking are two critical problems for autonomous driving systems. The solutions to these problems are generally based on their respective assumptions, \ie{the static world assumption for simultaneous localization and mapping (SLAM) and the accurate ego-pose assumption for object tracking}. However, these assumptions are challenging to hold in dynamic road scenarios, where SLAM and object tracking become closely correlated. Therefore, we propose DL-SLOT, a dynamic LiDAR SLAM and object tracking method, to simultaneously address these two coupled problems. This method integrates the state estimations of both the autonomous vehicle and the stationary and dynamic objects in the environment into a unified optimization framework. First, we used object detection to identify all points belonging to potentially dynamic objects. Subsequently, a LiDAR odometry was conducted using the filtered point cloud. Simultaneously, we proposed a sliding window-based object association method that accurately associates objects according to the historical trajectories of tracked objects. The ego-states and those of the stationary and dynamic objects are integrated into the sliding window-based collaborative graph optimization. The stationary objects are subsequently restored from the potentially dynamic object set. Finally, a global pose-graph is implemented to eliminate the accumulated error. Experiments on KITTI datasets demonstrate that our method achieves better accuracy than SLAM and object tracking baseline methods. This confirms that solving SLAM and object tracking simultaneously is mutually advantageous, dramatically improving the robustness and accuracy of SLAM and object tracking in dynamic road scenarios.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2209.00993.pdf' target='_blank'>https://arxiv.org/pdf/2209.00993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mario Quiles PÃ©rez, Enrique TomÃ¡s MartÃ­nez BeltrÃ¡n, Sergio LÃ³pez Bernal, Eduardo Horna Prat, Luis Montesano Del Campo, Lorenzo FernÃ¡ndez MaimÃ³, Alberto Huertas CeldrÃ¡n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00993">Data Fusion in Neuromarketing: Multimodal Analysis of Biosignals, Lifecycle Stages, Current Advances, Datasets, Trends, and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The primary goal of any company is to increase its profits by improving both the quality of its products and how they are advertised. In this context, neuromarketing seeks to enhance the promotion of products and generate a greater acceptance on potential buyers. Traditionally, neuromarketing studies have relied on a single biosignal to obtain feedback from presented stimuli. However, thanks to new devices and technological advances studying this area of knowledge, recent trends indicate a shift towards the fusion of diverse biosignals. An example is the usage of electroencephalography for understanding the impact of an advertisement at the neural level and visual tracking to identify the stimuli that induce such impacts. This emerging pattern determines which biosignals to employ for achieving specific neuromarketing objectives. Furthermore, the fusion of data from multiple sources demands advanced processing methodologies. Despite these complexities, there is a lack of literature that adequately collates and organizes the various data sources and the applied processing techniques for the research objectives pursued. To address these challenges, the current paper conducts a comprehensive analysis of the objectives, biosignals, and data processing techniques employed in neuromarketing research. This study provides both the technical definition and a graphical distribution of the elements under revision. Additionally, it presents a categorization based on research objectives and provides an overview of the combinatory methodologies employed. After this, the paper examines primary public datasets designed for neuromarketing research together with others whose main purpose is not neuromarketing, but can be used for this matter. Ultimately, this work provides a historical perspective on the evolution of techniques across various phases over recent years and enumerates key lessons learned.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2207.04551.pdf' target='_blank'>https://arxiv.org/pdf/2207.04551.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kha Gia Quach, Huu Le, Pha Nguyen, Chi Nhan Duong, Tien Dai Bui, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.04551">Depth Perspective-aware Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper aims to tackle Multiple Object Tracking (MOT), an important problem in computer vision but remains challenging due to many practical issues, especially occlusions. Indeed, we propose a new real-time Depth Perspective-aware Multiple Object Tracking (DP-MOT) approach to tackle the occlusion problem in MOT. A simple yet efficient Subject-Ordered Depth Estimation (SODE) is first proposed to automatically order the depth positions of detected subjects in a 2D scene in an unsupervised manner. Using the output from SODE, a new Active pseudo-3D Kalman filter, a simple but effective extension of Kalman filter with dynamic control variables, is then proposed to dynamically update the movement of objects. In addition, a new high-order association approach is presented in the data association step to incorporate first-order and second-order relationships between the detected objects. The proposed approach consistently achieves state-of-the-art performance compared to recent MOT methods on standard MOT benchmarks.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2206.02619.pdf' target='_blank'>https://arxiv.org/pdf/2206.02619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Illia Oleksiienko, Paraskevi Nousi, Nikolaos Passalis, Anastasios Tefas, Alexandros Iosifidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.02619">VPIT: Real-time Embedded Single Object 3D Tracking Using Voxel Pseudo Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel voxel-based 3D single object tracking (3D SOT) method called Voxel Pseudo Image Tracking (VPIT). VPIT is the first method that uses voxel pseudo images for 3D SOT. The input point cloud is structured by pillar-based voxelization, and the resulting pseudo image is used as an input to a 2D-like Siamese SOT method. The pseudo image is created in the Bird's-eye View (BEV) coordinates, and therefore the objects in it have constant size. Thus, only the object rotation can change in the new coordinate system and not the object scale. For this reason, we replace multi-scale search with a multi-rotation search, where differently rotated search regions are compared against a single target representation to predict both position and rotation of the object. Experiments on KITTI Tracking dataset show that VPIT is the fastest 3D SOT method and maintains competitive Success and Precision values. Application of a SOT method in a real-world scenario meets with limitations such as lower computational capabilities of embedded devices and a latency-unforgiving environment, where the method is forced to skip certain data frames if the inference speed is not high enough. We implement a real-time evaluation protocol and show that other methods lose most of their performance on embedded devices, while VPIT maintains its ability to track the object.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2205.13575.pdf' target='_blank'>https://arxiv.org/pdf/2205.13575.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subha Maity, Debarghya Mukherjee, Moulinath Banerjee, Yuekai Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.13575">Predictor-corrector algorithms for stochastic optimization under gradual distribution shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Time-varying stochastic optimization problems frequently arise in machine learning practice (e.g. gradual domain shift, object tracking, strategic classification). Although most problems are solved in discrete time, the underlying process is often continuous in nature. We exploit this underlying continuity by developing predictor-corrector algorithms for time-varying stochastic optimizations. We provide error bounds for the iterates, both in presence of pure and noisy access to the queries from the relevant derivatives of the loss function. Furthermore, we show (theoretically and empirically in several examples) that our method outperforms non-predictor corrector methods that do not exploit the underlying continuous process.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2205.10766.pdf' target='_blank'>https://arxiv.org/pdf/2205.10766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaoang Wang, Mingli Song, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.10766">Recent Advances in Embedding Methods for Multi-Object Tracking: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to associate target objects across video frames in order to obtain entire moving trajectories. With the advancement of deep neural networks and the increasing demand for intelligent video analysis, MOT has gained significantly increased interest in the computer vision community. Embedding methods play an essential role in object location estimation and temporal identity association in MOT. Unlike other computer vision tasks, such as image classification, object detection, re-identification, and segmentation, embedding methods in MOT have large variations, and they have never been systematically analyzed and summarized. In this survey, we first conduct a comprehensive overview with in-depth analysis for embedding methods in MOT from seven different perspectives, including patch-level embedding, single-frame embedding, cross-frame joint embedding, correlation embedding, sequential embedding, tracklet embedding, and cross-track relational embedding. We further summarize the existing widely used MOT datasets and analyze the advantages of existing state-of-the-art methods according to their embedding strategies. Finally, some critical yet under-investigated areas and future research directions are discussed.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2506.18737.pdf' target='_blank'>https://arxiv.org/pdf/2506.18737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shanliang Yao, Runwei Guan, Yi Ni, Sen Xu, Yong Yue, Xiaohui Zhu, Ryan Wen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.18737">USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking in inland waterways plays a crucial role in safe and cost-effective applications, including waterborne transportation, sightseeing tours, environmental monitoring and surface rescue. Our Unmanned Surface Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU, delivers robust tracking capabilities in complex waterborne environments. By leveraging these sensors, our USV collected comprehensive object tracking data, which we present as USVTrack, the first 4D radar-camera tracking dataset tailored for autonomous driving in new generation waterborne transportation systems. Our USVTrack dataset presents rich scenarios, featuring diverse various waterways, varying times of day, and multiple weather and lighting conditions. Moreover, we present a simple but effective radar-camera matching method, termed RCM, which can be plugged into popular two-stage association trackers. Experimental results utilizing RCM demonstrate the effectiveness of the radar-camera matching in improving object tracking accuracy and reliability for autonomous driving in waterborne environments. The USVTrack dataset is public on https://usvtrack.github.io.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2502.01357.pdf' target='_blank'>https://arxiv.org/pdf/2502.01357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong-In Kim, Dong-Hee Paek, Seung-Hyun Song, Seung-Hyun Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01357">Bayesian Approximation-Based Trajectory Prediction and Tracking with 4D Radar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D multi-object tracking (MOT) is vital for autonomous vehicles, yet LiDAR and camera-based methods degrade in adverse weather. Meanwhile, Radar-based solutions remain robust but often suffer from limited vertical resolution and simplistic motion models. Existing Kalman filter-based approaches also rely on fixed noise covariance, hampering adaptability when objects make sudden maneuvers. We propose Bayes-4DRTrack, a 4D Radar-based MOT framework that adopts a transformer-based motion prediction network to capture nonlinear motion dynamics and employs Bayesian approximation in both detection and prediction steps. Moreover, our two-stage data association leverages Doppler measurements to better distinguish closely spaced targets. Evaluated on the K-Radar dataset (including adverse weather scenarios), Bayes-4DRTrack demonstrates a 5.7% gain in Average Multi-Object Tracking Accuracy (AMOTA) over methods with traditional motion models and fixed noise covariance. These results showcase enhanced robustness and accuracy in demanding, real-world conditions.
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2411.15600.pdf' target='_blank'>https://arxiv.org/pdf/2411.15600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15600">How Texts Help? A Fine-grained Evaluation to Reveal the Role of Language in Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language tracking (VLT) extends traditional single object tracking by incorporating textual information, providing semantic guidance to enhance tracking performance under challenging conditions like fast motion and deformations. However, current VLT trackers often underperform compared to single-modality methods on multiple benchmarks, with semantic information sometimes becoming a "distraction." To address this, we propose VLTVerse, the first fine-grained evaluation framework for VLT trackers that comprehensively considers multiple challenge factors and diverse semantic information, hoping to reveal the role of language in VLT. Our contributions include: (1) VLTVerse introduces 10 sequence-level challenge labels and 6 types of multi-granularity semantic information, creating a flexible and multi-dimensional evaluation space for VLT; (2) leveraging 60 subspaces formed by combinations of challenge factors and semantic types, we conduct systematic fine-grained evaluations of three mainstream SOTA VLT trackers, uncovering their performance bottlenecks across complex scenarios and offering a novel perspective on VLT evaluation; (3) through decoupled analysis of experimental results, we examine the impact of various semantic types on specific challenge factors in relation to different algorithms, providing essential guidance for enhancing VLT across data, evaluation, and algorithmic dimensions. The VLTVerse, toolkit, and results will be available at \url{http://metaverse.aitestunion.com}.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2411.13183.pdf' target='_blank'>https://arxiv.org/pdf/2411.13183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuiran Wang, Xuehui Yu, Wenwen Yu, Guorong Li, Xiangyuan Lan, Qixiang Ye, Jianbin Jiao, Zhenjun Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13183">ClickTrack: Towards Real-time Interactive Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking(SOT) relies on precise object bounding box initialization. In this paper, we reconsidered the deficiencies in the current approaches to initializing single object trackers and propose a new paradigm for single object tracking algorithms, ClickTrack, a new paradigm using clicking interaction for real-time scenarios. Moreover, click as an input type inherently lack hierarchical information. To address ambiguity in certain special scenarios, we designed the Guided Click Refiner(GCR), which accepts point and optional textual information as inputs, transforming the point into the bounding box expected by the operator. The bounding box will be used as input of single object trackers. Experiments on LaSOT and GOT-10k benchmarks show that tracker combined with GCR achieves stable performance in real-time interactive scenarios. Furthermore, we explored the integration of GCR into the Segment Anything model(SAM), significantly reducing ambiguity issues when SAM receives point inputs.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2410.02492.pdf' target='_blank'>https://arxiv.org/pdf/2410.02492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02492">DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual language tracking (VLT) has emerged as a cutting-edge research area, harnessing linguistic data to enhance algorithms with multi-modal inputs and broadening the scope of traditional single object tracking (SOT) to encompass video understanding applications. Despite this, most VLT benchmarks still depend on succinct, human-annotated text descriptions for each video. These descriptions often fall short in capturing the nuances of video content dynamics and lack stylistic variety in language, constrained by their uniform level of detail and a fixed annotation frequency. As a result, algorithms tend to default to a "memorize the answer" strategy, diverging from the core objective of achieving a deeper understanding of video content. Fortunately, the emergence of large language models (LLMs) has enabled the generation of diverse text. This work utilizes LLMs to generate varied semantic annotations (in terms of text lengths and granularities) for representative SOT benchmarks, thereby establishing a novel multi-modal benchmark. Specifically, we (1) propose a new visual language tracking benchmark with diverse texts, named DTVLT, based on five prominent VLT and SOT benchmarks, including three sub-tasks: short-term tracking, long-term tracking, and global instance tracking. (2) We offer four granularity texts in our benchmark, considering the extent and density of semantic information. We expect this multi-granular generation strategy to foster a favorable environment for VLT and video understanding research. (3) We conduct comprehensive experimental analyses on DTVLT, evaluating the impact of diverse text on tracking performance and hope the identified performance bottlenecks of existing algorithms can support further research in VLT and video understanding. The proposed benchmark, experimental results and toolkit will be released gradually on http://videocube.aitestunion.com/.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2409.14543.pdf' target='_blank'>https://arxiv.org/pdf/2409.14543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Raj, Lei Wang, Tom Gedeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14543">TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurately detecting and tracking high-speed, small objects, such as balls in sports videos, is challenging due to factors like motion blur and occlusion. Although recent deep learning frameworks like TrackNetV1, V2, and V3 have advanced tennis ball and shuttlecock tracking, they often struggle in scenarios with partial occlusion or low visibility. This is primarily because these models rely heavily on visual features without explicitly incorporating motion information, which is crucial for precise tracking and trajectory prediction. In this paper, we introduce an enhancement to the TrackNet family by fusing high-level visual features with learnable motion attention maps through a motion-aware fusion mechanism, effectively emphasizing the moving ball's location and improving tracking performance. Our approach leverages frame differencing maps, modulated by a motion prompt layer, to highlight key motion regions over time. Experimental results on the tennis ball and shuttlecock datasets show that our method enhances the tracking performance of both TrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built on top of the existing TrackNet, as TrackNetV4.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2409.00618.pdf' target='_blank'>https://arxiv.org/pdf/2409.00618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lipeng Gu, Mingqiang Wei, Xuefeng Yan, Dingkun Zhu, Wei Zhao, Haoran Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00618">YOLOO: You Only Learn from Others Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal 3D multi-object tracking (MOT) typically necessitates extensive computational costs of deep neural networks (DNNs) to extract multi-modal representations. In this paper, we propose an intriguing question: May we learn from multiple modalities only during training to avoid multi-modal input in the inference phase? To answer it, we propose \textbf{YOLOO}, a novel multi-modal 3D MOT paradigm: You Only Learn from Others Once. YOLOO empowers the point cloud encoder to learn a unified tri-modal representation (UTR) from point clouds and other modalities, such as images and textual cues, all at once. Leveraging this UTR, YOLOO achieves efficient tracking solely using the point cloud encoder without compromising its performance, fundamentally obviating the need for computationally intensive DNNs. Specifically, YOLOO includes two core components: a unified tri-modal encoder (UTEnc) and a flexible geometric constraint (F-GC) module. UTEnc integrates a point cloud encoder with image and text encoders adapted from pre-trained CLIP. It seamlessly fuses point cloud information with rich visual-textual knowledge from CLIP into the point cloud encoder, yielding highly discriminative UTRs that facilitate the association between trajectories and detections. Additionally, F-GC filters out mismatched associations with similar representations but significant positional discrepancies. It further enhances the robustness of UTRs without requiring any scene-specific tuning, addressing a key limitation of customized geometric constraints (e.g., 3D IoU). Lastly, high-quality 3D trajectories are generated by a traditional data association component. By integrating these advancements into a multi-modal 3D MOT scheme, our YOLOO achieves substantial gains in both robustness and efficiency.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2405.12139.pdf' target='_blank'>https://arxiv.org/pdf/2405.12139.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuchen Li, Xiaokun Feng, Shiyu Hu, Meiqi Wu, Dailing Zhang, Jing Zhang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12139">DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Tracking (VLT) enhances single object tracking (SOT) by integrating natural language descriptions from a video, for the precise tracking of a specified object. By leveraging high-level semantic information, VLT guides object tracking, alleviating the constraints associated with relying on a visual modality. Nevertheless, most VLT benchmarks are annotated in a single granularity and lack a coherent semantic framework to provide scientific guidance. Moreover, coordinating human annotators for high-quality annotations is laborious and time-consuming. To address these challenges, we introduce DTLLM-VLT, which automatically generates extensive and multi-granularity text to enhance environmental diversity. (1) DTLLM-VLT generates scientific and multi-granularity text descriptions using a cohesive prompt framework. Its succinct and highly adaptable design allows seamless integration into various visual tracking benchmarks. (2) We select three prominent benchmarks to deploy our approach: short-term tracking, long-term tracking, and global instance tracking. We offer four granularity combinations for these benchmarks, considering the extent and density of semantic information, thereby showcasing the practicality and versatility of DTLLM-VLT. (3) We conduct comparative experiments on VLT benchmarks with different text granularities, evaluating and analyzing the impact of diverse text on tracking performance. Conclusionally, this work leverages LLM to provide multi-granularity semantic information for VLT task from efficient and diverse perspectives, enabling fine-grained evaluation of multi-modal trackers. In the future, we believe this work can be extended to more datasets to support vision datasets understanding.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2402.15895.pdf' target='_blank'>https://arxiv.org/pdf/2402.15895.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinkun Cao, Jiangmiao Pang, Kris Kitani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.15895">Multi-Object Tracking by Hierarchical Visual Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a new visual hierarchical representation paradigm for multi-object tracking. It is more effective to discriminate between objects by attending to objects' compositional visual regions and contrasting with the background contextual information instead of sticking to only the semantic visual cue such as bounding boxes. This compositional-semantic-contextual hierarchy is flexible to be integrated in different appearance-based multi-object tracking methods. We also propose an attention-based visual feature module to fuse the hierarchical visual representations. The proposed method achieves state-of-the-art accuracy and time efficiency among query-based methods on multiple multi-object tracking benchmarks.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2311.04749.pdf' target='_blank'>https://arxiv.org/pdf/2311.04749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbing Wang, Junyi Ji, William Barbour, Daniel B. Work
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04749">Online Min Cost Circulation for Multi-Object Tracking on Fragments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) or global data association problem is commonly approached as a minimum-cost-flow or minimum-cost-circulation problem on a graph. While there have been numerous studies aimed at enhancing algorithm efficiency, most of them focus on the batch problem, where all the data must be available simultaneously to construct a static graph. However, with the growing number of applications that generate streaming data, an efficient online algorithm is required to handle the streaming nature of the input. In this paper, we present an online extension of the well-known negative cycle canceling algorithm for solving the multi-object tracking problem with streaming fragmented data. We provide a proof of correctness for the proposed algorithm and demonstrate its efficiency through numerical experiments.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2305.08628.pdf' target='_blank'>https://arxiv.org/pdf/2305.08628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viktoria Ehm, Daniel Cremers, Florian Bernard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08628">Non-Separable Multi-Dimensional Network Flows for Visual Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Flows in networks (or graphs) play a significant role in numerous computer vision tasks. The scalar-valued edges in these graphs often lead to a loss of information and thereby to limitations in terms of expressiveness. For example, oftentimes high-dimensional data (e.g. feature descriptors) are mapped to a single scalar value (e.g. the similarity between two feature descriptors). To overcome this limitation, we propose a novel formalism for non-separable multi-dimensional network flows. By doing so, we enable an automatic and adaptive feature selection strategy - since the flow is defined on a per-dimension basis, the maximizing flow automatically chooses the best matching feature dimensions. As a proof of concept, we apply our formalism to the multi-object tracking problem and demonstrate that our approach outperforms scalar formulations on the MOT16 benchmark in terms of robustness to noise.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2209.02339.pdf' target='_blank'>https://arxiv.org/pdf/2209.02339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hua Ma, Yinshan Li, Yansong Gao, Zhi Zhang, Alsharif Abuadbba, Anmin Fu, Said F. Al-Sarawi, Nepal Surya, Derek Abbott
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.02339">TransCAB: Transferable Clean-Annotation Backdoor to Object Detection with Natural Trigger in Real-World</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection is the foundation of various critical computer-vision tasks such as segmentation, object tracking, and event detection. To train an object detector with satisfactory accuracy, a large amount of data is required. However, due to the intensive workforce involved with annotating large datasets, such a data curation task is often outsourced to a third party or relied on volunteers. This work reveals severe vulnerabilities of such data curation pipeline. We propose MACAB that crafts clean-annotated images to stealthily implant the backdoor into the object detectors trained on them even when the data curator can manually audit the images. We observe that the backdoor effect of both misclassification and the cloaking are robustly achieved in the wild when the backdoor is activated with inconspicuously natural physical triggers. Backdooring non-classification object detection with clean-annotation is challenging compared to backdooring existing image classification tasks with clean-label, owing to the complexity of having multiple objects within each frame, including victim and non-victim objects. The efficacy of the MACAB is ensured by constructively i abusing the image-scaling function used by the deep learning framework, ii incorporating the proposed adversarial clean image replica technique, and iii combining poison data selection criteria given constrained attacking budget. Extensive experiments demonstrate that MACAB exhibits more than 90% attack success rate under various real-world scenes. This includes both cloaking and misclassification backdoor effect even restricted with a small attack budget. The poisoned samples cannot be effectively identified by state-of-the-art detection techniques.The comprehensive video demo is at https://youtu.be/MA7L_LpXkp4, which is based on a poison rate of 0.14% for YOLOv4 cloaking backdoor and Faster R-CNN misclassification backdoor.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2012.02689.pdf' target='_blank'>https://arxiv.org/pdf/2012.02689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maolin Gao, Zorah LÃ¤hner, Johan Thunberg, Daniel Cremers, Florian Bernard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2012.02689">Isometric Multi-Shape Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding correspondences between shapes is a fundamental problem in computer vision and graphics, which is relevant for many applications, including 3D reconstruction, object tracking, and style transfer. The vast majority of correspondence methods aim to find a solution between pairs of shapes, even if multiple instances of the same class are available. While isometries are often studied in shape correspondence problems, they have not been considered explicitly in the multi-matching setting. This paper closes this gap by proposing a novel optimisation formulation for isometric multi-shape matching. We present a suitable optimisation algorithm for solving our formulation and provide a convergence and complexity analysis. Our algorithm obtains multi-matchings that are by construction provably cycle-consistent. We demonstrate the superior performance of our method on various datasets and set the new state-of-the-art in isometric multi-shape matching.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2507.21460.pdf' target='_blank'>https://arxiv.org/pdf/2507.21460.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mianzhao Wang, Fan Shi, Xu Cheng, Feifei Zhang, Shengyong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21460">An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality 4D light field representation with efficient angular feature modeling is crucial for scene perception, as it can provide discriminative spatial-angular cues to identify moving targets. However, recent developments still struggle to deliver reliable angular modeling in the temporal domain, particularly in complex low-light scenes. In this paper, we propose a novel light field epipolar-plane structure image (ESI) representation that explicitly defines the geometric structure within the light field. By capitalizing on the abrupt changes in the angles of light rays within the epipolar plane, this representation can enhance visual expression in low-light scenes and reduce redundancy in high-dimensional light fields. We further propose an angular-temporal interaction network (ATINet) for light field object tracking that learns angular-aware representations from the geometric structural cues and angular-temporal interaction cues of light fields. Furthermore, ATINet can also be optimized in a self-supervised manner to enhance the geometric feature interaction across the temporal domain. Finally, we introduce a large-scale light field low-light dataset for object tracking. Extensive experimentation demonstrates that ATINet achieves state-of-the-art performance in single object tracking. Furthermore, we extend the proposed method to multiple object tracking, which also shows the effectiveness of high-quality light field angular-temporal modeling.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2507.09880.pdf' target='_blank'>https://arxiv.org/pdf/2507.09880.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keito Suzuki, Bang Du, Runfa Blark Li, Kunyao Chen, Lei Wang, Peng Liu, Ning Bi, Truong Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09880">OpenHuman4D: Open-Vocabulary 4D Human Parsing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding dynamic 3D human representation has become increasingly critical in virtual and extended reality applications. However, existing human part segmentation methods are constrained by reliance on closed-set datasets and prolonged inference times, which significantly restrict their applicability. In this paper, we introduce the first 4D human parsing framework that simultaneously addresses these challenges by reducing the inference time and introducing open-vocabulary capabilities. Building upon state-of-the-art open-vocabulary 3D human parsing techniques, our approach extends the support to 4D human-centric video with three key innovations: 1) We adopt mask-based video object tracking to efficiently establish spatial and temporal correspondences, avoiding the necessity of segmenting all frames. 2) A novel Mask Validation module is designed to manage new target identification and mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating memory-conditioned attention and logits equalization for robust embedding fusion. Extensive experiments demonstrate the effectiveness and flexibility of the proposed method on 4D human-centric parsing tasks, achieving up to 93.3% acceleration compared to the previous state-of-the-art method, which was limited to parsing fixed classes.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2405.17903.pdf' target='_blank'>https://arxiv.org/pdf/2405.17903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongze Sun, Rui Liu, Wuque Cai, Jun Wang, Yue Wang, Huajin Tang, Yan Cui, Dezhong Yao, Daqing Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17903">Reliable Object Tracking by Multimodal Hybrid Feature Extraction and Transformer-Based Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking, which is primarily based on visible light image sequences, encounters numerous challenges in complicated scenarios, such as low light conditions, high dynamic ranges, and background clutter. To address these challenges, incorporating the advantages of multiple visual modalities is a promising solution for achieving reliable object tracking. However, the existing approaches usually integrate multimodal inputs through adaptive local feature interactions, which cannot leverage the full potential of visual cues, thus resulting in insufficient feature modeling. In this study, we propose a novel multimodal hybrid tracker (MMHT) that utilizes frame-event-based data for reliable single object tracking. The MMHT model employs a hybrid backbone consisting of an artificial neural network (ANN) and a spiking neural network (SNN) to extract dominant features from different visual modalities and then uses a unified encoder to align the features across different domains. Moreover, we propose an enhanced transformer-based module to fuse multimodal features using attention mechanisms. With these methods, the MMHT model can effectively construct a multiscale and multidimensional visual feature space and achieve discriminative feature modeling. Extensive experiments demonstrate that the MMHT model exhibits competitive performance in comparison with that of other state-of-the-art methods. Overall, our results highlight the effectiveness of the MMHT model in terms of addressing the challenges faced in visual object tracking tasks.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2404.12963.pdf' target='_blank'>https://arxiv.org/pdf/2404.12963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Rapado-Rincon, Akshay K. Burusa, Eldert J. van Henten, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12963">A comparison between single-stage and two-stage 3D tracking algorithms for greenhouse robotics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the current demand for automation in the agro-food industry, accurately detecting and localizing relevant objects in 3D is essential for successful robotic operations. However, this is a challenge due the presence of occlusions. Multi-view perception approaches allow robots to overcome occlusions, but a tracking component is needed to associate the objects detected by the robot over multiple viewpoints. Multi-object tracking (MOT) algorithms can be categorized between two-stage and single-stage methods. Two-stage methods tend to be simpler to adapt and implement to custom applications, while single-stage methods present a more complex end-to-end tracking method that can yield better results in occluded situations at the cost of more training data. The potential advantages of single-stage methods over two-stage methods depends on the complexity of the sequence of viewpoints that a robot needs to process. In this work, we compare a 3D two-stage MOT algorithm, 3D-SORT, against a 3D single-stage MOT algorithm, MOT-DETR, in three different types of sequences with varying levels of complexity. The sequences represent simpler and more complex motions that a robot arm can perform in a tomato greenhouse. Our experiments in a tomato greenhouse show that the single-stage algorithm consistently yields better tracking accuracy, especially in the more challenging sequences where objects are fully occluded or non-visible during several viewpoints.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2404.07495.pdf' target='_blank'>https://arxiv.org/pdf/2404.07495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weisheng Xu, Sifan Zhou, Jiaqi Xiong, Ziyu Zhao, Zhihang Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07495">PillarTrack:Boosting Pillar Representation for Transformer-based 3D Single Object Tracking on Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D single object tracking (3D SOT) is a critical issue in robotics and autonomous driving. Existing 3D SOT methods typically adhere to a point-based processing pipeline, wherein the re-sampling operation invariably leads to either redundant or missing information, thereby impacting performance. To address these issues, we propose PillarTrack, a novel pillar-based 3D SOT framework. First, we transform sparse point clouds into dense pillars to preserve the local and global geometrics. Second, we propose a Pyramid-Encoded Pillar Feature Encoder (PE-PFE) design to enhance the robustness of pillar feature for translation/rotation/scale. Third, we present an efficient Transformer-based backbone from the perspective of modality differences. Finally, we construct our PillarTrack based on above designs. Extensive experiments show that our method achieves comparable performance on the KITTI and NuScenes datasets, significantly enhancing the performance of the baseline.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2311.17085.pdf' target='_blank'>https://arxiv.org/pdf/2311.17085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Ge, Xiangmei Chen, Jiuxin Cao, Xuelin Zhu, Bo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17085">Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking aims to locate one specific target in video sequences, given its initial state. Classical trackers rely solely on visual cues, restricting their ability to handle challenges such as appearance variations, ambiguity, and distractions. Hence, Vision-Language (VL) tracking has emerged as a promising approach, incorporating language descriptions to directly provide high-level semantics and enhance tracking performance. However, current VL trackers have not fully exploited the power of VL learning, as they suffer from limitations such as heavily relying on off-the-shelf backbones for feature extraction, ineffective VL fusion designs, and the absence of VL-related loss functions. Consequently, we present a novel tracker that progressively explores target-centric semantics for VL tracking. Specifically, we propose the first Synchronous Learning Backbone (SLB) for VL tracking, which consists of two novel modules: the Target Enhance Module (TEM) and the Semantic Aware Module (SAM). These modules enable the tracker to perceive target-related semantics and comprehend the context of both visual and textual modalities at the same pace, facilitating VL feature extraction and fusion at different semantic levels. Moreover, we devise the dense matching loss to further strengthen multi-modal representation learning. Extensive experiments on VL tracking datasets demonstrate the superiority and effectiveness of our methods.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2310.04781.pdf' target='_blank'>https://arxiv.org/pdf/2310.04781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Saviolo, Pratyaksh Rao, Vivek Radhakrishnan, Jiuhong Xiao, Giuseppe Loianno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.04781">Unifying Foundation Models with Quadrotor Control for Visual Tracking Beyond Object Categories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual control enables quadrotors to adaptively navigate using real-time sensory data, bridging perception with action. Yet, challenges persist, including generalization across scenarios, maintaining reliability, and ensuring real-time responsiveness. This paper introduces a perception framework grounded in foundation models for universal object detection and tracking, moving beyond specific training categories. Integral to our approach is a multi-layered tracker integrated with the foundation detector, ensuring continuous target visibility, even when faced with motion blur, abrupt light shifts, and occlusions. Complementing this, we introduce a model-free controller tailored for resilient quadrotor visual tracking. Our system operates efficiently on limited hardware, relying solely on an onboard camera and an inertial measurement unit. Through extensive validation in diverse challenging indoor and outdoor environments, we demonstrate our system's effectiveness and adaptability. In conclusion, our research represents a step forward in quadrotor visual tracking, moving from task-specific methods to more versatile and adaptable operations.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2308.15266.pdf' target='_blank'>https://arxiv.org/pdf/2308.15266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim Meinhardt, Matt Feiszli, Yuchen Fan, Laura Leal-Taixe, Rakesh Ranjan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15266">NOVIS: A Case for End-to-End Near-Online Video Instance Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Until recently, the Video Instance Segmentation (VIS) community operated under the common belief that offline methods are generally superior to a frame by frame online processing. However, the recent success of online methods questions this belief, in particular, for challenging and long video sequences. We understand this work as a rebuttal of those recent observations and an appeal to the community to focus on dedicated near-online VIS approaches. To support our argument, we present a detailed analysis on different processing paradigms and the new end-to-end trainable NOVIS (Near-Online Video Instance Segmentation) method. Our transformer-based model directly predicts spatio-temporal mask volumes for clips of frames and performs instance tracking between clips via overlap embeddings. NOVIS represents the first near-online VIS approach which avoids any handcrafted tracking heuristics. We outperform all existing VIS methods by large margins and provide new state-of-the-art results on both YouTube-VIS (2019/2021) and the OVIS benchmarks.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2308.14833.pdf' target='_blank'>https://arxiv.org/pdf/2308.14833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Derek Gloudemans, Yanbing Wang, Gracie Gumm, William Barbour, Daniel B. Work
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14833">The Interstate-24 3D Dataset: a new benchmark for 3D multi-camera vehicle tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work presents a novel video dataset recorded from overlapping highway traffic cameras along an urban interstate, enabling multi-camera 3D object tracking in a traffic monitoring context. Data is released from 3 scenes containing video from at least 16 cameras each, totaling 57 minutes in length. 877,000 3D bounding boxes and corresponding object tracklets are fully and accurately annotated for each camera field of view and are combined into a spatially and temporally continuous set of vehicle trajectories for each scene. Lastly, existing algorithms are combined to benchmark a number of 3D multi-camera tracking pipelines on the dataset, with results indicating that the dataset is challenging due to the difficulty of matching objects traveling at high speeds across cameras and heavy object occlusion, potentially for hundreds of frames, during congested traffic. This work aims to enable the development of accurate and automatic vehicle trajectory extraction algorithms, which will play a vital role in understanding impacts of autonomous vehicle technologies on the safety and efficiency of traffic.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2308.07207.pdf' target='_blank'>https://arxiv.org/pdf/2308.07207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufeng Yao, Jiaqi Wang, Jinlong Peng, Mingmin Chi, Chao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07207">FOLT: Fast Multiple Object Tracking from UAV-captured Videos Based on Optical Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) has been successfully investigated in computer vision.
  However, MOT for the videos captured by unmanned aerial vehicles (UAV) is still challenging due to small object size, blurred object appearance, and very large and/or irregular motion in both ground objects and UAV platforms.
  In this paper, we propose FOLT to mitigate these problems and reach fast and accurate MOT in UAV view.
  Aiming at speed-accuracy trade-off, FOLT adopts a modern detector and light-weight optical flow extractor to extract object detection features and motion features at a minimum cost.
  Given the extracted flow, the flow-guided feature augmentation is designed to augment the object detection feature based on its optical flow, which improves the detection of small objects.
  Then the flow-guided motion prediction is also proposed to predict the object's position in the next frame, which improves the tracking performance of objects with very large displacements between adjacent frames.
  Finally, the tracker matches the detected objects and predicted objects using a spatially matching scheme to generate tracks for every object.
  Experiments on Visdrone and UAVDT datasets show that our proposed model can successfully track small objects with large and irregular motion and outperform existing state-of-the-art methods in UAV-MOT tasks.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2307.05219.pdf' target='_blank'>https://arxiv.org/pdf/2307.05219.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Rapado-Rincon, Eldert J. van Henten, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05219">MinkSORT: A 3D deep feature extractor using sparse convolutions to improve 3D multi-object tracking in greenhouse tomato plants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The agro-food industry is turning to robots to address the challenge of labour shortage. However, agro-food environments pose difficulties for robots due to high variation and occlusions. In the presence of these challenges, accurate world models, with information about object location, shape, and properties, are crucial for robots to perform tasks accurately. Building such models is challenging due to the complex and unique nature of agro-food environments, and errors in the model can lead to task execution issues. In this paper, MinkSORT, a novel method for generating tracking features using a 3D sparse convolutional network in a deepSORT-like approach, is proposed to improve the accuracy of world models in agro-food environments. MinkSORT was evaluated using real-world data collected in a tomato greenhouse, where it significantly improved the performance of a baseline model that tracks tomato positions in 3D using a Kalman filter and Mahalanobis distance. MinkSORT improved the HOTA from 42.8% to 44.77%, the association accuracy from 32.55% to 35.55%, and the MOTA from 57.63% to 58.81%. Different contrastive loss functions for training MinkSORT were also evaluated, and it was demonstrated that it leads to improved performance in terms of three separate precision and recall detection outcomes. The proposed method improves world model accuracy, enabling robots to perform tasks such as harvesting and plant maintenance with greater efficiency and accuracy, which is essential for meeting the growing demand for food in a sustainable manner.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2304.08408.pdf' target='_blank'>https://arxiv.org/pdf/2304.08408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Li, Tobias Fischer, Lei Ke, Henghui Ding, Martin Danelljan, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.08408">OVTrack: Open-Vocabulary Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to recognize, localize and track dynamic objects in a scene is fundamental to many real-world applications, such as self-driving and robotic systems. Yet, traditional multiple object tracking (MOT) benchmarks rely only on a few object categories that hardly represent the multitude of possible objects that are encountered in the real world. This leaves contemporary MOT methods limited to a small set of pre-defined object categories. In this paper, we address this limitation by tackling a novel task, open-vocabulary MOT, that aims to evaluate tracking beyond pre-defined training categories. We further develop OVTrack, an open-vocabulary tracker that is capable of tracking arbitrary object classes. Its design is based on two key ingredients: First, leveraging vision-language models for both classification and association via knowledge distillation; second, a data hallucination strategy for robust appearance feature learning from denoising diffusion probabilistic models. The result is an extremely data-efficient open-vocabulary tracker that sets a new state-of-the-art on the large-scale, large-vocabulary TAO benchmark, while being trained solely on static images. Project page: https://www.vis.xyz/pub/ovtrack/
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2303.05071.pdf' target='_blank'>https://arxiv.org/pdf/2303.05071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian-Xing Xu, Yuan-Chen Guo, Yu-Kun Lai, Song-Hai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05071">MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking has been a crucial problem for decades with numerous applications such as autonomous driving. Despite its wide-ranging use, this task remains challenging due to the significant appearance variation caused by occlusion and size differences among tracked targets. To address these issues, we present MBPTrack, which adopts a Memory mechanism to utilize past information and formulates localization in a coarse-to-fine scheme using Box Priors given in the first frame. Specifically, past frames with targetness masks serve as an external memory, and a transformer-based module propagates tracked target cues from the memory to the current frame. To precisely localize objects of all sizes, MBPTrack first predicts the target center via Hough voting. By leveraging box priors given in the first frame, we adaptively sample reference points around the target center that roughly cover the target of different sizes. Then, we obtain dense feature maps by aggregating point features into the reference points, where localization can be performed more effectively. Extensive experiments demonstrate that MBPTrack achieves state-of-the-art performance on KITTI, nuScenes and Waymo Open Dataset, while running at 50 FPS on a single RTX3090 GPU.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2211.08542.pdf' target='_blank'>https://arxiv.org/pdf/2211.08542.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian-Xing Xu, Yuan-Chen Guo, Yu-Kun Lai, Song-Hai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.08542">CXTrack: Improving 3D Point Cloud Tracking with Contextual Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking plays an essential role in many applications, such as autonomous driving. It remains a challenging problem due to the large appearance variation and the sparsity of points caused by occlusion and limited sensor capabilities. Therefore, contextual information across two consecutive frames is crucial for effective object tracking. However, points containing such useful information are often overlooked and cropped out in existing methods, leading to insufficient use of important contextual knowledge. To address this issue, we propose CXTrack, a novel transformer-based network for 3D object tracking, which exploits ConteXtual information to improve the tracking results. Specifically, we design a target-centric transformer network that directly takes point features from two consecutive frames and the previous bounding box as input to explore contextual information and implicitly propagate target cues. To achieve accurate localization for objects of all sizes, we propose a transformer-based localization head with a novel center embedding module to distinguish the target from distractors. Extensive experiments on three large-scale datasets, KITTI, nuScenes and Waymo Open Dataset, show that CXTrack achieves state-of-the-art tracking performance while running at 34 FPS.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2211.02760.pdf' target='_blank'>https://arxiv.org/pdf/2211.02760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Rapado Rincon, Eldert J. van Henten, Gert Kootstra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.02760">Development and evaluation of automated localisation and reconstruction of all fruits on tomato plants in a greenhouse based on multi-view perception and 3D multi-object tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to accurately represent and localise relevant objects is essential for robots to carry out tasks effectively. Traditional approaches, where robots simply capture an image, process that image to take an action, and then forget the information, have proven to struggle in the presence of occlusions. Methods using multi-view perception, which have the potential to address some of these problems, require a world model that guides the collection, integration and extraction of information from multiple viewpoints. Furthermore, constructing a generic representation that can be applied in various environments and tasks is a difficult challenge. In this paper, a novel approach for building generic representations in occluded agro-food environments using multi-view perception and 3D multi-object tracking is introduced. The method is based on a detection algorithm that generates partial point clouds for each detected object, followed by a 3D multi-object tracking algorithm that updates the representation over time. The accuracy of the representation was evaluated in a real-world environment, where successful representation and localisation of tomatoes in tomato plants were achieved, despite high levels of occlusion, with the total count of tomatoes estimated with a maximum error of 5.08% and the tomatoes tracked with an accuracy up to 71.47%. Novel tracking metrics were introduced, demonstrating that valuable insight into the errors in localising and representing the fruits can be provided by their use. This approach presents a novel solution for building representations in occluded agro-food environments, demonstrating potential to enable robots to perform tasks effectively in these challenging environments.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2507.04762.pdf' target='_blank'>https://arxiv.org/pdf/2507.04762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Damanaki, Ioulia Kapsali, Nikos Piperigkos, Alexandros Gkillas, Aris S. Lalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04762">Robustifying 3D Perception via Least-Squares Graphs for Multi-Agent Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The critical perception capabilities of EdgeAI systems, such as autonomous vehicles, are required to be resilient against adversarial threats, by enabling accurate identification and localization of multiple objects in the scene over time, mitigating their impact. Single-agent tracking offers resilience to adversarial attacks but lacks situational awareness, underscoring the need for multi-agent cooperation to enhance context understanding and robustness. This paper proposes a novel mitigation framework on 3D LiDAR scene against adversarial noise by tracking objects based on least-squares graph on multi-agent adversarial bounding boxes. Specifically, we employ the least-squares graph tool to reduce the induced positional error of each detection's centroid utilizing overlapped bounding boxes on a fully connected graph via differential coordinates and anchor points. Hence, the multi-vehicle detections are fused and refined mitigating the adversarial impact, and associated with existing tracks in two stages performing tracking to further suppress the adversarial threat. An extensive evaluation study on the real-world V2V4Real dataset demonstrates that the proposed method significantly outperforms both state-of-the-art single and multi-agent tracking frameworks by up to 23.3% under challenging adversarial conditions, operating as a resilient approach without relying on additional defense mechanisms.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2506.09469.pdf' target='_blank'>https://arxiv.org/pdf/2506.09469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Damanaki, Nikos Piperigkos, Alexandros Gkillas, Aris S. Lalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09469">Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) plays a crucial role in autonomous driving systems, as it lays the foundations for advanced perception and precise path planning modules. Nonetheless, single agent based MOT lacks in sensing surroundings due to occlusions, sensors failures, etc. Hence, the integration of multiagent information is essential for comprehensive understanding of the environment. This paper proposes a novel Cooperative MOT framework for tracking objects in 3D LiDAR scene by formulating and solving a graph topology-aware optimization problem so as to fuse information coming from multiple vehicles. By exploiting a fully connected graph topology defined by the detected bounding boxes, we employ the Graph Laplacian processing optimization technique to smooth the position error of bounding boxes and effectively combine them. In that manner, we reveal and leverage inherent coherences of diverse multi-agent detections, and associate the refined bounding boxes to tracked objects at two stages, optimizing localization and tracking accuracies. An extensive evaluation study has been conducted, using the real-world V2V4Real dataset, where the proposed method significantly outperforms the baseline frameworks, including the state-of-the-art deep-learning DMSTrack and V2V4Real, in various testing sequences.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2503.12968.pdf' target='_blank'>https://arxiv.org/pdf/2503.12968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12968">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2412.01543.pdf' target='_blank'>https://arxiv.org/pdf/2412.01543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufeng Jin, Vignesh Prasad, Snehal Jauhri, Mathias Franzius, Georgia Chalvatzaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01543">6DOPE-GS: Online 6D Object Pose Estimation using Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and accurate object pose estimation is an essential component for modern vision systems in many applications such as Augmented Reality, autonomous driving, and robotics. While research in model-based 6D object pose estimation has delivered promising results, model-free methods are hindered by the high computational load in rendering and inferring consistent poses of arbitrary objects in a live RGB-D video stream. To address this issue, we present 6DOPE-GS, a novel method for online 6D object pose estimation \& tracking with a single RGB-D camera by effectively leveraging advances in Gaussian Splatting. Thanks to the fast differentiable rendering capabilities of Gaussian Splatting, 6DOPE-GS can simultaneously optimize for 6D object poses and 3D object reconstruction. To achieve the necessary efficiency and accuracy for live tracking, our method uses incremental 2D Gaussian Splatting with an intelligent dynamic keyframe selection procedure to achieve high spatial object coverage and prevent erroneous pose updates. We also propose an opacity statistic-based pruning mechanism for adaptive Gaussian density control, to ensure training stability and efficiency. We evaluate our method on the HO3D and YCBInEOAT datasets and show that 6DOPE-GS matches the performance of state-of-the-art baselines for model-free simultaneous 6D pose tracking and reconstruction while providing a 5$\times$ speedup. We also demonstrate the method's suitability for live, dynamic object tracking and reconstruction in a real-world setting.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2406.20083.pdf' target='_blank'>https://arxiv.org/pdf/2406.20083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, Luca Weihs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.20083">PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present PoliFormer (Policy Transformer), an RGB-only indoor navigation agent trained end-to-end with reinforcement learning at scale that generalizes to the real-world without adaptation despite being trained purely in simulation. PoliFormer uses a foundational vision transformer encoder with a causal transformer decoder enabling long-term memory and reasoning. It is trained for hundreds of millions of interactions across diverse environments, leveraging parallelized, multi-machine rollouts for efficient training with high throughput. PoliFormer is a masterful navigator, producing state-of-the-art results across two distinct embodiments, the LoCoBot and Stretch RE-1 robots, and four navigation benchmarks. It breaks through the plateaus of previous work, achieving an unprecedented 85.5% success rate in object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement. PoliFormer can also be trivially extended to a variety of downstream applications such as object tracking, multi-object navigation, and open-vocabulary navigation with no finetuning.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2406.01380.pdf' target='_blank'>https://arxiv.org/pdf/2406.01380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Liu, Wenhan Cao, Chang Liu, Tianyi Zhang, Shengbo Eben Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01380">Convolutional Unscented Kalman Filter for Multi-Object Tracking with Outliers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is an essential technique for navigation in autonomous driving. In tracking-by-detection systems, biases, false positives, and misses, which are referred to as outliers, are inevitable due to complex traffic scenarios. Recent tracking methods are based on filtering algorithms that overlook these outliers, leading to reduced tracking accuracy or even loss of the objects trajectory. To handle this challenge, we adopt a probabilistic perspective, regarding the generation of outliers as misspecification between the actual distribution of measurement data and the nominal measurement model used for filtering. We further demonstrate that, by designing a convolutional operation, we can mitigate this misspecification. Incorporating this operation into the widely used unscented Kalman filter (UKF) in commonly adopted tracking algorithms, we derive a variant of the UKF that is robust to outliers, called the convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian conjugate property, thus allowing for real-time tracking. We also prove that ConvUKF has a bounded tracking error in the presence of outliers, which implies robust stability. The experimental results on the KITTI and nuScenes datasets show improved accuracy compared to representative baseline algorithms for MOT tasks.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2405.17698.pdf' target='_blank'>https://arxiv.org/pdf/2405.17698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isla Duporge, Maksim Kholiavchenko, Roi Harel, Scott Wolf, Dan Rubenstein, Meg Crofoot, Tanya Berger-Wolf, Stephen Lee, Julie Barreau, Jenna Kline, Michelle Ramirez, Charles Stewart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17698">BaboonLand Dataset: Tracking Primates in the Wild and Automating Behaviour Recognition from Drone Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Using drones to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding group primate behavior. Previous studies have demonstrated that it is possible to automate the classification of primate behavior from video data, but these studies have been carried out in captivity or from ground-based cameras. To understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can be seen in relation to the natural environment in which ecological decisions are made. This study presents a novel dataset from drone videos for baboon detection, tracking, and behavior recognition. The baboon detection dataset was created by manually annotating all baboons in drone videos with bounding boxes. A tiling method was subsequently applied to create a pyramid of images at various scales from the original 5.3K resolution images, resulting in approximately 30K images used for baboon detection. The tracking dataset is derived from the detection dataset, where all bounding boxes are assigned the same ID throughout the video. This process resulted in half an hour of very dense tracking data. The behavior recognition dataset was generated by converting tracks into mini-scenes, a video subregion centered on each animal; each mini-scene was manually annotated with 12 distinct behavior types, resulting in over 20 hours of data. Benchmark results show mean average precision (mAP) of 92.62\% for the YOLOv8-X detection model, multiple object tracking precision (MOTA) of 63.81\% for the BotSort tracking algorithm, and micro top-1 accuracy of 63.97\% for the X3D behavior recognition model. Using deep learning to classify wildlife behavior from drone footage facilitates non-invasive insight into the collective behavior of an entire group.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2405.17323.pdf' target='_blank'>https://arxiv.org/pdf/2405.17323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingwei Liu, Yasutomo Kawanishi, Takahiro Komamizu, Ichiro Ide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.17323">Tracking Small Birds by Detection Candidate Region Filtering and Detection History-aware Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on tracking birds that appear small in a panoramic video. When the size of the tracked object is small in the image (small object tracking) and move quickly, object detection and association suffers. To address these problems, we propose Adaptive Slicing Aided Hyper Inference (Adaptive SAHI), which reduces the candidate regions to apply detection, and Detection History-aware Similarity Criterion (DHSC), which accurately associates objects in consecutive frames based on the detection history. Experiments on the NUBird2022 dataset verifies the effectiveness of the proposed method by showing improvements in both accuracy and speed.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2402.17098.pdf' target='_blank'>https://arxiv.org/pdf/2402.17098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Gao, Shi-Min Li, Feng Gao, Fei Wang, Ru-Yue Yuan, Hamido Fujita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17098">In Defense and Revival of Bayesian Filtering for Thermal Infrared Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based methods monopolize the latest research in the field of thermal infrared (TIR) object tracking. However, relying solely on deep learning models to obtain better tracking results requires carefully selecting feature information that is beneficial to representing the target object and designing a reasonable template update strategy, which undoubtedly increases the difficulty of model design. Thus, recent TIR tracking methods face many challenges in complex scenarios. This paper introduces a novel Deep Bayesian Filtering (DBF) method to enhance TIR tracking in these challenging situations. DBF is distinctive in its dual-model structure: the system and observation models. The system model leverages motion data to estimate the potential positions of the target object based on two-dimensional Brownian motion, thus generating a prior probability. Following this, the observation model comes into play upon capturing the TIR image. It serves as a classifier and employs infrared information to ascertain the likelihood of these estimated positions, creating a likelihood probability. According to the guidance of the two models, the position of the target object can be determined, and the template can be dynamically updated. Experimental analysis across several benchmark datasets reveals that DBF achieves competitive performance, surpassing most existing TIR tracking methods in complex scenarios.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2402.07677.pdf' target='_blank'>https://arxiv.org/pdf/2402.07677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Li, Hannah Schieber, Niklas Corell, Bernhard Egger, Julian Kreimeier, Daniel Roth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07677">GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user's hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly graph. We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this graph enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2301.12057.pdf' target='_blank'>https://arxiv.org/pdf/2301.12057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaijie Zhao, Haitao Zhao, Zhongze Wang, Jingchao Peng, Zhengwei Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12057">Object Preserving Siamese Network for Single Object Tracking on Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Obviously, the object is the key factor of the 3D single object tracking (SOT) task. However, previous Siamese-based trackers overlook the negative effects brought by randomly dropped object points during backbone sampling, which hinder trackers to predict accurate bounding boxes (BBoxes). Exploring an approach that seeks to maximize the preservation of object points and their object-aware features is of particular significance. Motivated by this, we propose an Object Preserving Siamese Network
  (OPSNet), which can significantly maintain object integrity and boost tracking performance. Firstly, the object highlighting module enhances the object-aware features and extracts discriminative features from template and search area. Then, the object-preserved sampling selects object candidates to obtain object-preserved search area seeds and drop the background points that contribute less to tracking. Finally, the object localization network precisely locates 3D BBoxes based on the object-preserved search area seeds. Extensive experiments demonstrate our method outperforms the state-of-the-art performance (9.4% and 2.5% success gain on KITTI and Waymo Open Dataset respectively).
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2212.11920.pdf' target='_blank'>https://arxiv.org/pdf/2212.11920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph Mayer, Martin Danelljan, Ming-Hsuan Yang, Vittorio Ferrari, Luc Van Gool, Alina Kuznetsova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.11920">Beyond SOT: Tracking Multiple Generic Objects at Once</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generic Object Tracking (GOT) is the problem of tracking target objects, specified by bounding boxes in the first frame of a video. While the task has received much attention in the last decades, researchers have almost exclusively focused on the single object setting. Multi-object GOT benefits from a wider applicability, rendering it more attractive in real-world applications. We attribute the lack of research interest into this problem to the absence of suitable benchmarks. In this work, we introduce a new large-scale GOT benchmark, LaGOT, containing multiple annotated target objects per sequence. Our benchmark allows users to tackle key remaining challenges in GOT, aiming to increase robustness and reduce computation through joint tracking of multiple objects simultaneously. In addition, we propose a transformer-based GOT tracker baseline capable of joint processing of multiple objects through shared computation. Our approach achieves a 4x faster run-time in case of 10 concurrent objects compared to tracking each object independently and outperforms existing single object trackers on our new benchmark. In addition, our approach achieves highly competitive results on single-object GOT datasets, setting a new state of the art on TrackingNet with a success rate AUC of 84.4%. Our benchmark, code, and trained models will be made publicly available.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2111.12728.pdf' target='_blank'>https://arxiv.org/pdf/2111.12728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianglong Ye, Yuntao Chen, Naiyan Wang, Xiaolong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2111.12728">Online Adaptation for Implicit Object Tracking and Shape Reconstruction in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking and reconstructing 3D objects from cluttered scenes are the key components for computer vision, robotics and autonomous driving systems. While recent progress in implicit function has shown encouraging results on high-quality 3D shape reconstruction, it is still very challenging to generalize to cluttered and partially observable LiDAR data. In this paper, we propose to leverage the continuity in video data. We introduce a novel and unified framework which utilizes a neural implicit function to simultaneously track and reconstruct 3D objects in the wild. Our approach adapts the DeepSDF model (i.e., an instantiation of the implicit function) in the video online, iteratively improving the shape reconstruction while in return improving the tracking, and vice versa. We experiment with both Waymo and KITTI datasets and show significant improvements over state-of-the-art methods for both tracking and shape reconstruction tasks. Our project page is at https://jianglongye.com/implicit-tracking .
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2509.14060.pdf' target='_blank'>https://arxiv.org/pdf/2509.14060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Du, Weiwei Xing, Ming Li, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14060">VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multi-object tracking (MOT) algorithms typically overlook issues inherent in low-quality videos, leading to significant degradation in tracking performance when confronted with real-world image deterioration. Therefore, advancing the application of MOT algorithms in real-world low-quality video scenarios represents a critical and meaningful endeavor. To address the challenges posed by low-quality scenarios, inspired by vision-language models, this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking framework (VSE-MOT). Specifically, we first design a tri-branch architecture that leverages a vision-language model to extract global visual semantic information from images and fuse it with query vectors. Subsequently, to further enhance the utilization of visual semantic information, we introduce the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic information to suit multi-object tracking tasks, while the VSFM improves the efficacy of feature fusion. Through extensive experiments, we validate the effectiveness and superiority of the proposed method in real-world low-quality video scenarios. Its tracking performance metrics outperform those of existing methods by approximately 8% to 20%, while maintaining robust performance in conventional scenarios.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2509.14060.pdf' target='_blank'>https://arxiv.org/pdf/2509.14060.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Du, Weiwei Xing, Ming Li, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14060">VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current multi-object tracking (MOT) algorithms typically overlook issues inherent in low-quality videos, leading to significant degradation in tracking performance when confronted with real-world image deterioration. Therefore, advancing the application of MOT algorithms in real-world low-quality video scenarios represents a critical and meaningful endeavor. To address the challenges posed by low-quality scenarios, inspired by vision-language models, this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking framework (VSE-MOT). Specifically, we first design a tri-branch architecture that leverages a vision-language model to extract global visual semantic information from images and fuse it with query vectors. Subsequently, to further enhance the utilization of visual semantic information, we introduce the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic information to suit multi-object tracking tasks, while the VSFM improves the efficacy of feature fusion. Through extensive experiments, we validate the effectiveness and superiority of the proposed method in real-world low-quality video scenarios. Its tracking performance metrics outperform those of existing methods by approximately 8% to 20%, while maintaining robust performance in conventional scenarios.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2509.02028.pdf' target='_blank'>https://arxiv.org/pdf/2509.02028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Halima Bouzidi, Haoyu Liu, Mohammad Abdullah Al Faruque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02028">See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-vision understanding has driven the development of advanced perception systems, most notably the emerging paradigm of Referring Multi-Object Tracking (RMOT). By leveraging natural-language queries, RMOT systems can selectively track objects that satisfy a given semantic description, guided through Transformer-based spatial-temporal reasoning modules. End-to-End (E2E) RMOT models further unify feature extraction, temporal memory, and spatial reasoning within a Transformer backbone, enabling long-range spatial-temporal modeling over fused textual-visual representations. Despite these advances, the reliability and robustness of RMOT remain underexplored. In this paper, we examine the security implications of RMOT systems from a design-logic perspective, identifying adversarial vulnerabilities that compromise both the linguistic-visual referring and track-object matching components. Additionally, we uncover a novel vulnerability in advanced RMOT models employing FIFO-based memory, whereby targeted and consistent attacks on their spatial-temporal reasoning introduce errors that persist within the history buffer over multiple subsequent frames. We present VEIL, a novel adversarial framework designed to disrupt the unified referring-matching mechanisms of RMOT models. We show that carefully crafted digital and physical perturbations can corrupt the tracking logic reliability, inducing track ID switches and terminations. We conduct comprehensive evaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL and demonstrate the urgent need for security-aware RMOT designs for critical large-scale applications.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2509.02028.pdf' target='_blank'>https://arxiv.org/pdf/2509.02028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Halima Bouzidi, Haoyu Liu, Mohammad Abdullah Al Faruque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02028">See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language-vision understanding has driven the development of advanced perception systems, most notably the emerging paradigm of Referring Multi-Object Tracking (RMOT). By leveraging natural-language queries, RMOT systems can selectively track objects that satisfy a given semantic description, guided through Transformer-based spatial-temporal reasoning modules. End-to-End (E2E) RMOT models further unify feature extraction, temporal memory, and spatial reasoning within a Transformer backbone, enabling long-range spatial-temporal modeling over fused textual-visual representations. Despite these advances, the reliability and robustness of RMOT remain underexplored. In this paper, we examine the security implications of RMOT systems from a design-logic perspective, identifying adversarial vulnerabilities that compromise both the linguistic-visual referring and track-object matching components. Additionally, we uncover a novel vulnerability in advanced RMOT models employing FIFO-based memory, whereby targeted and consistent attacks on their spatial-temporal reasoning introduce errors that persist within the history buffer over multiple subsequent frames. We present VEIL, a novel adversarial framework designed to disrupt the unified referring-matching mechanisms of RMOT models. We show that carefully crafted digital and physical perturbations can corrupt the tracking logic reliability, inducing track ID switches and terminations. We conduct comprehensive evaluations using the Refer-KITTI dataset to validate the effectiveness of VEIL and demonstrate the urgent need for security-aware RMOT designs for critical large-scale applications.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2508.13647.pdf' target='_blank'>https://arxiv.org/pdf/2508.13647.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrejÄÃ­, Oliver Kost, Yuxuan Xia, Lennart Svensson, OndÅej Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13647">Model-based Multi-object Visual Tracking: Identification and Standard Model Limitations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper uses multi-object tracking methods known from the radar tracking community to address the problem of pedestrian tracking using 2D bounding box detections. The standard point-object (SPO) model is adopted, and the posterior density is computed using the Poisson multi-Bernoulli mixture (PMBM) filter. The selection of the model parameters rooted in continuous time is discussed, including the birth and survival probabilities. Some parameters are selected from the first principles, while others are identified from the data, which is, in this case, the publicly available MOT-17 dataset. Although the resulting PMBM algorithm yields promising results, a mismatch between the SPO model and the data is revealed. The model-based approach assumes that modifying the problematic components causing the SPO model-data mismatch will lead to better model-based algorithms in future developments.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2507.13706.pdf' target='_blank'>https://arxiv.org/pdf/2507.13706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ãngel F. GarcÃ­a-FernÃ¡ndez, Jinhao Gu, Lennart Svensson, Yuxuan Xia, Jan KrejÄÃ­, Oliver Kost, OndÅej Straka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13706">GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces two quasi-metrics for performance assessment of multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an extension of the generalised optimal subpattern assignment (GOSPA) metric and measures the discrepancy between sets of objects. The other quasi-metric is an extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy between sets of trajectories. Similar to the GOSPA-based metrics, these quasi-metrics include costs for localisation error for properly detected objects, the number of false objects and the number of missed objects. The T-GOSPA quasi-metric also includes a track switching cost. Differently from the GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of penalising missed and false objects with different costs, and the localisation costs are not required to be symmetric. These properties can be useful in MOT evaluation in certain applications. The performance of several Bayesian MOT algorithms is assessed with the T-GOSPA quasi-metric via simulations.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2506.19621.pdf' target='_blank'>https://arxiv.org/pdf/2506.19621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noel JosÃ© Rodrigues Vicente, Enrique Lehner, Angel Villar-Corrales, Jan Nogga, Sven Behnke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19621">VideoPCDNet: Video Parsing and Prediction with Phase Correlation Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding and predicting video content is essential for planning and reasoning in dynamic environments. Despite advancements, unsupervised learning of object representations and dynamics remains challenging. We present VideoPCDNet, an unsupervised framework for object-centric video decomposition and prediction. Our model uses frequency-domain phase correlation techniques to recursively parse videos into object components, which are represented as transformed versions of learned object prototypes, enabling accurate and interpretable tracking. By explicitly modeling object motion through a combination of frequency domain operations and lightweight learned modules, VideoPCDNet enables accurate unsupervised object tracking and prediction of future video frames. In our experiments, we demonstrate that VideoPCDNet outperforms multiple object-centric baseline models for unsupervised tracking and prediction on several synthetic datasets, while learning interpretable object and motion representations.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2506.15148.pdf' target='_blank'>https://arxiv.org/pdf/2506.15148.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xia, Ãngel F. GarcÃ­a-FernÃ¡ndez, Johan Karlsson, Yu Ge, Lennart Svensson, Ting Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15148">Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a generalization of the trajectory general optimal sub-pattern assignment (GOSPA) metric for evaluating multi-object tracking algorithms that provide trajectory estimates with track-level uncertainties. This metric builds on the recently introduced probabilistic GOSPA metric to account for both the existence and state estimation uncertainties of individual object states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a multidimensional assignment problem, and its linear programming relaxation--also a valid metric--is computable in polynomial time. Additionally, this metric retains the interpretability of TGOSPA, and we show that its decomposition yields intuitive costs terms associated to expected localization error and existence probability mismatch error for properly detected objects, expected missed and false detection error, and track switch error. The effectiveness of the proposed metric is demonstrated through a simulation study.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2505.20381.pdf' target='_blank'>https://arxiv.org/pdf/2505.20381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijia Chen, Yanqiu Yu, En Yu, Wenbing Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20381">ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-object tracking (RMOT) is an important research field in computer vision. Its task form is to guide the models to track the objects that conform to the language instruction. However, the RMOT task commonly requires clear language instructions, such methods often fail to work when complex language instructions with reasoning characteristics appear. In this work, we propose a new task, called Reasoning-based Multi-Object Tracking (ReaMOT). ReaMOT is a more challenging task that requires accurate reasoning about objects that match the language instruction with reasoning characteristic and tracking the objects' trajectories. To advance the ReaMOT task and evaluate the reasoning capabilities of tracking models, we construct ReaMOT Challenge, a reasoning-based multi-object tracking benchmark built upon 12 datasets. Specifically, it comprises 1,156 language instructions with reasoning characteristic, 423,359 image-language pairs, and 869 diverse scenes, which is divided into three levels of reasoning difficulty. In addition, we propose a set of evaluation metrics tailored for the ReaMOT task. Furthermore, we propose ReaTrack, a training-free framework for reasoning-based multi-object tracking based on large vision-language models (LVLM) and SAM2, as a baseline for the ReaMOT task. Extensive experiments on the ReaMOT Challenge benchmark demonstrate the effectiveness of our ReaTrack framework.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2505.12606.pdf' target='_blank'>https://arxiv.org/pdf/2505.12606.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Xuan, Zechao Li, Jinhui Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12606">Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-modal object tracking integrates auxiliary modalities such as depth, thermal infrared, event flow, and language to provide additional information beyond RGB images, showing great potential in improving tracking stabilization in complex scenarios. Existing methods typically start from an RGB-based tracker and learn to understand auxiliary modalities only from training data. Constrained by the limited multi-modal training data, the performance of these methods is unsatisfactory. To alleviate this limitation, this work proposes a unified multi-modal tracker Diff-MM by exploiting the multi-modal understanding capability of the pre-trained text-to-image generation model. Diff-MM leverages the UNet of pre-trained Stable Diffusion as a tracking feature extractor through the proposed parallel feature extraction pipeline, which enables pairwise image inputs for object tracking. We further introduce a multi-modal sub-module tuning method that learns to gain complementary information between different modalities. By harnessing the extensive prior knowledge in the generation model, we achieve a unified tracker with uniform parameters for RGB-N/D/T/E tracking. Experimental results demonstrate the promising performance of our method compared with recently proposed trackers, e.g., its AUC outperforms OneTracker by 8.3% on TNL2K.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2501.16753.pdf' target='_blank'>https://arxiv.org/pdf/2501.16753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hy Nguyen, Srikanth Thudumu, Hung Du, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16753">Overcoming Semantic Dilution in Transformer-Based Next Frame Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next-frame prediction in videos is crucial for applications such as autonomous driving, object tracking, and motion prediction. The primary challenge in next-frame prediction lies in effectively capturing and processing both spatial and temporal information from previous video sequences. The transformer architecture, known for its prowess in handling sequence data, has made remarkable progress in this domain. However, transformer-based next-frame prediction models face notable issues: (a) The multi-head self-attention (MHSA) mechanism requires the input embedding to be split into $N$ chunks, where $N$ is the number of heads. Each segment captures only a fraction of the original embeddings information, which distorts the representation of the embedding in the latent space, resulting in a semantic dilution problem; (b) These models predict the embeddings of the next frames rather than the frames themselves, but the loss function based on the errors of the reconstructed frames, not the predicted embeddings -- this creates a discrepancy between the training objective and the model output. We propose a Semantic Concentration Multi-Head Self-Attention (SCMHSA) architecture, which effectively mitigates semantic dilution in transformer-based next-frame prediction. Additionally, we introduce a loss function that optimizes SCMHSA in the latent space, aligning the training objective more closely with the model output. Our method demonstrates superior performance compared to the original transformer-based predictors.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2501.13994.pdf' target='_blank'>https://arxiv.org/pdf/2501.13994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hy Nguyen, Bao Pham, Hung Du, Srikanth Thudumu, Rajesh Vasa, Kon Mouzakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13994">CSAOT: Cooperative Multi-Agent System for Active Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object Tracking is essential for many computer vision applications, such as autonomous navigation, surveillance, and robotics. Unlike Passive Object Tracking (POT), which relies on static camera viewpoints to detect and track objects across consecutive frames, Active Object Tracking (AOT) requires a controller agent to actively adjust its viewpoint to maintain visual contact with a moving target in complex environments. Existing AOT solutions are predominantly single-agent-based, which struggle in dynamic and complex scenarios due to limited information gathering and processing capabilities, often resulting in suboptimal decision-making. Alleviating these limitations necessitates the development of a multi-agent system where different agents perform distinct roles and collaborate to enhance learning and robustness in dynamic and complex environments. Although some multi-agent approaches exist for AOT, they typically rely on external auxiliary agents, which require additional devices, making them costly. In contrast, we introduce the Collaborative System for Active Object Tracking (CSAOT), a method that leverages multi-agent deep reinforcement learning (MADRL) and a Mixture of Experts (MoE) framework to enable multiple agents to operate on a single device, thereby improving tracking performance and reducing costs. Our approach enhances robustness against occlusions and rapid motion while optimizing camera movements to extend tracking duration. We validated the effectiveness of CSAOT on various interactive maps with dynamic and stationary obstacles.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2412.08321.pdf' target='_blank'>https://arxiv.org/pdf/2412.08321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrejÄÃ­, Oliver Kost, OndÅej Straka, Yuxuan Xia, Lennart Svensson, Ãngel F. GarcÃ­a-FernÃ¡ndez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08321">TGOSPA Metric Parameters Selection and Evaluation for Visual Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking algorithms are deployed in various applications, each with different performance requirements. For example, track switches pose significant challenges for offline scene understanding, as they hinder the accuracy of data interpretation. Conversely, in online surveillance applications, their impact is often minimal. This disparity underscores the need for application-specific performance evaluations that are both simple and mathematically sound. The trajectory generalized optimal sub-pattern assignment (TGOSPA) metric offers a principled approach to evaluate multi-object tracking performance. It accounts for localization errors, the number of missed and false objects, and the number of track switches, providing a comprehensive assessment framework. This paper illustrates the effective use of the TGOSPA metric in computer vision tasks, addressing challenges posed by the need for application-specific scoring methodologies. By exploring the TGOSPA parameter selection, we enable users to compare, comprehend, and optimize the performance of algorithms tailored for specific tasks, such as target tracking and training of detector or re-ID modules.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2411.19941.pdf' target='_blank'>https://arxiv.org/pdf/2411.19941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Heyward, JoÃ£o Carreira, Dima Damen, Andrew Zisserman, Viorica PÄtrÄucean
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19941">Perception Test 2024: Challenge Summary and a Novel Hour-Long VideoQA Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Following the successful 2023 edition, we organised the Second Perception Test challenge as a half-day workshop alongside the IEEE/CVF European Conference on Computer Vision (ECCV) 2024, with the goal of benchmarking state-of-the-art video models and measuring the progress since last year using the Perception Test benchmark. This year, the challenge had seven tracks (up from six last year) and covered low-level and high-level tasks, with language and non-language interfaces, across video, audio, and text modalities; the additional track covered hour-long video understanding and introduced a novel video QA benchmark 1h-walk VQA. Overall, the tasks in the different tracks were: object tracking, point tracking, temporal action localisation, temporal sound localisation, multiple-choice video question-answering, grounded video question-answering, and hour-long video question-answering. We summarise in this report the challenge tasks and results, and introduce in detail the novel hour-long video QA benchmark 1h-walk VQA.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2411.15459.pdf' target='_blank'>https://arxiv.org/pdf/2411.15459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinqi Liu, Li Zhou, Zikun Zhou, Jianqiu Chen, Zhenyu He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15459">MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision-language tracking task aims to perform object tracking based on various modality references. Existing Transformer-based vision-language tracking methods have made remarkable progress by leveraging the global modeling ability of self-attention. However, current approaches still face challenges in effectively exploiting the temporal information and dynamically updating reference features during tracking. Recently, the State Space Model (SSM), known as Mamba, has shown astonishing ability in efficient long-sequence modeling. Particularly, its state space evolving process demonstrates promising capabilities in memorizing multimodal temporal information with linear complexity. Witnessing its success, we propose a Mamba-based vision-language tracking model to exploit its state space evolving ability in temporal space for robust multimodal tracking, dubbed MambaVLT. In particular, our approach mainly integrates a time-evolving hybrid state space block and a selective locality enhancement block, to capture contextual information for multimodal modeling and adaptive reference feature update. Besides, we introduce a modality-selection module that dynamically adjusts the weighting between visual and language references, mitigating potential ambiguities from either reference type. Extensive experimental results show that our method performs favorably against state-of-the-art trackers across diverse benchmarks.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2410.02638.pdf' target='_blank'>https://arxiv.org/pdf/2410.02638.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian Herzog, Johannes Gilg, Philipp Wolters, Torben Teepe, Gerhard Rigoll
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02638">Spatial-Temporal Multi-Cuts for Online Multiple-Camera Vehicle Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate online multiple-camera vehicle tracking is essential for intelligent transportation systems, autonomous driving, and smart city applications. Like single-camera multiple-object tracking, it is commonly formulated as a graph problem of tracking-by-detection. Within this framework, existing online methods usually consist of two-stage procedures that cluster temporally first, then spatially, or vice versa. This is computationally expensive and prone to error accumulation. We introduce a graph representation that allows spatial-temporal clustering in a single, combined step: New detections are spatially and temporally connected with existing clusters. By keeping sparse appearance and positional cues of all detections in a cluster, our method can compare clusters based on the strongest available evidence. The final tracks are obtained online using a simple multicut assignment procedure. Our method does not require any training on the target scene, pre-extraction of single-camera tracks, or additional annotations. Notably, we outperform the online state-of-the-art on the CityFlow dataset in terms of IDF1 by more than 14%, and on the Synthehicle dataset by more than 25%, respectively. The code is publicly available.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2410.01806.pdf' target='_blank'>https://arxiv.org/pdf/2410.01806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Segu, Luigi Piccinelli, Siyuan Li, Yung-Hsu Yang, Bernt Schiele, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01806">Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their trajectories. However, it remains a key open research question on how to model long-range dependencies within tracklets, interdependencies among tracklets, and the associated temporal occlusions. To this end, we introduce Samba, a novel linear-time set-of-sequences model designed to jointly process multiple tracklets by synchronizing the multiple selective state-spaces used to model each tracklet. Samba autoregressively predicts the future track query for each sequence while maintaining synchronized long-term memory representations across tracklets. By integrating Samba into a tracking-by-propagation framework, we propose SambaMOTR, the first tracker effectively addressing the aforementioned issues, including long-range dependencies, tracklet interdependencies, and temporal occlusions. Additionally, we introduce an effective technique for dealing with uncertain observations (MaskObs) and an efficient training recipe to scale SambaMOTR to longer sequences. By modeling long-range dependencies and interactions among tracked objects, SambaMOTR implicitly learns to track objects accurately through occlusions without any hand-crafted heuristics. Our approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT, and SportsMOT datasets.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2409.17221.pdf' target='_blank'>https://arxiv.org/pdf/2409.17221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mattia Segu, Luigi Piccinelli, Siyuan Li, Luc Van Gool, Fisher Yu, Bernt Schiele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17221">Walker: Self-supervised Multiple Object Tracking by Walking on Temporal Appearance Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The supervision of state-of-the-art multiple object tracking (MOT) methods requires enormous annotation efforts to provide bounding boxes for all frames of all videos, and instance IDs to associate them through time. To this end, we introduce Walker, the first self-supervised tracker that learns from videos with sparse bounding box annotations, and no tracking labels. First, we design a quasi-dense temporal object appearance graph, and propose a novel multi-positive contrastive objective to optimize random walks on the graph and learn instance similarities. Then, we introduce an algorithm to enforce mutually-exclusive connective properties across instances in the graph, optimizing the learned topology for MOT. At inference time, we propose to associate detected instances to tracklets based on the max-likelihood transition state under motion-constrained bi-directional walks. Walker is the first self-supervised tracker to achieve competitive performance on MOT17, DanceTrack, and BDD100K. Remarkably, our proposal outperforms the previous self-supervised trackers even when drastically reducing the annotation requirements by up to 400x.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2408.02049.pdf' target='_blank'>https://arxiv.org/pdf/2408.02049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Wu, Kun Sun, Pei An, Mathieu Salzmann, Yanning Zhang, Jiaqi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02049">3D Single-object Tracking in Point Clouds with High Temporal Variation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The high temporal variation of the point clouds is the key challenge of 3D single-object tracking (3D SOT). Existing approaches rely on the assumption that the shape variation of the point clouds and the motion of the objects across neighboring frames are smooth, failing to cope with high temporal variation data. In this paper, we present a novel framework for 3D SOT in point clouds with high temporal variation, called HVTrack. HVTrack proposes three novel components to tackle the challenges in the high temporal variation scenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud shape variations; 2) a Base-Expansion Feature Cross-Attention module to deal with similar object distractions in expanded search areas; 3) a Contextual Point Guided Self-Attention module for suppressing heavy background noise. We construct a dataset with high temporal variation (KITTI-HV) by setting different frame intervals for sampling in the KITTI dataset. On the KITTI-HV with 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker CXTracker by 11.3%/15.7% in Success/Precision.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2407.08222.pdf' target='_blank'>https://arxiv.org/pdf/2407.08222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing Wang, Joel Janek Dabrowski, Josh Pinskier, Lois Liow, Vinoth Viswanathan, Richard Scalzo, David Howard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08222">PINN-Ray: A Physics-Informed Neural Network to Model Soft Robotic Fin Ray Fingers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modelling complex deformation for soft robotics provides a guideline to understand their behaviour, leading to safe interaction with the environment. However, building a surrogate model with high accuracy and fast inference speed can be challenging for soft robotics due to the nonlinearity from complex geometry, large deformation, material nonlinearity etc. The reality gap from surrogate models also prevents their further deployment in the soft robotics domain. In this study, we proposed a physics-informed Neural Networks (PINNs) named PINN-Ray to model complex deformation for a Fin Ray soft robotic gripper, which embeds the minimum potential energy principle from elastic mechanics and additional high-fidelity experimental data into the loss function of neural network for training. This method is significant in terms of its generalisation to complex geometry and robust to data scarcity as compared to other data-driven neural networks. Furthermore, it has been extensively evaluated to model the deformation of the Fin Ray finger under external actuation. PINN-Ray demonstrates improved accuracy as compared with Finite element modelling (FEM) after applying the data assimilation scheme to treat the sim-to-real gap. Additionally, we introduced our automated framework to design, fabricate soft robotic fingers, and characterise their deformation by visual tracking, which provides a guideline for the fast prototype of soft robotics.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2407.01959.pdf' target='_blank'>https://arxiv.org/pdf/2407.01959.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Li, Yubo Cui, Zhiheng Li, Zheng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01959">FlowTrack: Point-level Flow Network for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (SOT) is a crucial task in fields of mobile robotics and autonomous driving. Traditional motion-based approaches achieve target tracking by estimating the relative movement of target between two consecutive frames. However, they usually overlook local motion information of the target and fail to exploit historical frame information effectively. To overcome the above limitations, we propose a point-level flow method with multi-frame information for 3D SOT task, called FlowTrack. Specifically, by estimating the flow for each point in the target, our method could capture the local motion details of target, thereby improving the tracking performance. At the same time, to handle scenes with sparse points, we present a learnable target feature as the bridge to efficiently integrate target information from past frames. Moreover, we design a novel Instance Flow Head to transform dense point-level flow into instance-level motion, effectively aggregating local motion information to obtain global target motion. Finally, our method achieves competitive performance with improvements of 5.9% on the KITTI dataset and 2.9% on NuScenes. The code will be made publicly available soon.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2312.17261.pdf' target='_blank'>https://arxiv.org/pdf/2312.17261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juliano Pinto, Georg Hess, Yuxuan Xia, Henk Wymeersch, Lennart Svensson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17261">Transformer-Based Multi-Object Smoothing with Decoupled Data Association and Smoothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is the task of estimating the state trajectories of an unknown and time-varying number of objects over a certain time window. Several algorithms have been proposed to tackle the multi-object smoothing task, where object detections can be conditioned on all the measurements in the time window. However, the best-performing methods suffer from intractable computational complexity and require approximations, performing suboptimally in complex settings. Deep learning based algorithms are a possible venue for tackling this issue but have not been applied extensively in settings where accurate multi-object models are available and measurements are low-dimensional. We propose a novel DL architecture specifically tailored for this setting that decouples the data association task from the smoothing task. We compare the performance of the proposed smoother to the state-of-the-art in different tasks of varying difficulty and provide, to the best of our knowledge, the first comparison between traditional Bayesian trackers and DL trackers in the smoothing problem setting.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2312.13090.pdf' target='_blank'>https://arxiv.org/pdf/2312.13090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joseph Heyward, JoÃ£o Carreira, Dima Damen, Andrew Zisserman, Viorica PÄtrÄucean
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13090">Perception Test 2023: A Summary of the First Challenge And Outcome</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The First Perception Test challenge was held as a half-day workshop alongside the IEEE/CVF International Conference on Computer Vision (ICCV) 2023, with the goal of benchmarking state-of-the-art video models on the recently proposed Perception Test benchmark. The challenge had six tracks covering low-level and high-level tasks, with both a language and non-language interface, across video, audio, and text modalities, and covering: object tracking, point tracking, temporal action localisation, temporal sound localisation, multiple-choice video question-answering, and grounded video question-answering. We summarise in this report the task descriptions, metrics, baselines, and results.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2311.02734.pdf' target='_blank'>https://arxiv.org/pdf/2311.02734.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Gorlo, Kenneth Blomqvist, Francesco Milano, Roland Siegwart
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02734">ISAR: A Benchmark for Single- and Few-Shot Object Instance Segmentation and Re-Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most object-level mapping systems in use today make use of an upstream learned object instance segmentation model. If we want to teach them about a new object or segmentation class, we need to build a large dataset and retrain the system. To build spatial AI systems that can quickly be taught about new objects, we need to effectively solve the problem of single-shot object detection, instance segmentation and re-identification. So far there is neither a method fulfilling all of these requirements in unison nor a benchmark that could be used to test such a method. Addressing this, we propose ISAR, a benchmark and baseline method for single- and few-shot object Instance Segmentation And Re-identification, in an effort to accelerate the development of algorithms that can robustly detect, segment, and re-identify objects from a single or a few sparse training examples. We provide a semi-synthetic dataset of video sequences with ground-truth semantic annotations, a standardized evaluation pipeline, and a baseline method. Our benchmark aligns with the emerging research trend of unifying Multi-Object Tracking, Video Object Segmentation, and Re-identification.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2307.08984.pdf' target='_blank'>https://arxiv.org/pdf/2307.08984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Wei, Long Chen, Wei Ji, Xiaoyu Yue, Roger Zimmermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08984">In Defense of Clip-based Video Relation Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Visual Relation Detection (VidVRD) aims to detect visual relationship triplets in videos using spatial bounding boxes and temporal boundaries. Existing VidVRD methods can be broadly categorized into bottom-up and top-down paradigms, depending on their approach to classifying relations. Bottom-up methods follow a clip-based approach where they classify relations of short clip tubelet pairs and then merge them into long video relations. On the other hand, top-down methods directly classify long video tubelet pairs. While recent video-based methods utilizing video tubelets have shown promising results, we argue that the effective modeling of spatial and temporal context plays a more significant role than the choice between clip tubelets and video tubelets. This motivates us to revisit the clip-based paradigm and explore the key success factors in VidVRD. In this paper, we propose a Hierarchical Context Model (HCM) that enriches the object-based spatial context and relation-based temporal context based on clips. We demonstrate that using clip tubelets can achieve superior performance compared to most video-based methods. Additionally, using clip tubelets offers more flexibility in model designs and helps alleviate the limitations associated with video tubelets, such as the challenging long-term object tracking problem and the loss of temporal information in long-term tubelet feature compression. Extensive experiments conducted on two challenging VidVRD benchmarks validate that our HCM achieves a new state-of-the-art performance, highlighting the effectiveness of incorporating advanced spatial and temporal context modeling within the clip-based paradigm.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2306.17440.pdf' target='_blank'>https://arxiv.org/pdf/2306.17440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yubo Cui, Zhiheng Li, Zheng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17440">STTracker: Spatio-Temporal Tracker for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking with point clouds is a critical task in 3D computer vision. Previous methods usually input the last two frames and use the predicted box to get the template point cloud in previous frame and the search area point cloud in the current frame respectively, then use similarity-based or motion-based methods to predict the current box. Although these methods achieved good tracking performance, they ignore the historical information of the target, which is important for tracking. In this paper, compared to inputting two frames of point clouds, we input multi-frame of point clouds to encode the spatio-temporal information of the target and learn the motion information of the target implicitly, which could build the correlations among different frames to track the target in the current frame efficiently. Meanwhile, rather than directly using the point feature for feature fusion, we first crop the point cloud features into many patches and then use sparse attention mechanism to encode the patch-level similarity and finally fuse the multi-frame features. Extensive experiments show that our method achieves competitive results on challenging large-scale benchmarks (62.6% in KITTI and 49.66% in NuScenes).
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2306.02585.pdf' target='_blank'>https://arxiv.org/pdf/2306.02585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changcheng Xiao, Qiong Cao, Yujie Zhong, Long Lan, Xiang Zhang, Zhigang Luo, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02585">MotionTrack: Learning Motion Predictor for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Significant progress has been achieved in multi-object tracking (MOT) through the evolution of detection and re-identification (ReID) techniques. Despite these advancements, accurately tracking objects in scenarios with homogeneous appearance and heterogeneous motion remains a challenge. This challenge arises from two main factors: the insufficient discriminability of ReID features and the predominant utilization of linear motion models in MOT. In this context, we introduce a novel motion-based tracker, MotionTrack, centered around a learnable motion predictor that relies solely on object trajectory information. This predictor comprehensively integrates two levels of granularity in motion features to enhance the modeling of temporal dynamics and facilitate precise future motion prediction for individual objects. Specifically, the proposed approach adopts a self-attention mechanism to capture token-level information and a Dynamic MLP layer to model channel-level features. MotionTrack is a simple, online tracking approach. Our experimental results demonstrate that MotionTrack yields state-of-the-art performance on datasets such as Dancetrack and SportsMOT, characterized by highly complex object motion.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2305.06794.pdf' target='_blank'>https://arxiv.org/pdf/2305.06794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Li, Yubo Cui, Yu Lin, Zheng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06794">MMF-Track: Multi-modal Multi-level Fusion for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking plays a crucial role in computer vision. Mainstream methods mainly rely on point clouds to achieve geometry matching between target template and search area. However, textureless and incomplete point clouds make it difficult for single-modal trackers to distinguish objects with similar structures. To overcome the limitations of geometry matching, we propose a Multi-modal Multi-level Fusion Tracker (MMF-Track), which exploits the image texture and geometry characteristic of point clouds to track 3D target. Specifically, we first propose a Space Alignment Module (SAM) to align RGB images with point clouds in 3D space, which is the prerequisite for constructing inter-modal associations. Then, in feature interaction level, we design a Feature Interaction Module (FIM) based on dual-stream structure, which enhances intra-modal features in parallel and constructs inter-modal semantic associations. Meanwhile, in order to refine each modal feature, we introduce a Coarse-to-Fine Interaction Module (CFIM) to realize the hierarchical feature interaction at different scales. Finally, in similarity fusion level, we propose a Similarity Fusion Module (SFM) to aggregate geometry and texture clues from the target. Experiments show that our method achieves state-of-the-art performance on KITTI (39% Success and 42% Precision gains against previous multi-modal method) and is also competitive on NuScenes.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2304.07705.pdf' target='_blank'>https://arxiv.org/pdf/2304.07705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Huaming Chen, Wei Bao, Zhongzheng Lai, Zao Zhang, Dong Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07705">Handling Heavy Occlusion in Dense Crowd Tracking by Focusing on the Heads</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of deep learning, object detection and tracking play a vital role in today's society. Being able to identify and track all the pedestrians in the dense crowd scene with computer vision approaches is a typical challenge in this field, also known as the Multiple Object Tracking (MOT) challenge. Modern trackers are required to operate on more and more complicated scenes. According to the MOT20 challenge result, the pedestrian is 4 times denser than the MOT17 challenge. Hence, improving the ability to detect and track in extremely crowded scenes is the aim of this work. In light of the occlusion issue with the human body, the heads are usually easier to identify. In this work, we have designed a joint head and body detector in an anchor-free style to boost the detection recall and precision performance of pedestrians in both small and medium sizes. Innovatively, our model does not require information on the statistical head-body ratio for common pedestrians detection for training. Instead, the proposed model learns the ratio dynamically. To verify the effectiveness of the proposed model, we evaluate the model with extensive experiments on different datasets, including MOT20, Crowdhuman, and HT21 datasets. As a result, our proposed method significantly improves both the recall and precision rate on small & medium sized pedestrians and achieves state-of-the-art results in these challenging datasets.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2304.03184.pdf' target='_blank'>https://arxiv.org/pdf/2304.03184.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuheng Jiang, Kaixin Yao, Zhuo Su, Zhehao Shen, Haimin Luo, Lan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03184">Instant-NVR: Instant Neural Volumetric Rendering for Human-object Interactions from Monocular RGBD Stream</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Convenient 4D modeling of human-object interactions is essential for numerous applications. However, monocular tracking and rendering of complex interaction scenarios remain challenging. In this paper, we propose Instant-NVR, a neural approach for instant volumetric human-object tracking and rendering using a single RGBD camera. It bridges traditional non-rigid tracking with recent instant radiance field techniques via a multi-thread tracking-rendering mechanism. In the tracking front-end, we adopt a robust human-object capture scheme to provide sufficient motion priors. We further introduce a separated instant neural representation with a novel hybrid deformation module for the interacting scene. We also provide an on-the-fly reconstruction scheme of the dynamic/static radiance fields via efficient motion-prior searching. Moreover, we introduce an online key frame selection scheme and a rendering-aware refinement strategy to significantly improve the appearance details for online novel-view synthesis. Extensive experiments demonstrate the effectiveness and efficiency of our approach for the instant generation of human-object radiance fields on the fly, notably achieving real-time photo-realistic novel view synthesis under complex human-object interactions.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2303.04670.pdf' target='_blank'>https://arxiv.org/pdf/2303.04670.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sankeerth Durvasula, Yushi Guan, Nandita Vijaykumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.04670">EvConv: Fast CNN Inference on Event Camera Inputs For High-Speed Robot Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras capture visual information with a high temporal resolution and a wide dynamic range. This enables capturing visual information at fine time granularities (e.g., microseconds) in rapidly changing environments. This makes event cameras highly useful for high-speed robotics tasks involving rapid motion, such as high-speed perception, object tracking, and control. However, convolutional neural network inference on event camera streams cannot currently perform real-time inference at the high speeds at which event cameras operate - current CNN inference times are typically closer in order of magnitude to the frame rates of regular frame-based cameras. Real-time inference at event camera rates is necessary to fully leverage the high frequency and high temporal resolution that event cameras offer. This paper presents EvConv, a new approach to enable fast inference on CNNs for inputs from event cameras. We observe that consecutive inputs to the CNN from an event camera have only small differences between them. Thus, we propose to perform inference on the difference between consecutive input tensors, or the increment. This enables a significant reduction in the number of floating-point operations required (and thus the inference latency) because increments are very sparse. We design EvConv to leverage the irregular sparsity in increments from event cameras and to retain the sparsity of these increments across all layers of the network. We demonstrate a reduction in the number of floating operations required in the forward pass by up to 98%. We also demonstrate a speedup of up to 1.6X for inference using CNNs for tasks such as depth estimation, object recognition, and optical flow estimation, with almost no loss in accuracy.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2301.03213.pdf' target='_blank'>https://arxiv.org/pdf/2301.03213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Tang, Kevin Liang, Matt Feiszli, Weiyao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.03213">EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking is a key component to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is underrepresented in many existing datasets; these tend to focus on relatively short, third-person videos. Egocentric video has several distinguishing characteristics from those commonly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to consistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection problem, and their "framed" nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, this new dataset presents a significant challenge to recent state-of-the-art single-object tracking models, which we find score poorly on traditional tracking metrics for our new dataset, compared to popular benchmarks. We further show improvements that can be made to a STARK tracker to significantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark, hoping our dataset leads to further advancements in tracking.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/1902.00615.pdf' target='_blank'>https://arxiv.org/pdf/1902.00615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Ding, Zhixin Lai, Siyang Li, Panfeng Li, Qikai Yang, Edward Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1902.00615">Confidence Trigger Detection: Accelerating Real-time Tracking-by-detection Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time object tracking necessitates a delicate balance between speed and accuracy, a challenge exacerbated by the computational demands of deep learning methods. In this paper, we propose Confidence-Triggered Detection (CTD), an innovative approach that strategically bypasses object detection for frames closely resembling intermediate states, leveraging tracker confidence scores. CTD not only enhances tracking speed but also preserves accuracy, surpassing existing tracking algorithms. Through extensive evaluation across various tracker confidence thresholds, we identify an optimal trade-off between tracking speed and accuracy, providing crucial insights for parameter fine-tuning and enhancing CTD's practicality in real-world scenarios. Our experiments across diverse detection models underscore the robustness and versatility of the CTD framework, demonstrating its potential to enable real-time tracking in resource-constrained environments.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2509.14147.pdf' target='_blank'>https://arxiv.org/pdf/2509.14147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanxing Li, Shengyang Wang, Fangyu Sun, Shuyu Wu, Dexin Zuo, Wenxian Yu, Danping Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14147">StableTracker: Learning to Stably Track Target via Differentiable Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>FPV object tracking methods heavily rely on handcraft modular designs, resulting in hardware overload and cumulative error, which seriously degrades the tracking performance, especially for rapidly accelerating or decelerating targets. To address these challenges, we present \textbf{StableTracker}, a learning-based control policy that enables quadrotors to robustly follow the moving target from arbitrary perspectives. The policy is trained using backpropagation-through-time via differentiable simulation, allowing the quadrotor to maintain the target at the center of the visual field in both horizontal and vertical directions, while keeping a fixed relative distance, thereby functioning as an autonomous aerial camera. We compare StableTracker against both state-of-the-art traditional algorithms and learning baselines. Simulation experiments demonstrate that our policy achieves superior accuracy, stability and generalization across varying safe distances, trajectories, and target velocities. Furthermore, a real-world experiment on a quadrotor with an onboard computer validated practicality of the proposed approach.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2509.14147.pdf' target='_blank'>https://arxiv.org/pdf/2509.14147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanxing Li, Shengyang Wang, Fangyu Sun, Shuyu Wu, Dexin Zuo, Wenxian Yu, Danping Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14147">StableTracker: Learning to Stably Track Target via Differentiable Simulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>FPV object tracking methods heavily rely on handcraft modular designs, resulting in hardware overload and cumulative error, which seriously degrades the tracking performance, especially for rapidly accelerating or decelerating targets. To address these challenges, we present \textbf{StableTracker}, a learning-based control policy that enables quadrotors to robustly follow the moving target from arbitrary perspectives. The policy is trained using backpropagation-through-time via differentiable simulation, allowing the quadrotor to maintain the target at the center of the visual field in both horizontal and vertical directions, while keeping a fixed relative distance, thereby functioning as an autonomous aerial camera. We compare StableTracker against both state-of-the-art traditional algorithms and learning baselines. Simulation experiments demonstrate that our policy achieves superior accuracy, stability and generalization across varying safe distances, trajectories, and target velocities. Furthermore, a real-world experiment on a quadrotor with an onboard computer validated practicality of the proposed approach.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2509.02111.pdf' target='_blank'>https://arxiv.org/pdf/2509.02111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Missaoui, Orcun Cetintas, Guillem BrasÃ³, Tim Meinhardt, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02111">NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The long-standing division between \textit{online} and \textit{offline} Multi-Object Tracking (MOT) has led to fragmented solutions that fail to address the flexible temporal requirements of real-world deployment scenarios. Current \textit{online} trackers rely on frame-by-frame hand-crafted association strategies and struggle with long-term occlusions, whereas \textit{offline} approaches can cover larger time gaps, but still rely on heuristic stitching for arbitrarily long sequences. In this paper, we introduce NOOUGAT, the first tracker designed to operate with arbitrary temporal horizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework that processes non-overlapping subclips, and fuses them through a novel Autoregressive Long-term Tracking (ALT) layer. The subclip size controls the trade-off between latency and temporal context, enabling a wide range of deployment scenarios, from frame-by-frame to batch processing. NOOUGAT achieves state-of-the-art performance across both tracking regimes, improving \textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 on MOT20, with even greater gains in \textit{offline} mode.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2509.02111.pdf' target='_blank'>https://arxiv.org/pdf/2509.02111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Missaoui, Orcun Cetintas, Guillem BrasÃ³, Tim Meinhardt, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02111">NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The long-standing division between \textit{online} and \textit{offline} Multi-Object Tracking (MOT) has led to fragmented solutions that fail to address the flexible temporal requirements of real-world deployment scenarios. Current \textit{online} trackers rely on frame-by-frame hand-crafted association strategies and struggle with long-term occlusions, whereas \textit{offline} approaches can cover larger time gaps, but still rely on heuristic stitching for arbitrarily long sequences. In this paper, we introduce NOOUGAT, the first tracker designed to operate with arbitrary temporal horizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework that processes non-overlapping subclips, and fuses them through a novel Autoregressive Long-term Tracking (ALT) layer. The subclip size controls the trade-off between latency and temporal context, enabling a wide range of deployment scenarios, from frame-by-frame to batch processing. NOOUGAT achieves state-of-the-art performance across both tracking regimes, improving \textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 on MOT20, with even greater gains in \textit{offline} mode.
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2508.11323.pdf' target='_blank'>https://arxiv.org/pdf/2508.11323.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Zhang, Xinyao Wang, Boxi Wu, Tu Zheng, Wang Yunhua, Zheng Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11323">Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking is a critical and challenging task in the field of autonomous driving. A common paradigm relies on modeling individual object motion, e.g., Kalman filters, to predict trajectories. While effective in simple scenarios, this approach often struggles in crowded environments or with inaccurate detections, as it overlooks the rich geometric relationships between objects. This highlights the need to leverage spatial cues. However, existing geometry-aware methods can be susceptible to interference from irrelevant objects, leading to ambiguous features and incorrect associations. To address this, we propose focusing on cue-consistency: identifying and matching stable spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency Tracker (DSC-Track) to implement this principle. Firstly, we design a unified spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative trajectory embeddings while suppressing interference. Secondly, our cue-consistency transformer module explicitly aligns consistent feature representations between historical tracks and current detections. Finally, a dynamic update mechanism preserves salient spatiotemporal information for stable online tracking. Extensive experiments on the nuScenes and Waymo Open Datasets validate the effectiveness and robustness of our approach. On the nuScenes benchmark, for instance, our method achieves state-of-the-art performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets, respectively.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2508.01730.pdf' target='_blank'>https://arxiv.org/pdf/2508.01730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Ma, Hui Luo, Qi Chen, Yuankai Qi, Yumei Sun, Amin Beheshti, Jianlin Zhang, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01730">Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2507.01535.pdf' target='_blank'>https://arxiv.org/pdf/2507.01535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingxi Liu, Calvin Chen, Junhao Li, Guyang Yu, Haoqian Song, Xuchen Liu, Jinqiang Cui, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01535">TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Vision Transformer (ViT) model has long struggled with the challenge of quadratic complexity, a limitation that becomes especially critical in unmanned aerial vehicle (UAV) tracking systems, where data must be processed in real time. In this study, we explore the recently proposed State-Space Model, Mamba, leveraging its computational efficiency and capability for long-sequence modeling to effectively process dense image sequences in tracking tasks. First, we highlight the issue of temporal inconsistency in existing Mamba-based methods, specifically the failure to account for temporal continuity in the Mamba scanning mechanism. Secondly, building upon this insight,we propose TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model for handling image sequence of tracking problem. In our framework, the mamba scan is performed in a nested way while independently process temporal and spatial coherent patch tokens. While the template frame is encoded as query token and utilized for tracking in every scan. Extensive experiments conducted on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves state-of-the-art precision while offering noticeable higher speed in UAV tracking.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2506.05543.pdf' target='_blank'>https://arxiv.org/pdf/2506.05543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sethuraman TV, Savya Khosla, Vignesh Srinivasakumar, Jiahui Huang, Seoung Wug Oh, Simon Jenni, Derek Hoiem, Joon-Young Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05543">FRAME: Pre-Training Video Feature Representations via Anticipation and Memory</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dense video prediction tasks, such as object tracking and semantic segmentation, require video encoders that generate temporally consistent, spatially dense features for every frame. However, existing approaches fall short: image encoders like DINO or CLIP lack temporal awareness, while video models such as VideoMAE underperform compared to image encoders on dense prediction tasks. We address this gap with FRAME, a self-supervised video frame encoder tailored for dense video understanding. FRAME learns to predict current and future DINO patch features from past and present RGB frames, leading to spatially precise and temporally coherent representations. To our knowledge, FRAME is the first video encoder to leverage image-based models for dense prediction while outperforming them on tasks requiring fine-grained visual correspondence. As an auxiliary capability, FRAME aligns its class token with CLIP's semantic space, supporting language-driven tasks such as video classification. We evaluate FRAME across six dense prediction tasks on seven datasets, where it consistently outperforms image encoders and existing self-supervised video models. Despite its versatility, FRAME maintains a compact architecture suitable for a range of downstream applications.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2506.03335.pdf' target='_blank'>https://arxiv.org/pdf/2506.03335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dheeraj Khanna, Jerrin Bright, Yuhao Chen, John S. Zelek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03335">SportMamba: Adaptive Non-Linear Multi-Object Tracking with State Space Models for Team Sports</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in team sports is particularly challenging due to the fast-paced motion and frequent occlusions resulting in motion blur and identity switches, respectively. Predicting player positions in such scenarios is particularly difficult due to the observed highly non-linear motion patterns. Current methods are heavily reliant on object detection and appearance-based tracking, which struggle to perform in complex team sports scenarios, where appearance cues are ambiguous and motion patterns do not necessarily follow a linear pattern. To address these challenges, we introduce SportMamba, an adaptive hybrid MOT technique specifically designed for tracking in dynamic team sports. The technical contribution of SportMamba is twofold. First, we introduce a mamba-attention mechanism that models non-linear motion by implicitly focusing on relevant embedding dependencies. Second, we propose a height-adaptive spatial association metric to reduce ID switches caused by partial occlusions by accounting for scale variations due to depth changes. Additionally, we extend the detection search space with adaptive buffers to improve associations in fast-motion scenarios. Our proposed technique, SportMamba, demonstrates state-of-the-art performance on various metrics in the SportsMOT dataset, which is characterized by complex motion and severe occlusion. Furthermore, we demonstrate its generalization capability through zero-shot transfer to VIP-HTD, an ice hockey dataset.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2505.20834.pdf' target='_blank'>https://arxiv.org/pdf/2505.20834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20834">Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency. The code will be released.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2505.20834.pdf' target='_blank'>https://arxiv.org/pdf/2505.20834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20834">Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2505.20834.pdf' target='_blank'>https://arxiv.org/pdf/2505.20834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20834">Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2505.07254.pdf' target='_blank'>https://arxiv.org/pdf/2505.07254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Nagy, Naoufel Werghi, Bilal Hassan, Jorge Dias, Majid Khonji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07254">Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\% and 0.81\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\% in higher order tracking accuracy (HOTA) and 1.55\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2503.01547.pdf' target='_blank'>https://arxiv.org/pdf/2503.01547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arash Nasr Esfahani, Hamed Hosseini, Mehdi Tale Masouleh, Ahmad Kalhor, Hedieh Sajedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01547">AI-Driven Relocation Tracking in Dynamic Kitchen Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As smart homes become more prevalent in daily life, the ability to understand dynamic environments is essential which is increasingly dependent on AI systems. This study focuses on developing an intelligent algorithm which can navigate a robot through a kitchen, recognizing objects, and tracking their relocation. The kitchen was chosen as the testing ground due to its dynamic nature as objects are frequently moved, rearranged and replaced. Various techniques, such as SLAM feature-based tracking and deep learning-based object detection (e.g., Faster R-CNN), are commonly used for object tracking. Additionally, methods such as optical flow analysis and 3D reconstruction have also been used to track the relocation of objects. These approaches often face challenges when it comes to problems such as lighting variations and partial occlusions, where parts of the object are hidden in some frames but visible in others. The proposed method in this study leverages the YOLOv5 architecture, initialized with pre-trained weights and subsequently fine-tuned on a custom dataset. A novel method was developed, introducing a frame-scoring algorithm which calculates a score for each object based on its location and features within all frames. This scoring approach helps to identify changes by determining the best-associated frame for each object and comparing the results in each scene, overcoming limitations seen in other methods while maintaining simplicity in design. The experimental results demonstrate an accuracy of 97.72%, a precision of 95.83% and a recall of 96.84% for this algorithm, which highlights the efficacy of the model in detecting spatial changes.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2502.05938.pdf' target='_blank'>https://arxiv.org/pdf/2502.05938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sourav Sanyal, Amogh Joshi, Manish Nagaraj, Rohan Kumar Manna, Kaushik Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05938">Energy-Efficient Autonomous Aerial Navigation with Dynamic Vision Sensors: A Physics-Guided Neuromorphic Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based object tracking is a critical component for achieving autonomous aerial navigation, particularly for obstacle avoidance. Neuromorphic Dynamic Vision Sensors (DVS) or event cameras, inspired by biological vision, offer a promising alternative to conventional frame-based cameras. These cameras can detect changes in intensity asynchronously, even in challenging lighting conditions, with a high dynamic range and resistance to motion blur. Spiking neural networks (SNNs) are increasingly used to process these event-based signals efficiently and asynchronously. Meanwhile, physics-based artificial intelligence (AI) provides a means to incorporate system-level knowledge into neural networks via physical modeling. This enhances robustness, energy efficiency, and provides symbolic explainability. In this work, we present a neuromorphic navigation framework for autonomous drone navigation. The focus is on detecting and navigating through moving gates while avoiding collisions. We use event cameras for detecting moving objects through a shallow SNN architecture in an unsupervised manner. This is combined with a lightweight energy-aware physics-guided neural network (PgNN) trained with depth inputs to predict optimal flight times, generating near-minimum energy paths. The system is implemented in the Gazebo simulator and integrates a sensor-fused vision-to-planning neuro-symbolic framework built with the Robot Operating System (ROS) middleware. This work highlights the future potential of integrating event-based vision with physics-guided planning for energy-efficient autonomous navigation, particularly for low-latency decision-making.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2501.15953.pdf' target='_blank'>https://arxiv.org/pdf/2501.15953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Chu, Yicong Li, Tat-Seng Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15953">Understanding Long Videos via LLM-Powered Entity Relation Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The analysis of extended video content poses unique challenges in artificial intelligence, particularly when dealing with the complexity of tracking and understanding visual elements across time. Current methodologies that process video frames sequentially struggle to maintain coherent tracking of objects, especially when these objects temporarily vanish and later reappear in the footage. A critical limitation of these approaches is their inability to effectively identify crucial moments in the video, largely due to their limited grasp of temporal relationships. To overcome these obstacles, we present GraphVideoAgent, a cutting-edge system that leverages the power of graph-based object tracking in conjunction with large language model capabilities. At its core, our framework employs a dynamic graph structure that maps and monitors the evolving relationships between visual entities throughout the video sequence. This innovative approach enables more nuanced understanding of how objects interact and transform over time, facilitating improved frame selection through comprehensive contextual awareness. Our approach demonstrates remarkable effectiveness when tested against industry benchmarks. In evaluations on the EgoSchema dataset, GraphVideoAgent achieved a 2.2 improvement over existing methods while requiring analysis of only 8.2 frames on average. Similarly, testing on the NExT-QA benchmark yielded a 2.0 performance increase with an average frame requirement of 8.1. These results underscore the efficiency of our graph-guided methodology in enhancing both accuracy and computational performance in long-form video understanding tasks.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2412.00692.pdf' target='_blank'>https://arxiv.org/pdf/2412.00692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Tim Meinhardt, Orcun Cetintas, Cheng-Yen Yang, Sameer Satish Pusegaonkar, Benjamin Missaoui, Sujit Biswas, Zheng Tang, Laura Leal-TaixÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00692">MCBLT: Multi-Camera Multi-Object 3D Tracking in Long Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object perception from multi-view cameras is crucial for intelligent systems, particularly in indoor environments, e.g., warehouses, retail stores, and hospitals. Most traditional multi-target multi-camera (MTMC) detection and tracking methods rely on 2D object detection, single-view multi-object tracking (MOT), and cross-view re-identification (ReID) techniques, without properly handling important 3D information by multi-view image aggregation. In this paper, we propose a 3D object detection and tracking framework, named MCBLT, which first aggregates multi-view images with necessary camera calibration parameters to obtain 3D object detections in bird's-eye view (BEV). Then, we introduce hierarchical graph neural networks (GNNs) to track these 3D detections in BEV for MTMC tracking results. Unlike existing methods, MCBLT has impressive generalizability across different scenes and diverse camera settings, with exceptional capability for long-term association handling. As a result, our proposed MCBLT establishes a new state-of-the-art on the AICity'24 dataset with $81.22$ HOTA, and on the WildTrack dataset with $95.6$ IDF1.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2411.18476.pdf' target='_blank'>https://arxiv.org/pdf/2411.18476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangtao Shuai, Martin Baerveldt, Manh Nguyen-Duc, Anh Le-Tuan, Manfred Hauswirth, Danh Le-Phuoc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18476">A comparison of extended object tracking with multi-modal sensors in indoor environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a preliminary study of an efficient object tracking approach, comparing the performance of two different 3D point cloud sensory sources: LiDAR and stereo cameras, which have significant price differences. In this preliminary work, we focus on single object tracking. We first developed a fast heuristic object detector that utilizes prior information about the environment and target. The resulting target points are subsequently fed into an extended object tracking framework, where the target shape is parameterized using a star-convex hypersurface model. Experimental results show that our object tracking method using a stereo camera achieves performance similar to that of a LiDAR sensor, with a cost difference of more than tenfold.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2410.08529.pdf' target='_blank'>https://arxiv.org/pdf/2410.08529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Qian, Ruize Han, Junhui Hou, Linqi Song, Wei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08529">VOVTrack: Exploring the Potentiality in Videos for Open-Vocabulary Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Open-vocabulary multi-object tracking (OVMOT) represents a critical new challenge involving the detection and tracking of diverse object categories in videos, encompassing both seen categories (base classes) and unseen categories (novel classes). This issue amalgamates the complexities of open-vocabulary object detection (OVD) and multi-object tracking (MOT). Existing approaches to OVMOT often merge OVD and MOT methodologies as separate modules, predominantly focusing on the problem through an image-centric lens. In this paper, we propose VOVTrack, a novel method that integrates object states relevant to MOT and video-centric training to address this challenge from a video object tracking standpoint. First, we consider the tracking-related state of the objects during tracking and propose a new prompt-guided attention mechanism for more accurate localization and classification (detection) of the time-varying objects. Subsequently, we leverage raw video data without annotations for training by formulating a self-supervised object similarity learning technique to facilitate temporal object association (tracking). Experimental results underscore that VOVTrack outperforms existing methods, establishing itself as a state-of-the-art solution for open-vocabulary tracking task.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2410.02094.pdf' target='_blank'>https://arxiv.org/pdf/2410.02094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sabine Muzellec, Drew Linsley, Alekh K. Ashok, Ennio Mingolla, Girik Malik, Rufin VanRullen, Thomas Serre
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02094">Tracking objects that change in appearance with phase synchrony</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objects we encounter often change appearance as we interact with them. Changes in illumination (shadows), object pose, or the movement of non-rigid objects can drastically alter available image features. How do biological visual systems track objects as they change? One plausible mechanism involves attentional mechanisms for reasoning about the locations of objects independently of their appearances -- a capability that prominent neuroscience theories have associated with computing through neural synchrony. Here, we describe a novel deep learning circuit that can learn to precisely control attention to features separately from their location in the world through neural synchrony: the complex-valued recurrent neural network (CV-RNN). Next, we compare object tracking in humans, the CV-RNN, and other deep neural networks (DNNs), using FeatureTracker: a large-scale challenge that asks observers to track objects as their locations and appearances change in precisely controlled ways. While humans effortlessly solved FeatureTracker, state-of-the-art DNNs did not. In contrast, our CV-RNN behaved similarly to humans on the challenge, providing a computational proof-of-concept for the role of phase synchronization as a neural substrate for tracking appearance-morphing objects as they move about.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2407.14047.pdf' target='_blank'>https://arxiv.org/pdf/2407.14047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Qian, Ruize Han, Wei Feng, Junhui Hou, Linqi Song, Song Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14047">OCTrack: Benchmarking the Open-Corpus Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study a novel yet practical problem of open-corpus multi-object tracking (OCMOT), which extends the MOT into localizing, associating, and recognizing generic-category objects of both seen (base) and unseen (novel) classes, but without the category text list as prompt. To study this problem, the top priority is to build a benchmark. In this work, we build OCTrackB, a large-scale and comprehensive benchmark, to provide a standard evaluation platform for the OCMOT problem. Compared to previous datasets, OCTrackB has more abundant and balanced base/novel classes and the corresponding samples for evaluation with less bias. We also propose a new multi-granularity recognition metric to better evaluate the generative object recognition in OCMOT. By conducting the extensive benchmark evaluation, we report and analyze the results of various state-of-the-art methods, which demonstrate the rationale of OCMOT, as well as the usefulness and advantages of OCTrackB.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2405.11536.pdf' target='_blank'>https://arxiv.org/pdf/2405.11536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed Nagy, Naoufel Werghi, Bilal Hassan, Jorge Dias, Majid Khonji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11536">RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State Estimation Drift Mitigation on LiDAR PointCloud</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses limitations in 3D tracking-by-detection methods, particularly in identifying legitimate trajectories and reducing state estimation drift in Kalman filters. Existing methods often use threshold-based filtering for detection scores, which can fail for distant and occluded objects, leading to false positives. To tackle this, we propose a novel track validity mechanism and multi-stage observational gating process, significantly reducing ghost tracks and enhancing tracking performance. Our method achieves a $29.47\%$ improvement in Multi-Object Tracking Accuracy (MOTA) on the KITTI validation dataset with the Second detector. Additionally, a refined Kalman filter term reduces localization noise, improving higher-order tracking accuracy (HOTA) by $4.8\%$. The online framework, RobMOT, outperforms state-of-the-art methods across multiple detectors, with HOTA improvements of up to $3.92\%$ on the KITTI testing dataset and $8.7\%$ on the validation dataset, while achieving low identity switch scores. RobMOT excels in challenging scenarios, tracking distant objects and prolonged occlusions, with a $1.77\%$ MOTA improvement on the Waymo Open dataset, and operates at a remarkable 3221 FPS on a single CPU, proving its efficiency for real-time multi-object tracking.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2405.06765.pdf' target='_blank'>https://arxiv.org/pdf/2405.06765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anastasios Arsenos, Vasileios Karampinis, Evangelos Petrongonas, Christos Skliros, Dimitrios Kollias, Stefanos Kollias, Athanasios Voulodimos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06765">Common Corruptions for Enhancing and Evaluating Robustness in Air-to-Air Visual Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The main barrier to achieving fully autonomous flights lies in autonomous aircraft navigation. Managing non-cooperative traffic presents the most important challenge in this problem. The most efficient strategy for handling non-cooperative traffic is based on monocular video processing through deep learning models. This study contributes to the vision-based deep learning aircraft detection and tracking literature by investigating the impact of data corruption arising from environmental and hardware conditions on the effectiveness of these methods. More specifically, we designed $7$ types of common corruptions for camera inputs taking into account real-world flight conditions. By applying these corruptions to the Airborne Object Tracking (AOT) dataset we constructed the first robustness benchmark dataset named AOT-C for air-to-air aerial object detection. The corruptions included in this dataset cover a wide range of challenging conditions such as adverse weather and sensor noise. The second main contribution of this letter is to present an extensive experimental evaluation involving $8$ diverse object detectors to explore the degradation in the performance under escalating levels of corruptions (domain shifts). Based on the evaluation results, the key observations that emerge are the following: 1) One-stage detectors of the YOLO family demonstrate better robustness, 2) Transformer-based and multi-stage detectors like Faster R-CNN are extremely vulnerable to corruptions, 3) Robustness against corruptions is related to the generalization ability of models. The third main contribution is to present that finetuning on our augmented synthetic data results in improvements in the generalisation ability of the object detector in real-world flight experiments.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2405.06749.pdf' target='_blank'>https://arxiv.org/pdf/2405.06749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasileios Karampinis, Anastasios Arsenos, Orfeas Filippopoulos, Evangelos Petrongonas, Christos Skliros, Dimitrios Kollias, Stefanos Kollias, Athanasios Voulodimos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06749">Ensuring UAV Safety: A Vision-only and Real-time Framework for Collision Avoidance Through Object Detection, Tracking, and Distance Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last twenty years, unmanned aerial vehicles (UAVs) have garnered growing interest due to their expanding applications in both military and civilian domains. Detecting non-cooperative aerial vehicles with efficiency and estimating collisions accurately are pivotal for achieving fully autonomous aircraft and facilitating Advanced Air Mobility (AAM). This paper presents a deep-learning framework that utilizes optical sensors for the detection, tracking, and distance estimation of non-cooperative aerial vehicles. In implementing this comprehensive sensing framework, the availability of depth information is essential for enabling autonomous aerial vehicles to perceive and navigate around obstacles. In this work, we propose a method for estimating the distance information of a detected aerial object in real time using only the input of a monocular camera. In order to train our deep learning components for the object detection, tracking and depth estimation tasks we utilize the Amazon Airborne Object Tracking (AOT) Dataset. In contrast to previous approaches that integrate the depth estimation module into the object detector, our method formulates the problem as image-to-image translation. We employ a separate lightweight encoder-decoder network for efficient and robust depth estimation. In a nutshell, the object detection module identifies and localizes obstacles, conveying this information to both the tracking module for monitoring obstacle movement and the depth estimation module for calculating distances. Our approach is evaluated on the Airborne Object Tracking (AOT) dataset which is the largest (to the best of our knowledge) air-to-air airborne object dataset.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2404.13953.pdf' target='_blank'>https://arxiv.org/pdf/2404.13953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinzhe Xu, Huajian Huang, Yingshu Chen, Sai-Kit Yeung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13953">360VOTS: Visual Object Tracking and Segmentation in Omnidirectional Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking and segmentation in omnidirectional videos are challenging due to the wide field-of-view and large spherical distortion brought by 360Â° images. To alleviate these problems, we introduce a novel representation, extended bounding field-of-view (eBFoV), for target localization and use it as the foundation of a general 360 tracking framework which is applicable for both omnidirectional visual object tracking and segmentation tasks. Building upon our previous work on omnidirectional visual object tracking (360VOT), we propose a comprehensive dataset and benchmark that incorporates a new component called omnidirectional video object segmentation (360VOS). The 360VOS dataset includes 290 sequences accompanied by dense pixel-wise masks and covers a broader range of target categories. To support both the development and evaluation of algorithms in this domain, we divide the dataset into a training subset with 170 sequences and a testing subset with 120 sequences. Furthermore, we tailor evaluation metrics for both omnidirectional tracking and segmentation to ensure rigorous assessment. Through extensive experiments, we benchmark state-of-the-art approaches and demonstrate the effectiveness of our proposed 360 tracking framework and training dataset. Homepage: https://360vots.hkustvgd.com/
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2404.09857.pdf' target='_blank'>https://arxiv.org/pdf/2404.09857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangwei Zhong, Kui Wu, Hai Ci, Churan Wang, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09857">Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embodied visual tracking is to follow a target object in dynamic 3D environments using an agent's egocentric vision. This is a vital and challenging skill for embodied agents. However, existing methods suffer from inefficient training and poor generalization. In this paper, we propose a novel framework that combines visual foundation models(VFM) and offline reinforcement learning(offline RL) to empower embodied visual tracking. We use a pre-trained VFM, such as "Tracking Anything", to extract semantic segmentation masks with text prompts. We then train a recurrent policy network with offline RL, e.g., Conservative Q-Learning, to learn from the collected demonstrations without online interactions. To further improve the robustness and generalization of the policy network, we also introduce a mask re-targeting mechanism and a multi-level data collection strategy. In this way, we can train a robust policy within an hour on a consumer-level GPU, e.g., Nvidia RTX 3090. We evaluate our agent on several high-fidelity environments with challenging situations, such as distraction and occlusion. The results show that our agent outperforms state-of-the-art methods in terms of sample efficiency, robustness to distractors, and generalization to unseen scenarios and targets. We also demonstrate the transferability of the learned agent from virtual environments to a real-world robot.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2403.10574.pdf' target='_blank'>https://arxiv.org/pdf/2403.10574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxia Xie, Bineng Zhong, Zhiyi Mo, Shengping Zhang, Liangtao Shi, Shuxiang Song, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10574">Autoregressive Queries for Adaptive Tracking with Spatio-TemporalTransformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rich spatio-temporal information is crucial to capture the complicated target appearance variations in visual tracking. However, most top-performing tracking algorithms rely on many hand-crafted components for spatio-temporal information aggregation. Consequently, the spatio-temporal information is far away from being fully explored. To alleviate this issue, we propose an adaptive tracker with spatio-temporal transformers (named AQATrack), which adopts simple autoregressive queries to effectively learn spatio-temporal information without many hand-designed components. Firstly, we introduce a set of learnable and autoregressive queries to capture the instantaneous target appearance changes in a sliding window fashion. Then, we design a novel attention mechanism for the interaction of existing queries to generate a new query in current frame. Finally, based on the initial target template and learnt autoregressive queries, a spatio-temporal information fusion module (STM) is designed for spatiotemporal formation aggregation to locate a target object. Benefiting from the STM, we can effectively combine the static appearance and instantaneous changes to guide robust tracking. Extensive experiments show that our method significantly improves the tracker's performance on six popular tracking benchmarks: LaSOT, LaSOText, TrackingNet, GOT-10k, TNL2K, and UAV123.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2401.09942.pdf' target='_blank'>https://arxiv.org/pdf/2401.09942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir M. Mansourian, Vladimir Somers, Christophe De Vleeschouwer, Shohreh Kasaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09942">Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective tracking and re-identification of players is essential for analyzing soccer videos. But, it is a challenging task due to the non-linear motion of players, the similarity in appearance of players from the same team, and frequent occlusions. Therefore, the ability to extract meaningful embeddings to represent players is crucial in developing an effective tracking and re-identification system. In this paper, a multi-purpose part-based person representation method, called PRTreID, is proposed that performs three tasks of role classification, team affiliation, and re-identification, simultaneously. In contrast to available literature, a single network is trained with multi-task supervision to solve all three tasks, jointly. The proposed joint method is computationally efficient due to the shared backbone. Also, the multi-task learning leads to richer and more discriminative representations, as demonstrated by both quantitative and qualitative results. To demonstrate the effectiveness of PRTreID, it is integrated with a state-of-the-art tracking method, using a part-based post-processing module to handle long-term tracking. The proposed tracking method outperforms all existing tracking methods on the challenging SoccerNet tracking dataset.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2311.03957.pdf' target='_blank'>https://arxiv.org/pdf/2311.03957.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes Tenhumberg, Leon Sievers, Berthold BÃ¤uml
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03957">Self-Contained and Automatic Calibration of a Multi-Fingered Hand Using Only Pairwise Contact Measurements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A self-contained calibration procedure that can be performed automatically without additional external sensors or tools is a significant advantage, especially for complex robotic systems. Here, we show that the kinematics of a multi-fingered robotic hand can be precisely calibrated only by moving the tips of the fingers pairwise into contact. The only prerequisite for this is sensitive contact detection, e.g., by torque-sensing in the joints (as in our DLR-Hand II) or tactile skin. The measurement function for a given joint configuration is the distance between the modeled fingertip geometries, but the actual measurement is always zero. In an in-depth analysis, we prove that this contact-based calibration determines all quantities needed for manipulating objects with the hand, i.e., the difference vectors of the fingertips, and that it is as sensitive as a calibration using an external visual tracking system and markers. We describe the complete calibration scheme, including the selection of optimal sample joint configurations and search motions for the contacts despite the initial kinematic uncertainties. In a real-world calibration experiment for the torque-controlled four-fingered DLR-Hand II, the maximal error of 17.7mm can be reduced to only 3.7mm.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2307.11349.pdf' target='_blank'>https://arxiv.org/pdf/2307.11349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sourav Sanyal, Rohan Kumar Manna, Kaushik Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11349">EV-Planner: Energy-Efficient Robot Navigation via Event-Based Physics-Guided Neuromorphic Planner</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based object tracking is an essential precursor to performing autonomous aerial navigation in order to avoid obstacles. Biologically inspired neuromorphic event cameras are emerging as a powerful alternative to frame-based cameras, due to their ability to asynchronously detect varying intensities (even in poor lighting conditions), high dynamic range, and robustness to motion blur. Spiking neural networks (SNNs) have gained traction for processing events asynchronously in an energy-efficient manner. On the other hand, physics-based artificial intelligence (AI) has gained prominence recently, as they enable embedding system knowledge via physical modeling inside traditional analog neural networks (ANNs). In this letter, we present an event-based physics-guided neuromorphic planner (EV-Planner) to perform obstacle avoidance using neuromorphic event cameras and physics-based AI. We consider the task of autonomous drone navigation where the mission is to detect moving gates and fly through them while avoiding a collision. We use event cameras to perform object detection using a shallow spiking neural network in an unsupervised fashion. Utilizing the physical equations of the brushless DC motors present in the drone rotors, we train a lightweight energy-aware physics-guided neural network (PgNN) with depth inputs. This predicts the optimal flight time responsible for generating near-minimum energy paths. We spawn the drone in the Gazebo simulator and implement a sensor-fused vision-to-planning neuro-symbolic framework using Robot Operating System (ROS). Simulation results for safe collision-free flight trajectories are presented with performance analysis, ablation study and potential future research directions
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2304.12175.pdf' target='_blank'>https://arxiv.org/pdf/2304.12175.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mason B. Peterson, Parker C. Lusk, Jonathan P. How
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12175">MOTLEE: Distributed Mobile Multi-Object Tracking with Localization Error Elimination</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MOTLEE, a distributed mobile multi-object tracking algorithm that enables a team of robots to collaboratively track moving objects in the presence of localization error. Existing approaches to distributed tracking make limiting assumptions regarding the relative spatial relationship of sensors, including assuming a static sensor network or that perfect localization is available. Instead, we develop an algorithm based on the Kalman-Consensus filter for distributed tracking that properly leverages localization uncertainty in collaborative tracking. Further, our method allows the team to maintain an accurate understanding of dynamic objects in the environment by realigning robot frames and incorporating frame alignment uncertainty into our object tracking formulation. We evaluate our method in hardware on a team of three mobile ground robots tracking four people. Compared to previous works that do not account for localization error, we show that MOTLEE is resilient to localization uncertainties, enabling accurate tracking in distributed, dynamic settings with mobile tracking sensors.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2303.03508.pdf' target='_blank'>https://arxiv.org/pdf/2303.03508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Kiefer, Yitong Quan, Andreas Zell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.03508">Memory Maps for Video Object Detection and Tracking on UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel approach to video object detection detection and tracking on Unmanned Aerial Vehicles (UAVs). By incorporating metadata, the proposed approach creates a memory map of object locations in actual world coordinates, providing a more robust and interpretable representation of object locations in both, image space and the real world. We use this representation to boost confidences, resulting in improved performance for several temporal computer vision tasks, such as video object detection, short and long-term single and multi-object tracking, and video anomaly detection. These findings confirm the benefits of metadata in enhancing the capabilities of UAVs in the field of temporal computer vision and pave the way for further advancements in this area.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2204.07414.pdf' target='_blank'>https://arxiv.org/pdf/2204.07414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Hu, Xin Zhao, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.07414">SOTVerse: A User-defined Task Space of Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking (SOT) research falls into a cycle -- trackers perform well on most benchmarks but quickly fail in challenging scenarios, causing researchers to doubt the insufficient data content and take more effort to construct larger datasets with more challenging situations. However, inefficient data utilization and limited evaluation methods more seriously hinder SOT research. The former causes existing datasets can not be exploited comprehensively, while the latter neglects challenging factors in the evaluation process. In this article, we systematize the representative benchmarks and form a Single Object Tracking metaverse (SOTVerse) -- a user-defined SOT task space to break through the bottleneck. We first propose a 3E Paradigm to describe tasks by three components (i.e., environment, evaluation, and executor). Then, we summarize task characteristics, clarify the organization standards, and construct SOTVerse with 12.56 million frames. Specifically, SOTVerse automatically labels challenging factors per frame, allowing users to generate user-defined spaces efficiently via construction rules. Besides, SOTVerse provides two mechanisms with new indicators and successfully evaluates trackers under various subtasks. Consequently, SOTVerse first provides a strategy to improve resource utilization in the computer vision area, making research more standardized and scientific. The SOTVerse, toolkit, evaluation server, and results are available at http://metaverse.aitestunion.com.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2510.05070.pdf' target='_blank'>https://arxiv.org/pdf/2510.05070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05070">ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body loco-manipulation promises transformative capabilities for daily service and warehouse tasks. While recent advances in general motion tracking (GMT) have enabled humanoids to reproduce diverse human motions, these policies lack the precision and object awareness required for loco-manipulation. To this end, we introduce ResMimic, a two-stage residual learning framework for precise and expressive humanoid control from human motion data. First, a GMT policy, trained on large-scale human-only motion, serves as a task-agnostic base for generating human-like whole-body movements. An efficient but precise residual policy is then learned to refine the GMT outputs to improve locomotion and incorporate object interaction. To further facilitate efficient training, we design (i) a point-cloud-based object tracking reward for smoother optimization, (ii) a contact reward that encourages accurate humanoid body-object interactions, and (iii) a curriculum-based virtual object controller to stabilize early training. We evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results show substantial gains in task success, training efficiency, and robustness over strong baselines. Videos are available at https://resmimic.github.io/ .
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2510.05070.pdf' target='_blank'>https://arxiv.org/pdf/2510.05070.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siheng Zhao, Yanjie Ze, Yue Wang, C. Karen Liu, Pieter Abbeel, Guanya Shi, Rocky Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05070">ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humanoid whole-body loco-manipulation promises transformative capabilities for daily service and warehouse tasks. While recent advances in general motion tracking (GMT) have enabled humanoids to reproduce diverse human motions, these policies lack the precision and object awareness required for loco-manipulation. To this end, we introduce ResMimic, a two-stage residual learning framework for precise and expressive humanoid control from human motion data. First, a GMT policy, trained on large-scale human-only motion, serves as a task-agnostic base for generating human-like whole-body movements. An efficient but precise residual policy is then learned to refine the GMT outputs to improve locomotion and incorporate object interaction. To further facilitate efficient training, we design (i) a point-cloud-based object tracking reward for smoother optimization, (ii) a contact reward that encourages accurate humanoid body-object interactions, and (iii) a curriculum-based virtual object controller to stabilize early training. We evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results show substantial gains in task success, training efficiency, and robustness over strong baselines. Videos are available at https://resmimic.github.io/ .
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2508.14776.pdf' target='_blank'>https://arxiv.org/pdf/2508.14776.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Li, Arren Glover, Chiara Bartolozzi, Lorenzo Natale
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14776">6-DoF Object Tracking with Event-based Optical Flow and Frames</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking the position and orientation of objects in space (i.e., in 6-DoF) in real time is a fundamental problem in robotics for environment interaction. It becomes more challenging when objects move at high-speed due to frame rate limitations in conventional cameras and motion blur. Event cameras are characterized by high temporal resolution, low latency and high dynamic range, that can potentially overcome the impacts of motion blur. Traditional RGB cameras provide rich visual information that is more suitable for the challenging task of single-shot object pose estimation. In this work, we propose using event-based optical flow combined with an RGB based global object pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the core advantages of both types of vision sensors. Specifically, we propose an event-based optical flow algorithm for object motion measurement to implement an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF velocity with low frequency estimated pose from the global pose estimator, the method can track pose when objects move at high-speed. The proposed algorithm is tested and validated on both synthetic and real world data, demonstrating its effectiveness, especially in high-speed motion scenarios.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2508.14607.pdf' target='_blank'>https://arxiv.org/pdf/2508.14607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengzhi Zhong, Xinzhe Wang, Dan Zeng, Qihua Zhou, Feixiang He, Shuiwang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14607">SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential for low-power computation, yet their application in visual tasks remains largely confined to image classification, object detection, and event-based tracking. In contrast, real-world vision systems still widely use conventional RGB video streams, where the potential of directly-trained SNNs for complex temporal tasks such as multi-object tracking (MOT) remains underexplored. To address this challenge, we propose SMTrack-the first directly trained deep SNN framework for end-to-end multi-object tracking on standard RGB videos. SMTrack introduces an adaptive and scale-aware Normalized Wasserstein Distance loss (Asa-NWDLoss) to improve detection and localization performance under varying object scales and densities. Specifically, the method computes the average object size within each training batch and dynamically adjusts the normalization factor, thereby enhancing sensitivity to small objects. For the association stage, we incorporate the TrackTrack identity module to maintain robust and consistent object trajectories. Extensive evaluations on BEE24, MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking in complex scenarios.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2508.09524.pdf' target='_blank'>https://arxiv.org/pdf/2508.09524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yipei Wang, Shiyu Hu, Shukun Jia, Panxi Xu, Hongfei Ma, Yiping Ma, Jing Zhang, Xiaobo Lu, Xin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09524">SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present the first systematic investigation and quantification of Similar Object Interference (SOI), a long-overlooked yet critical bottleneck in Single Object Tracking (SOT). Through controlled Online Interference Masking (OIM) experiments, we quantitatively demonstrate that eliminating interference sources leads to substantial performance improvements (AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a primary constraint for robust tracking and highlighting the feasibility of external cognitive guidance. Building upon these insights, we adopt natural language as a practical form of external guidance, and construct SOIBench-the first semantic cognitive guidance benchmark specifically targeting SOI challenges. It automatically mines SOI frames through multi-tracker collective judgment and introduces a multi-level annotation protocol to generate precise semantic guidance texts. Systematic evaluation on SOIBench reveals a striking finding: existing vision-language tracking (VLT) methods fail to effectively exploit semantic cognitive guidance, achieving only marginal improvements or even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we propose a novel paradigm employing large-scale vision-language models (VLM) as external cognitive engines that can be seamlessly integrated into arbitrary RGB trackers. This approach demonstrates substantial improvements under semantic cognitive guidance (AUC gains up to 0.93), representing a significant advancement over existing VLT methods. We hope SOIBench will serve as a standardized evaluation platform to advance semantic cognitive tracking research and contribute new insights to the tracking research community.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2507.23251.pdf' target='_blank'>https://arxiv.org/pdf/2507.23251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fereshteh Aghaee Meibodi, Shadi Alijani, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23251">A Deep Dive into Generic Object Tracking: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generic object tracking remains an important yet challenging task in computer vision due to complex spatio-temporal dynamics, especially in the presence of occlusions, similar distractors, and appearance variations. Over the past two decades, a wide range of tracking paradigms, including Siamese-based trackers, discriminative trackers, and, more recently, prominent transformer-based approaches, have been introduced to address these challenges. While a few existing survey papers in this field have either concentrated on a single category or widely covered multiple ones to capture progress, our paper presents a comprehensive review of all three categories, with particular emphasis on the rapidly evolving transformer-based methods. We analyze the core design principles, innovations, and limitations of each approach through both qualitative and quantitative comparisons. Our study introduces a novel categorization and offers a unified visual and tabular comparison of representative methods. Additionally, we organize existing trackers from multiple perspectives and summarize the major evaluation benchmarks, highlighting the fast-paced advancements in transformer-based tracking driven by their robust spatio-temporal modeling capabilities.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2507.19908.pdf' target='_blank'>https://arxiv.org/pdf/2507.19908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengmeng Wang, Haonan Wang, Yulong Li, Xiangjie Kong, Jiaxin Du, Guojiang Shen, Feng Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19908">TrackAny3D: Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D LiDAR-based single object tracking (SOT) relies on sparse and irregular point clouds, posing challenges from geometric variations in scale, motion patterns, and structural complexity across object categories. Current category-specific approaches achieve good accuracy but are impractical for real-world use, requiring separate models for each category and showing limited generalization. To tackle these issues, we propose TrackAny3D, the first framework to transfer large-scale pretrained 3D models for category-agnostic 3D SOT. We first integrate parameter-efficient adapters to bridge the gap between pretraining and tracking tasks while preserving geometric priors. Then, we introduce a Mixture-of-Geometry-Experts (MoGE) architecture that adaptively activates specialized subnetworks based on distinct geometric characteristics. Additionally, we design a temporal context optimization strategy that incorporates learnable temporal tokens and a dynamic mask weighting module to propagate historical information and mitigate temporal drift. Experiments on three commonly-used benchmarks show that TrackAny3D establishes new state-of-the-art performance on category-agnostic 3D SOT, demonstrating strong generalization and competitiveness. We hope this work will enlighten the community on the importance of unified models and further expand the use of large-scale pretrained models in this field.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2505.04088.pdf' target='_blank'>https://arxiv.org/pdf/2505.04088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shang Zhang, Huanbin Zhang, Dali Feng, Yujie Cui, Ruoyan Xiong, Cen He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04088">SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Thermal infrared (TIR) object tracking often suffers from challenges such as target occlusion, motion blur, and background clutter, which significantly degrade the performance of trackers. To address these issues, this paper pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a bidirectional state-space model and a self-attention mechanism. Specifically, we introduce the Motion Mamba module into the Siamese architecture to ex-tract motion features and recover overlooked edge details using bidirectional modeling and self-attention. We propose a Siamese parameter-sharing strate-gy that allows certain convolutional layers to share weights. This approach reduces computational redundancy while preserving strong feature represen-tation. In addition, we design a motion edge-aware regression loss to improve tracking accuracy, especially for motion-blurred targets. Extensive experi-ments are conducted on four TIR tracking benchmarks, including LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT achieves superior performance in TIR target tracking.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2505.00739.pdf' target='_blank'>https://arxiv.org/pdf/2505.00739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiushi Yang, Yuan Yao, Miaomiao Cui, Liefeng Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00739">MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional capabilities in interactive object segmentation for both images and videos. However, as a foundational model on interactive segmentation, SAM2 performs segmentation directly based on mask memory from the past six frames, leading to two significant challenges. Firstly, during inference in videos, objects may disappear since SAM2 relies solely on memory without accounting for object motion information, which limits its long-range object tracking capabilities. Secondly, its memory is constructed from fixed past frames, making it susceptible to challenges associated with object disappearance or occlusion, due to potentially inaccurate segmentation results in memory. To address these problems, we present MoSAM, incorporating two key strategies to integrate object motion cues into the model and establish more reliable feature memory. Firstly, we propose Motion-Guided Prompting (MGP), which represents the object motion in both sparse and dense manners, then injects them into SAM2 through a set of motion-guided prompts. MGP enables the model to adjust its focus towards the direction of motion, thereby enhancing the object tracking capabilities. Furthermore, acknowledging that past segmentation results may be inaccurate, we devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically identifies frames likely to contain accurate segmentation in both pixel- and frame-level. By eliminating potentially inaccurate mask predictions from memory, we can leverage more reliable memory features to exploit similar regions for improving segmentation results. Extensive experiments on various benchmarks of video object segmentation and video instance segmentation demonstrate that our MoSAM achieves state-of-the-art results compared to other competitors.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2502.16569.pdf' target='_blank'>https://arxiv.org/pdf/2502.16569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Safa, Waqas Aman, Ali Al-Zawqari, Saif Al-Kuwari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16569">Benchmarking Online Object Trackers for Underwater Robot Position Locking Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomously controlling the position of Remotely Operated underwater Vehicles (ROVs) is of crucial importance for a wide range of underwater engineering applications, such as in the inspection and maintenance of underwater industrial structures. Consequently, studying vision-based underwater robot navigation and control has recently gained increasing attention to counter the numerous challenges faced in underwater conditions, such as lighting variability, turbidity, camera image distortions (due to bubbles), and ROV positional disturbances (due to underwater currents). In this paper, we propose (to the best of our knowledge) a first rigorous unified benchmarking of more than seven Machine Learning (ML)-based one-shot object tracking algorithms for vision-based position locking of ROV platforms. We propose a position-locking system that processes images of an object of interest in front of which the ROV must be kept stable. Then, our proposed system uses the output result of different object tracking algorithms to automatically correct the position of the ROV against external disturbances. We conducted numerous real-world experiments using a BlueROV2 platform within an indoor pool and provided clear demonstrations of the strengths and weaknesses of each tracking approach. Finally, to help alleviate the scarcity of underwater ROV data, we release our acquired data base as open-source with the hope of benefiting future research.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2501.04336.pdf' target='_blank'>https://arxiv.org/pdf/2501.04336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Huang, Yuyang Ji, Xiaofang Wang, Nikhil Mehta, Tong Xiao, Donghyun Lee, Sigmund Vanvalkenburgh, Shengxin Zha, Bolin Lai, Licheng Yu, Ning Zhang, Yong Jae Lee, Miao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.04336">Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-form video understanding with Large Vision Language Models is challenged by the need to analyze temporally dispersed yet spatially concentrated key moments within limited context windows. In this work, we introduce VideoMindPalace, a new framework inspired by the "Mind Palace", which organizes critical video moments into a topologically structured semantic graph. VideoMindPalace organizes key information through (i) hand-object tracking and interaction, (ii) clustered activity zones representing specific areas of recurring activities, and (iii) environment layout mapping, allowing natural language parsing by LLMs to provide grounded insights on spatio-temporal and 3D context. In addition, we propose the Video MindPalace Benchmark (VMB), to assess human-like reasoning, including spatial localization, temporal reasoning, and layout-aware sequential understanding. Evaluated on VMB and established video QA datasets, including EgoSchema, NExT-QA, IntentQA, and the Active Memories Benchmark, VideoMindPalace demonstrates notable gains in spatio-temporal coherence and human-aligned reasoning, advancing long-form video analysis capabilities in VLMs.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2411.08144.pdf' target='_blank'>https://arxiv.org/pdf/2411.08144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangge Li, Benjamin C Yang, Sayan Mitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08144">Visual Tracking with Intermittent Visibility: Switched Control Design and Implementation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of visual target tracking in scenarios where a pursuer may experience intermittent loss of visibility of the target. The design of a Switched Visual Tracker (SVT) is presented which aims to meet the competing requirements of maintaining both proximity and visibility. SVT alternates between a visual tracking mode for following the target, and a recovery mode for regaining visual contact when the target falls out of sight. We establish the stability of SVT by extending the average dwell time theorem from switched systems theory, which may be of independent interest. Our implementation of SVT on an Agilicious drone [1] illustrates its effectiveness on tracking various target trajectories: it reduces the average tracking error by up to 45% and significantly improves visibility duration compared to a baseline algorithm. The results show that our approach effectively handles intermittent vision loss, offering enhanced robustness and adaptability for real-world autonomous missions. Additionally, we demonstrate how the stability analysis provides valuable guidance for selecting parameters, such as tracking speed and recovery distance, to optimize the SVT's performance.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2408.12727.pdf' target='_blank'>https://arxiv.org/pdf/2408.12727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojin Shin, Donghwa Kang, Daejin Choi, Brent Kang, Jinkyu Lee, Hyeongboo Baek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12727">BankTweak: Adversarial Attack against Multi-Object Trackers by Manipulating Feature Banks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to construct moving trajectories for objects, and modern multi-object trackers mainly utilize the tracking-by-detection methodology. Initial approaches to MOT attacks primarily aimed to degrade the detection quality of the frames under attack, thereby reducing accuracy only in those specific frames, highlighting a lack of \textit{efficiency}. To improve efficiency, recent advancements manipulate object positions to cause persistent identity (ID) switches during the association phase, even after the attack ends within a few frames. However, these position-manipulating attacks have inherent limitations, as they can be easily counteracted by adjusting distance-related parameters in the association phase, revealing a lack of \textit{robustness}. In this paper, we present \textsf{BankTweak}, a novel adversarial attack designed for MOT trackers, which features efficiency and robustness. \textsf{BankTweak} focuses on the feature extractor in the association phase and reveals vulnerability in the Hungarian matching method used by feature-based MOT systems. Exploiting the vulnerability, \textsf{BankTweak} induces persistent ID switches (addressing \textit{efficiency}) even after the attack ends by strategically injecting altered features into the feature banks without modifying object positions (addressing \textit{robustness}). To demonstrate the applicability, we apply \textsf{BankTweak} to three multi-object trackers (DeepSORT, StrongSORT, and MOTDT) with one-stage, two-stage, anchor-free, and transformer detectors. Extensive experiments on the MOT17 and MOT20 datasets show that our method substantially surpasses existing attacks, exposing the vulnerability of the tracking-by-detection framework to \textsf{BankTweak}.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2405.15137.pdf' target='_blank'>https://arxiv.org/pdf/2405.15137.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pratyusha Musunuru, Yuchao Li, Jamison Weber, Dimitri Bertsekas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15137">An Approximate Dynamic Programming Framework for Occlusion-Robust Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we consider data association problems involving multi-object tracking (MOT). In particular, we address the challenges arising from object occlusions. We propose a framework called approximate dynamic programming track (ADPTrack), which applies dynamic programming principles to improve an existing method called the base heuristic. Given a set of tracks and the next target frame, the base heuristic extends the tracks by matching them to the objects of this target frame directly. In contrast, ADPTrack first processes a few subsequent frames and applies the base heuristic starting from the next target frame to obtain tentative tracks. It then leverages the tentative tracks to match the objects of the target frame. This tends to reduce the occlusion-based errors and leads to an improvement over the base heuristic. When tested on the MOT17 video dataset, the proposed method demonstrates a 0.7% improvement in the association accuracy (IDF1 metric) over a state-of-the-art method that is used as the base heuristic. It also obtains improvements with respect to all the other standard metrics. Empirically, we found that the improvements are particularly pronounced in scenarios where the video data is obtained by fixed-position cameras.
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2404.09504.pdf' target='_blank'>https://arxiv.org/pdf/2404.09504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiangqiang Wu, Antoni B. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09504">Learning Tracking Representations from Single Point Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing deep trackers are typically trained with largescale video frames with annotated bounding boxes. However, these bounding boxes are expensive and time-consuming to annotate, in particular for large scale datasets. In this paper, we propose to learn tracking representations from single point annotations (i.e., 4.5x faster to annotate than the traditional bounding box) in a weakly supervised manner. Specifically, we propose a soft contrastive learning (SoCL) framework that incorporates target objectness prior into end-to-end contrastive learning. Our SoCL consists of adaptive positive and negative sample generation, which is memory-efficient and effective for learning tracking representations. We apply the learned representation of SoCL to visual tracking and show that our method can 1) achieve better performance than the fully supervised baseline trained with box annotations under the same annotation time cost; 2) achieve comparable performance of the fully supervised baseline by using the same number of training frames and meanwhile reducing annotation time cost by 78% and total fees by 85%; 3) be robust to annotation noise.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2404.02562.pdf' target='_blank'>https://arxiv.org/pdf/2404.02562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonglin Liu, Shujie Chen, Jianfeng Dong, Xun Wang, Di Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02562">Representation Alignment Contrastive Regularization for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving high-performance in multi-object tracking algorithms heavily relies on modeling spatio-temporal relationships during the data association stage. Mainstream approaches encompass rule-based and deep learning-based methods for spatio-temporal relationship modeling. While the former relies on physical motion laws, offering wider applicability but yielding suboptimal results for complex object movements, the latter, though achieving high-performance, lacks interpretability and involves complex module designs. This work aims to simplify deep learning-based spatio-temporal relationship models and introduce interpretability into features for data association. Specifically, a lightweight single-layer transformer encoder is utilized to model spatio-temporal relationships. To make features more interpretative, two contrastive regularization losses based on representation alignment are proposed, derived from spatio-temporal consistency rules. By applying weighted summation to affinity matrices, the aligned features can seamlessly integrate into the data association stage of the original tracking workflow. Experimental results showcase that our model enhances the majority of existing tracking networks' performance without excessive complexity, with minimal increase in training overhead and nearly negligible computational and storage costs.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2403.20225.pdf' target='_blank'>https://arxiv.org/pdf/2403.20225.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sanghyun Woo, Kwanyong Park, Inkyu Shin, Myungchul Kim, In So Kweon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.20225">MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-target multi-camera tracking is a crucial task that involves identifying and tracking individuals over time using video streams from multiple cameras. This task has practical applications in various fields, such as visual surveillance, crowd behavior analysis, and anomaly detection. However, due to the difficulty and cost of collecting and labeling data, existing datasets for this task are either synthetically generated or artificially constructed within a controlled camera network setting, which limits their ability to model real-world dynamics and generalize to diverse camera configurations. To address this issue, we present MTMMC, a real-world, large-scale dataset that includes long video sequences captured by 16 multi-modal cameras in two different environments - campus and factory - across various time, weather, and season conditions. This dataset provides a challenging test-bed for studying multi-camera tracking under diverse real-world complexities and includes an additional input modality of spatially aligned and temporally synchronized RGB and thermal cameras, which enhances the accuracy of multi-camera tracking. MTMMC is a super-set of existing datasets, benefiting independent fields such as person detection, re-identification, and multiple object tracking. We provide baselines and new learning setups on this dataset and set the reference scores for future studies. The datasets, models, and test server will be made publicly available.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2402.04519.pdf' target='_blank'>https://arxiv.org/pdf/2402.04519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhao, Shiyu Hu, Yipei Wang, Jing Zhang, Yimin Hu, Rongshuai Liu, Haibin Ling, Yin Li, Renshu Li, Kun Liu, Jiadong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04519">BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking (SOT) is a fundamental problem in computer vision, with a wide range of applications, including autonomous driving, augmented reality, and robot navigation. The robustness of SOT faces two main challenges: tiny target and fast motion. These challenges are especially manifested in videos captured by unmanned aerial vehicles (UAV), where the target is usually far away from the camera and often with significant motion relative to the camera. To evaluate the robustness of SOT methods, we propose BioDrone -- the first bionic drone-based visual benchmark for SOT. Unlike existing UAV datasets, BioDrone features videos captured from a flapping-wing UAV system with a major camera shake due to its aerodynamics. BioDrone hence highlights the tracking of tiny targets with drastic changes between consecutive frames, providing a new robust vision benchmark for SOT. To date, BioDrone offers the largest UAV-based SOT benchmark with high-quality fine-grained manual annotations and automatically generates frame-level labels, designed for robust vision analyses. Leveraging our proposed BioDrone, we conduct a systematic evaluation of existing SOT methods, comparing the performance of 20 representative models and studying novel means of optimizing a SOTA method (KeepTrack KeepTrack) for robust SOT. Our evaluation leads to new baselines and insights for robust SOT. Moving forward, we hope that BioDrone will not only serve as a high-quality benchmark for robust SOT, but also invite future research into robust computer vision. The database, toolkits, evaluation server, and baseline results are available at http://biodrone.aitestunion.com.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2311.01423.pdf' target='_blank'>https://arxiv.org/pdf/2311.01423.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jen-Hao Cheng, Sheng-Yao Kuan, Hugo Latapie, Gaowen Liu, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01423">CenterRadarNet: Joint 3D Object Detection and Tracking Framework using 4D FMCW Radar</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust perception is a vital component for ensuring safe autonomous and assisted driving. Automotive radar (77 to 81 GHz), which offers weather-resilient sensing, provides a complementary capability to the vision- or LiDAR-based autonomous driving systems. Raw radio-frequency (RF) radar tensors contain rich spatiotemporal semantics besides 3D location information. The majority of previous methods take in 3D (Doppler-range-azimuth) RF radar tensors, allowing prediction of an object's location, heading angle, and size in bird's-eye-view (BEV). However, they lack the ability to at the same time infer objects' size, orientation, and identity in the 3D space. To overcome this limitation, we propose an efficient joint architecture called CenterRadarNet, designed to facilitate high-resolution representation learning from 4D (Doppler-range-azimuth-elevation) radar data for 3D object detection and re-identification (re-ID) tasks. As a single-stage 3D object detector, CenterRadarNet directly infers the BEV object distribution confidence maps, corresponding 3D bounding box attributes, and appearance embedding for each pixel. Moreover, we build an online tracker utilizing the learned appearance embedding for re-ID. CenterRadarNet achieves the state-of-the-art result on the K-Radar 3D object detection benchmark. In addition, we present the first 3D object-tracking result using radar on the K-Radar dataset V2. In diverse driving scenarios, CenterRadarNet shows consistent, robust performance, emphasizing its wide applicability.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2309.09078.pdf' target='_blank'>https://arxiv.org/pdf/2309.09078.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiruo Zhou, Suya You, C. -C. Jay Kuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.09078">Unsupervised Green Object Tracker (GOT) without Offline Pre-training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised trackers trained on labeled data dominate the single object tracking field for superior tracking accuracy. The labeling cost and the huge computational complexity hinder their applications on edge devices. Unsupervised learning methods have also been investigated to reduce the labeling cost but their complexity remains high. Aiming at lightweight high-performance tracking, feasibility without offline pre-training, and algorithmic transparency, we propose a new single object tracking method, called the green object tracker (GOT), in this work. GOT conducts an ensemble of three prediction branches for robust box tracking: 1) a global object-based correlator to predict the object location roughly, 2) a local patch-based correlator to build temporal correlations of small spatial units, and 3) a superpixel-based segmentator to exploit the spatial information of the target frame. GOT offers competitive tracking accuracy with state-of-the-art unsupervised trackers, which demand heavy offline pre-training, at a lower computation cost. GOT has a tiny model size (<3k parameters) and low inference complexity (around 58M FLOPs per frame). Since its inference complexity is between 0.1%-10% of DL trackers, it can be easily deployed on mobile and edge devices.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2308.05026.pdf' target='_blank'>https://arxiv.org/pdf/2308.05026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Cheng, Mengmeng Liu, Lin Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05026">An End-to-End Framework of Road User Detection, Tracking, and Prediction from Monocular Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception that involves multi-object detection and tracking, and trajectory prediction are two major tasks of autonomous driving. However, they are currently mostly studied separately, which results in most trajectory prediction modules being developed based on ground truth trajectories without taking into account that trajectories extracted from the detection and tracking modules in real-world scenarios are noisy. These noisy trajectories can have a significant impact on the performance of the trajectory predictor and can lead to serious prediction errors. In this paper, we build an end-to-end framework for detection, tracking, and trajectory prediction called ODTP (Online Detection, Tracking and Prediction). It adopts the state-of-the-art online multi-object tracking model, QD-3DT, for perception and trains the trajectory predictor, DCENet++, directly based on the detection results without purely relying on ground truth trajectories. We evaluate the performance of ODTP on the widely used nuScenes dataset for autonomous driving. Extensive experiments show that ODPT achieves high performance end-to-end trajectory prediction. DCENet++, with the enhanced dynamic maps, predicts more accurate trajectories than its base model. It is also more robust when compared with other generative and deterministic trajectory prediction models trained on noisy detection results.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2307.10046.pdf' target='_blank'>https://arxiv.org/pdf/2307.10046.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingzhe Guo, Zhipeng Zhang, Liping Jing, Haibin Ling, Heng Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.10046">Divert More Attention to Vision-Language Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal vision-language (VL) learning has noticeably pushed the tendency toward generic intelligence owing to emerging large foundation models. However, tracking, as a fundamental vision problem, surprisingly enjoys less bonus from recent flourishing VL learning. We argue that the reasons are two-fold: the lack of large-scale vision-language annotated videos and ineffective vision-language interaction learning of current works. These nuisances motivate us to design more effective vision-language representation for tracking, meanwhile constructing a large database with language annotation for model learning. Particularly, in this paper, we first propose a general attribute annotation strategy to decorate videos in six popular tracking benchmarks, which contributes a large-scale vision-language tracking database with more than 23,000 videos. We then introduce a novel framework to improve tracking by learning a unified-adaptive VL representation, where the cores are the proposed asymmetric architecture search and modality mixer (ModaMixer). To further improve VL representation, we introduce a contrastive loss to align different modalities. To thoroughly evidence the effectiveness of our method, we integrate the proposed framework on three tracking methods with different designs, i.e., the CNN-based SiamCAR, the Transformer-based OSTrack, and the hybrid structure TransT. The experiments demonstrate that our framework can significantly improve all baselines on six benchmarks. Besides empirical results, we theoretically analyze our approach to show its rationality. By revealing the potential of VL representation, we expect the community to divert more attention to VL tracking and hope to open more possibilities for future tracking with diversified multimodal messages.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2306.11724.pdf' target='_blank'>https://arxiv.org/pdf/2306.11724.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>V. A. Coutinho, R. J. Cintra, F. M. Bayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.11724">Low-complexity Multidimensional DCT Approximations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce low-complexity multidimensional discrete cosine transform (DCT) approximations. Three dimensional DCT (3D DCT) approximations are formalized in terms of high-order tensor theory. The formulation is extended to higher dimensions with arbitrary lengths. Several multiplierless $8\times 8\times 8$ approximate methods are proposed and the computational complexity is discussed for the general multidimensional case. The proposed methods complexity cost was assessed, presenting considerably lower arithmetic operations when compared with the exact 3D DCT. The proposed approximations were embedded into 3D DCT-based video coding scheme and a modified quantization step was introduced. The simulation results showed that the approximate 3D DCT coding methods offer almost identical output visual quality when compared with exact 3D DCT scheme. The proposed 3D approximations were also employed as a tool for visual tracking. The approximate 3D DCT-based proposed system performs similarly to the original exact 3D DCT-based method. In general, the suggested methods showed competitive performance at a considerably lower computational cost.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2304.00571.pdf' target='_blank'>https://arxiv.org/pdf/2304.00571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiangqiang Wu, Tianyu Yang, Ziquan Liu, Wei Lin, Baoyuan Wu, Antoni B. Chan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.00571">DropMAE: Learning Representations via Masked Autoencoders with Spatial-Attention Dropout for Temporal Matching Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper studies masked autoencoder (MAE) video pre-training for various temporal matching-based downstream tasks, i.e., object-level tracking tasks including video object tracking (VOT) and video object segmentation (VOS), self-supervised visual correspondence learning, dense tracking tasks including optical flow estimation and long-term point tracking, and 3D point cloud tracking. Specifically, our work explores to provide a general representation to boost the temporal matching ability in various downstream tracking tasks. To achieve this, we firstly find that a simple extension of MAE, which randomly masks out frame patches in videos and reconstruct the frame pixels, heavily relies on spatial cues while ignoring temporal relations for frame reconstruction, thus leading to sub-optimal temporal matching representations. To alleviate this, we propose DropMAE, which adaptively performs spatial-attention dropout in the frame reconstruction to facilitate temporal correspondence learning in videos. We obtain several important findings with DropMAE: 1) DropMAE is a strong and efficient temporal matching learner, which achieves better fine-tuning results on matching-based tasks than the ImageNet-based MAE with 2x faster pre-training speed. 2) DropMAE is effective for different tracking tasks, i.e., object-level matching tasks including VOT and VOS, dense tracking tasks including optical flow estimation and tracking any point (TAP), and even 3D tracking in the different modality of point cloud data. Since none exists, we build ViT-based trackers for different downstream tracking tasks, and our pre-trained DropMAE model can be directly loaded in these ViT-based trackers for fine-tuning without further modifications. Experiments on 6 downstream tracking tasks demonstrate the effectiveness of DropMAE as a general pre-trained representation for diverse tracking tasks.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2303.07605.pdf' target='_blank'>https://arxiv.org/pdf/2303.07605.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng Luo, Gongjie Zhang, Changqing Zhou, Zhonghua Wu, Qingyi Tao, Lewei Lu, Shijian Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07605">Modeling Continuous Motion for 3D Point Cloud Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of 3D single object tracking (SOT) with LiDAR point clouds is crucial for various applications, such as autonomous driving and robotics. However, existing approaches have primarily relied on appearance matching or motion modeling within only two successive frames, thereby overlooking the long-range continuous motion property of objects in 3D space. To address this issue, this paper presents a novel approach that views each tracklet as a continuous stream: at each timestamp, only the current frame is fed into the network to interact with multi-frame historical features stored in a memory bank, enabling efficient exploitation of sequential information. To achieve effective cross-frame message passing, a hybrid attention mechanism is designed to account for both long-range relation modeling and local geometric feature extraction. Furthermore, to enhance the utilization of multi-frame features for robust tracking, a contrastive sequence enhancement strategy is proposed, which uses ground truth tracklets to augment training sequences and promote discrimination against false positives in a contrastive manner. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art method by significant margins on multiple benchmarks.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2302.07344.pdf' target='_blank'>https://arxiv.org/pdf/2302.07344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Levi Cai, Nathan E. McGuire, Roger Hanlon, T. Aran Mooney, Yogesh Girdhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.07344">Semi-Supervised Visual Tracking of Marine Animals using Autonomous Underwater Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-situ visual observations of marine organisms is crucial to developing behavioural understandings and their relations to their surrounding ecosystem. Typically, these observations are collected via divers, tags, and remotely-operated or human-piloted vehicles. Recently, however, autonomous underwater vehicles equipped with cameras and embedded computers with GPU capabilities are being developed for a variety of applications, and in particular, can be used to supplement these existing data collection mechanisms where human operation or tags are more difficult. Existing approaches have focused on using fully-supervised tracking methods, but labelled data for many underwater species are severely lacking. Semi-supervised trackers may offer alternative tracking solutions because they require less data than fully-supervised counterparts. However, because there are not existing realistic underwater tracking datasets, the performance of semi-supervised tracking algorithms in the marine domain is not well understood. To better evaluate their performance and utility, in this paper we provide (1) a novel dataset specific to marine animals located at http://warp.whoi.edu/vmat/, (2) an evaluation of state-of-the-art semi-supervised algorithms in the context of underwater animal tracking, and (3) an evaluation of real-world performance through demonstrations using a semi-supervised algorithm on-board an autonomous underwater vehicle to track marine animals in the wild.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2302.03793.pdf' target='_blank'>https://arxiv.org/pdf/2302.03793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangxiao Lu, Ninad Khargonkar, Zesheng Xu, Charles Averill, Kamalesh Palanisamy, Kaiyu Hang, Yunhui Guo, Nicholas Ruozzi, Yu Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03793">Self-Supervised Unseen Object Instance Segmentation via Long-Term Robot Interaction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel robotic system for improving unseen object instance segmentation in the real world by leveraging long-term robot interaction with objects. Previous approaches either grasp or push an object and then obtain the segmentation mask of the grasped or pushed object after one action. Instead, our system defers the decision on segmenting objects after a sequence of robot pushing actions. By applying multi-object tracking and video object segmentation on the images collected via robot pushing, our system can generate segmentation masks of all the objects in these images in a self-supervised way. These include images where objects are very close to each other, and segmentation errors usually occur on these images for existing object segmentation networks. We demonstrate the usefulness of our system by fine-tuning segmentation networks trained on synthetic data with real-world data collected by our system. We show that, after fine-tuning, the segmentation accuracy of the networks is significantly improved both in the same domain and across different domains. In addition, we verify that the fine-tuned networks improve top-down robotic grasping of unseen objects in the real world.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2211.11629.pdf' target='_blank'>https://arxiv.org/pdf/2211.11629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Li, Ziyuan Huang, Junjie Ye, Yiming Li, Sebastian Scherer, Hang Zhao, Changhong Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11629">PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking is essential to intelligent robots. Most existing approaches have ignored the online latency that can cause severe performance degradation during real-world processing. Especially for unmanned aerial vehicles (UAVs), where robust tracking is more challenging and onboard computation is limited, the latency issue can be fatal. In this work, we present a simple framework for end-to-end latency-aware tracking, i.e., end-to-end predictive visual tracking (PVT++). Unlike existing solutions that naively append Kalman Filters after trackers, PVT++ can be jointly optimized, so that it takes not only motion information but can also leverage the rich visual knowledge in most pre-trained tracker models for robust prediction. Besides, to bridge the training-evaluation domain gap, we propose a relative motion factor, empowering PVT++ to generalize to the challenging and complex UAV tracking scenes. These careful designs have made the small-capacity lightweight PVT++ a widely effective solution. Additionally, this work presents an extended latency-aware evaluation benchmark for assessing an any-speed tracker in the online setting. Empirical results on a robotic platform from the aerial perspective show that PVT++ can achieve significant performance gain on various trackers and exhibit higher accuracy than prior solutions, largely mitigating the degradation brought by latency.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2509.13396.pdf' target='_blank'>https://arxiv.org/pdf/2509.13396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinan Wang, Di Shi, Fengyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13396">Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel three-stage framework for real-time foreign object intrusion (FOI) detection and tracking in power transmission systems. The framework integrates: (1) a YOLOv7 segmentation model for fast and robust object localization, (2) a ConvNeXt-based feature extractor trained with triplet loss to generate discriminative embeddings, and (3) a feature-assisted IoU tracker that ensures resilient multi-object tracking under occlusion and motion. To enable scalable field deployment, the pipeline is optimized for deployment on low-cost edge hardware using mixed-precision inference. The system supports incremental updates by adding embeddings from previously unseen objects into a reference database without requiring model retraining. Extensive experiments on real-world surveillance and drone video datasets demonstrate the framework's high accuracy and robustness across diverse FOI scenarios. In addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's practicality and scalability for real-world edge applications.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2509.13396.pdf' target='_blank'>https://arxiv.org/pdf/2509.13396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinan Wang, Di Shi, Fengyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13396">Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel three-stage framework for real-time foreign object intrusion (FOI) detection and tracking in power transmission systems. The framework integrates: (1) a YOLOv7 segmentation model for fast and robust object localization, (2) a ConvNeXt-based feature extractor trained with triplet loss to generate discriminative embeddings, and (3) a feature-assisted IoU tracker that ensures resilient multi-object tracking under occlusion and motion. To enable scalable field deployment, the pipeline is optimized for deployment on low-cost edge hardware using mixed-precision inference. The system supports incremental updates by adding embeddings from previously unseen objects into a reference database without requiring model retraining. Extensive experiments on real-world surveillance and drone video datasets demonstrate the framework's high accuracy and robustness across diverse FOI scenarios. In addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's practicality and scalability for real-world edge applications.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2507.23473.pdf' target='_blank'>https://arxiv.org/pdf/2507.23473.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bin Xie, Congxuan Zhang, Fagan Wang, Peng Liu, Feng Lu, Zhen Chen, Weiming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23473">CST Anti-UAV: A Thermal Infrared Benchmark for Tiny UAV Tracking in Complex Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread application of Unmanned Aerial Vehicles (UAVs) has raised serious public safety and privacy concerns, making UAV perception crucial for anti-UAV tasks. However, existing UAV tracking datasets predominantly feature conspicuous objects and lack diversity in scene complexity and attribute representation, limiting their applicability to real-world scenarios. To overcome these limitations, we present the CST Anti-UAV, a new thermal infrared dataset specifically designed for Single Object Tracking (SOT) in Complex Scenes with Tiny UAVs (CST). It contains 220 video sequences with over 240k high-quality bounding box annotations, highlighting two key properties: a significant number of tiny-sized UAV targets and the diverse and complex scenes. To the best of our knowledge, CST Anti-UAV is the first dataset to incorporate complete manual frame-level attribute annotations, enabling precise evaluations under varied challenges. To conduct an in-depth performance analysis for CST Anti-UAV, we evaluate 20 existing SOT methods on the proposed dataset. Experimental results demonstrate that tracking tiny UAVs in complex environments remains a challenge, as the state-of-the-art method achieves only 35.92% state accuracy, much lower than the 67.69% observed on the Anti-UAV410 dataset. These findings underscore the limitations of existing benchmarks and the need for further advancements in UAV tracking research. The CST Anti-UAV benchmark is about to be publicly released, which not only fosters the development of more robust SOT methods but also drives innovation in anti-UAV systems.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2507.21411.pdf' target='_blank'>https://arxiv.org/pdf/2507.21411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kentaro Takahira, Yue Yu, Takanori Fujiwara, Ryo Suzuki, Huamin Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21411">InSituTale: Enhancing Augmented Data Storytelling with Physical Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented data storytelling enhances narrative delivery by integrating visualizations with physical environments and presenter actions. Existing systems predominantly rely on body gestures or speech to control visualizations, leaving interactions with physical objects largely underexplored. We introduce augmented physical data storytelling, an approach enabling presenters to manipulate visualizations through physical object interactions. To inform this approach, we first conducted a survey of data-driven presentations to identify common visualization commands. We then conducted workshops with nine HCI/VIS researchers to collect mappings between physical manipulations and these commands. Guided by these insights, we developed InSituTale, a prototype that combines object tracking via a depth camera with Vision-LLM for detecting real-world events. Through physical manipulations, presenters can dynamically execute various visualization commands, delivering cohesive data storytelling experiences that blend physical and digital elements. A user study with 12 participants demonstrated that InSituTale enables intuitive interactions, offers high utility, and facilitates an engaging presentation experience.
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2507.12832.pdf' target='_blank'>https://arxiv.org/pdf/2507.12832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Kondo, Norimichi Ukita, Riku Kanayama, Yuki Yoshida, Takayuki Yamaguchi, Xiang Yu, Guang Liang, Xinyao Liu, Guan-Zhang Wang, Wei-Ta Chu, Bing-Cheng Chuang, Jia-Hua Lee, Pin-Tseng Kuo, I-Hsuan Chu, Yi-Shein Hsiao, Cheng-Han Wu, Po-Yi Wu, Jui-Chien Tsou, Hsuan-Chi Liu, Chun-Yi Lee, Yuan-Fu Yang, Kosuke Shigematsu, Asuka Shin, Ba Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12832">MVA 2025 Small Multi-Object Tracking for Spotting Birds Challenge: Dataset, Methods, and Results</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Small Multi-Object Tracking (SMOT) is particularly challenging when targets occupy only a few dozen pixels, rendering detection and appearance-based association unreliable. Building on the success of the MVA2023 SOD4SB challenge, this paper introduces the SMOT4SB challenge, which leverages temporal information to address limitations of single-frame detection. Our three main contributions are: (1) the SMOT4SB dataset, consisting of 211 UAV video sequences with 108,192 annotated frames under diverse real-world conditions, designed to capture motion entanglement where both camera and targets move freely in 3D; (2) SO-HOTA, a novel metric combining Dot Distance with HOTA to mitigate the sensitivity of IoU-based metrics to small displacements; and (3) a competitive MVA2025 challenge with 78 participants and 308 submissions, where the winning method achieved a 5.1x improvement over the baseline. This work lays a foundation for advancing SMOT in UAV scenarios with applications in bird strike avoidance, agriculture, fisheries, and ecological monitoring.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2507.02393.pdf' target='_blank'>https://arxiv.org/pdf/2507.02393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seokyeong Lee, Sithu Aung, Junyong Choi, Seungryong Kim, Ig-Jae Kim, Junghyun Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02393">PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Monocular 3D object detection (M3OD) has long faced challenges due to data scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity. Although various weakly supervised methods and pseudo-labeling methods have been proposed to address these issues, they are mostly limited by domain-specific learning or rely solely on shape information from a single observation. In this paper, we propose a novel pseudo-labeling framework that uses only video data and is more robust to occlusion, without requiring a multi-view setup, additional sensors, camera poses, or domain-specific training. Specifically, we explore a technique for aggregating the pseudo-LiDARs of both static and dynamic objects across temporally adjacent frames using object point tracking, enabling 3D attribute extraction in scenarios where 3D data acquisition is infeasible. Extensive experiments demonstrate that our method ensures reliable accuracy and strong scalability, making it a practical and effective solution for M3OD.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2504.04097.pdf' target='_blank'>https://arxiv.org/pdf/2504.04097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohang Han, Matti Vahs, Jana Tumova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04097">Risk-Aware Robot Control in Dynamic Environments Using Belief Control Barrier Functions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring safety for autonomous robots operating in dynamic environments can be challenging due to factors such as unmodeled dynamics, noisy sensor measurements, and partial observability. To account for these limitations, it is common to maintain a belief distribution over the true state. This belief could be a non-parametric, sample-based representation to capture uncertainty more flexibly. In this paper, we propose a novel form of Belief Control Barrier Functions (BCBFs) specifically designed to ensure safety in dynamic environments under stochastic dynamics and a sample-based belief about the environment state. Our approach incorporates provable concentration bounds on tail risk measures into BCBFs, effectively addressing possible multimodal and skewed belief distributions represented by samples. Moreover, the proposed method demonstrates robustness against distributional shifts up to a predefined bound. We validate the effectiveness and real-time performance (approximately 1kHz) of the proposed method through two simulated underwater robotic applications: object tracking and dynamic collision avoidance.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2503.13023.pdf' target='_blank'>https://arxiv.org/pdf/2503.13023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michal Danilowicz, Tomasz Kryjak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13023">Real-Time Multi-Object Tracking using YOLOv8 and SORT on a SoC FPGA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is one of the most important problems in computer vision and a key component of any vision-based perception system used in advanced autonomous mobile robotics. Therefore, its implementation on low-power and real-time embedded platforms is highly desirable. Modern MOT algorithms should be able to track objects of a given class (e.g. people or vehicles). In addition, the number of objects to be tracked is not known in advance, and they may appear and disappear at any time, as well as be obscured. For these reasons, the most popular and successful approaches have recently been based on the tracking paradigm. Therefore, the presence of a high quality object detector is essential, which in practice accounts for the vast majority of the computational and memory complexity of the whole MOT system. In this paper, we propose an FPGA (Field-Programmable Gate Array) implementation of an embedded MOT system based on a quantized YOLOv8 detector and the SORT (Simple Online Realtime Tracker) tracker. We use a modified version of the FINN framework to utilize external memory for model parameters and to support operations necessary required by YOLOv8. We discuss the evaluation of detection and tracking performance using the COCO and MOT15 datasets, where we achieve 0.21 mAP and 38.9 MOTA respectively. As the computational platform, we use an MPSoC system (Zynq UltraScale+ device from AMD/Xilinx) where the detector is deployed in reprogrammable logic and the tracking algorithm is implemented in the processor system.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2501.07133.pdf' target='_blank'>https://arxiv.org/pdf/2501.07133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiantong Zhao, Xiuping Liu, Shengjing Tian, Yinan Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.07133">Robust Single Object Tracking in LiDAR Point Clouds under Adverse Weather Conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking (3DSOT) in LiDAR point clouds is a critical task for outdoor perception, enabling real-time perception of object location, orientation, and motion. Despite the impressive performance of current 3DSOT methods, evaluating them on clean datasets inadequately reflects their comprehensive performance, as the adverse weather conditions in real-world surroundings has not been considered. One of the main obstacles is the lack of adverse weather benchmarks for the evaluation of 3DSOT. To this end, this work proposes a challenging benchmark for LiDAR-based 3DSOT in adverse weather, which comprises two synthetic datasets (KITTI-A and nuScenes-A) and one real-world dataset (CADC-SOT) spanning three weather types: rain, fog, and snow. Based on this benchmark, five representative 3D trackers from different tracking frameworks conducted robustness evaluation, resulting in significant performance degradations. This prompts the question: What are the factors that cause current advanced methods to fail on such adverse weather samples? Consequently, we explore the impacts of adverse weather and answer the above question from three perspectives: 1) target distance; 2) template shape corruption; and 3) target shape corruption. Finally, based on domain randomization and contrastive learning, we designed a dual-branch tracking framework for adverse weather, named DRCT, achieving excellent performance in benchmarks.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2411.19167.pdf' target='_blank'>https://arxiv.org/pdf/2411.19167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Shangchen Han, Fan Zhang, Linguang Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19167">HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2411.08433.pdf' target='_blank'>https://arxiv.org/pdf/2411.08433.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxiang Wang, Jiaxin Liu, Miaojie Feng, Zhaoxing Zhang, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08433">3D Multi-Object Tracking with Semi-Supervised GRU-Kalman Filter</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT), a fundamental component of environmental perception, is essential for intelligent systems like autonomous driving and robotic sensing. Although Tracking-by-Detection frameworks have demonstrated excellent performance in recent years, their application in real-world scenarios faces significant challenges. Object movement in complex environments is often highly nonlinear, while existing methods typically rely on linear approximations of motion. Furthermore, system noise is frequently modeled as a Gaussian distribution, which fails to capture the true complexity of the noise dynamics. These oversimplified modeling assumptions can lead to significant reductions in tracking precision. To address this, we propose a GRU-based MOT method, which introduces a learnable Kalman filter into the motion module. This approach is able to learn object motion characteristics through data-driven learning, thereby avoiding the need for manual model design and model error. At the same time, to avoid abnormal supervision caused by the wrong association between annotations and trajectories, we design a semi-supervised learning strategy to accelerate the convergence speed and improve the robustness of the model. Evaluation experiment on the nuScenes and Argoverse2 datasets demonstrates that our system exhibits superior performance and significant potential compared to traditional TBD methods.
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2411.00608.pdf' target='_blank'>https://arxiv.org/pdf/2411.00608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Cheng Chen, Yuan-yao Lou, Mustafa Abdallah, Kwang Taik Kim, Saurabh Bagchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00608">HopTrack: A Real-time Multi-Object Tracking System for Embedded Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) poses significant challenges in computer vision. Despite its wide application in robotics, autonomous driving, and smart manufacturing, there is limited literature addressing the specific challenges of running MOT on embedded devices. State-of-the-art MOT trackers designed for high-end GPUs often experience low processing rates (<11fps) when deployed on embedded devices. Existing MOT frameworks for embedded devices proposed strategies such as fusing the detector model with the feature embedding model to reduce inference latency or combining different trackers to improve tracking accuracy, but tend to compromise one for the other. This paper introduces HopTrack, a real-time multi-object tracking system tailored for embedded devices. Our system employs a novel discretized static and dynamic matching approach along with an innovative content-aware dynamic sampling technique to enhance tracking accuracy while meeting the real-time requirement. Compared with the best high-end GPU modified baseline Byte (Embed) and the best existing baseline on embedded devices MobileNet-JDE, HopTrack achieves a processing speed of up to 39.29 fps on NVIDIA AGX Xavier with a multi-object tracking accuracy (MOTA) of up to 63.12% on the MOT16 benchmark, outperforming both counterparts by 2.15% and 4.82%, respectively. Additionally, the accuracy improvement is coupled with the reduction in energy consumption (20.8%), power (5%), and memory usage (8%), which are crucial resources on embedded devices. HopTrack is also detector agnostic allowing the flexibility of plug-and-play.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2410.20893.pdf' target='_blank'>https://arxiv.org/pdf/2410.20893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengjing Tian, Yinan Han, Xiantong Zhao, Bin Liu, Xiuping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20893">Evaluating the Robustness of LiDAR Point Cloud Tracking Against Adversarial Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we delve into the robustness of neural network-based LiDAR point cloud tracking models under adversarial attacks, a critical aspect often overlooked in favor of performance enhancement. These models, despite incorporating advanced architectures like Transformer or Bird's Eye View (BEV), tend to neglect robustness in the face of challenges such as adversarial attacks, domain shifts, or data corruption. We instead focus on the robustness of the tracking models under the threat of adversarial attacks. We begin by establishing a unified framework for conducting adversarial attacks within the context of 3D object tracking, which allows us to thoroughly investigate both white-box and black-box attack strategies. For white-box attacks, we tailor specific loss functions to accommodate various tracking paradigms and extend existing methods such as FGSM, C\&W, and PGD to the point cloud domain. In addressing black-box attack scenarios, we introduce a novel transfer-based approach, the Target-aware Perturbation Generation (TAPG) algorithm, with the dual objectives of achieving high attack performance and maintaining low perceptibility. This method employs a heuristic strategy to enforce sparse attack constraints and utilizes random sub-vector factorization to bolster transferability. Our experimental findings reveal a significant vulnerability in advanced tracking methods when subjected to both black-box and white-box attacks, underscoring the necessity for incorporating robustness against adversarial attacks into the design of LiDAR point cloud tracking models. Notably, compared to existing methods, the TAPG also strikes an optimal balance between the effectiveness of the attack and the concealment of the perturbations.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2410.15518.pdf' target='_blank'>https://arxiv.org/pdf/2410.15518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thinh Phan, Isaac Phillips, Andrew Lockett, Michael T. Kidd, Ngan Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15518">TrackMe:A Simple and Effective Multiple Object Tracking Annotation Tool</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking, especially animal tracking, is one of the key topics that attract a lot of attention due to its benefits of animal behavior understanding and monitoring. Recent state-of-the-art tracking methods are founded on deep learning architectures for object detection, appearance feature extraction and track association. Despite the good tracking performance, these methods are trained and evaluated on common objects such as human and cars. To perform on the animal, there is a need to create large datasets of different types in multiple conditions. The dataset construction comprises of data collection and data annotation. In this work, we put more focus on the latter task. Particularly, we renovate the well-known tool, LabelMe, so as to assist common user with or without in-depth knowledge about computer science to annotate the data with less effort. The new tool named as TrackMe inherits the simplicity, high compatibility with varied systems, minimal hardware requirement and convenient feature utilization from the predecessor. TrackMe is an upgraded version with essential features for multiple object tracking annotation.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2410.14093.pdf' target='_blank'>https://arxiv.org/pdf/2410.14093.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kosuke Tatsumura, Yohei Hamakawa, Masaya Yamasaki, Koji Oya, Hiroshi Fujimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14093">Enhancing In-vehicle Multiple Object Tracking Systems with Embeddable Ising Machines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A cognitive function of tracking multiple objects, needed in autonomous mobile vehicles, comprises object detection and their temporal association. While great progress owing to machine learning has been recently seen for elaborating the similarity matrix between the objects that have been recognized and the objects detected in a current video frame, less for the assignment problem that finally determines the temporal association, which is a combinatorial optimization problem. Here we show an in-vehicle multiple object tracking system with a flexible assignment function for tracking through multiple long-term occlusion events. To solve the flexible assignment problem formulated as a nondeterministic polynomial time-hard problem, the system relies on an embeddable Ising machine based on a quantum-inspired algorithm called simulated bifurcation. Using a vehicle-mountable computing platform, we demonstrate a realtime system-wide throughput (23 frames per second on average) with the enhanced functionality.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2410.10527.pdf' target='_blank'>https://arxiv.org/pdf/2410.10527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqing Guo, Canlun Zheng, Shiyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10527">Motion-guided small MAV detection in complex and non-planar scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been a growing interest in the visual detection of micro aerial vehicles (MAVs) due to its importance in numerous applications. However, the existing methods based on either appearance or motion features encounter difficulties when the background is complex or the MAV is too small. In this paper, we propose a novel motion-guided MAV detector that can accurately identify small MAVs in complex and non-planar scenes. This detector first exploits a motion feature enhancement module to capture the motion features of small MAVs. Then it uses multi-object tracking and trajectory filtering to eliminate false positives caused by motion parallax. Finally, an appearance-based classifier and an appearance-based detector that operates on the cropped regions are used to achieve precise detection results. Our proposed method can effectively and efficiently detect extremely small MAVs from dynamic and complex backgrounds because it aggregates pixel-level motion features and eliminates false positives based on the motion and appearance features of MAVs. Experiments on the ARD-MAV dataset demonstrate that the proposed method could achieve high performance in small MAV detection under challenging conditions and outperform other state-of-the-art methods across various metrics
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2409.16111.pdf' target='_blank'>https://arxiv.org/pdf/2409.16111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yannik Blei, Michael Krawez, Nisarga Nilavadi, Tanja Katharina Kaiser, Wolfram Burgard
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16111">CloudTrack: Scalable UAV Tracking with Cloud Semantics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and rescue scenarios to gather information in the search area. The automatic identification of the person searched for in aerial footage could increase the autonomy of such systems, reduce the search time, and thus increase the missed person's chances of survival. In this paper, we present a novel approach to perform semantically conditioned open vocabulary object tracking that is specifically designed to cope with the limitations of UAV hardware. Our approach has several advantages. It can run with verbal descriptions of the missing person, e.g., the color of the shirt, it does not require dedicated training to execute the mission and can efficiently track a potentially moving person. Our experimental results demonstrate the versatility and efficacy of our approach.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2408.07157.pdf' target='_blank'>https://arxiv.org/pdf/2408.07157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Alotaibi, Brian L. Mark, Mohammad Reza Fasihi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07157">Object Tracking Incorporating Transfer Learning into Unscented and Cubature Kalman Filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel filtering algorithm that employs Bayesian transfer learning to address the challenges posed by mismatched intensity of the noise in a pair of sensors, each of which tracks an object using a nonlinear dynamic system model. In this setting, the primary sensor experiences a higher noise intensity in tracking the object than the source sensor. To improve the estimation accuracy of the primary sensor, we propose a framework that integrates Bayesian transfer learning into an Unscented Kalman Filter (UKF) and a Cubature Kalman Filter (CKF). In this approach, the parameters of the predicted observations in the source sensor are transferred to the primary sensor and used as an additional prior in the filtering process. Our simulation results show that the transfer learning approach significantly outperforms the conventional isolated UKF and CKF. Comparisons to a form of measurement vector fusion are also presented.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2408.07157.pdf' target='_blank'>https://arxiv.org/pdf/2408.07157.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Alotaibi, Brian L. Mark, Mohammad Reza Fasihi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07157">Object Tracking Incorporating Transfer Learning into Unscented and Cubature Kalman Filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel filtering algorithm that employs Bayesian transfer learning to address the challenges posed by mismatched intensity of the noise in a pair of sensors, each of which tracks an object using a nonlinear dynamic system model. In this setting, the primary sensor experiences a higher noise intensity in tracking the object than the source sensor. To improve the estimation accuracy of the primary sensor, we propose a framework that integrates Bayesian transfer learning into an Unscented Kalman Filter (UKF) and a Cubature Kalman Filter (CKF). In this approach, the parameters of the predicted observations in the source sensor are transferred to the primary sensor and used as an additional prior in the filtering process. Our simulation results show that the transfer learning approach significantly outperforms the conventional isolated UKF and CKF. Comparisons to a form of measurement vector fusion are also presented.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2407.08049.pdf' target='_blank'>https://arxiv.org/pdf/2407.08049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lei Cheng, Arindam Sengupta, Siyang Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08049">Deep Learning-Based Robust Multi-Object Tracking via Fusion of mmWave Radar and Camera Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving holds great promise in addressing traffic safety concerns by leveraging artificial intelligence and sensor technology. Multi-Object Tracking plays a critical role in ensuring safer and more efficient navigation through complex traffic scenarios. This paper presents a novel deep learning-based method that integrates radar and camera data to enhance the accuracy and robustness of Multi-Object Tracking in autonomous driving systems. The proposed method leverages a Bi-directional Long Short-Term Memory network to incorporate long-term temporal information and improve motion prediction. An appearance feature model inspired by FaceNet is used to establish associations between objects across different frames, ensuring consistent tracking. A tri-output mechanism is employed, consisting of individual outputs for radar and camera sensors and a fusion output, to provide robustness against sensor failures and produce accurate tracking results. Through extensive evaluations of real-world datasets, our approach demonstrates remarkable improvements in tracking accuracy, ensuring reliable performance even in low-visibility scenarios.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2407.04170.pdf' target='_blank'>https://arxiv.org/pdf/2407.04170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markus Krimmel, Jan Achterhold, Joerg Stueckler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04170">Attention Normalization Impacts Cardinality Generalization in Slot Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object-centric scene decompositions are important representations for downstream tasks in fields such as computer vision and robotics. The recently proposed Slot Attention module, already leveraged by several derivative works for image segmentation and object tracking in videos, is a deep learning component which performs unsupervised object-centric scene decomposition on input images. It is based on an attention architecture, in which latent slot vectors, which hold compressed information on objects, attend to localized perceptual features from the input image. In this paper, we demonstrate that design decisions on normalizing the aggregated values in the attention architecture have considerable impact on the capabilities of Slot Attention to generalize to a higher number of slots and objects as seen during training. We propose and investigate alternatives to the original normalization scheme which increase the generalization capabilities of Slot Attention to varying slot and object counts, resulting in performance gains on the task of unsupervised image segmentation. The newly proposed normalizations represent minimal and easy to implement modifications of the usual Slot Attention module, changing the value aggregation mechanism from a weighted mean operation to a scaled weighted sum operation.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2406.18414.pdf' target='_blank'>https://arxiv.org/pdf/2406.18414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kemiao Huang, Yinqi Chen, Meiying Zhang, Qi Hao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.18414">BiTrack: Bidirectional Offline 3D Multi-Object Tracking Using Camera-LiDAR Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compared with real-time multi-object tracking (MOT), offline multi-object tracking (OMOT) has the advantages to perform 2D-3D detection fusion, erroneous link correction, and full track optimization but has to deal with the challenges from bounding box misalignment and track evaluation, editing, and refinement. This paper proposes "BiTrack", a 3D OMOT framework that includes modules of 2D-3D detection fusion, initial trajectory generation, and bidirectional trajectory re-optimization to achieve optimal tracking results from camera-LiDAR data. The novelty of this paper includes threefold: (1) development of a point-level object registration technique that employs a density-based similarity metric to achieve accurate fusion of 2D-3D detection results; (2) development of a set of data association and track management skills that utilizes a vertex-based similarity metric as well as false alarm rejection and track recovery mechanisms to generate reliable bidirectional object trajectories; (3) development of a trajectory re-optimization scheme that re-organizes track fragments of different fidelities in a greedy fashion, as well as refines each trajectory with completion and smoothing techniques. The experiment results on the KITTI dataset demonstrate that BiTrack achieves the state-of-the-art performance for 3D OMOT tasks in terms of accuracy and efficiency.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2404.05136.pdf' target='_blank'>https://arxiv.org/pdf/2404.05136.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijia Lu, Bing Shuai, Yanbei Chen, Zhenlin Xu, Davide Modolo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05136">Self-Supervised Multi-Object Tracking with Path Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel concept of path consistency to learn robust object matching without using manual object identity supervision. Our key idea is that, to track a object through frames, we can obtain multiple different association results from a model by varying the frames it can observe, i.e., skipping frames in observation. As the differences in observations do not alter the identities of objects, the obtained association results should be consistent. Based on this rationale, we generate multiple observation paths, each specifying a different set of frames to be skipped, and formulate the Path Consistency Loss that enforces the association results are consistent across different observation paths. We use the proposed loss to train our object matching model with only self-supervision. By extensive experiments on three tracking datasets (MOT17, PersonPath22, KITTI), we demonstrate that our method outperforms existing unsupervised methods with consistent margins on various evaluation metrics, and even achieves performance close to supervised methods.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2403.04112.pdf' target='_blank'>https://arxiv.org/pdf/2403.04112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Pieroni, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04112">Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel multi-modal Multi-Object Tracking (MOT) algorithm for self-driving cars that combines camera and LiDAR data. Camera frames are processed with a state-of-the-art 3D object detector, whereas classical clustering techniques are used to process LiDAR observations. The proposed MOT algorithm comprises a three-step association process, an Extended Kalman filter for estimating the motion of each detected dynamic obstacle, and a track management phase. The EKF motion model requires the current measured relative position and orientation of the observed object and the longitudinal and angular velocities of the ego vehicle as inputs. Unlike most state-of-the-art multi-modal MOT approaches, the proposed algorithm does not rely on maps or knowledge of the ego global pose. Moreover, it uses a 3D detector exclusively for cameras and is agnostic to the type of LiDAR sensor used. The algorithm is validated both in simulation and with real-world data, with satisfactory results.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2401.13285.pdf' target='_blank'>https://arxiv.org/pdf/2401.13285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengjing Tian, Yinan Han, Xiuping Liu, Xiantong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13285">Small Object Tracking in LiDAR Point Cloud: Learning the Target-awareness Prototype and Fine-grained Search Region</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single Object Tracking in LiDAR point cloud is one of the most essential parts of environmental perception, in which small objects are inevitable in real-world scenarios and will bring a significant barrier to the accurate location. However, the existing methods concentrate more on exploring universal architectures for common categories and overlook the challenges that small objects have long been thorny due to the relative deficiency of foreground points and a low tolerance for disturbances. To this end, we propose a Siamese network-based method for small object tracking in the LiDAR point cloud, which is composed of the target-awareness prototype mining (TAPM) module and the regional grid subdivision (RGS) module. The TAPM module adopts the reconstruction mechanism of the masked decoder to learn the prototype in the feature space, aiming to highlight the presence of foreground points that will facilitate the subsequent location of small objects. Through the above prototype is capable of accentuating the small object of interest, the positioning deviation in feature maps still leads to high tracking errors. To alleviate this issue, the RGS module is proposed to recover the fine-grained features of the search region based on ViT and pixel shuffle layers. In addition, apart from the normal settings, we elaborately design a scaling experiment to evaluate the robustness of the different trackers on small objects. Extensive experiments on KITTI and nuScenes demonstrate that our method can effectively improve the tracking performance of small targets without affecting normal-sized objects.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2312.09723.pdf' target='_blank'>https://arxiv.org/pdf/2312.09723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Dunnhofer, Luca Sordi, Niki Martinel, Christian Micheloni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09723">Tracking Skiers from the Top to the Bottom</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Skiing is a popular winter sport discipline with a long history of competitive events. In this domain, computer vision has the potential to enhance the understanding of athletes' performance, but its application lags behind other sports due to limited studies and datasets. This paper makes a step forward in filling such gaps. A thorough investigation is performed on the task of skier tracking in a video capturing his/her complete performance. Obtaining continuous and accurate skier localization is preemptive for further higher-level performance analyses. To enable the study, the largest and most annotated dataset for computer vision in skiing, SkiTB, is introduced. Several visual object tracking algorithms, including both established methodologies and a newly introduced skier-optimized baseline algorithm, are tested using the dataset. The results provide valuable insights into the applicability of different tracking methods for vision-based skiing analysis. SkiTB, code, and results are available at https://machinelearning.uniud.it/datasets/skitb.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2312.04167.pdf' target='_blank'>https://arxiv.org/pdf/2312.04167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Lin, Laurent Girin, Xavier Alameda-Pineda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04167">Mixture of Dynamical Variational Autoencoders for Multi-Source Trajectory Modeling and Separation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a latent-variable generative model called mixture of dynamical variational autoencoders (MixDVAE) to model the dynamics of a system composed of multiple moving sources. A DVAE model is pre-trained on a single-source dataset to capture the source dynamics. Then, multiple instances of the pre-trained DVAE model are integrated into a multi-source mixture model with a discrete observation-to-source assignment latent variable. The posterior distributions of both the discrete observation-to-source assignment variable and the continuous DVAE variables representing the sources content/position are estimated using a variational expectation-maximization algorithm, leading to multi-source trajectories estimation. We illustrate the versatility of the proposed MixDVAE model on two tasks: a computer vision task, namely multi-object tracking, and an audio processing task, namely single-channel audio source separation. Experimental results show that the proposed method works well on these two tasks, and outperforms several baseline methods.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2311.12592.pdf' target='_blank'>https://arxiv.org/pdf/2311.12592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changxing Huang, Nanlin Shi, Yining Miao, Xiaogang Chen, Yijun Wang, Xiaorong Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12592">Visual tracking brain computer interface</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Brain-computer interfaces (BCIs) offer a way to interact with computers without relying on physical movements. Non-invasive electroencephalography (EEG)-based visual BCIs, known for efficient speed and calibration ease, face limitations in continuous tasks due to discrete stimulus design and decoding methods. To achieve continuous control, we implemented a novel spatial encoding stimulus paradigm and devised a corresponding projection method to enable continuous modulation of decoded velocity. Subsequently, we conducted experiments involving 17 participants and achieved Fitt's ITR of 0.55 bps for the fixed tracking task and 0.37 bps for the random tracking task. The proposed BCI with a high Fitt's ITR was then integrated into two applications, including painting and gaming. In conclusion, this study proposed a visual BCI-based control method to go beyond discrete commands, allowing natural continuous control based on neural activity.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2308.15795.pdf' target='_blank'>https://arxiv.org/pdf/2308.15795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Su, Ruizhou Sun, Xin Shu, Yu Zhang, Qingyao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15795">Occlusion-Aware Detection and Re-ID Calibrated Network for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) is a crucial computer vision task that aims to predict the bounding boxes and identities of objects simultaneously. While state-of-the-art methods have made remarkable progress by jointly optimizing the multi-task problems of detection and Re-ID feature learning, yet, few approaches explore to tackle the occlusion issue, which is a long-standing challenge in the MOT field. Generally, occluded objects may hinder the detector from estimating the bounding boxes, resulting in fragmented trajectories. And the learned occluded Re-ID embeddings are less distinct since they contain interferer. To this end, we propose an occlusion-aware detection and Re-ID calibrated network for multi-object tracking, termed as ORCTrack. Specifically, we propose an Occlusion-Aware Attention (OAA) module in the detector that highlights the object features while suppressing the occluded background regions. OAA can serve as a modulator that enhances the detector for some potentially occluded objects. Furthermore, we design a Re-ID embedding matching block based on the optimal transport problem, which focuses on enhancing and calibrating the Re-ID representations through different adjacent frames complementarily. To validate the effectiveness of the proposed method, extensive experiments are conducted on two challenging VisDrone2021-MOT and KITTI benchmarks. Experimental evaluations demonstrate the superiority of our approach, which can achieve new state-of-the-art performance and enjoy high run-time efficiency.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2305.10210.pdf' target='_blank'>https://arxiv.org/pdf/2305.10210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin ThÃ©rien, Chengjie Huang, Adrian Chow, Krzysztof Czarnecki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10210">Object Re-Identification from Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object re-identification (ReID) from images plays a critical role in application domains of image retrieval (surveillance, retail analytics, etc.) and multi-object tracking (autonomous driving, robotics, etc.). However, systems that additionally or exclusively perceive the world from depth sensors are becoming more commonplace without any corresponding methods for object ReID. In this work, we fill the gap by providing the first large-scale study of object ReID from point clouds and establishing its performance relative to image ReID. To enable such a study, we create two large-scale ReID datasets with paired image and LiDAR observations and propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in Siamese style, our proposed point cloud ReID networks can make thousands of pairwise comparisons in real-time ($10$ Hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Our strongest network trained at the largest scale achieves ReID accuracy exceeding $90\%$ for rigid objects and $85\%$ for deformable objects (without any explicit skeleton normalization). To our knowledge, we are the first to study object re-identification from real point cloud observations.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2305.03052.pdf' target='_blank'>https://arxiv.org/pdf/2305.03052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li, Carl Vondrick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03052">Tracking through Containers and Occluders in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce $\textbf{TCOW}$, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2211.04517.pdf' target='_blank'>https://arxiv.org/pdf/2211.04517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Russell Buchanan, Varun Agrawal, Marco Camurri, Frank Dellaert, Maurice Fallon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.04517">Deep IMU Bias Inference for Robust Visual-Inertial Odometry with Factor Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Inertial Odometry (VIO) is one of the most established state estimation methods for mobile platforms. However, when visual tracking fails, VIO algorithms quickly diverge due to rapid error accumulation during inertial data integration. This error is typically modeled as a combination of additive Gaussian noise and a slowly changing bias which evolves as a random walk. In this work, we propose to train a neural network to learn the true bias evolution. We implement and compare two common sequential deep learning architectures: LSTMs and Transformers. Our approach follows from recent learning-based inertial estimators, but, instead of learning a motion model, we target IMU bias explicitly, which allows us to generalize to locomotion patterns unseen in training. We show that our proposed method improves state estimation in visually challenging situations across a wide range of motions by quadrupedal robots, walking humans, and drones. Our experiments show an average 15% reduction in drift rate, with much larger reductions when there is total vision failure. Importantly, we also demonstrate that models trained with one locomotion pattern (human walking) can be applied to another (quadruped robot trotting) without retraining.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2210.08518.pdf' target='_blank'>https://arxiv.org/pdf/2210.08518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiantong Zhao, Yinan Han, Shengjing Tian, Jian Liu, Xiuping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.08518">OST: Efficient One-stream Network for 3D Single Object Tracking in Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although recent Siamese network-based trackers have achieved impressive perceptual accuracy for single object tracking in LiDAR point clouds, they usually utilized heavy correlation operations to capture category-level characteristics only, and overlook the inherent merit of arbitrariness in contrast to multiple object tracking. In this work, we propose a radically novel one-stream network with the strength of the instance-level encoding, which avoids the correlation operations occurring in previous Siamese network, thus considerably reducing the computational effort. In particular, the proposed method mainly consists of a Template-aware Transformer Module (TTM) and a Multi-scale Feature Aggregation (MFA) module capable of fusing spatial and semantic information. The TTM stitches the specified template and the search region together and leverages an attention mechanism to establish the information flow, breaking the previous pattern of independent \textit{extraction-and-correlation}. As a result, this module makes it possible to directly generate template-aware features that are suitable for the arbitrary and continuously changing nature of the target, enabling the model to deal with unseen categories. In addition, the MFA is proposed to make spatial and semantic information complementary to each other, which is characterized by reverse directional feature propagation that aggregates information from shallow to deep layers. Extensive experiments on KITTI and nuScenes demonstrate that our method has achieved considerable performance not only for class-specific tracking but also for class-agnostic tracking with less computation and higher efficiency.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2202.13524.pdf' target='_blank'>https://arxiv.org/pdf/2202.13524.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengjing Tian, Jun Liu, Xiuping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.13524">Towards Class-agnostic Tracking Using Feature Decorrelation in Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking in point clouds has been attracting more and more attention owing to the presence of LiDAR sensors in 3D vision. However, the existing methods based on deep neural networks focus mainly on training different models for different categories, which makes them unable to perform well in real-world applications when encountering classes unseen during the training phase. In this work, we investigate a more challenging task in the LiDAR point clouds, class-agnostic tracking, where a general model is supposed to be learned for any specified targets of both observed and unseen categories. In particular, we first investigate the class-agnostic performances of the state-of-the-art trackers via exposing the unseen categories to them during testing, finding that a key factor for class-agnostic tracking is how to constrain fused features between the template and search region to maintain generalization when the distribution is shifted from observed to unseen classes. Therefore, we propose a feature decorrelation method to address this problem, which eliminates the spurious correlations of the fused features through a set of learned weights and further makes the search region consistent among foreground points and distinctive between foreground and background points. Experiments on the KITTI and NuScenes demonstrate that the proposed method can achieve considerable improvements by benchmarking against the advanced trackers P2B and BAT, especially when tracking unseen objects.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2508.12982.pdf' target='_blank'>https://arxiv.org/pdf/2508.12982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrejÄÃ­, OndÅej Straka, Petr Girg, JiÅÃ­ Benedikt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12982">Revisiting Functional Derivatives in Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probability generating functionals (PGFLs) are efficient and powerful tools for tracking independent objects in clutter. It was shown that PGFLs could be used for the elegant derivation of practical multi-object tracking algorithms, e.g., the probability hypothesis density (PHD) filter. However, derivations using PGFLs use the so-called functional derivatives whose definitions usually appear too complicated or heuristic, involving Dirac delta ``functions''. This paper begins by comparing different definitions of functional derivatives and exploring their relationships and implications for practical applications. It then proposes a rigorous definition of the functional derivative, utilizing straightforward yet precise mathematics for clarity. Key properties of the functional derivative are revealed and discussed.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2507.18594.pdf' target='_blank'>https://arxiv.org/pdf/2507.18594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuecheng Bai, Yuxiang Wang, Boyu Hu, Qinyuan Jie, Chuanzhi Xu, Hongru Xiao, Kechen Li, Vera Chung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18594">DRWKV: Focusing on Object Edges for Low-Light Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2507.16015.pdf' target='_blank'>https://arxiv.org/pdf/2507.16015.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Dunnhofer, Zaira Manigrasso, Christian Micheloni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16015">Is Tracking really more challenging in First Person Egocentric Vision?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking and segmentation are becoming fundamental tasks for understanding human activities in egocentric vision. Recent research has benchmarked state-of-the-art methods and concluded that first person egocentric vision presents challenges compared to previously studied domains. However, these claims are based on evaluations conducted across significantly different scenarios. Many of the challenging characteristics attributed to egocentric vision are also present in third person videos of human-object activities. This raises a critical question: how much of the observed performance drop stems from the unique first person viewpoint inherent to egocentric vision versus the domain of human-object activities? To address this question, we introduce a new benchmark study designed to disentangle such factors. Our evaluation strategy enables a more precise separation of challenges related to the first person perspective from those linked to the broader domain of human-object activity understanding. By doing so, we provide deeper insights into the true sources of difficulty in egocentric tracking and segmentation, facilitating more targeted advancements on this task.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2505.20455.pdf' target='_blank'>https://arxiv.org/pdf/2505.20455.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Hong, Anthony Liang, Kevin Kim, Harshitha Rajaprakash, Jesse Thomason, Erdem BÄ±yÄ±k, Jesse Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.20455">HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We hand the community HAND, a simple and time-efficient method for teaching robots new manipulation tasks through human hand demonstrations. Instead of relying on task-specific robot demonstrations collected via teleoperation, HAND uses easy-to-provide hand demonstrations to retrieve relevant behaviors from task-agnostic robot play data. Using a visual tracking pipeline, HAND extracts the motion of the human hand from the hand demonstration and retrieves robot sub-trajectories in two stages: first filtering by visual similarity, then retrieving trajectories with similar behaviors to the hand. Fine-tuning a policy on the retrieved data enables real-time learning of tasks in under four minutes, without requiring calibrated cameras or detailed hand pose estimation. Experiments also show that HAND outperforms retrieval baselines by over 2x in average task success rates on real robots. Videos can be found at our project website: https://liralab.usc.edu/handretrieval/.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2504.20391.pdf' target='_blank'>https://arxiv.org/pdf/2504.20391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tran Thien Dat Nguyen, Ba Tuong Vo, Ba-Ngu Vo, Hoa Van Nguyen, Changbeom Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20391">The Mean of Multi-Object Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the concept of a mean for trajectories and multi-object trajectories (defined as sets or multi-sets of trajectories) along with algorithms for computing them. Specifically, we use the FrÃ©chet mean, and metrics based on the optimal sub-pattern assignment (OSPA) construct, to extend the notion of average from vectors to trajectories and multi-object trajectories. Further, we develop efficient algorithms to compute these means using greedy search and Gibbs sampling. Using distributed multi-object tracking as an application, we demonstrate that the FrÃ©chet mean approach to multi-object trajectory consensus significantly outperforms state-of-the-art distributed multi-object tracking methods.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2504.20391.pdf' target='_blank'>https://arxiv.org/pdf/2504.20391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tran Thien Dat Nguyen, Ba Tuong Vo, Ba-Ngu Vo, Hoa Van Nguyen, Changbeom Shim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20391">The Mean of Multi-Object Trajectories</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces the concept of a mean for trajectories and multi-object trajectories (defined as sets or multi-sets of trajectories) along with algorithms for computing them. Specifically, we use the FrÃ©chet mean, and metrics based on the optimal sub-pattern assignment (OSPA) construct, to extend the notion of average from vectors to trajectories and multi-object trajectories. Further, we develop efficient algorithms to compute these means using greedy search and Gibbs sampling. Using distributed multi-object tracking as an application, we demonstrate that the FrÃ©chet mean approach to multi-object trajectory consensus significantly outperforms state-of-the-art distributed multi-object tracking methods.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2504.03047.pdf' target='_blank'>https://arxiv.org/pdf/2504.03047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reef Alturki, Adrian Hilton, Jean-Yves Guillemaut
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03047">Attention-Aware Multi-View Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In spite of the recent advancements in multi-object tracking, occlusion poses a significant challenge. Multi-camera setups have been used to address this challenge by providing a comprehensive coverage of the scene. Recent multi-view pedestrian detection models have highlighted the potential of an early-fusion strategy, projecting feature maps of all views to a common ground plane or the Bird's Eye View (BEV), and then performing detection. This strategy has been shown to improve both detection and tracking performance. However, the perspective transformation results in significant distortion on the ground plane, affecting the robustness of the appearance features of the pedestrians. To tackle this limitation, we propose a novel model that incorporates attention mechanisms in a multi-view pedestrian tracking scenario. Our model utilizes an early-fusion strategy for detection, and a cross-attention mechanism to establish robust associations between pedestrians in different frames, while efficiently propagating pedestrian features across frames, resulting in a more robust feature representation for each pedestrian. Extensive experiments demonstrate that our model outperforms state-of-the-art models, with an IDF1 score of $96.1\%$ on Wildtrack dataset, and $85.7\%$ on MultiviewX dataset.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2502.17434.pdf' target='_blank'>https://arxiv.org/pdf/2502.17434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Li, Mingxi Jia, Tuluhan Akbulut, Yu Xiang, George Konidaris, Srinath Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17434">V-HOP: Visuo-Haptic 6D Object Pose Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Project website: https://ivl.cs.brown.edu/research/v-hop
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2502.17434.pdf' target='_blank'>https://arxiv.org/pdf/2502.17434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Li, Mingxi Jia, Tuluhan Akbulut, Yu Xiang, George Konidaris, Srinath Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17434">V-HOP: Visuo-Haptic 6D Object Pose Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans naturally integrate vision and haptics for robust object perception during manipulation. The loss of either modality significantly degrades performance. Inspired by this multisensory integration, prior object pose estimation research has attempted to combine visual and haptic/tactile feedback. Although these works demonstrate improvements in controlled environments or synthetic datasets, they often underperform vision-only approaches in real-world settings due to poor generalization across diverse grippers, sensor layouts, or sim-to-real environments. Furthermore, they typically estimate the object pose for each frame independently, resulting in less coherent tracking over sequences in real-world deployments. To address these limitations, we introduce a novel unified haptic representation that effectively handles multiple gripper embodiments. Building on this representation, we introduce a new visuo-haptic transformer-based object pose tracker that seamlessly integrates visual and haptic input. We validate our framework in our dataset and the Feelsight dataset, demonstrating significant performance improvement on challenging sequences. Notably, our method achieves superior generalization and robustness across novel embodiments, objects, and sensor types (both taxel-based and vision-based tactile sensors). In real-world experiments, we demonstrate that our approach outperforms state-of-the-art visual trackers by a large margin. We further show that we can achieve precise manipulation tasks by incorporating our real-time object tracking result into motion plans, underscoring the advantages of visuo-haptic perception. Project website: https://ivl.cs.brown.edu/research/v-hop
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2501.10129.pdf' target='_blank'>https://arxiv.org/pdf/2501.10129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Futian Wang, Fengxiang Liu, Xiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.10129">Spatio-temporal Graph Learning on Adaptive Mined Key Frames for High-performance Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of multi-object tracking, the challenge of accurately capturing the spatial and temporal relationships between objects in video sequences remains a significant hurdle. This is further complicated by frequent occurrences of mutual occlusions among objects, which can lead to tracking errors and reduced performance in existing methods. Motivated by these challenges, we propose a novel adaptive key frame mining strategy that addresses the limitations of current tracking approaches. Specifically, we introduce a Key Frame Extraction (KFE) module that leverages reinforcement learning to adaptively segment videos, thereby guiding the tracker to exploit the intrinsic logic of the video content. This approach allows us to capture structured spatial relationships between different objects as well as the temporal relationships of objects across frames. To tackle the issue of object occlusions, we have developed an Intra-Frame Feature Fusion (IFF) module. Unlike traditional graph-based methods that primarily focus on inter-frame feature fusion, our IFF module uses a Graph Convolutional Network (GCN) to facilitate information exchange between the target and surrounding objects within a frame. This innovation significantly enhances target distinguishability and mitigates tracking loss and appearance similarity due to occlusions. By combining the strengths of both long and short trajectories and considering the spatial relationships between objects, our proposed tracker achieves impressive results on the MOT17 dataset, i.e., 68.6 HOTA, 81.0 IDF1, 66.6 AssA, and 893 IDS, proving its effectiveness and accuracy.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2411.13346.pdf' target='_blank'>https://arxiv.org/pdf/2411.13346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karolina Trajkovska, MatjaÅ¾ Kljun, Klen ÄopiÄ Pucihar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13346">Gaze2AOI: Open Source Deep-learning Based System for Automatic Area of Interest Annotation with Eye Tracking Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Eye gaze is considered an important indicator for understanding and predicting user behaviour, as well as directing their attention across various domains including advertisement design, human-computer interaction and film viewing. In this paper, we present a novel method to enhance the analysis of user behaviour and attention by (i) augmenting video streams with automatically annotating and labelling areas of interest (AOIs), and (ii) integrating AOIs with collected eye gaze and fixation data. The tool provides key features such as time to first fixation, dwell time, and frequency of AOI revisits. By incorporating the YOLOv8 object tracking algorithm, the tool supports over 600 different object classes, providing a comprehensive set for a variety of video streams. This tool will be made available as open-source software, thereby contributing to broader research and development efforts in the field.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2410.13437.pdf' target='_blank'>https://arxiv.org/pdf/2410.13437.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changcheng Xiao, Qiong Cao, Yujie Zhong, Xiang Zhang, Tao Wang, Canqun Yang, Long Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13437">Temporal-Enhanced Multimodal Transformer for Referring Multi-Object Tracking and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to locate an arbitrary number of target objects and maintain their identities referred by a language expression in a video. This intricate task involves the reasoning of linguistic and visual modalities, along with the temporal association of target objects. However, the seminal work employs only loose feature fusion and overlooks the utilization of long-term information on tracked objects. In this study, we introduce a compact Transformer-based method, termed TenRMOT. We conduct feature fusion at both encoding and decoding stages to fully exploit the advantages of Transformer architecture. Specifically, we incrementally perform cross-modal fusion layer-by-layer during the encoding phase. In the decoding phase, we utilize language-guided queries to probe memory features for accurate prediction of the desired objects. Moreover, we introduce a query update module that explicitly leverages temporal prior information of the tracked objects to enhance the consistency of their trajectories. In addition, we introduce a novel task called Referring Multi-Object Tracking and Segmentation (RMOTS) and construct a new dataset named Ref-KITTI Segmentation. Our dataset consists of 18 videos with 818 expressions, and each expression averages 10.7 masks, which poses a greater challenge compared to the typical single mask in most existing referring video segmentation datasets. TenRMOT demonstrates superior performance on both the referring multi-object tracking and the segmentation tasks.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2408.12232.pdf' target='_blank'>https://arxiv.org/pdf/2408.12232.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.12232">BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2408.04979.pdf' target='_blank'>https://arxiv.org/pdf/2408.04979.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lennart Niecksch, Alexander Mock, Felix Igelbrink, Thomas Wiemann, Joachim Hertzberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04979">Mesh-based Object Tracking for Dynamic Semantic 3D Scene Graphs via Ray Tracing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel method for 3D geometric scene graph generation using range sensors and RGB cameras. We initially detect instance-wise keypoints with a YOLOv8s model to compute 6D pose estimates of known objects by solving PnP. We use a ray tracing approach to track a geometric scene graph consisting of mesh models of object instances. In contrast to classical point-to-point matching, this leads to more robust results, especially under occlusions between objects instances. We show that using this hybrid strategy leads to robust self-localization, pre-segmentation of the range sensor data and accurate pose tracking of objects using the same environmental representation. All detected objects are integrated into a semantic scene graph. This scene graph then serves as a front end to a semantic mapping framework to allow spatial reasoning.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2408.02263.pdf' target='_blank'>https://arxiv.org/pdf/2408.02263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Lu, Jiahao Nie, Zhiwei He, Hongjie Gu, Xudong Lv
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02263">VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current LiDAR point cloud-based 3D single object tracking (SOT) methods typically rely on point-based representation network. Despite demonstrated success, such networks suffer from some fundamental problems: 1) It contains pooling operation to cope with inherently disordered point clouds, hindering the capture of 3D spatial information that is useful for tracking, a regression task. 2) The adopted set abstraction operation hardly handles density-inconsistent point clouds, also preventing 3D spatial information from being modeled. To solve these problems, we introduce a novel tracking framework, termed VoxelTrack. By voxelizing inherently disordered point clouds into 3D voxels and extracting their features via sparse convolution blocks, VoxelTrack effectively models precise and robust 3D spatial information, thereby guiding accurate position prediction for tracked objects. Moreover, VoxelTrack incorporates a dual-stream encoder with cross-iterative feature fusion module to further explore fine-grained 3D spatial information for tracking. Benefiting from accurate 3D spatial information being modeled, our VoxelTrack simplifies tracking pipeline with a single regression loss. Extensive experiments are conducted on three widely-adopted datasets including KITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that VoxelTrack achieves state-of-the-art performance (88.3%, 71.4% and 63.6% mean precision on the three datasets, respectively), and outperforms the existing trackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source code and model will be released.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2407.01907.pdf' target='_blank'>https://arxiv.org/pdf/2407.01907.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailiang Zhang, Dian Chao, Zhihao Guan, Yang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01907">The Solution for the ICCV 2023 Perception Test Challenge 2023 -- Task 6 -- Grounded videoQA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a grounded video question-answering solution. Our research reveals that the fixed official baseline method for video question answering involves two main steps: visual grounding and object tracking. However, a significant challenge emerges during the initial step, where selected frames may lack clearly identifiable target objects. Furthermore, single images cannot address questions like "Track the container from which the person pours the first time." To tackle this issue, we propose an alternative two-stage approach:(1) First, we leverage the VALOR model to answer questions based on video information.(2) concatenate the answered questions with their respective answers. Finally, we employ TubeDETR to generate bounding boxes for the targets.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2406.19844.pdf' target='_blank'>https://arxiv.org/pdf/2406.19844.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaheng Zhuang, Guoan Wang, Siyu Zhang, Xiyang Wang, Hangning Zhou, Ziyao Xu, Chi Zhang, Zhiheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.19844">StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object Tracking and Trajectory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking and trajectory prediction are two crucial modules in autonomous driving systems. Generally, the two tasks are handled separately in traditional paradigms and a few methods have started to explore modeling these two tasks in a joint manner recently. However, these approaches suffer from the limitations of single-frame training and inconsistent coordinate representations between tracking and prediction tasks. In this paper, we propose a streaming and unified framework for joint 3D Multi-Object Tracking and trajectory Prediction (StreamMOTP) to address the above challenges. Firstly, we construct the model in a streaming manner and exploit a memory bank to preserve and leverage the long-term latent features for tracked objects more effectively. Secondly, a relative spatio-temporal positional encoding strategy is introduced to bridge the gap of coordinate representations between the two tasks and maintain the pose-invariance for trajectory prediction. Thirdly, we further improve the quality and consistency of predicted trajectories with a dual-stream predictor. We conduct extensive experiments on popular nuSences dataset and the experimental results demonstrate the effectiveness and superiority of StreamMOTP, which outperforms previous methods significantly on both tasks. Furthermore, we also prove that the proposed framework has great potential and advantages in actual applications of autonomous driving.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2406.01011.pdf' target='_blank'>https://arxiv.org/pdf/2406.01011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Palmer, Martin KrÃ¼ger, Richard Altendorfer, Torsten Bertram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01011">Multi-Object Tracking based on Imaging Radar 3D Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective tracking of surrounding traffic participants allows for an accurate state estimation as a necessary ingredient for prediction of future behavior and therefore adequate planning of the ego vehicle trajectory. One approach for detecting and tracking surrounding traffic participants is the combination of a learning based object detector with a classical tracking algorithm. Learning based object detectors have been shown to work adequately on lidar and camera data, while learning based object detectors using standard radar data input have proven to be inferior. Recently, with the improvements to radar sensor technology in the form of imaging radars, the object detection performance on radar was greatly improved but is still limited compared to lidar sensors due to the sparsity of the radar point cloud. This presents a unique challenge for the task of multi-object tracking. The tracking algorithm must overcome the limited detection quality while generating consistent tracks. To this end, a comparison between different multi-object tracking methods on imaging radar data is required to investigate its potential for downstream tasks. The work at hand compares multiple approaches and analyzes their limitations when applied to imaging radar data. Furthermore, enhancements to the presented approaches in the form of probabilistic association algorithms are considered for this task.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2406.00589.pdf' target='_blank'>https://arxiv.org/pdf/2406.00589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Qi, Junlin Zhang, Xin Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00589">Robust Visual Tracking via Iterative Gradient Descent and Threshold Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual tracking fundamentally involves regressing the state of the target in each frame of a video. Despite significant progress, existing regression-based trackers still tend to experience failures and inaccuracies. To enhance the precision of target estimation, this paper proposes a tracking technique based on robust regression. Firstly, we introduce a novel robust linear regression estimator, which achieves favorable performance when the error vector follows i.i.d Gaussian-Laplacian distribution. Secondly, we design an iterative process to quickly solve the problem of outliers. In fact, the coefficients are obtained by Iterative Gradient Descent and Threshold Selection algorithm (IGDTS). In addition, we expend IGDTS to a generative tracker, and apply IGDTS-distance to measure the deviation between the sample and the model. Finally, we propose an update scheme to capture the appearance changes of the tracked object and ensure that the model is updated correctly. Experimental results on several challenging image sequences show that the proposed tracker outperformance existing trackers.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2405.06600.pdf' target='_blank'>https://arxiv.org/pdf/2405.06600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzhe Wang, Kang Ma, Qiankun Liu, Yunhao Zou, Ying Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.06600">Multi-Object Tracking in the Dark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-light scenes are prevalent in real-world applications (e.g. autonomous driving and surveillance at night). Recently, multi-object tracking in various practical use cases have received much attention, but multi-object tracking in dark scenes is rarely considered. In this paper, we focus on multi-object tracking in dark scenes. To address the lack of datasets, we first build a Low-light Multi-Object Tracking (LMOT) dataset. LMOT provides well-aligned low-light video pairs captured by our dual-camera system, and high-quality multi-object tracking annotations for all videos. Then, we propose a low-light multi-object tracking method, termed as LTrack. We introduce the adaptive low-pass downsample module to enhance low-frequency components of images outside the sensor noises. The degradation suppression learning strategy enables the model to learn invariant information under noise disturbance and image quality degradation. These components improve the robustness of multi-object tracking in dark scenes. We conducted a comprehensive analysis of our LMOT dataset and proposed LTrack. Experimental results demonstrate the superiority of the proposed method and its competitiveness in real night low-light scenes. Dataset and Code: https: //github.com/ying-fu/LMOT
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2403.11978.pdf' target='_blank'>https://arxiv.org/pdf/2403.11978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan KrejÄÃ­, Oliver Kost, OndÅej Straka, JindÅich DunÃ­k
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.11978">Pedestrian Tracking with Monocular Camera using Unconstrained 3D Motion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A first-principle single-object model is proposed for pedestrian tracking. It is assumed that the extent of the moving object can be described via known statistics in 3D, such as pedestrian height. The proposed model thus need not constrain the object motion in 3D to a common ground plane, which is usual in 3D visual tracking applications. A nonlinear filter for this model is implemented using the unscented Kalman filter (UKF) and tested using the publicly available MOT-17 dataset. The proposed solution yields promising results in 3D while maintaining perfect results when projected into the 2D image. Moreover, the estimation error covariance matches the true one. Unlike conventional methods, the introduced model parameters have convenient meaning and can readily be adjusted for a problem.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2403.05852.pdf' target='_blank'>https://arxiv.org/pdf/2403.05852.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du, Jing Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05852">SSF-Net: Spatial-Spectral Fusion Network with Spectral Angle Awareness for Hyperspectral Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral video (HSV) offers valuable spatial, spectral, and temporal information simultaneously, making it highly suitable for handling challenges such as background clutter and visual similarity in object tracking. However, existing methods primarily focus on band regrouping and rely on RGB trackers for feature extraction, resulting in limited exploration of spectral information and difficulties in achieving complementary representations of object features. In this paper, a spatial-spectral fusion network with spectral angle awareness (SST-Net) is proposed for hyperspectral (HS) object tracking. Firstly, to address the issue of insufficient spectral feature extraction in existing networks, a spatial-spectral feature backbone ($S^2$FB) is designed. With the spatial and spectral extraction branch, a joint representation of texture and spectrum is obtained. Secondly, a spectral attention fusion module (SAFM) is presented to capture the intra- and inter-modality correlation to obtain the fused features from the HS and RGB modalities. It can incorporate the visual information into the HS spectral context to form a robust representation. Thirdly, to ensure a more accurate response of the tracker to the object position, a spectral angle awareness module (SAAM) investigates the region-level spectral similarity between the template and search images during the prediction stage. Furthermore, we develop a novel spectral angle awareness loss (SAAL) to offer guidance for the SAAM based on similar regions. Finally, to obtain the robust tracking results, a weighted prediction method is considered to combine the HS and RGB predicted motions of objects to leverage the strengths of each modality. Extensive experiments on the HOTC dataset demonstrate the effectiveness of the proposed SSF-Net, compared with state-of-the-art trackers.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2402.13146.pdf' target='_blank'>https://arxiv.org/pdf/2402.13146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adnen Abdessaied, Manuel von Hochmeister, Andreas Bulling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.13146">OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the Object Language Video Transformer (OLViT) - a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2402.08774.pdf' target='_blank'>https://arxiv.org/pdf/2402.08774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angus Fung, Beno Benhabib, Goldie Nejat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.08774">LDTrack: Dynamic People Tracking by Service Robots using Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking of dynamic people in cluttered and crowded human-centered environments is a challenging robotics problem due to the presence of intraclass variations including occlusions, pose deformations, and lighting variations. This paper introduces a novel deep learning architecture, using conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for tracking multiple dynamic people under intraclass variations. By uniquely utilizing conditional latent diffusion models to capture temporal person embeddings, our architecture can adapt to appearance changes of people over time. We incorporated a latent feature encoder network which enables the diffusion process to operate within a high-dimensional latent space to allow for the extraction and spatial-temporal refinement of such rich features as person appearance, motion, location, identity, and contextual information. Extensive experiments demonstrate the effectiveness of LDTrack over other state-of-the-art tracking methods in cluttered and crowded human-centered environments under intraclass variations. Namely, the results show our method outperforms existing deep learning robotic people tracking methods in both tracking accuracy and tracking precision with statistical significance. Additionally, a comprehensive multi-object tracking comparison study was performed against the state-of-the-art methods in urban environments, demonstrating the generalizability of LDTrack. An ablation study was performed to validate the design choices of LDTrack.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2401.11204.pdf' target='_blank'>https://arxiv.org/pdf/2401.11204.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Nie, Zhiwei He, Xudong Lv, Xueyi Zhou, Dong-Kyu Chae, Fei Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.11204">Towards Category Unification of 3D Single Object Tracking on Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Category-specific models are provenly valuable methods in 3D single object tracking (SOT) regardless of Siamese or motion-centric paradigms. However, such over-specialized model designs incur redundant parameters, thus limiting the broader applicability of 3D SOT task. This paper first introduces unified models that can simultaneously track objects across all categories using a single network with shared model parameters. Specifically, we propose to explicitly encode distinct attributes associated to different object categories, enabling the model to adapt to cross-category data. We find that the attribute variances of point cloud objects primarily occur from the varying size and shape (e.g., large and square vehicles v.s. small and slender humans). Based on this observation, we design a novel point set representation learning network inheriting transformer architecture, termed AdaFormer, which adaptively encodes the dynamically varying shape and size information from cross-category data in a unified manner. We further incorporate the size and shape prior derived from the known template targets into the model's inputs and learning objective, facilitating the learning of unified representation. Equipped with such designs, we construct two category-unified models SiamCUT and MoCUT.Extensive experiments demonstrate that SiamCUT and MoCUT exhibit strong generalization and training stability. Furthermore, our category-unified models outperform the category-specific counterparts by a significant margin (e.g., on KITTI dataset, 12% and 3% performance gains on the Siamese and motion paradigms). Our code will be available.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2401.03872.pdf' target='_blank'>https://arxiv.org/pdf/2401.03872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alan Lukezic, Ziga Trojer, Jiri Matas, Matej Kristan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03872">A New Dataset and a Distractor-Aware Architecture for Transparent Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Performance of modern trackers degrades substantially on transparent objects compared to opaque objects. This is largely due to two distinct reasons. Transparent objects are unique in that their appearance is directly affected by the background. Furthermore, transparent object scenes often contain many visually similar objects (distractors), which often lead to tracking failure. However, development of modern tracking architectures requires large training sets, which do not exist in transparent object tracking. We present two contributions addressing the aforementioned issues. We propose the first transparent object tracking training dataset Trans2k that consists of over 2k sequences with 104,343 images overall, annotated by bounding boxes and segmentation masks. Standard trackers trained on this dataset consistently improve by up to 16%. Our second contribution is a new distractor-aware transparent object tracker (DiTra) that treats localization accuracy and target identification as separate tasks and implements them by a novel architecture. DiTra sets a new state-of-the-art in transparent object tracking and generalizes well to opaque objects.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2312.05955.pdf' target='_blank'>https://arxiv.org/pdf/2312.05955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxi Li, Xiongjie Chen, Yunpeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05955">Learning Differentiable Particle Filter on the Fly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Differentiable particle filters are an emerging class of sequential Bayesian inference techniques that use neural networks to construct components in state space models. Existing approaches are mostly based on offline supervised training strategies. This leads to the delay of the model deployment and the obtained filters are susceptible to distribution shift of test-time data. In this paper, we propose an online learning framework for differentiable particle filters so that model parameters can be updated as data arrive. The technical constraint is that there is no known ground truth state information in the online inference setting. We address this by adopting an unsupervised loss to construct the online model updating procedure, which involves a sequence of filtering operations for online maximum likelihood-based parameter estimation. We empirically evaluate the effectiveness of the proposed method, and compare it with supervised learning methods in simulation settings including a multivariate linear Gaussian state-space model and a simulated object tracking experiment.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2312.01650.pdf' target='_blank'>https://arxiv.org/pdf/2312.01650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linh Van Ma, Muhammad Ishfaq Hussain, JongHyun Park, Jeongbae Kim, Moongu Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.01650">Adaptive Confidence Threshold for ByteTrack in Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the application of ByteTrack in the realm of multiple object tracking. ByteTrack, a simple tracking algorithm, enables the simultaneous tracking of multiple objects by strategically incorporating detections with a low confidence threshold. Conventionally, objects are initially associated with high confidence threshold detections. When the association between objects and detections becomes ambiguous, ByteTrack extends the association to lower confidence threshold detections. One notable drawback of the existing ByteTrack approach is its reliance on a fixed threshold to differentiate between high and low-confidence detections. In response to this limitation, we introduce a novel and adaptive approach. Our proposed method entails a dynamic adjustment of the confidence threshold, leveraging insights derived from overall detections. Through experimentation, we demonstrate the effectiveness of our adaptive confidence threshold technique while maintaining running time compared to ByteTrack.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2311.18199.pdf' target='_blank'>https://arxiv.org/pdf/2311.18199.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Aminul Islam, Wangzhi Xing, Jun Zhou, Yongsheng Gao, Kuldip K. Paliwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18199">Hy-Tracker: A Novel Framework for Enhancing Efficiency and Accuracy of Object Tracking in Hyperspectral Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral object tracking has recently emerged as a topic of great interest in the remote sensing community. The hyperspectral image, with its many bands, provides a rich source of material information of an object that can be effectively used for object tracking. While most hyperspectral trackers are based on detection-based techniques, no one has yet attempted to employ YOLO for detecting and tracking the object. This is due to the presence of multiple spectral bands, the scarcity of annotated hyperspectral videos, and YOLO's performance limitation in managing occlusions, and distinguishing object in cluttered backgrounds. Therefore, in this paper, we propose a novel framework called Hy-Tracker, which aims to bridge the gap between hyperspectral data and state-of-the-art object detection methods to leverage the strengths of YOLOv7 for object tracking in hyperspectral videos. Hy-Tracker not only introduces YOLOv7 but also innovatively incorporates a refined tracking module on top of YOLOv7. The tracker refines the initial detections produced by YOLOv7, leading to improved object-tracking performance. Furthermore, we incorporate Kalman-Filter into the tracker, which addresses the challenges posed by scale variation and occlusion. The experimental results on hyperspectral benchmark datasets demonstrate the effectiveness of Hy-Tracker in accurately tracking objects across frames.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2311.07268.pdf' target='_blank'>https://arxiv.org/pdf/2311.07268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edison P. Velasco SÃ¡nchez, Miguel Ãngel MuÃ±oz-BaÃ±Ã³n, Francisco A. Candelas, Santiago T. Puente, Fernando Torres
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07268">ViKi-HyCo: A Hybrid-Control approach for complex car-like maneuvers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Visual Servoing is deeply studied to perform simple maneuvers, the literature does not commonly address complex cases where the target is far out of the camera's field of view (FOV) during the maneuver. For this reason, in this paper, we present ViKi-HyCo (Visual Servoing and Kinematic Hybrid-Controller). This approach generates the necessary maneuvers for the complex positioning of a non-holonomic mobile robot in outdoor environments. In this method, we use \hbox{LiDAR-camera} fusion to estimate objects bounding boxes using image and metrics modalities. With the multi-modality nature of our representation, we can automatically obtain a target for a visual servoing controller. At the same time, we also have a metric target, which allows us to hybridize with a kinematic controller. Given this hybridization, we can perform complex maneuvers even when the target is far away from the camera's FOV. The proposed approach does not require an object-tracking algorithm and can be applied to any robotic positioning task where its kinematic model is known. ViKi-HyCo has an error of 0.0428 \pm 0.0467 m in the X-axis and 0.0515 \pm 0.0323 m in the Y-axis at the end of a complete positioning task.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2310.14506.pdf' target='_blank'>https://arxiv.org/pdf/2310.14506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ji Youn Lee, Changbeom Shim, Hoa Van Nguyen, Tran Thien Dat Nguyen, Hyunjin Choi, Youngho Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14506">Label Space Partition Selection for Multi-Object Tracking Using Two-Layer Partitioning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Estimating the trajectories of multi-objects poses a significant challenge due to data association ambiguity, which leads to a substantial increase in computational requirements. To address such problems, a divide-and-conquer manner has been employed with parallel computation. In this strategy, distinguished objects that have unique labels are grouped based on their statistical dependencies, the intersection of predicted measurements. Several geometry approaches have been used for label grouping since finding all intersected label pairs is clearly infeasible for large-scale tracking problems. This paper proposes an efficient implementation of label grouping for label-partitioned generalized labeled multi-Bernoulli filter framework using a secondary partitioning technique. This allows for parallel computation in the label graph indexing step, avoiding generating and eliminating duplicate comparisons. Additionally, we compare the performance of the proposed technique with several efficient spatial searching algorithms. The results demonstrate the superior performance of the proposed approach on large-scale data sets, enabling scalable trajectory estimation.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2310.03333.pdf' target='_blank'>https://arxiv.org/pdf/2310.03333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Syuen Lim, Ziwei Wang, Jiajun Liu, Abdelwahed Khamis, Reza Arablouei, Robert Barlow, Ryan McAllister
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03333">Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Regulatory compliance auditing across diverse industrial domains requires heightened quality assurance and traceability. Present manual and intermittent approaches to such auditing yield significant challenges, potentially leading to oversights in the monitoring process. To address these issues, we introduce a real-time, multi-modal sensing system employing 3D time-of-flight and RGB cameras, coupled with unsupervised learning techniques on edge AI devices. This enables continuous object tracking thereby enhancing efficiency in record-keeping and minimizing manual interventions. While we validate the system in a knife sanitization context within agrifood facilities, emphasizing its prowess against occlusion and low-light issues with RGB cameras, its potential spans various industrial monitoring settings.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2309.13393.pdf' target='_blank'>https://arxiv.org/pdf/2309.13393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonardo Saraceni, Ionut M. Motoi, Daniele Nardi, Thomas A. Ciarfuglia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13393">AgriSORT: A Simple Online Real-time Tracking-by-Detection framework for robotics in precision agriculture</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The problem of multi-object tracking (MOT) consists in detecting and tracking all the objects in a video sequence while keeping a unique identifier for each object. It is a challenging and fundamental problem for robotics. In precision agriculture the challenge of achieving a satisfactory solution is amplified by extreme camera motion, sudden illumination changes, and strong occlusions. Most modern trackers rely on the appearance of objects rather than motion for association, which can be ineffective when most targets are static objects with the same appearance, as in the agricultural case. To this end, on the trail of SORT [5], we propose AgriSORT, a simple, online, real-time tracking-by-detection pipeline for precision agriculture based only on motion information that allows for accurate and fast propagation of tracks between frames. The main focuses of AgriSORT are efficiency, flexibility, minimal dependencies, and ease of deployment on robotic platforms. We test the proposed pipeline on a novel MOT benchmark specifically tailored for the agricultural context, based on video sequences taken in a table grape vineyard, particularly challenging due to strong self-similarity and density of the instances. Both the code and the dataset are available for future comparisons.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2306.02407.pdf' target='_blank'>https://arxiv.org/pdf/2306.02407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Colin Samplawski, Shiwei Fang, Ziqi Wang, Deepak Ganesan, Mani Srivastava, Benjamin M. Marlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02407">Heteroskedastic Geospatial Tracking with Distributed Camera Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual object tracking has seen significant progress in recent years. However, the vast majority of this work focuses on tracking objects within the image plane of a single camera and ignores the uncertainty associated with predicted object locations. In this work, we focus on the geospatial object tracking problem using data from a distributed camera network. The goal is to predict an object's track in geospatial coordinates along with uncertainty over the object's location while respecting communication constraints that prohibit centralizing raw image data. We present a novel single-object geospatial tracking data set that includes high-accuracy ground truth object locations and video data from a network of four cameras. We present a modeling framework for addressing this task including a novel backbone model and explore how uncertainty calibration and fine-tuning through a differentiable tracker affect performance.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2211.13769.pdf' target='_blank'>https://arxiv.org/pdf/2211.13769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saksham Aggarwal, Taneesh Gupta, Pawan Kumar Sahu, Arnav Chavan, Rishabh Tiwari, Dilip K. Prasad, Deepak K. Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13769">On Designing Light-Weight Object Trackers through Network Pruning: Use CNNs or Transformers?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object trackers deployed on low-power devices need to be light-weight, however, most of the current state-of-the-art (SOTA) methods rely on using compute-heavy backbones built using CNNs or transformers. Large sizes of such models do not allow their deployment in low-power conditions and designing compressed variants of large tracking models is of great importance. This paper demonstrates how highly compressed light-weight object trackers can be designed using neural architectural pruning of large CNN and transformer based trackers. Further, a comparative study on architectural choices best suited to design light-weight trackers is provided. A comparison between SOTA trackers using CNNs, transformers as well as the combination of the two is presented to study their stability at various compression ratios. Finally results for extreme pruning scenarios going as low as 1% in some cases are shown to study the limits of network pruning in object tracking. This work provides deeper insights into designing highly efficient trackers from existing SOTA methods.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2211.11077.pdf' target='_blank'>https://arxiv.org/pdf/2211.11077.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peirong Liu, Rui Wang, Pengchuan Zhang, Omid Poursaeed, Yipin Zhou, Xuefei Cao, Sreya Dutta Roy, Ashish Shah, Ser-Nam Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.11077">Unifying Tracking and Image-Video Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Objection detection (OD) has been one of the most fundamental tasks in computer vision. Recent developments in deep learning have pushed the performance of image OD to new heights by learning-based, data-driven approaches. On the other hand, video OD remains less explored, mostly due to much more expensive data annotation needs. At the same time, multi-object tracking (MOT) which requires reasoning about track identities and spatio-temporal trajectories, shares similar spirits with video OD. However, most MOT datasets are class-specific (e.g., person-annotated only), which constrains a model's flexibility to perform tracking on other objects. We propose TrIVD (Tracking and Image-Video Detection), the first framework that unifies image OD, video OD, and MOT within one end-to-end model. To handle the discrepancies and semantic overlaps of category labels across datasets, TrIVD formulates detection/tracking as grounding and reasons about object categories via visual-text alignments. The unified formulation enables cross-dataset, multi-task training, and thus equips TrIVD with the ability to leverage frame-level features, video-level spatio-temporal relations, as well as track identity associations. With such joint training, we can now extend the knowledge from OD data, that comes with much richer object category annotations, to MOT and achieve zero-shot tracking capability. Experiments demonstrate that multi-task co-trained TrIVD outperforms single-task baselines across all image/video OD and MOT tasks. We further set the first baseline on the new task of zero-shot tracking.
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2211.06739.pdf' target='_blank'>https://arxiv.org/pdf/2211.06739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Udbhav Bamba, Neeraj Anand, Saksham Aggarwal, Dilip K. Prasad, Deepak K. Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.06739">Partial Binarization of Neural Networks for Budget-Aware Efficient Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Binarization is a powerful compression technique for neural networks, significantly reducing FLOPs, but often results in a significant drop in model performance. To address this issue, partial binarization techniques have been developed, but a systematic approach to mixing binary and full-precision parameters in a single network is still lacking. In this paper, we propose a controlled approach to partial binarization, creating a budgeted binary neural network (B2NN) with our MixBin strategy. This method optimizes the mixing of binary and full-precision components, allowing for explicit selection of the fraction of the network to remain binary. Our experiments show that B2NNs created using MixBin outperform those from random or iterative searches and state-of-the-art layer selection methods by up to 3% on the ImageNet-1K dataset. We also show that B2NNs outperform the structured pruning baseline by approximately 23% at the extreme FLOP budget of 15%, and perform well in object tracking, with up to a 12.4% relative improvement over other baselines. Additionally, we demonstrate that B2NNs developed by MixBin can be transferred across datasets, with some cases showing improved performance over directly applying MixBin on the downstream data.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2205.03721.pdf' target='_blank'>https://arxiv.org/pdf/2205.03721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nick Heppert, Toki Migimatsu, Brent Yi, Claire Chen, Jeannette Bohg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.03721">Category-Independent Articulated Object Tracking with Factor Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robots deployed in human-centric environments may need to manipulate a diverse range of articulated objects, such as doors, dishwashers, and cabinets. Articulated objects often come with unexpected articulation mechanisms that are inconsistent with categorical priors: for example, a drawer might rotate about a hinge joint instead of sliding open. We propose a category-independent framework for predicting the articulation models of unknown objects from sequences of RGB-D images. The prediction is performed by a two-step process: first, a visual perception module tracks object part poses from raw images, and second, a factor graph takes these poses and infers the articulation model including the current configuration between the parts as a 6D twist. We also propose a manipulation-oriented metric to evaluate predicted joint twists in terms of how well a compliant robot controller would be able to manipulate the articulated object given the predicted twist. We demonstrate that our visual perception and factor graph modules outperform baselines on simulated data and show the applicability of our factor graph on real world data.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/1802.01346.pdf' target='_blank'>https://arxiv.org/pdf/1802.01346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Price, Guilherme Lawless, Heinrich H. BÃ¼lthoff, Michael Black, Aamir Ahmad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1802.01346">Deep Neural Network-based Cooperative Visual Tracking through Multiple Micro Aerial Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-camera full-body pose capture of humans and animals in outdoor environments is a highly challenging problem. Our approach to it involves a team of cooperating micro aerial vehicles (MAVs) with on-board cameras only. The key enabling-aspect of our approach is the on-board person detection and tracking method. Recent state-of-the-art methods based on deep neural networks (DNN) are highly promising in this context. However, real time DNNs are severely constrained in input data dimensions, in contrast to available camera resolutions. Therefore, DNNs often fail at objects with small scale or far away from the camera, which are typical characteristics of a scenario with aerial robots. Thus, the core problem addressed in this paper is how to achieve on-board, real-time, continuous and accurate vision-based detections using DNNs for visual person tracking through MAVs. Our solution leverages cooperation among multiple MAVs. First, each MAV fuses its own detections with those obtained by other MAVs to perform cooperative visual tracking. This allows for predicting future poses of the tracked person, which are used to selectively process only the relevant regions of future images, even at high resolutions. Consequently, using our DNN-based detector we are able to continuously track even distant humans with high accuracy and speed. We demonstrate the efficiency of our approach through real robot experiments involving two aerial robots tracking a person, while maintaining an active perception-driven formation. Our solution runs fully on-board our MAV's CPU and GPU, with no remote processing. ROS-based source code is provided for the benefit of the community.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2509.19096.pdf' target='_blank'>https://arxiv.org/pdf/2509.19096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilhan Skender, Kailin Tong, Selim Solmaz, Daniel Watzenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19096">Investigating Traffic Accident Detection Using Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic safety remains a critical global concern, with timely and accurate accident detection essential for hazard reduction and rapid emergency response. Infrastructure-based vision sensors offer scalable and efficient solutions for continuous real-time monitoring, facilitating automated detection of accidents directly from captured images. This research investigates the zero-shot capabilities of multimodal large language models (MLLMs) for detecting and describing traffic accidents using images from infrastructure cameras, thus minimizing reliance on extensive labeled datasets. Main contributions include: (1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA, explicitly addressing the scarcity of diverse, realistic, infrastructure-based accident data through controlled simulations; (2) Comparative performance analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in accident identification and descriptive capabilities without prior fine-tuning; and (3) Integration of advanced visual analytics, specifically YOLO for object detection, Deep SORT for multi-object tracking, and Segment Anything (SAM) for instance segmentation, into enhanced prompts to improve model accuracy and explainability. Key numerical results show Pixtral as the top performer with an F1-score of 0.71 and 83% recall, while Gemini models gained precision with enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and recall losses. Gemma 3 offered the most balanced performance with minimal metric fluctuation. These findings demonstrate the substantial potential of integrating MLLMs with advanced visual analytics techniques, enhancing their applicability in real-world automated traffic monitoring systems.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2509.19096.pdf' target='_blank'>https://arxiv.org/pdf/2509.19096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ilhan Skender, Kailin Tong, Selim Solmaz, Daniel Watzenig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19096">Investigating Traffic Accident Detection Using Multimodal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traffic safety remains a critical global concern, with timely and accurate accident detection essential for hazard reduction and rapid emergency response. Infrastructure-based vision sensors offer scalable and efficient solutions for continuous real-time monitoring, facilitating automated detection of accidents directly from captured images. This research investigates the zero-shot capabilities of multimodal large language models (MLLMs) for detecting and describing traffic accidents using images from infrastructure cameras, thus minimizing reliance on extensive labeled datasets. Main contributions include: (1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA, explicitly addressing the scarcity of diverse, realistic, infrastructure-based accident data through controlled simulations; (2) Comparative performance analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in accident identification and descriptive capabilities without prior fine-tuning; and (3) Integration of advanced visual analytics, specifically YOLO for object detection, Deep SORT for multi-object tracking, and Segment Anything (SAM) for instance segmentation, into enhanced prompts to improve model accuracy and explainability. Key numerical results show Pixtral as the top performer with an F1-score of 0.71 and 83% recall, while Gemini models gained precision with enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and recall losses. Gemma 3 offered the most balanced performance with minimal metric fluctuation. These findings demonstrate the substantial potential of integrating MLLMs with advanced visual analytics techniques, enhancing their applicability in real-world automated traffic monitoring systems.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2509.18272.pdf' target='_blank'>https://arxiv.org/pdf/2509.18272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tornike Karchkhadze, Kuan-Lin Chen, Mojtaba Heydari, Robert Henzel, Alessandro Toso, Mehrez Souden, Joshua Atkins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18272">StereoFoley: Object-Aware Stereo Audio Generation from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present StereoFoley, a video-to-audio generation framework that produces semantically aligned, temporally synchronized, and spatially accurate stereo sound at 48 kHz. While recent generative video-to-audio models achieve strong semantic and temporal fidelity, they largely remain limited to mono or fail to deliver object-aware stereo imaging, constrained by the lack of professionally mixed, spatially accurate video-to-audio datasets. First, we develop and train a base model that generates stereo audio from video, achieving state-of-the-art in both semantic accuracy and synchronization. Next, to overcome dataset limitations, we introduce a synthetic data generation pipeline that combines video analysis, object tracking, and audio synthesis with dynamic panning and distance-based loudness controls, enabling spatially accurate object-aware sound. Finally, we fine-tune the base model on this synthetic dataset, yielding clear object-audio correspondence. Since no established metrics exist, we introduce stereo object-awareness measures and validate it through a human listening study, showing strong correlation with perception. This work establishes the first end-to-end framework for stereo object-aware video-to-audio generation, addressing a critical gap and setting a new benchmark in the field.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2509.18272.pdf' target='_blank'>https://arxiv.org/pdf/2509.18272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tornike Karchkhadze, Kuan-Lin Chen, Mojtaba Heydari, Robert Henzel, Alessandro Toso, Mehrez Souden, Joshua Atkins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18272">StereoFoley: Object-Aware Stereo Audio Generation from Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present StereoFoley, a video-to-audio generation framework that produces semantically aligned, temporally synchronized, and spatially accurate stereo sound at 48 kHz. While recent generative video-to-audio models achieve strong semantic and temporal fidelity, they largely remain limited to mono or fail to deliver object-aware stereo imaging, constrained by the lack of professionally mixed, spatially accurate video-to-audio datasets. First, we develop and train a base model that generates stereo audio from video, achieving state-of-the-art in both semantic accuracy and synchronization. Next, to overcome dataset limitations, we introduce a synthetic data generation pipeline that combines video analysis, object tracking, and audio synthesis with dynamic panning and distance-based loudness controls, enabling spatially accurate object-aware sound. Finally, we fine-tune the base model on this synthetic dataset, yielding clear object-audio correspondence. Since no established metrics exist, we introduce stereo object-awareness measures and validate it through a human listening study, showing strong correlation with perception. This work establishes the first end-to-end framework for stereo object-aware video-to-audio generation, addressing a critical gap and setting a new benchmark in the field.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2509.12924.pdf' target='_blank'>https://arxiv.org/pdf/2509.12924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik ForssÃ©n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12924">MATTER: Multiscale Attention for Registration Error Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking. This makes detecting and quantifying registration misalignment, i.e., PCR quality validation, an important task. All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes. In this work, we instead use regression for PCR validation, allowing for a more fine-grained quantification of the registration quality. We also extend previously used misalignment-related features by using multiscale extraction and attention-based aggregation. This leads to accurate and robust registration error estimation on diverse datasets, especially for point clouds with heterogeneous spatial densities. Furthermore, when used to guide a mapping downstream task, our method significantly improves the mapping quality for a given amount of re-registered frames, compared to the state-of-the-art classification-based method.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2509.12924.pdf' target='_blank'>https://arxiv.org/pdf/2509.12924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik ForssÃ©n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12924">MATTER: Multiscale Attention for Registration Error Regression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Point cloud registration (PCR) is crucial for many downstream tasks, such as simultaneous localization and mapping (SLAM) and object tracking. This makes detecting and quantifying registration misalignment, i.e., PCR quality validation, an important task. All existing methods treat validation as a classification task, aiming to assign the PCR quality to a few classes. In this work, we instead use regression for PCR validation, allowing for a more fine-grained quantification of the registration quality. We also extend previously used misalignment-related features by using multiscale extraction and attention-based aggregation. This leads to accurate and robust registration error estimation on diverse datasets, especially for point clouds with heterogeneous spatial densities. Furthermore, when used to guide a mapping downstream task, our method significantly improves the mapping quality for a given amount of re-registered frames, compared to the state-of-the-art classification-based method.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2509.11873.pdf' target='_blank'>https://arxiv.org/pdf/2509.11873.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anne Marthe Sophie Ngo Bibinbe, Patrick Gagnon, Jamie Ahloy-Dallaire, Eric R. Paquet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11873">Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision livestock farming requires advanced monitoring tools to meet the increasing management needs of the industry. Computer vision systems capable of long-term multi-animal tracking (MAT) are essential for continuous behavioral monitoring in livestock production. MAT, a specialized subset of multi-object tracking (MOT), shares many challenges with MOT, but also faces domain-specific issues including frequent animal occlusion, highly similar appearances among animals, erratic motion patterns, and a wide range of behavior types. While some existing MAT tools are user-friendly and widely adopted, they often underperform compared to state-of-the-art MOT methods, which can result in inaccurate downstream tasks such as behavior analysis, health state estimation, and related applications. In this study, we benchmarked both MAT and MOT approaches for long-term tracking of pigs. We compared tools such as DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT, cross-input consistency, and newer approaches like Track-Anything and PromptTrack. All methods were evaluated on a 10-minute pig tracking dataset. Our results demonstrate that, overall, MOT approaches outperform traditional MAT tools, even for long-term tracking scenarios. These findings highlight the potential of recent MOT techniques to enhance the accuracy and reliability of automated livestock tracking.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2509.11873.pdf' target='_blank'>https://arxiv.org/pdf/2509.11873.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anne Marthe Sophie Ngo Bibinbe, Patrick Gagnon, Jamie Ahloy-Dallaire, Eric R. Paquet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11873">Multi-animal tracking in Transition: Comparative Insights into Established and Emerging Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Precision livestock farming requires advanced monitoring tools to meet the increasing management needs of the industry. Computer vision systems capable of long-term multi-animal tracking (MAT) are essential for continuous behavioral monitoring in livestock production. MAT, a specialized subset of multi-object tracking (MOT), shares many challenges with MOT, but also faces domain-specific issues including frequent animal occlusion, highly similar appearances among animals, erratic motion patterns, and a wide range of behavior types. While some existing MAT tools are user-friendly and widely adopted, they often underperform compared to state-of-the-art MOT methods, which can result in inaccurate downstream tasks such as behavior analysis, health state estimation, and related applications. In this study, we benchmarked both MAT and MOT approaches for long-term tracking of pigs. We compared tools such as DeepLabCut and idTracker with MOT-based methods including ByteTrack, DeepSORT, cross-input consistency, and newer approaches like Track-Anything and PromptTrack. All methods were evaluated on a 10-minute pig tracking dataset. Our results demonstrate that, overall, MOT approaches outperform traditional MAT tools, even for long-term tracking scenarios. These findings highlight the potential of recent MOT techniques to enhance the accuracy and reliability of automated livestock tracking.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2509.11453.pdf' target='_blank'>https://arxiv.org/pdf/2509.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11453">Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics and autonomous systems. Existing methods typically follow frame-wise motion estimation or a sequence-based paradigm. However, the two-frame methods are efficient but lack long-term temporal context, making them vulnerable in sparse or occluded scenes, while sequence-based methods that process multiple point clouds gain robustness at a significant computational cost. To resolve this dilemma, we propose a novel trajectory-based paradigm and its instantiation, TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame tracker by implicitly learning motion continuity from historical bounding box trajectories alone-without requiring additional, costly point cloud inputs. It first generates a fast, explicit motion proposal and then uses an implicit motion modeling module to predict the future trajectory, which in turn refines and corrects the initial proposal. Extensive experiments on the large-scale NuScenes benchmark show that TrajTrack achieves new state-of-the-art performance, dramatically improving tracking precision by 4.48% over a strong baseline while running at 56 FPS. Besides, we also demonstrate the strong generalizability of TrajTrack across different base trackers. Video is available at https://www.bilibili.com/video/BV1ahYgzmEWP.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2509.11453.pdf' target='_blank'>https://arxiv.org/pdf/2509.11453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11453">Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics and autonomous systems. Existing methods typically follow frame-wise motion estimation or a sequence-based paradigm. However, the two-frame methods are efficient but lack long-term temporal context, making them vulnerable in sparse or occluded scenes, while sequence-based methods that process multiple point clouds gain robustness at a significant computational cost. To resolve this dilemma, we propose a novel trajectory-based paradigm and its instantiation, TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame tracker by implicitly learning motion continuity from historical bounding box trajectories alone-without requiring additional, costly point cloud inputs. It first generates a fast, explicit motion proposal and then uses an implicit motion modeling module to predict the future trajectory, which in turn refines and corrects the initial proposal. Extensive experiments on the large-scale NuScenes benchmark show that TrajTrack achieves new state-of-the-art performance, dramatically improving tracking precision by 4.48% over a strong baseline while running at 56 FPS. Besides, we also demonstrate the strong generalizability of TrajTrack across different base trackers. Video is available at https://www.bilibili.com/video/BV1ahYgzmEWP.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2509.08421.pdf' target='_blank'>https://arxiv.org/pdf/2509.08421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Toida, Taigo Sakai, Naoki Kato, Kazutoyo Yokota, Takeshi Nakamura, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08421">Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-View Multi-Object Tracking (MVMOT) is essential for applications such as surveillance, autonomous driving, and sports analytics. However, maintaining consistent object identities across multiple cameras remains challenging due to viewpoint changes, lighting variations, and occlusions, which often lead to tracking errors.Recent methods project features from multiple cameras into a unified Bird's-Eye-View (BEV) space to improve robustness against occlusion. However, this projection introduces feature distortion and non-uniform density caused by variations in object scale with distance. These issues degrade the quality of the fused representation and reduce detection and tracking accuracy.To address these problems, we propose SCFusion, a framework that combines three techniques to improve multi-view feature integration. First, it applies a sparse transformation to avoid unnatural interpolation during projection. Next, it performs density-aware weighting to adaptively fuse features based on spatial confidence and camera distance. Finally, it introduces a multi-view consistency loss that encourages each camera to learn discriminative features independently before fusion.Experiments show that SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline method TrackTacular. These results demonstrate that SCFusion effectively mitigates the limitations of conventional BEV projection and provides a robust and accurate solution for multi-view object detection and tracking.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2509.08421.pdf' target='_blank'>https://arxiv.org/pdf/2509.08421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Toida, Taigo Sakai, Naoki Kato, Kazutoyo Yokota, Takeshi Nakamura, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08421">Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-View Multi-Object Tracking (MVMOT) is essential for applications such as surveillance, autonomous driving, and sports analytics. However, maintaining consistent object identities across multiple cameras remains challenging due to viewpoint changes, lighting variations, and occlusions, which often lead to tracking errors.Recent methods project features from multiple cameras into a unified Bird's-Eye-View (BEV) space to improve robustness against occlusion. However, this projection introduces feature distortion and non-uniform density caused by variations in object scale with distance. These issues degrade the quality of the fused representation and reduce detection and tracking accuracy.To address these problems, we propose SCFusion, a framework that combines three techniques to improve multi-view feature integration. First, it applies a sparse transformation to avoid unnatural interpolation during projection. Next, it performs density-aware weighting to adaptively fuse features based on spatial confidence and camera distance. Finally, it introduces a multi-view consistency loss that encourages each camera to learn discriminative features independently before fusion.Experiments show that SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9% on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline method TrackTacular. These results demonstrate that SCFusion effectively mitigates the limitations of conventional BEV projection and provides a robust and accurate solution for multi-view object detection and tracking.
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2509.03499.pdf' target='_blank'>https://arxiv.org/pdf/2509.03499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Barnard, Elaine Liu, Kristine Walz, Brian Schlining, Nancy Jacobsen Stout, Lonny Lundsten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03499">DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarking multi-object tracking and object detection model performance is an essential step in machine learning model development, as it allows researchers to evaluate model detection and tracker performance on human-generated 'test' data, facilitating consistent comparisons between models and trackers and aiding performance optimization. In this study, a novel benchmark video dataset was developed and used to assess the performance of several Monterey Bay Aquarium Research Institute object detection models and a FathomNet single-class object detection model together with several trackers. The dataset consists of four video sequences representing midwater and benthic deep-sea habitats. Performance was evaluated using Higher Order Tracking Accuracy, a metric that balances detection, localization, and association accuracy. To the best of our knowledge, this is the first publicly available benchmark for multi-object tracking in deep-sea video footage. We provide the benchmark data, a clearly documented workflow for generating additional benchmark videos, as well as example Python notebooks for computing metrics.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2509.03499.pdf' target='_blank'>https://arxiv.org/pdf/2509.03499.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Barnard, Elaine Liu, Kristine Walz, Brian Schlining, Nancy Jacobsen Stout, Lonny Lundsten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03499">DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmarking multi-object tracking and object detection model performance is an essential step in machine learning model development, as it allows researchers to evaluate model detection and tracker performance on human-generated 'test' data, facilitating consistent comparisons between models and trackers and aiding performance optimization. In this study, a novel benchmark video dataset was developed and used to assess the performance of several Monterey Bay Aquarium Research Institute object detection models and a FathomNet single-class object detection model together with several trackers. The dataset consists of four video sequences representing midwater and benthic deep-sea habitats. Performance was evaluated using Higher Order Tracking Accuracy, a metric that balances detection, localization, and association accuracy. To the best of our knowledge, this is the first publicly available benchmark for multi-object tracking in deep-sea video footage. We provide the benchmark data, a clearly documented workflow for generating additional benchmark videos, as well as example Python notebooks for computing metrics.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2508.14370.pdf' target='_blank'>https://arxiv.org/pdf/2508.14370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Hashempoor, Yu Dong Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14370">FastTracker: Real-Time and Accurate Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional multi-object tracking (MOT) systems are predominantly designed for pedestrian tracking and often exhibit limited generalization to other object categories. This paper presents a generalized tracking framework capable of handling multiple object types, with a particular emphasis on vehicle tracking in complex traffic scenes. The proposed method incorporates two key components: (1) an occlusion-aware re-identification mechanism that enhances identity preservation for heavily occluded objects, and (2) a road-structure-aware tracklet refinement strategy that utilizes semantic scene priors such as lane directions, crosswalks, and road boundaries to improve trajectory continuity and accuracy. In addition, we introduce a new benchmark dataset comprising diverse vehicle classes with frame-level tracking annotations, specifically curated to support evaluation of vehicle-focused tracking methods. Extensive experimental results demonstrate that the proposed approach achieves robust performance on both the newly introduced dataset and several public benchmarks, highlighting its effectiveness in general-purpose object tracking. While our framework is designed for generalized multi-class tracking, it also achieves strong performance on conventional benchmarks, with HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark are available: github.com/Hamidreza-Hashempoor/FastTracker, huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2508.14370.pdf' target='_blank'>https://arxiv.org/pdf/2508.14370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Hashempoor, Yu Dong Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14370">FastTracker: Real-Time and Accurate Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional multi-object tracking (MOT) systems are predominantly designed for pedestrian tracking and often exhibit limited generalization to other object categories. This paper presents a generalized tracking framework capable of handling multiple object types, with a particular emphasis on vehicle tracking in complex traffic scenes. The proposed method incorporates two key components: (1) an occlusion-aware re-identification mechanism that enhances identity preservation for heavily occluded objects, and (2) a road-structure-aware tracklet refinement strategy that utilizes semantic scene priors such as lane directions, crosswalks, and road boundaries to improve trajectory continuity and accuracy. In addition, we introduce a new benchmark dataset comprising diverse vehicle classes with frame-level tracking annotations, specifically curated to support evaluation of vehicle-focused tracking methods. Extensive experimental results demonstrate that the proposed approach achieves robust performance on both the newly introduced dataset and several public benchmarks, highlighting its effectiveness in general-purpose object tracking. While our framework is designed for generalized multi-class tracking, it also achieves strong performance on conventional benchmarks, with HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark are available: github.com/Hamidreza-Hashempoor/FastTracker, huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2508.02238.pdf' target='_blank'>https://arxiv.org/pdf/2508.02238.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Dong, Yiwei Zhang, Yangjie Cui, Jinwu Xiang, Daochun Li, Zhan Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02238">An Event-based Fast Intensity Reconstruction Scheme for UAV Real-time Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras offer significant advantages, including a wide dynamic range, high temporal resolution, and immunity to motion blur, making them highly promising for addressing challenging visual conditions. Extracting and utilizing effective information from asynchronous event streams is essential for the onboard implementation of event cameras. In this paper, we propose a streamlined event-based intensity reconstruction scheme, event-based single integration (ESI), to address such implementation challenges. This method guarantees the portability of conventional frame-based vision methods to event-based scenarios and maintains the intrinsic advantages of event cameras. The ESI approach reconstructs intensity images by performing a single integration of the event streams combined with an enhanced decay algorithm. Such a method enables real-time intensity reconstruction at a high frame rate, typically 100 FPS. Furthermore, the relatively low computation load of ESI fits onboard implementation suitably, such as in UAV-based visual tracking scenarios. Extensive experiments have been conducted to evaluate the performance comparison of ESI and state-of-the-art algorithms. Compared to state-of-the-art algorithms, ESI demonstrates remarkable runtime efficiency improvements, superior reconstruction quality, and a high frame rate. As a result, ESI enhances UAV onboard perception significantly under visual adversary surroundings. In-flight tests, ESI demonstrates effective performance for UAV onboard visual tracking under extremely low illumination conditions(2-10lux), whereas other comparative algorithms fail due to insufficient frame rate, poor image quality, or limited real-time performance.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2507.17793.pdf' target='_blank'>https://arxiv.org/pdf/2507.17793.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Joel Brogan, Matthew Yohe, David Cornett
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17793">CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>What if you could piece together your own custom biometrics and AI analysis system, a bit like LEGO blocks? We aim to bring that technology to field operators in the field who require flexible, high-performance edge AI system that can be adapted on a moment's notice. This paper introduces CHAMP (Configurable Hot-swappable Architecture for Machine Perception), a modular edge computing platform that allows operators to dynamically swap in specialized AI "capability cartridges" for tasks like face recognition, object tracking, and document analysis. CHAMP leverages low-power FPGA-based accelerators on a high-throughput bus, orchestrated by a custom operating system (VDiSK) to enable plug-and-play AI pipelines and cryptographically secured biometric datasets. In this paper we describe the CHAMP design, including its modular scaling with multiple accelerators and the VDiSK operating system for runtime reconfiguration, along with its cryptographic capabilities to keep data stored on modules safe and private. Experiments demonstrate near-linear throughput scaling from 1 to 5 neural compute accelerators, highlighting both the performance gains and saturation limits of the USB3-based bus. Finally, we discuss applications of CHAMP in field biometrics, surveillance, and disaster response, and outline future improvements in bus protocols, cartridge capabilities, and system software.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2506.09159.pdf' target='_blank'>https://arxiv.org/pdf/2506.09159.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Antonio Calagna, Yenchia Yu, Paolo Giaccone, Carla Fabiana Chiasserini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.09159">MOSE: A Novel Orchestration Framework for Stateful Microservice Migration at the Edge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stateful migration has emerged as the dominant technology to support microservice mobility at the network edge while ensuring a satisfying experience to mobile end users. This work addresses two pivotal challenges, namely, the implementation and the orchestration of the migration process. We first introduce a novel framework that efficiently implements stateful migration and effectively orchestrates the migration process by fulfilling both network and application KPI targets. Through experimental validation using realistic microservices, we then show that our solution (i) greatly improves migration performance, yielding up to 77% decrease of the migration downtime with respect to the state of the art, and (ii) successfully addresses the strict user QoE requirements of critical scenarios featuring latency-sensitive microservices. Further, we consider two practical use cases, featuring, respectively, a UAV autopilot microservice and a multi-object tracking task, and demonstrate how our framework outperforms current state-of-the-art approaches in configuring the migration process and in meeting KPI targets.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2505.22882.pdf' target='_blank'>https://arxiv.org/pdf/2505.22882.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Yang, Zhixian Xie, Xuechao Zhang, Heni Ben Amor, Shan Lin, Wanxin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22882">TwinTrack: Bridging Vision and Contact Physics for Real-Time Tracking of Unknown Dynamic Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time tracking of previously unseen, highly dynamic objects in contact-rich environments -- such as during dexterous in-hand manipulation -- remains a significant challenge. Purely vision-based tracking often suffers from heavy occlusions due to the frequent contact interactions and motion blur caused by abrupt motion during contact impacts. We propose TwinTrack, a physics-aware visual tracking framework that enables robust and real-time 6-DoF pose tracking of unknown dynamic objects in a contact-rich scene by leveraging the contact physics of the observed scene. At the core of TwinTrack is an integration of Real2Sim and Sim2Real. In Real2Sim, we combine the complementary strengths of vision and contact physics to estimate object's collision geometry and physical properties: object's geometry is first reconstructed from vision, then updated along with other physical parameters from contact dynamics for physical accuracy. In Sim2Real, robust pose estimation of the object is achieved by adaptive fusion between visual tracking and prediction of the learned contact physics. TwinTrack is built on a GPU-accelerated, deeply customized physics engine to ensure real-time performance. We evaluate our method on two contact-rich scenarios: object falling with rich contact impacts against the environment, and contact-rich in-hand manipulation. Experimental results demonstrate that, compared to baseline methods, TwinTrack achieves significantly more robust, accurate, and real-time 6-DoF tracking in these challenging scenarios, with tracking speed exceeding 20 Hz. Project page: https://irislab.tech/TwinTrack-webpage/
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2505.18727.pdf' target='_blank'>https://arxiv.org/pdf/2505.18727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohe Li, Pengfei Li, Zide Fan, Ying Geng, Fangli Mou, Haohua Wu, Yunping Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18727">FusionTrack: End-to-End Multi-Object Tracking in Arbitrary Multi-View Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-view multi-object tracking (MVMOT) has found widespread applications in intelligent transportation, surveillance systems, and urban management. However, existing studies rarely address genuinely free-viewpoint MVMOT systems, which could significantly enhance the flexibility and scalability of cooperative tracking systems. To bridge this gap, we first construct the Multi-Drone Multi-Object Tracking (MDMOT) dataset, captured by mobile drone swarms across diverse real-world scenarios, initially establishing the first benchmark for multi-object tracking in arbitrary multi-view environment. Building upon this foundation, we propose \textbf{FusionTrack}, an end-to-end framework that reasonably integrates tracking and re-identification to leverage multi-view information for robust trajectory association. Extensive experiments on our MDMOT and other benchmark datasets demonstrate that FusionTrack achieves state-of-the-art performance in both single-view and multi-view tracking.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2505.07110.pdf' target='_blank'>https://arxiv.org/pdf/2505.07110.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhang, Fenghua Shao, Runsheng Zhang, Yifan Zhuang, Liuqingqing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07110">DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Based on the DeepSORT algorithm, this study explores the application of visual tracking technology in intelligent human-computer interaction, especially in the field of gesture recognition and tracking. With the rapid development of artificial intelligence and deep learning technology, visual-based interaction has gradually replaced traditional input devices and become an important way for intelligent systems to interact with users. The DeepSORT algorithm can achieve accurate target tracking in dynamic environments by combining Kalman filters and deep learning feature extraction methods. It is especially suitable for complex scenes with multi-target tracking and fast movements. This study experimentally verifies the superior performance of DeepSORT in gesture recognition and tracking. It can accurately capture and track the user's gesture trajectory and is superior to traditional tracking methods in terms of real-time and accuracy. In addition, this study also combines gesture recognition experiments to evaluate the recognition ability and feedback response of the DeepSORT algorithm under different gestures (such as sliding, clicking, and zooming). The experimental results show that DeepSORT can not only effectively deal with target occlusion and motion blur but also can stably track in a multi-target environment, achieving a smooth user interaction experience. Finally, this paper looks forward to the future development direction of intelligent human-computer interaction systems based on visual tracking and proposes future research focuses such as algorithm optimization, data fusion, and multimodal interaction in order to promote a more intelligent and personalized interactive experience. Keywords-DeepSORT, visual tracking, gesture recognition, human-computer interaction
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2504.18708.pdf' target='_blank'>https://arxiv.org/pdf/2504.18708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longfei Han, Klaus KefferpÃ¼tz, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18708">Decentralized Fusion of 3D Extended Object Tracking based on a B-Spline Shape Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extended Object Tracking (EOT) exploits the high resolution of modern sensors for detailed environmental perception. Combined with decentralized fusion, it contributes to a more scalable and robust perception system. This paper investigates the decentralized fusion of 3D EOT using a B-spline curve based model. The spline curve is used to represent the side-view profile, which is then extruded with a width to form a 3D shape. We use covariance intersection (CI) for the decentralized fusion and discuss the challenge of applying it to EOT. We further evaluate the tracking result of the decentralized fusion with simulated and real datasets of traffic scenarios. We show that the CI-based fusion can significantly improve the tracking performance for sensors with unfavorable perspective.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2504.03258.pdf' target='_blank'>https://arxiv.org/pdf/2504.03258.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuxiao Ding, Yutong Yang, Julian Wiederer, Markus Braun, Peizheng Li, Juergen Gall, Bin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03258">TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Query denoising has become a standard training strategy for DETR-based detectors by addressing the slow convergence issue. Besides that, query denoising can be used to increase the diversity of training samples for modeling complex scenarios which is critical for Multi-Object Tracking (MOT), showing its potential in MOT application. Existing approaches integrate query denoising within the tracking-by-attention paradigm. However, as the denoising process only happens within the single frame, it cannot benefit the tracker to learn temporal-related information. In addition, the attention mask in query denoising prevents information exchange between denoising and object queries, limiting its potential in improving association using self-attention. To address these issues, we propose TQD-Track, which introduces Temporal Query Denoising (TQD) tailored for MOT, enabling denoising queries to carry temporal information and instance-specific feature representation. We introduce diverse noise types onto denoising queries that simulate real-world challenges in MOT. We analyze our proposed TQD for different tracking paradigms, and find out the paradigm with explicit learned data association module, e.g. tracking-by-detection or alternating detection and association, benefit from TQD by a larger margin. For these paradigms, we further design an association mask in the association module to ensure the consistent interaction between track and detection queries as during inference. Extensive experiments on the nuScenes dataset demonstrate that our approach consistently enhances different tracking methods by only changing the training process, especially the paradigms with explicit association module.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2503.10730.pdf' target='_blank'>https://arxiv.org/pdf/2503.10730.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longfei Han, Klaus KefferpÃ¼tz, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10730">3D Extended Object Tracking based on Extruded B-Spline Side View Profiles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is an essential task for autonomous systems. With the advancement of 3D sensors, these systems can better perceive their surroundings using effective 3D Extended Object Tracking (EOT) methods. Based on the observation that common road users are symmetrical on the right and left sides in the traveling direction, we focus on the side view profile of the object. In order to leverage of the development in 2D EOT and balance the number of parameters of a shape model in the tracking algorithms, we propose a method for 3D extended object tracking (EOT) by describing the side view profile of the object with B-spline curves and forming an extrusion to obtain a 3D extent. The use of B-spline curves exploits their flexible representation power by allowing the control points to move freely. The algorithm is developed into an Extended Kalman Filter (EKF). For a through evaluation of this method, we use simulated traffic scenario of different vehicle models and realworld open dataset containing both radar and lidar data.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2503.09807.pdf' target='_blank'>https://arxiv.org/pdf/2503.09807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingwu Liu, Nicolas Saunier, Guillaume-Alexandre Bilodeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09807">How good are deep learning methods for automated road safety analysis using video data? An experimental study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based multi-object detection (MOD) and multi-object tracking (MOT) are advancing at a fast pace. A variety of 2D and 3D MOD and MOT methods have been developed for monocular and stereo cameras. Road safety analysis can benefit from those advancements. As crashes are rare events, surrogate measures of safety (SMoS) have been developed for safety analyses. (Semi-)Automated safety analysis methods extract road user trajectories to compute safety indicators, for example, Time-to-Collision (TTC) and Post-encroachment Time (PET). Inspired by the success of deep learning in MOD and MOT, we investigate three MOT methods, including one based on a stereo-camera, using the annotated KITTI traffic video dataset. Two post-processing steps, IDsplit and SS, are developed to improve the tracking results and investigate the factors influencing the TTC. The experimental results show that, despite some advantages in terms of the numbers of interactions or similarity to the TTC distributions, all the tested methods systematically over-estimate the number of interactions and under-estimate the TTC: they report more interactions and more severe interactions, making the road user interactions appear less safe than they are. Further efforts will be directed towards testing more methods and more data, in particular from roadside sensors, to verify the results and improve the performance.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2502.17399.pdf' target='_blank'>https://arxiv.org/pdf/2502.17399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liuchuan Yu, Ching-I Huang, Hsueh-Cheng Wang, Lap-Fai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17399">Enriching Physical-Virtual Interaction in AR Gaming by Tracking Identical Real Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Augmented reality (AR) games, particularly those designed for headsets, have become increasingly prevalent with advancements in both hardware and software. However, the majority of AR games still rely on pre-scanned or static scenes, and interaction mechanisms are often limited to controllers or hand-tracking. Additionally, the presence of identical objects in AR games poses challenges for conventional object tracking techniques, which often struggle to differentiate between identical objects or necessitate the installation of fixed cameras for global object movement tracking. In response to these limitations, we present a novel approach to address the tracking of identical objects in an AR scene to enrich physical-virtual interaction. Our method leverages partial scene observations captured by an AR headset, utilizing the perspective and spatial data provided by this technology. Object identities within the scene are determined through the solution of a label assignment problem using integer programming. To enhance computational efficiency, we incorporate a Voronoi diagram-based pruning method into our approach. Our implementation of this approach in a farm-to-table AR game demonstrates its satisfactory performance and robustness. Furthermore, we showcase the versatility and practicality of our method through applications in AR storytelling and a simulated gaming robot. Our video demo is available at: https://youtu.be/rPGkLYuKvCQ.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2501.13710.pdf' target='_blank'>https://arxiv.org/pdf/2501.13710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>IÃ±aki Erregue, Kamal Nasrollahi, Sergio Escalera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13710">YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised Re-ID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT) solution that combines real-time object detection with self-supervised Re-Identification (Re-ID). By incorporating a dedicated Re-ID branch into YOLO11s, our model performs Joint Detection and Embedding (JDE), generating appearance features for each detection. The Re-ID branch is trained in a fully self-supervised setting while simultaneously training for detection, eliminating the need for costly identity-labeled datasets. The triplet loss, with hard positive and semi-hard negative mining strategies, is used for learning discriminative embeddings. Data association is enhanced with a custom tracking implementation that successfully integrates motion, appearance, and location cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20 benchmarks, surpassing existing JDE methods in terms of FPS and using up to ten times fewer parameters. Thus, making our method a highly attractive solution for real-world applications.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2412.13273.pdf' target='_blank'>https://arxiv.org/pdf/2412.13273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrei Znobishchev, Valerii Filev, Oleg Kudashev, Nikita Orlov, Humphrey Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13273">CompactFlowNet: Efficient Real-time Optical Flow Estimation on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present CompactFlowNet, the first real-time mobile neural network for optical flow prediction, which involves determining the displacement of each pixel in an initial frame relative to the corresponding pixel in a subsequent frame. Optical flow serves as a fundamental building block for various video-related tasks, such as video restoration, motion estimation, video stabilization, object tracking, action recognition, and video generation. While current state-of-the-art methods prioritize accuracy, they often overlook constraints regarding speed and memory usage. Existing light models typically focus on reducing size but still exhibit high latency, compromise significantly on quality, or are optimized for high-performance GPUs, resulting in sub-optimal performance on mobile devices. This study aims to develop a mobile-optimized optical flow model by proposing a novel mobile device-compatible architecture, as well as enhancements to the training pipeline, which optimize the model for reduced weight, low memory utilization, and increased speed while maintaining minimal error. Our approach demonstrates superior or comparable performance to the state-of-the-art lightweight models on the challenging KITTI and Sintel benchmarks. Furthermore, it attains a significantly accelerated inference speed, thereby yielding real-time operational efficiency on the iPhone 8, while surpassing real-time performance levels on more advanced mobile devices.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2411.15811.pdf' target='_blank'>https://arxiv.org/pdf/2411.15811.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Wenhui Zhao, Dingwen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15811">FastTrackTr:Towards Fast Multi-Object Tracking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Transformer-based multi-object tracking (MOT) methods have captured the attention of many researchers in recent years. However, these models often suffer from slow inference speeds due to their structure or other issues. To address this problem, we revisited the Joint Detection and Tracking (JDT) method by looking back at past approaches. By integrating the original JDT approach with some advanced theories, this paper employs an efficient method of information transfer between frames on the DETR, constructing a fast and novel JDT-type MOT framework: FastTrackTr. Thanks to the superiority of this information transfer method, our approach not only reduces the number of queries required during tracking but also avoids the excessive introduction of network structures, ensuring model simplicity. Experimental results indicate that our method has the potential to achieve real-time tracking and exhibits competitive tracking accuracy across multiple datasets.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2411.11514.pdf' target='_blank'>https://arxiv.org/pdf/2411.11514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Li, Michael Burke, Subramanian Ramamoorthy, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11514">Learning a Neural Association Network for Self-supervised Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework to learn data association for multi-object tracking in a self-supervised manner. Fully-supervised learning methods are known to achieve excellent tracking performances, but acquiring identity-level annotations is tedious and time-consuming. Motivated by the fact that in real-world scenarios object motion can be usually represented by a Markov process, we present a novel expectation maximization (EM) algorithm that trains a neural network to associate detections for tracking, without requiring prior knowledge of their temporal correspondences. At the core of our method lies a neural Kalman filter, with an observation model conditioned on associations of detections parameterized by a neural network. Given a batch of frames as input, data associations between detections from adjacent frames are predicted by a neural network followed by a Sinkhorn normalization that determines the assignment probabilities of detections to states. Kalman smoothing is then used to obtain the marginal probability of observations given the inferred states, producing a training objective to maximize this marginal probability using gradient descent. The proposed framework is fully differentiable, allowing the underlying neural model to be trained end-to-end. We evaluate our approach on the challenging MOT17, MOT20, and BDD100K datasets and achieve state-of-the-art results in comparison to self-supervised trackers using public detections.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2411.11514.pdf' target='_blank'>https://arxiv.org/pdf/2411.11514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Li, Michael Burke, Subramanian Ramamoorthy, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11514">Learning a Neural Association Network for Self-supervised Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel framework to learn data association for multi-object tracking in a self-supervised manner. Fully-supervised learning methods are known to achieve excellent tracking performances, but acquiring identity-level annotations is tedious and time-consuming. Motivated by the fact that in real-world scenarios object motion can be usually represented by a Markov process, we present a novel expectation maximization (EM) algorithm that trains a neural network to associate detections for tracking, without requiring prior knowledge of their temporal correspondences. At the core of our method lies a neural Kalman filter, with an observation model conditioned on associations of detections parameterized by a neural network. Given a batch of frames as input, data associations between detections from adjacent frames are predicted by a neural network followed by a Sinkhorn normalization that determines the assignment probabilities of detections to states. Kalman smoothing is then used to obtain the marginal probability of observations given the inferred states, producing a training objective to maximize this marginal probability using gradient descent. The proposed framework is fully differentiable, allowing the underlying neural model to be trained end-to-end. We evaluate our approach on the challenging MOT17, MOT20, and BDD100K datasets and achieve state-of-the-art results in comparison to self-supervised trackers using public detections.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2411.09020.pdf' target='_blank'>https://arxiv.org/pdf/2411.09020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirvan Dutta, Etienne Burdet, Mohsen Kaboli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09020">Predictive Visuo-Tactile Interactive Perception Framework for Object Properties Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interactive exploration of the unknown physical properties of objects such as stiffness, mass, center of mass, friction coefficient, and shape is crucial for autonomous robotic systems operating continuously in unstructured environments. Precise identification of these properties is essential to manipulate objects in a stable and controlled way, and is also required to anticipate the outcomes of (prehensile or non-prehensile) manipulation actions such as pushing, pulling, lifting, etc. Our study focuses on autonomously inferring the physical properties of a diverse set of various homogeneous, heterogeneous, and articulated objects utilizing a robotic system equipped with vision and tactile sensors. We propose a novel predictive perception framework for identifying object properties of the diverse objects by leveraging versatile exploratory actions: non-prehensile pushing and prehensile pulling. As part of the framework, we propose a novel active shape perception to seamlessly initiate exploration. Our innovative dual differentiable filtering with Graph Neural Networks learns the object-robot interaction and performs consistent inference of indirectly observable time-invariant object properties. In addition, we formulate a $N$-step information gain approach to actively select the most informative actions for efficient learning and inference. Extensive real-robot experiments with planar objects show that our predictive perception framework results in better performance than the state-of-the-art baseline and demonstrate our framework in three major applications for i) object tracking, ii) goal-driven task, and iii) change in environment detection.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2411.06197.pdf' target='_blank'>https://arxiv.org/pdf/2411.06197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shukun Jia, Shiyu Hu, Yichao Cao, Feng Yang, Xin Lu, Xiaobo Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06197">Tracking by Detection and Query: An Efficient End-to-End Framework for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is dominated by two paradigms: tracking-by-detection (TBD) and tracking-by-query (TBQ). While TBD is decoupled and efficient, its fragmented association steps and heuristic matching pipelines often compromise robustness in complex scenarios. TBQ provides stronger semantic modeling through end-to-end learning, but suffers from high training cost and slow inference due to tight coupling between detection and association. To address these challenges, we propose TBDQ-Net, a unified tracking-by-detection-and-query (TBDQ) framework that effectively combines the strengths of both paradigms. Our method efficiently integrates pretrained, high-performance detectors with an MOT-tailored associator. The associator is lightweight and directly fetches information from the inference of detectors, enhancing the overall efficiency of the framework. The associator is also learnable, making it essential for fully end-to-end optimization, ensuring robust tracking capabilities. Specifically, the associator comprises two key modules: basic information interaction (BII) for comprehensive semantic interaction, and content-position alignment (CPA) for semantic and positional consistency. TBDQ-Net's effectiveness is extensively demonstrated on DanceTrack, SportsMOT and MOT20 benchmarks. As a structurally efficient and semantically robust tracking framework, it outperforms the leading TBD method by 6.0 IDF1 points on DanceTrack and achieves at least 37.5% faster inference than prominent TBQ methods.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2411.02220.pdf' target='_blank'>https://arxiv.org/pdf/2411.02220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryoma Yataka, Pu Perry Wang, Petros Boufounos, Ryuhei Takahashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.02220">SIRA: Scalable Inter-frame Relation and Association for Radar Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Conventional radar feature extraction faces limitations due to low spatial resolution, noise, multipath reflection, the presence of ghost targets, and motion blur. Such limitations can be exacerbated by nonlinear object motion, particularly from an ego-centric viewpoint. It becomes evident that to address these challenges, the key lies in exploiting temporal feature relation over an extended horizon and enforcing spatial motion consistency for effective association. To this end, this paper proposes SIRA (Scalable Inter-frame Relation and Association) with two designs. First, inspired by Swin Transformer, we introduce extended temporal relation, generalizing the existing temporal relation layer from two consecutive frames to multiple inter-frames with temporally regrouped window attention for scalability. Second, we propose motion consistency track with the concept of a pseudo-tracklet generated from observational data for better trajectory prediction and subsequent object association. Our approach achieves 58.11 mAP@0.5 for oriented object detection and 47.79 MOTA for multiple object tracking on the Radiate dataset, surpassing previous state-of-the-art by a margin of +4.11 mAP@0.5 and +9.94 MOTA, respectively.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2410.24183.pdf' target='_blank'>https://arxiv.org/pdf/2410.24183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matteo Tesori, Giorgio Battistelli, Luigi Chisci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.24183">Extended Object Tracking and Classification based on Linear Splines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a framework based on linear splines for 2-dimensional extended object tracking and classification. Unlike state of the art models, linear splines allow to represent extended objects whose contour is an arbitrarily complex curve. An exact likelihood is derived for the case in which noisy measurements can be scattered from any point on the contour of the extended object, while an approximate Monte Carlo likelihood is provided for the case wherein scattering points can be anywhere, i.e. inside or on the contour, on the object surface. Exploiting such likelihood to measure how well the observed data fit a given shape, a suitable estimator is developed. The proposed estimator models the extended object in terms of a kinematic state, providing object position and orientation, along with a shape vector, characterizing object contour and surface. The kinematic state is estimated via a nonlinear Kalman filter, while the shape vector is estimated via a Bayesian classifier so that classification is implicitly solved during shape estimation. Numerical experiments are provided to assess, compared to state of the art extended object estimators, the effectiveness of the proposed one.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2410.16329.pdf' target='_blank'>https://arxiv.org/pdf/2410.16329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Zhong, Yang Yang, Fengqiang Wan, Henglu Wei, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16329">The Solution for Single Object Tracking Task of Perception Test Challenge 2024</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This report presents our method for Single Object Tracking (SOT), which aims to track a specified object throughout a video sequence. We employ the LoRAT method. The essence of the work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. We train our model using the extensive LaSOT and GOT-10k datasets, which provide a solid foundation for robust performance. Additionally, we implement the alpha-refine technique for post-processing the bounding box outputs. Although the alpha-refine method does not yield the anticipated results, our overall approach achieves a score of 0.813, securing first place in the competition.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2409.11785.pdf' target='_blank'>https://arxiv.org/pdf/2409.11785.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiming Ge, Zhao Luo, Chunhui Zhang, Yingying Hua, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11785">Distilling Channels for Efficient Deep Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep trackers have proven success in visual tracking. Typically, these trackers employ optimally pre-trained deep networks to represent all diverse objects with multi-channel features from some fixed layers. The deep networks employed are usually trained to extract rich knowledge from massive data used in object classification and so they are capable to represent generic objects very well. However, these networks are too complex to represent a specific moving object, leading to poor generalization as well as high computational and memory costs. This paper presents a novel and general framework termed channel distillation to facilitate deep trackers. To validate the effectiveness of channel distillation, we take discriminative correlation filter (DCF) and ECO for example. We demonstrate that an integrated formulation can turn feature compression, response map generation, and model update into a unified energy minimization problem to adaptively select informative feature channels that improve the efficacy of tracking moving objects on the fly. Channel distillation can accurately extract good channels, alleviating the influence of noisy channels and generally reducing the number of channels, as well as adaptively generalizing to different channels and networks. The resulting deep tracker is accurate, fast, and has low memory requirements. Extensive experimental evaluations on popular benchmarks clearly demonstrate the effectiveness and generalizability of our framework.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2409.11749.pdf' target='_blank'>https://arxiv.org/pdf/2409.11749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Li, Peidong Li, Lijun Zhao, Dedong Liu, Jinghan Gao, Xian Wu, Yitao Wu, Dixiao Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.11749">RockTrack: A 3D Robust Multi-Camera-Ken Multi-Object Tracking Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Multi-Object Tracking (MOT) obtains significant performance improvements with the rapid advancements in 3D object detection, particularly in cost-effective multi-camera setups. However, the prevalent end-to-end training approach for multi-camera trackers results in detector-specific models, limiting their versatility. Moreover, current generic trackers overlook the unique features of multi-camera detectors, i.e., the unreliability of motion observations and the feasibility of visual information. To address these challenges, we propose RockTrack, a 3D MOT method for multi-camera detectors. Following the Tracking-By-Detection framework, RockTrack is compatible with various off-the-shelf detectors. RockTrack incorporates a confidence-guided preprocessing module to extract reliable motion and image observations from distinct representation spaces from a single detector. These observations are then fused in an association module that leverages geometric and appearance cues to minimize mismatches. The resulting matches are propagated through a staged estimation process, forming the basis for heuristic noise modeling. Additionally, we introduce a novel appearance similarity metric for explicitly characterizing object affinities in multi-camera settings. RockTrack achieves state-of-the-art performance on the nuScenes vision-only tracking leaderboard with 59.1% AMOTA while demonstrating impressive computational efficiency.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2409.07904.pdf' target='_blank'>https://arxiv.org/pdf/2409.07904.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongzihan Song, Zhenyu Weng, Huiping Zhuang, Jinchang Ren, Yongming Chen, Zhiping Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07904">FACT: Feature Adaptive Continual-learning Tracker for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT) involves identifying multiple targets and assigning them corresponding IDs within a video sequence, where occlusions are often encountered. Recent methods address occlusions using appearance cues through online learning techniques to improve adaptivity or offline learning techniques to utilize temporal information from videos. However, most existing online learning-based MOT methods are unable to learn from all past tracking information to improve adaptivity on long-term occlusions while maintaining real-time tracking speed. On the other hand, temporal information-based offline learning methods maintain a long-term memory to store past tracking information, but this approach restricts them to use only local past information during tracking. To address these challenges, we propose a new MOT framework called the Feature Adaptive Continual-learning Tracker (FACT), which enables real-time tracking and feature learning for targets by utilizing all past tracking information. We demonstrate that the framework can be integrated with various state-of-the-art feature-based trackers, thereby improving their tracking ability. Specifically, we develop the feature adaptive continual-learning (FAC) module, a neural network that can be trained online to learn features adaptively using all past tracking information during tracking. Moreover, we also introduce a two-stage association module specifically designed for the proposed continual learning-based tracking. Extensive experiment results demonstrate that the proposed method achieves state-of-the-art online tracking performance on MOT17 and MOT20 benchmarks. The code will be released upon acceptance.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2409.00339.pdf' target='_blank'>https://arxiv.org/pdf/2409.00339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Makoto M. Itoh, Qingrui Hu, Takayuki Niizato, Hiroaki Kawashima, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00339">Fish Tracking Challenge 2024: A Multi-Object Tracking Competition with Sweetfish Schooling Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The study of collective animal behavior, especially in aquatic environments, presents unique challenges and opportunities for understanding movement and interaction patterns in the field of ethology, ecology, and bio-navigation. The Fish Tracking Challenge 2024 (https://ftc-2024.github.io/) introduces a multi-object tracking competition focused on the intricate behaviors of schooling sweetfish. Using the SweetFish dataset, participants are tasked with developing advanced tracking models to accurately monitor the locations of 10 sweetfishes simultaneously. This paper introduces the competition's background, objectives, the SweetFish dataset, and the appraoches of the 1st to 3rd winners and our baseline. By leveraging video data and bounding box annotations, the competition aims to foster innovation in automatic detection and tracking algorithms, addressing the complexities of aquatic animal movements. The challenge provides the importance of multi-object tracking for discovering the dynamics of collective animal behavior, with the potential to significantly advance scientific understanding in the above fields.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2408.17098.pdf' target='_blank'>https://arxiv.org/pdf/2408.17098.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgardo Solano-Carrillo, Felix Sattler, Antje Alex, Alexander Klein, Bruno Pereira Costa, Angel Bueno Rodriguez, Jannis Stoppe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.17098">UTrack: Multi-Object Tracking with Uncertain Detections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The tracking-by-detection paradigm is the mainstream in multi-object tracking, associating tracks to the predictions of an object detector. Although exhibiting uncertainty through a confidence score, these predictions do not capture the entire variability of the inference process. For safety and security critical applications like autonomous driving, surveillance, etc., knowing this predictive uncertainty is essential though. Therefore, we introduce, for the first time, a fast way to obtain the empirical predictive distribution during object detection and incorporate that knowledge in multi-object tracking. Our mechanism can easily be integrated into state-of-the-art trackers, enabling them to fully exploit the uncertainty in the detections. Additionally, novel association methods are introduced that leverage the proposed mechanism. We demonstrate the effectiveness of our contribution on a variety of benchmarks, such as MOT17, MOT20, DanceTrack, and KITTI.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2407.04249.pdf' target='_blank'>https://arxiv.org/pdf/2407.04249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Hashempoor, Rosemary Koikara, Yu Dong Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04249">FeatureSORT: Essential Features for Effective Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FeatureSORT, a simple yet effective online multiple object tracker that reinforces the DeepSORT baseline with a redesigned detector and additional feature cues. In contrast to conventional detectors that only provide bounding boxes, our modified YOLOX architecture is extended to output multiple appearance attributes, including clothing color, clothing style, and motion direction, alongside the bounding boxes. These feature cues, together with a ReID network, form complementary embeddings that substantially improve association accuracy. Furthermore, we incorporate stronger post-processing strategies, such as global linking and Gaussian Smoothing Process interpolation, to handle missing associations and detections. During online tracking, we define a measurement-to-track distance function that jointly considers IoU, direction, color, style, and ReID similarity. This design enables FeatureSORT to maintain consistent identities through longer occlusions while reducing identity switches. Extensive experiments on standard MOT benchmarks demonstrate that FeatureSORT achieves state-of-the-art online performance, with MOTA scores of 79.7 on MOT16, 80.6 on MOT17, 77.9 on MOT20, and 92.2 on DanceTrack, underscoring the effectiveness of feature-enriched detection and modular post processing in advancing multi-object tracking.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2407.04249.pdf' target='_blank'>https://arxiv.org/pdf/2407.04249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Hashempoor, Rosemary Koikara, Yu Dong Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04249">FeatureSORT: Essential Features for Effective Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce FeatureSORT, a simple yet effective online multiple object tracker that reinforces the DeepSORT baseline with a redesigned detector and additional feature cues. In contrast to conventional detectors that only provide bounding boxes, our modified YOLOX architecture is extended to output multiple appearance attributes, including clothing color, clothing style, and motion direction, alongside the bounding boxes. These feature cues, together with a ReID network, form complementary embeddings that substantially improve association accuracy. Furthermore, we incorporate stronger post-processing strategies, such as global linking and Gaussian Smoothing Process interpolation, to handle missing associations and detections. During online tracking, we define a measurement-to-track distance function that jointly considers IoU, direction, color, style, and ReID similarity. This design enables FeatureSORT to maintain consistent identities through longer occlusions while reducing identity switches. Extensive experiments on standard MOT benchmarks demonstrate that FeatureSORT achieves state-of-the-art online performance, with MOTA scores of 79.7 on MOT16, 80.6 on MOT17, 77.9 on MOT20, and 92.2 on DanceTrack, underscoring the effectiveness of feature-enriched detection and modular post processing in advancing multi-object tracking.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2407.03084.pdf' target='_blank'>https://arxiv.org/pdf/2407.03084.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longfei Han, Qiuyu Xu, Klaus KefferpÃ¼tz, Gordon Elger, JÃ¼rgen Beyerer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03084">Applying Extended Object Tracking for Self-Localization of Roadside Radar Sensors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent Transportation Systems (ITS) can benefit from roadside 4D mmWave radar sensors for large-scale traffic monitoring due to their weatherproof functionality, long sensing range and low manufacturing cost. However, the localization method using external measurement devices has limitations in urban environments. Furthermore, if the sensor mount exhibits changes due to environmental influences, they cannot be corrected when the measurement is performed only during the installation. In this paper, we propose self-localization of roadside radar data using Extended Object Tracking (EOT). The method analyses both the tracked trajectories of the vehicles observed by the sensor and the aerial laser scan of city streets, assigns labels of driving behaviors such as "straight ahead", "left turn", "right turn" to trajectory sections and road segments, and performs Semantic Iterative Closest Points (SICP) algorithm to register the point cloud. The method exploits the result from a down stream task -- object tracking -- for localization. We demonstrate high accuracy in the sub-meter range along with very low orientation error. The method also shows good data efficiency. The evaluation is done in both simulation and real-world tests.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2406.20024.pdf' target='_blank'>https://arxiv.org/pdf/2406.20024.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Chen, Lin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.20024">eMoE-Tracker: Environmental MoE-based Transformer for Robust Event-guided Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The unique complementarity of frame-based and event cameras for high frame rate object tracking has recently inspired some research attempts to develop multi-modal fusion approaches. However, these methods directly fuse both modalities and thus ignore the environmental attributes, e.g., motion blur, illumination variance, occlusion, scale variation, etc. Meanwhile, insufficient interaction between search and template features makes distinguishing target objects and backgrounds difficult. As a result, performance degradation is induced especially in challenging conditions. This paper proposes a novel and effective Transformer-based event-guided tracking framework, called eMoE-Tracker, which achieves new SOTA performance under various conditions. Our key idea is to disentangle the environment into several learnable attributes to dynamically learn the attribute-specific features and strengthen the target information by improving the interaction between the target template and search regions. To achieve the goal, we first propose an environmental Mix-of-Experts (eMoE) module that is built upon the environmental Attributes Disentanglement to learn attribute-specific features and environmental Attributes Assembling to assemble the attribute-specific features by the learnable attribute scores dynamically. The eMoE module is a subtle router that prompt-tunes the transformer backbone more efficiently. We then introduce a contrastive relation modeling (CRM) module to emphasize target information by leveraging a contrastive learning strategy between the target template and search regions. Extensive experiments on diverse event-based benchmark datasets showcase the superior performance of our eMoE-Tracker compared to the prior arts.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2406.12081.pdf' target='_blank'>https://arxiv.org/pdf/2406.12081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matias Gran-Henriksen, Hans Andreas Lindgaard, Gabriel Kiss, Frank Lindseth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.12081">Deep HM-SORT: Enhancing Multi-Object Tracking in Sports with Deep Features, Harmonic Mean, and Expansion IOU</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces Deep HM-SORT, a novel online multi-object tracking algorithm specifically designed to enhance the tracking of athletes in sports scenarios. Traditional multi-object tracking methods often struggle with sports environments due to the similar appearances of players, irregular and unpredictable movements, and significant camera motion. Deep HM-SORT addresses these challenges by integrating deep features, harmonic mean, and Expansion IOU. By leveraging the harmonic mean, our method effectively balances appearance and motion cues, significantly reducing ID-swaps. Additionally, our approach retains all tracklets indefinitely, improving the re-identification of players who leave and re-enter the frame. Experimental results demonstrate that Deep HM-SORT achieves state-of-the-art performance on two large-scale public benchmarks, SportsMOT and SoccerNet Tracking Challenge 2023. Specifically, our method achieves 80.1 HOTA on the SportsMOT dataset and 85.4 HOTA on the SoccerNet-Tracking dataset, outperforming existing trackers in key metrics such as HOTA, IDF1, AssA, and MOTA. This robust solution provides enhanced accuracy and reliability for automated sports analytics, offering significant improvements over previous methods without introducing additional computational cost.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2405.08909.pdf' target='_blank'>https://arxiv.org/pdf/2405.08909.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuxiao Ding, Lukas Schneider, Marius Cordts, Juergen Gall
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08909">ADA-Track++: End-to-End Multi-Camera 3D Multi-Object Tracking with Alternating Detection and Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the tracking-by-attention paradigm, utilizing track queries for identity-consistent detection and object queries for identity-agnostic track spawning. Tracking-by-attention, however, entangles detection and tracking queries in one embedding for both the detection and tracking task, which is sub-optimal. Other approaches resemble the tracking-by-detection paradigm and detect objects using decoupled track and detection queries followed by a subsequent association. These methods, however, do not leverage synergies between the detection and association task. Combining the strengths of both paradigms, we introduce ADA-Track++, a novel end-to-end framework for 3D MOT from multi-view cameras. We introduce a learnable data association module based on edge-augmented cross-attention, leveraging appearance and geometric features. We also propose an auxiliary token in this attention-based association module, which helps mitigate disproportionately high attention to incorrect association targets caused by attention normalization. Furthermore, we integrate this association module into the decoder layer of a DETR-based 3D detector, enabling simultaneous DETR-like query-to-image cross-attention for detection and query-to-query cross-attention for data association. By stacking these decoder layers, queries are refined for the detection and association task alternately, effectively harnessing the task dependencies. We evaluate our method on the nuScenes dataset and demonstrate the advantage of our approach compared to the two previous paradigms.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2405.05646.pdf' target='_blank'>https://arxiv.org/pdf/2405.05646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gerardo Duran-Martin, Matias Altamirano, Alexander Y. Shestopaloff, Leandro SÃ¡nchez-Betancourt, Jeremias Knoblauch, Matt Jones, FranÃ§ois-Xavier Briol, Kevin Murphy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05646">Outlier-robust Kalman Filtering through Generalised Bayes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We derive a novel, provably robust, and closed-form Bayesian update rule for online filtering in state-space models in the presence of outliers and misspecified measurement models. Our method combines generalised Bayesian inference with filtering methods such as the extended and ensemble Kalman filter. We use the former to show robustness and the latter to ensure computational efficiency in the case of nonlinear models. Our method matches or outperforms other robust filtering methods (such as those based on variational Bayes) at a much lower computational cost. We show this empirically on a range of filtering problems with outlier measurements, such as object tracking, state estimation in high-dimensional chaotic systems, and online learning of neural networks.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2403.08018.pdf' target='_blank'>https://arxiv.org/pdf/2403.08018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Miah, Guillaume-Alexandre Bilodeau, Nicolas Saunier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.08018">Learning Data Association for Multi-Object Tracking using Only Coordinates</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel Transformer-based module to address the data association problem for multi-object tracking. From detections obtained by a pretrained detector, this module uses only coordinates from bounding boxes to estimate an affinity score between pairs of tracks extracted from two distinct temporal windows. This module, named TWiX, is trained on sets of tracks with the objective of discriminating pairs of tracks coming from the same object from those which are not. Our module does not use the intersection over union measure, nor does it requires any motion priors or any camera motion compensation technique. By inserting TWiX within an online cascade matching pipeline, our tracker C-TWiX achieves state-of-the-art performance on the DanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17 dataset. The code will be made available upon publication.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2403.02075.pdf' target='_blank'>https://arxiv.org/pdf/2403.02075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiyi Lv, Yuhang Huang, Ning Zhang, Ruei-Sung Lin, Mei Han, Dan Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02075">DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In Multiple Object Tracking, objects often exhibit non-linear motion of acceleration and deceleration, with irregular direction changes. Tacking-by-detection (TBD) trackers with Kalman Filter motion prediction work well in pedestrian-dominant scenarios but fall short in complex situations when multiple objects perform non-linear and diverse motion simultaneously. To tackle the complex non-linear motion, we propose a real-time diffusion-based MOT approach named DiffMOT. Specifically, for the motion predictor component, we propose a novel Decoupled Diffusion-based Motion Predictor (D$^2$MP). It models the entire distribution of various motion presented by the data as a whole. It also predicts an individual object's motion conditioning on an individual's historical motion information. Furthermore, it optimizes the diffusion process with much fewer sampling steps. As a MOT tracker, the DiffMOT is real-time at 22.7FPS, and also outperforms the state-of-the-art on DanceTrack and SportsMOT datasets with $62.3\%$ and $76.2\%$ in HOTA metrics, respectively. To the best of our knowledge, DiffMOT is the first to introduce a diffusion probabilistic model into the MOT to tackle non-linear motion prediction.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2401.13950.pdf' target='_blank'>https://arxiv.org/pdf/2401.13950.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vitaliy Kim, Gunho Jung, Seong-Whan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13950">AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many multi-object tracking (MOT) approaches, which employ the Kalman Filter as a motion predictor, assume constant velocity and Gaussian-distributed filtering noises. These assumptions render the Kalman Filter-based trackers effective in linear motion scenarios. However, these linear assumptions serve as a key limitation when estimating future object locations within scenarios involving non-linear motion and occlusions. To address this issue, we propose a motion-based MOT approach with an adaptable motion predictor, called AM-SORT, which adapts to estimate non-linear uncertainties. AM-SORT is a novel extension of the SORT-series trackers that supersedes the Kalman Filter with the transformer architecture as a motion predictor. We introduce a historical trajectory embedding that empowers the transformer to extract spatio-temporal features from a sequence of bounding boxes. AM-SORT achieves competitive performance compared to state-of-the-art trackers on DanceTrack, with 56.3 IDF1 and 55.6 HOTA. We conduct extensive experiments to demonstrate the effectiveness of our method in predicting non-linear movement under occlusions.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2401.12743.pdf' target='_blank'>https://arxiv.org/pdf/2401.12743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Xie, Wankou Yang, Chunyu Wang, Lei Chu, Yue Cao, Chao Ma, Wenjun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12743">Correlation-Embedded Transformer Tracking: A Single-Branch Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing robust and discriminative appearance models has been a long-standing research challenge in visual object tracking. In the prevalent Siamese-based paradigm, the features extracted by the Siamese-like networks are often insufficient to model the tracked targets and distractor objects, thereby hindering them from being robust and discriminative simultaneously. While most Siamese trackers focus on designing robust correlation operations, we propose a novel single-branch tracking framework inspired by the transformer. Unlike the Siamese-like feature extraction, our tracker deeply embeds cross-image feature correlation in multiple layers of the feature network. By extensively matching the features of the two images through multiple layers, it can suppress non-target features, resulting in target-aware feature extraction. The output features can be directly used for predicting target locations without additional correlation steps. Thus, we reformulate the two-branch Siamese tracking as a conceptually simple, fully transformer-based Single-Branch Tracking pipeline, dubbed SBT. After conducting an in-depth analysis of the SBT baseline, we summarize many effective design principles and propose an improved tracker dubbed SuperSBT. SuperSBT adopts a hierarchical architecture with a local modeling layer to enhance shallow-level features. A unified relation modeling is proposed to remove complex handcrafted layer pattern designs. SuperSBT is further improved by masked image modeling pre-training, integrating temporal modeling, and equipping with dedicated prediction heads. Thus, SuperSBT outperforms the SBT baseline by 4.7%,3.0%, and 4.5% AUC scores in LaSOT, TrackingNet, and GOT-10K. Notably, SuperSBT greatly raises the speed of SBT from 37 FPS to 81 FPS. Extensive experiments show that our method achieves superior results on eight VOT benchmarks.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2401.10805.pdf' target='_blank'>https://arxiv.org/pdf/2401.10805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paritosh Parmar, Eric Peh, Basura Fernando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10805">Learning to Visually Connect Actions and their Effects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We identify and explore two different aspects of the concept of CATE: Action Selection (AS) and Effect-Affinity Assessment (EAA), where video understanding models connect actions and effects at semantic and fine-grained levels, respectively. We design various baseline models for AS and EAA. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. Our experiments show that in solving AS and EAA, models learn intuitive properties like object tracking and pose encoding without explicit supervision. We demonstrate that CATE can be an effective self-supervised task for learning video representations from unlabeled videos. The study aims to showcase the fundamental nature and versatility of CATE, with the hope of inspiring advanced formulations and models.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2312.08411.pdf' target='_blank'>https://arxiv.org/pdf/2312.08411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John Lloyd, Nathan F. Lepora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08411">Pose and shear-based tactile servoing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile servoing is an important technique because it enables robots to manipulate objects with precision and accuracy while adapting to changes in their environments in real-time. One approach for tactile servo control with high-resolution soft tactile sensors is to estimate the contact pose relative to an object surface using a convolutional neural network (CNN) for use as a feedback signal. In this paper, we investigate how the surface pose estimation model can be extended to include shear, and utilize these combined pose-and-shear models to develop a tactile robotic system that can be programmed for diverse non-prehensile manipulation tasks, such as object tracking, surface following, single-arm object pushing and dual-arm object pushing. In doing this, two technical challenges had to be overcome. Firstly, the use of tactile data that includes shear-induced slippage can lead to error-prone estimates unsuitable for accurate control, and so we modified the CNN into a Gaussian-density neural network and used a discriminative Bayesian filter to improve the predictions with a state dynamics model that utilizes the robot kinematics. Secondly, to achieve smooth robot motion in 3D space while interacting with objects, we used SE(3) velocity-based servo control, which required re-deriving the Bayesian filter update equations using Lie group theory, as many standard assumptions do not hold for state variables defined on non-Euclidean manifolds. In future, we believe that pose and shear-based tactile servoing will enable many object manipulation tasks and the fully-dexterous utilization of multi-fingered tactile robot hands. Video: https://www.youtube.com/watch?v=xVs4hd34ek0
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2311.08043.pdf' target='_blank'>https://arxiv.org/pdf/2311.08043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre-FranÃ§ois De Plaen, Nicola Marinello, Marc Proesmans, Tinne Tuytelaars, Luc Van Gool
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08043">Contrastive Learning for Multi-Object Tracking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The DEtection TRansformer (DETR) opened new possibilities for object detection by modeling it as a translation task: converting image features into object-level representations. Previous works typically add expensive modules to DETR to perform Multi-Object Tracking (MOT), resulting in more complicated architectures. We instead show how DETR can be turned into a MOT model by employing an instance-level contrastive loss, a revised sampling strategy and a lightweight assignment method. Our training scheme learns object appearances while preserving detection capabilities and with little overhead. Its performance surpasses the previous state-of-the-art by +2.6 mMOTA on the challenging BDD100K dataset and is comparable to existing transformer-based methods on the MOT17 dataset.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2311.02749.pdf' target='_blank'>https://arxiv.org/pdf/2311.02749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elham Amin Mansour, Hehui Zheng, Robert K. Katzschmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02749">Fast Point Cloud to Mesh Reconstruction for Deformable Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The world around us is full of soft objects we perceive and deform with dexterous hand movements. For a robotic hand to control soft objects, it has to acquire online state feedback of the deforming object. While RGB-D cameras can collect occluded point clouds at a rate of 30Hz, this does not represent a continuously trackable object surface. Hence, in this work, we developed a method that takes as input a template mesh which is the mesh of an object in its non-deformed state and a deformed point cloud of the same object, and then shapes the template mesh such that it matches the deformed point cloud. The reconstruction of meshes from point clouds has long been studied in the field of Computer graphics under 3D reconstruction and 4D reconstruction, however, both lack the speed and generalizability needed for robotics applications. Our model is designed using a point cloud auto-encoder and a Real-NVP architecture. Our trained model can perform mesh reconstruction and tracking at a rate of 58Hz on a template mesh of 3000 vertices and a deformed point cloud of 5000 points and is generalizable to the deformations of six different object categories which are assumed to be made of soft material in our experiments (scissors, hammer, foam brick, cleanser bottle, orange, and dice). The object meshes are taken from the YCB benchmark dataset. An instance of a downstream application can be the control algorithm for a robotic hand that requires online feedback from the state of the manipulated object which would allow online grasp adaptation in a closed-loop manner. Furthermore, the tracking capacity of our method can help in the system identification of deforming objects in a marker-free approach. In future work, we will extend our trained model to generalize beyond six object categories and additionally to real-world deforming point clouds.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2310.19413.pdf' target='_blank'>https://arxiv.org/pdf/2310.19413.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Rollo, Andrea Zunino, Nikolaos Tsagarakis, Enrico Mingo Hoffman, Arash Ajoudani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19413">CARPE-ID: Continuously Adaptable Re-identification for Personalized Robot Assistance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In today's Human-Robot Interaction (HRI) scenarios, a prevailing tendency exists to assume that the robot shall cooperate with the closest individual or that the scene involves merely a singular human actor. However, in realistic scenarios, such as shop floor operations, such an assumption may not hold and personalized target recognition by the robot in crowded environments is required. To fulfil this requirement, in this work, we propose a person re-identification module based on continual visual adaptation techniques that ensure the robot's seamless cooperation with the appropriate individual even subject to varying visual appearances or partial or complete occlusions. We test the framework singularly using recorded videos in a laboratory environment and an HRI scenario, i.e., a person-following task by a mobile robot. The targets are asked to change their appearance during tracking and to disappear from the camera field of view to test the challenging cases of occlusion and outfit variations. We compare our framework with one of the state-of-the-art Multi-Object Tracking (MOT) methods and the results show that the CARPE-ID can accurately track each selected target throughout the experiments in all the cases (except two limit cases). At the same time, the s-o-t-a MOT has a mean of 4 tracking errors for each video.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2310.14194.pdf' target='_blank'>https://arxiv.org/pdf/2310.14194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingkai Fu, Meng Li, Wenxi Liu, Yuanchen Wang, Jiqing Zhang, Baocai Yin, Xiaopeng Wei, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14194">Distractor-aware Event-based Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event cameras, or dynamic vision sensors, have recently achieved success from fundamental vision tasks to high-level vision researches. Due to its ability to asynchronously capture light intensity changes, event camera has an inherent advantage to capture moving objects in challenging scenarios including objects under low light, high dynamic range, or fast moving objects. Thus event camera are natural for visual object tracking. However, the current event-based trackers derived from RGB trackers simply modify the input images to event frames and still follow conventional tracking pipeline that mainly focus on object texture for target distinction. As a result, the trackers may not be robust dealing with challenging scenarios such as moving cameras and cluttered foreground. In this paper, we propose a distractor-aware event-based tracker that introduces transformer modules into Siamese network architecture (named DANet). Specifically, our model is mainly composed of a motion-aware network and a target-aware network, which simultaneously exploits both motion cues and object contours from event data, so as to discover motion objects and identify the target object by removing dynamic distractors. Our DANet can be trained in an end-to-end manner without any post-processing and can run at over 80 FPS on a single V100. We conduct comprehensive experiments on two large event tracking datasets to validate the proposed model. We demonstrate that our tracker has superior performance against the state-of-the-art trackers in terms of both accuracy and efficiency.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2310.00242.pdf' target='_blank'>https://arxiv.org/pdf/2310.00242.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Tay Yu Liang, Kanji Tanaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00242">Walking = Traversable? : Traversability Prediction via Multiple Human Object Tracking under Occlusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emerging ``Floor plan from human trails (PfH)" technique has great potential for improving indoor robot navigation by predicting the traversability of occluded floors. This study presents an innovative approach that replaces first-person-view sensors with a third-person-view monocular camera mounted on the observer robot. This approach can gather measurements from multiple humans, expanding its range of applications. The key idea is to use two types of trackers, SLAM and MOT, to monitor stationary objects and moving humans and assess their interactions. This method achieves stable predictions of traversability even in challenging visual scenarios, such as occlusions, nonlinear perspectives, depth uncertainty, and intersections involving multiple humans. Additionally, we extend map quality metrics to apply to traversability maps, facilitating future research. We validate our proposed method through fusion and comparison with established techniques.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2309.13257.pdf' target='_blank'>https://arxiv.org/pdf/2309.13257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guotian Zeng, Bi Zeng, Hong Zhang, Jianqi Liu, Qingmao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13257">RTrack: Accelerating Convergence for Visual Object Tracking via Pseudo-Boxes Exploration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking (SOT) heavily relies on the representation of the target object as a bounding box. However, due to the potential deformation and rotation experienced by the tracked targets, the genuine bounding box fails to capture the appearance information explicitly and introduces cluttered background. This paper proposes RTrack, a novel object representation baseline tracker that utilizes a set of sample points to get a pseudo bounding box. RTrack automatically arranges these points to define the spatial extents and highlight local areas. Building upon the baseline, we conducted an in-depth exploration of the training potential and introduced a one-to-many leading assignment strategy. It is worth noting that our approach achieves competitive performance to the state-of-the-art trackers on the GOT-10k dataset while reducing training time to just 10% of the previous state-of-the-art (SOTA) trackers' training costs. The substantial reduction in training costs brings single-object tracking (SOT) closer to the object detection (OD) task. Extensive experiments demonstrate that our proposed RTrack achieves SOTA results with faster convergence.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2309.02903.pdf' target='_blank'>https://arxiv.org/pdf/2309.02903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingmao Wei, Bi Zeng, Guotian Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02903">Towards Efficient Training with Negative Samples in Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current state-of-the-art (SOTA) methods in visual object tracking often require extensive computational resources and vast amounts of training data, leading to a risk of overfitting. This study introduces a more efficient training strategy to mitigate overfitting and reduce computational requirements. We balance the training process with a mix of negative and positive samples from the outset, named as Joint learning with Negative samples (JN). Negative samples refer to scenarios where the object from the template is not present in the search region, which helps to prevent the model from simply memorizing the target, and instead encourages it to use the template for object location. To handle the negative samples effectively, we adopt a distribution-based head, which modeling the bounding box as distribution of distances to express uncertainty about the target's location in the presence of negative samples, offering an efficient way to manage the mixed sample training. Furthermore, our approach introduces a target-indicating token. It encapsulates the target's precise location within the template image. This method provides exact boundary details with negligible computational cost but improving performance. Our model, JN-256, exhibits superior performance on challenging benchmarks, achieving 75.8% AO on GOT-10k and 84.1% AUC on TrackingNet. Notably, JN-256 outperforms previous SOTA trackers that utilize larger models and higher input resolutions, even though it is trained with only half the number of data sampled used in those works.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2309.02676.pdf' target='_blank'>https://arxiv.org/pdf/2309.02676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingmao Wei, Guotian Zeng, Bi Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02676">Efficient Training for Visual Tracking with Deformable Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent Transformer-based visual tracking models have showcased superior performance. Nevertheless, prior works have been resource-intensive, requiring prolonged GPU training hours and incurring high GFLOPs during inference due to inefficient training methods and convolution-based target heads. This intensive resource use renders them unsuitable for real-world applications. In this paper, we present DETRack, a streamlined end-to-end visual object tracking framework. Our framework utilizes an efficient encoder-decoder structure where the deformable transformer decoder acting as a target head, achieves higher sparsity than traditional convolution heads, resulting in decreased GFLOPs. For training, we introduce a novel one-to-many label assignment and an auxiliary denoising technique, significantly accelerating model's convergence. Comprehensive experiments affirm the effectiveness and efficiency of our proposed method. For instance, DETRack achieves 72.9% AO on challenging GOT-10k benchmarks using only 20% of the training epochs required by the baseline, and runs with lower GFLOPs than all the transformer-based trackers.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2305.15688.pdf' target='_blank'>https://arxiv.org/pdf/2305.15688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiqing Zhang, Yuanchen Wang, Wenxi Liu, Meng Li, Jinpeng Bai, Baocai Yin, Xin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15688">Frame-Event Alignment and Fusion Network for High Frame Rate Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker's functionality in the real world, especially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture information like conventional cameras. This unique complementarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. Inthispaper, we propose an end-to-end network consisting of multi-modality alignment and fusion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-style and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events. While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Extensive experiments show that the proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2305.05062.pdf' target='_blank'>https://arxiv.org/pdf/2305.05062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeokhyen Kwon, Chaitra Hegde, Yashar Kiarashi, Venkata Siva Krishna Madala, Ratan Singh, ArjunSinh Nakum, Robert Tweedy, Leandro Miletto Tonetto, Craig M. Zimring, Matthew Doiron, Amy D. Rodriguez, Allan I. Levey, Gari D. Clifford
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05062">A Feasibility Study on Indoor Localization and Multi-person Tracking Using Sparsely Distributed Camera Network with Edge Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Camera-based activity monitoring systems are becoming an attractive solution for smart building applications with the advances in computer vision and edge computing technologies. In this paper, we present a feasibility study and systematic analysis of a camera-based indoor localization and multi-person tracking system implemented on edge computing devices within a large indoor space. To this end, we deployed an end-to-end edge computing pipeline that utilizes multiple cameras to achieve localization, body orientation estimation and tracking of multiple individuals within a large therapeutic space spanning $1700m^2$, all while maintaining a strong focus on preserving privacy. Our pipeline consists of 39 edge computing camera systems equipped with Tensor Processing Units (TPUs) placed in the indoor space's ceiling. To ensure the privacy of individuals, a real-time multi-person pose estimation algorithm runs on the TPU of the computing camera system. This algorithm extracts poses and bounding boxes, which are utilized for indoor localization, body orientation estimation, and multi-person tracking. Our pipeline demonstrated an average localization error of 1.41 meters, a multiple-object tracking accuracy score of 88.6\%, and a mean absolute body orientation error of 29\degree. These results shows that localization and tracking of individuals in a large indoor space is feasible even with the privacy constrains.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2304.11262.pdf' target='_blank'>https://arxiv.org/pdf/2304.11262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sourav Sinha, Mazen Farhood
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11262">Stochastic MPC Based Attacks on Object Tracking in Autonomous Driving Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decision making in advanced driver assistance systems involves in general the estimated trajectories of the surrounding objects. Multiple object tracking refers to the process of estimating in real time these trajectories, leveraging for this purpose sensors to detect the objects. This paper deals with devising attacks on object tracking in automated vehicles. The vehicle is assumed to have a detection-based object tracking system that relies on multiple sensors and uses an estimator such as a Kalman filter for sensor fusion and state estimation. The attack goal is to modify the object's state estimated by the victim vehicle to put the vehicle in an unsafe situation. This goal is achieved by judiciously perturbing some or all of the sensor outputs corresponding to the object of interest over a desired horizon. A stochastic model predictive control (SMPC) problem is formulated to compute the sequence of perturbations, whereby hard constraints on the perturbations and probabilistic chance constraints on the object's state are imposed. The chance constraints ensure that some desired conditions for a successful attack are satisfied with a prespecified probability. Reasonable assumptions are then made to obtain a computationally tractable linear SMPC program. The approach is demonstrated on an adaptive cruise control system in a simulation environment, where successful sequential attacks are generated, leading the victim vehicle into dangerous driving situations including collisions.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2302.10645.pdf' target='_blank'>https://arxiv.org/pdf/2302.10645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malte Pedersen, Daniel LehotskÃ½, Ivan Nikolov, Thomas B. Moeslund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10645">BrackishMOT: The Brackish Multi-Object Tracking Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>There exist no publicly available annotated underwater multi-object tracking (MOT) datasets captured in turbid environments. To remedy this we propose the BrackishMOT dataset with focus on tracking schools of small fish, which is a notoriously difficult MOT task. BrackishMOT consists of 98 sequences captured in the wild. Alongside the novel dataset, we present baseline results by training a state-of-the-art tracker. Additionally, we propose a framework for creating synthetic sequences in order to expand the dataset. The framework consists of animated fish models and realistic underwater environments. We analyse the effects of including synthetic data during training and show that a combination of real and synthetic underwater training data can enhance tracking performance. Links to code and data can be found at https://www.vap.aau.dk/brackishmot
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2301.08189.pdf' target='_blank'>https://arxiv.org/pdf/2301.08189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mihir Durve, Sibilla Orsini, Adriano Tiribocchi, Andrea Montessori, Jean-Michel Tucny, Marco Lauricella, Andrea Camposeo, Dario Pisignano, Sauro Succi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.08189">Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking droplets in microfluidics is a challenging task. The difficulty arises in choosing a tool to analyze general microfluidic videos to infer physical quantities. The state-of-the-art object detector algorithm You Only Look Once (YOLO) and the object tracking algorithm Simple Online and Realtime Tracking with a Deep Association Metric (DeepSORT) are customizable for droplet identification and tracking. The customization includes training YOLO and DeepSORT networks to identify and track the objects of interest. We trained several YOLOv5 and YOLOv7 models and the DeepSORT network for droplet identification and tracking from microfluidic experimental videos. We compare the performance of the droplet tracking applications with YOLOv5 and YOLOv7 in terms of training time and time to analyze a given video across various hardware configurations. Despite the latest YOLOv7 being 10% faster, the real-time tracking is only achieved by lighter YOLO models on RTX 3070 Ti GPU machine due to additional significant droplet tracking costs arising from the DeepSORT algorithm. This work is a benchmark study for the YOLOv5 and YOLOv7 networks with DeepSORT in terms of the training time and inference time for a custom dataset of microfluidic droplets.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2212.13930.pdf' target='_blank'>https://arxiv.org/pdf/2212.13930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Francesca Meneghello, Cheng Chen, Carlos Cordeiro, Francesco Restuccia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.13930">Toward Integrated Sensing and Communications in IEEE 802.11bf Wi-Fi Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Wi-Fi becomes ubiquitous in public and private spaces, it becomes natural to leverage its intrinsic ability to sense the surrounding environment to implement groundbreaking wireless sensing applications such as human presence detection, activity recognition, and object tracking. For this reason, the IEEE 802.11bf Task Group is defining the appropriate modifications to existing Wi-Fi standards to enhance sensing capabilities through 802.11-compliant devices. However, the new standard is expected to leave the specific sensing algorithms open to implementation. To fill this gap, this article explores the practical implications of integrating sensing into Wi-Fi networks. We provide an overview of the physical and medium access control layers sensing enablers, together with the application layer perspective. We analyze the impact of communication parameters on sensing performance and detail the main research challenges. To make our evaluation replicable, we pledge to release all of our dataset and code to the community.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2212.05861.pdf' target='_blank'>https://arxiv.org/pdf/2212.05861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihong Ren, Denglu Wu, Hui Cao, Xi'ai Chen, Zhi Han, Honghai Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.05861">Joint Counting, Detection and Re-Identification for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent trend in 2D multiple object tracking (MOT) is jointly solving detection and tracking, where object detection and appearance feature (or motion) are learned simultaneously. Despite competitive performance, in crowded scenes, joint detection and tracking usually fail to find accurate object associations due to missed or false detections. In this paper, we jointly model counting, detection and re-identification in an end-to-end framework, named CountingMOT, tailored for crowded scenes. By imposing mutual object-count constraints between detection and counting, the CountingMOT tries to find a balance between object detection and crowd density map estimation, which can help it to recover missed detections or reject false detections. Our approach is an attempt to bridge the gap of object detection, counting, and re-Identification. This is in contrast to prior MOT methods that either ignore the crowd density and thus are prone to failure in crowded scenes,or depend on local correlations to build a graphical relationship for matching targets. The proposed MOT tracker can perform online and real-time tracking, and achieves the state-of-the-art results on public benchmarks MOT16 (MOTA of 79.7), MOT17 (MOTA of 81.3%) and MOT20 (MOTA of 78.9%).
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2210.09531.pdf' target='_blank'>https://arxiv.org/pdf/2210.09531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Yang, Ling Liu, Shengjie Zheng, Lang Qian, Gang Gao, Xin Chen, Xiaojian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.09531">The Brain-Inspired Cooperative Shared Control Framework for Brain-Machine Interface</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In brain-machine interface (BMI) applications, a key challenge is the low information content and high noise level in neural signals, severely affecting stable robotic control. To address this challenge, we proposes a cooperative shared control framework based on brain-inspired intelligence, where control signals are decoded from neural activity, and the robot handles the fine control. This allows for a combination of flexible and adaptive interaction control between the robot and the brain, making intricate human-robot collaboration feasible. The proposed framework utilizes spiking neural networks (SNNs) for controlling robotic arm and wheel, including speed and steering. While full integration of the system remains a future goal, individual modules for robotic arm control, object tracking, and map generation have been successfully implemented. The framework is expected to significantly enhance the performance of BMI. In practical settings, the BMI with cooperative shared control, utilizing a brain-inspired algorithm, will greatly enhance the potential for clinical applications.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2209.00522.pdf' target='_blank'>https://arxiv.org/pdf/2209.00522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pan Wang, Liangliang Ren, Shengkai Wu, Jinrong Yang, En Yu, Hangcheng Yu, Xiaoping Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.00522">Implicit and Efficient Point Cloud Completion for 3D Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The point cloud based 3D single object tracking has drawn increasing attention. Although many breakthroughs have been achieved, we also reveal two severe issues. By extensive analysis, we find the prediction manner of current approaches is non-robust, i.e., exposing a misalignment gap between prediction score and actually localization accuracy. Another issue is the sparse point returns will damage the feature matching procedure of the SOT task. Based on these insights, we introduce two novel modules, i.e., Adaptive Refine Prediction (ARP) and Target Knowledge Transfer (TKT), to tackle them, respectively. To this end, we first design a strong pipeline to extract discriminative features and conduct the matching with the attention mechanism. Then, ARP module is proposed to tackle the misalignment issue by aggregating all predicted candidates with valuable clues. Finally, TKT module is designed to effectively overcome incomplete point cloud due to sparse and occlusion issues. We call our overall framework PCET. By conducting extensive experiments on the KITTI and Waymo Open Dataset, our model achieves state-of-the-art performance while maintaining a lower computational cost.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2208.08829.pdf' target='_blank'>https://arxiv.org/pdf/2208.08829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chuanming Tang, Xiao Wang, Yuanchao Bai, Zhe Wu, Jianlin Zhang, Yongmei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.08829">Learning Spatial-Frequency Transformer for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent trackers adopt the Transformer to combine or replace the widely used ResNet as their new backbone network. Although their trackers work well in regular scenarios, however, they simply flatten the 2D features into a sequence to better match the Transformer. We believe these operations ignore the spatial prior of the target object which may lead to sub-optimal results only. In addition, many works demonstrate that self-attention is actually a low-pass filter, which is independent of input features or key/queries. That is to say, it may suppress the high-frequency component of the input features and preserve or even amplify the low-frequency information. To handle these issues, in this paper, we propose a unified Spatial-Frequency Transformer that models the Gaussian spatial Prior and High-frequency emphasis Attention (GPHA) simultaneously. To be specific, Gaussian spatial prior is generated using dual Multi-Layer Perceptrons (MLPs) and injected into the similarity matrix produced by multiplying Query and Key features in self-attention. The output will be fed into a Softmax layer and then decomposed into two components, i.e., the direct signal and high-frequency signal. The low- and high-pass branches are rescaled and combined to achieve all-pass, therefore, the high-frequency features will be protected well in stacked self-attention layers. We further integrate the Spatial-Frequency Transformer into the Siamese tracking framework and propose a novel tracking algorithm, termed SFTransT. The cross-scale fusion based SwinTransformer is adopted as the backbone, and also a multi-head cross-attention module is used to boost the interaction between search and template features. The output will be fed into the tracking head for target localization. Extensive experiments on both short-term and long-term tracking benchmarks all demonstrate the effectiveness of our proposed framework.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2110.11284.pdf' target='_blank'>https://arxiv.org/pdf/2110.11284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehdi Miah, Guillaume-Alexandre Bilodeau, Nicolas Saunier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.11284">Multi-Object Tracking and Segmentation with a Space-Time Memory Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a method for multi-object tracking and segmentation based on a novel memory-based mechanism to associate tracklets. The proposed tracker, MeNToS, addresses particularly the long-term data association problem, when objects are not observable for long time intervals. Indeed, the recently introduced HOTA metric (High Order Tracking Accuracy), which has a better alignment than the formerly established MOTA (Multiple Object Tracking Accuracy) with the human visual assessment of tracking, has shown that improvements are still needed for data association, despite the recent improvement in object detection. In MeNToS, after creating tracklets using instance segmentation and optical flow, the proposed method relies on a space-time memory network originally developed for one-shot video object segmentation to improve the association of sequence of detections (tracklets) with temporal gaps. We evaluate our tracker on KITTIMOTS and MOTSChallenge and we show the benefit of our data association strategy with the HOTA metric. Additional ablation studies demonstrate that our approach using a space-time memory network gives better and more robust long-term association than those based on a re-identification network. Our project page is at \url{www.mehdimiah.com/mentos+}.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2509.22910.pdf' target='_blank'>https://arxiv.org/pdf/2509.22910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwei Du, Jing-Chen Peng, Patricio A. Vela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22910">Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given that Visual SLAM relies on appearance cues for localization and scene understanding, texture-less or visually degraded environments (e.g., plain walls or low lighting) lead to poor pose estimation and track loss. However, robots are typically equipped with sensors that provide some form of dead reckoning odometry with reasonable short-time performance but unreliable long-time performance. The Good Weights (GW) algorithm described here provides a framework to adaptively integrate dead reckoning (DR) with passive visual SLAM for continuous and accurate frame-level pose estimation. Importantly, it describes how all modules in a comprehensive SLAM system must be modified to incorporate DR into its design. Adaptive weighting increases DR influence when visual tracking is unreliable and reduces when visual feature information is strong, maintaining pose track without overreliance on DR. Good Weights yields a practical solution for mobile navigation that improves visual SLAM performance and robustness. Experiments on collected datasets and in real-world deployment demonstrate the benefits of Good Weights.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2509.22910.pdf' target='_blank'>https://arxiv.org/pdf/2509.22910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanwei Du, Jing-Chen Peng, Patricio A. Vela
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22910">Good Weights: Proactive, Adaptive Dead Reckoning Fusion for Continuous and Robust Visual SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given that Visual SLAM relies on appearance cues for localization and scene understanding, texture-less or visually degraded environments (e.g., plain walls or low lighting) lead to poor pose estimation and track loss. However, robots are typically equipped with sensors that provide some form of dead reckoning odometry with reasonable short-time performance but unreliable long-time performance. The Good Weights (GW) algorithm described here provides a framework to adaptively integrate dead reckoning (DR) with passive visual SLAM for continuous and accurate frame-level pose estimation. Importantly, it describes how all modules in a comprehensive SLAM system must be modified to incorporate DR into its design. Adaptive weighting increases DR influence when visual tracking is unreliable and reduces when visual feature information is strong, maintaining pose track without overreliance on DR. Good Weights yields a practical solution for mobile navigation that improves visual SLAM performance and robustness. Experiments on collected datasets and in real-world deployment demonstrate the benefits of Good Weights.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2509.18451.pdf' target='_blank'>https://arxiv.org/pdf/2509.18451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithvi Raj Singh, Raju Gottumukkala, Anthony Maida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18451">An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unpredictable movement patterns and small visual mark make precise tracking of fast-moving tiny objects like a racquetball one of the challenging problems in computer vision. This challenge is particularly relevant for sport robotics applications, where lightweight and accurate tracking systems can improve robot perception and planning capabilities. While Kalman filter-based tracking methods have shown success in general object tracking scenarios, their performance degrades substantially when dealing with rapidly moving objects that exhibit irregular bouncing behavior. In this study, we evaluate the performance of five state-of-the-art Kalman filter-based tracking methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom dataset containing 10,000 annotated racquetball frames captured at 720p-1280p resolution. We focus our analysis on two critical performance factors: inference speed and update frequency per image, examining how these parameters affect tracking accuracy and reliability for fast-moving tiny objects. Our experimental evaluation across four distinct scenarios reveals that DeepOCSORT achieves the lowest tracking error with an average ADE of 31.15 pixels compared to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms. However, our results show that all Kalman filter-based trackers exhibit significant tracking drift with spatial errors ranging from 3-11cm (ADE values: 31-114 pixels), indicating fundamental limitations in handling the unpredictable motion patterns of fast-moving tiny objects like racquetballs. Our analysis demonstrates that current tracking approaches require substantial improvements, with error rates 3-4x higher than standard object tracking benchmarks, highlighting the need for specialized methodologies for fast-moving tiny object tracking applications.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2509.18451.pdf' target='_blank'>https://arxiv.org/pdf/2509.18451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Prithvi Raj Singh, Raju Gottumukkala, Anthony Maida
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18451">An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unpredictable movement patterns and small visual mark make precise tracking of fast-moving tiny objects like a racquetball one of the challenging problems in computer vision. This challenge is particularly relevant for sport robotics applications, where lightweight and accurate tracking systems can improve robot perception and planning capabilities. While Kalman filter-based tracking methods have shown success in general object tracking scenarios, their performance degrades substantially when dealing with rapidly moving objects that exhibit irregular bouncing behavior. In this study, we evaluate the performance of five state-of-the-art Kalman filter-based tracking methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom dataset containing 10,000 annotated racquetball frames captured at 720p-1280p resolution. We focus our analysis on two critical performance factors: inference speed and update frequency per image, examining how these parameters affect tracking accuracy and reliability for fast-moving tiny objects. Our experimental evaluation across four distinct scenarios reveals that DeepOCSORT achieves the lowest tracking error with an average ADE of 31.15 pixels compared to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms. However, our results show that all Kalman filter-based trackers exhibit significant tracking drift with spatial errors ranging from 3-11cm (ADE values: 31-114 pixels), indicating fundamental limitations in handling the unpredictable motion patterns of fast-moving tiny objects like racquetballs. Our analysis demonstrates that current tracking approaches require substantial improvements, with error rates 3-4x higher than standard object tracking benchmarks, highlighting the need for specialized methodologies for fast-moving tiny object tracking applications.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2508.08117.pdf' target='_blank'>https://arxiv.org/pdf/2508.08117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Han, Pengcheng Fang, Yueying Tian, Jianhui Yu, Xiaohao Cai, Daniel Roggen, Philip Birch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08117">GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in monocular videos is fundamentally challenged by occlusions and depth ambiguity, issues that conventional tracking-by-detection (TBD) methods struggle to resolve owing to a lack of geometric awareness. To address these limitations, we introduce GRASPTrack, a novel depth-aware MOT framework that integrates monocular depth estimation and instance segmentation into a standard TBD pipeline to generate high-fidelity 3D point clouds from 2D detections, thereby enabling explicit 3D geometric reasoning. These 3D point clouds are then voxelized to enable a precise and robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To further enhance tracking robustness, our approach incorporates Depth-aware Adaptive Noise Compensation, which dynamically adjusts the Kalman filter process noise based on occlusion severity for more reliable state estimation. Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which extends the motion direction consistency from the image plane into 3D space to improve motion-based association cues, particularly for objects with complex trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack benchmarks demonstrate that our method achieves competitive performance, significantly improving tracking robustness in complex scenes with frequent occlusions and intricate motion patterns.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2505.18795.pdf' target='_blank'>https://arxiv.org/pdf/2505.18795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Li, Runze Gan, James R. Hopgood, Michael E. Davies, Simon J. Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18795">Distributed Expectation Propagation for Multi-Object Tracking over Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a novel distributed expectation propagation algorithm for multiple sensors, multiple objects tracking in cluttered environments. The proposed framework enables each sensor to operate locally while collaboratively exchanging moment estimates with other sensors, thus eliminating the need to transmit all data to a central processing node. Specifically, we introduce a fast and parallelisable Rao-Blackwellised Gibbs sampling scheme to approximate the tilted distributions, which enhances the accuracy and efficiency of expectation propagation updates. Results demonstrate that the proposed algorithm improves both communication and inference efficiency for multi-object tracking tasks with dynamic sensor connectivity and varying clutter levels.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2505.17201.pdf' target='_blank'>https://arxiv.org/pdf/2505.17201.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaim Chai Elchik, Fatemeh Karimi Nejadasl, Seyed Sahand Mohammadi Ziabari, Ali Mohammed Mansoor Alsahag
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17201">A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in computer vision has made significant advancements, yet tracking small fish in underwater environments presents unique challenges due to complex 3D motions and data noise. Traditional single-view MOT models often fall short in these settings. This thesis addresses these challenges by adapting state-of-the-art single-view MOT models, FairMOT and YOLOv8, for underwater fish detecting and tracking in ecological studies. The core contribution of this research is the development of a multi-view framework that utilizes stereo video inputs to enhance tracking accuracy and fish behavior pattern recognition. By integrating and evaluating these models on underwater fish video datasets, the study aims to demonstrate significant improvements in precision and reliability compared to single-view approaches. The proposed framework detects fish entities with a relative accuracy of 47% and employs stereo-matching techniques to produce a novel 3D output, providing a more comprehensive understanding of fish movements and interactions
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2505.16029.pdf' target='_blank'>https://arxiv.org/pdf/2505.16029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shichao Li, Peiliang Li, Qing Lian, Peng Yun, Xiaozhi Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16029">Learning better representations for crowded pedestrians in offboard LiDAR-camera 3D tracking-by-detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perceiving pedestrians in highly crowded urban environments is a difficult long-tail problem for learning-based autonomous perception. Speeding up 3D ground truth generation for such challenging scenes is performance-critical yet very challenging. The difficulties include the sparsity of the captured pedestrian point cloud and a lack of suitable benchmarks for a specific system design study. To tackle the challenges, we first collect a new multi-view LiDAR-camera 3D multiple-object-tracking benchmark of highly crowded pedestrians for in-depth analysis. We then build an offboard auto-labeling system that reconstructs pedestrian trajectories from LiDAR point cloud and multi-view images. To improve the generalization power for crowded scenes and the performance for small objects, we propose to learn high-resolution representations that are density-aware and relationship-aware. Extensive experiments validate that our approach significantly improves the 3D pedestrian tracking performance towards higher auto-labeling efficiency. The code will be publicly available at this HTTP URL.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2505.08126.pdf' target='_blank'>https://arxiv.org/pdf/2505.08126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angus Apps, Ziwei Wang, Vladimir Perejogin, Timothy Molloy, Robert Mahony
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08126">Asynchronous Multi-Object Tracking with an Event Camera</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Events cameras are ideal sensors for enabling robots to detect and track objects in highly dynamic environments due to their low latency output, high temporal resolution, and high dynamic range. In this paper, we present the Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and tracking multiple objects by processing individual raw events asynchronously. AEMOT detects salient event blob features by identifying regions of consistent optical flow using a novel Field of Active Flow Directions built from the Surface of Active Events. Detected features are tracked as candidate objects using the recently proposed Asynchronous Event Blob (AEB) tracker in order to construct small intensity patches of each candidate object. A novel learnt validation stage promotes or discards candidate objects based on classification of their intensity patches, with promoted objects having their position, velocity, size, and orientation estimated at their event rate. We evaluate AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with precision and recall performance exceeding that of alternative event-based detection and tracking algorithms by over 37%. Source code and the labelled event Bee Swarm Dataset will be open sourced
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2505.00995.pdf' target='_blank'>https://arxiv.org/pdf/2505.00995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taewook Park, Jinwoo Lee, Hyondong Oh, Won-Jae Yun, Kyu-Wha Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00995">Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the agricultural workforce declines and labor costs rise, robotic yield estimation has become increasingly important. While unmanned ground vehicles (UGVs) are commonly used for indoor farm monitoring, their deployment in greenhouses is often constrained by infrastructure limitations, sensor placement challenges, and operational inefficiencies. To address these issues, we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial odometry algorithm for precise navigation in GNSS-denied environments and utilizes a 3D multi-object tracking algorithm to estimate the count and weight of cherry tomatoes. We evaluate the system using two dataset: one from a harvesting row and another from a growing row. In the harvesting-row dataset, the proposed system achieves 94.4\% counting accuracy and 87.5\% weight estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For the growing-row dataset, which consists of occluded unripened fruits, we qualitatively analyze tracking performance and highlight future research directions for improving perception in greenhouse with strong occlusions. Our findings demonstrate the potential of UAVs for efficient robotic yield estimation in commercial greenhouses.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2504.12506.pdf' target='_blank'>https://arxiv.org/pdf/2504.12506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Nan Fernandez-Ayala, Jorge Silva, Meng Guo, Dimos V. Dimarogonas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12506">Robust Visual Servoing under Human Supervision for Assembly Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a framework enabling mobile manipulators to reliably complete pick-and-place tasks for assembling structures from construction blocks. The picking uses an eye-in-hand visual servoing controller for object tracking with Control Barrier Functions (CBFs) to ensure fiducial markers in the blocks remain visible. An additional robot with an eye-to-hand setup ensures precise placement, critical for structural stability. We integrate human-in-the-loop capabilities for flexibility and fault correction and analyze robustness to camera pose errors, proposing adapted barrier functions to handle them. Lastly, experiments validate the framework on 6-DoF mobile arms.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2504.09328.pdf' target='_blank'>https://arxiv.org/pdf/2504.09328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sonia Laguna, Alberto Garcia-Garcia, Marie-Julie Rakotosaona, Stylianos Moschoglou, Leonhard Helminger, Sergio Orts-Escolano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09328">Text To 3D Object Generation For Scalable Room Assembly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2504.07163.pdf' target='_blank'>https://arxiv.org/pdf/2504.07163.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordi Serra, Anton Aguilar, Ebrahim Abu-Helalah, RaÃºl Parada, Paolo Dini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07163">Multi-Object Tracking for Collision Avoidance Using Multiple Cameras in Open RAN Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper deals with the multi-object detection and tracking problem, within the scope of open Radio Access Network (RAN), for collision avoidance in vehicular scenarios. To this end, a set of distributed intelligent agents collocated with cameras are considered. The fusion of detected objects is done at an edge service, considering Open RAN connectivity. Then, the edge service predicts the objects trajectories for collision avoidance. Compared to the related work a more realistic Open RAN network is implemented and multiple cameras are used.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2504.01457.pdf' target='_blank'>https://arxiv.org/pdf/2504.01457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Meng, Chunyun Fu, Xiangyan Yan, Zheng Liang, Pan Ji, Jianwen Wang, Tao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01457">Deep LG-Track: An Enhanced Localization-Confidence-Guided Multi-Object Tracker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking plays a crucial role in various applications, such as autonomous driving and security surveillance. This study introduces Deep LG-Track, a novel multi-object tracker that incorporates three key enhancements to improve the tracking accuracy and robustness. First, an adaptive Kalman filter is developed to dynamically update the covariance of measurement noise based on detection confidence and trajectory disappearance. Second, a novel cost matrix is formulated to adaptively fuse motion and appearance information, leveraging localization confidence and detection confidence as weighting factors. Third, a dynamic appearance feature updating strategy is introduced, adjusting the relative weighting of historical and current appearance features based on appearance clarity and localization accuracy. Comprehensive evaluations on the MOT17 and MOT20 datasets demonstrate that the proposed Deep LG-Track consistently outperforms state-of-the-art trackers across multiple performance metrics, highlighting its effectiveness in multi-object tracking tasks.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2502.13875.pdf' target='_blank'>https://arxiv.org/pdf/2502.13875.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huu-Thien Tran, Phuoc-Sang Pham, Thai-Son Tran, Khoa Luu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13875">MEX: Memory-efficient Approach to Referring Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Referring Multi-Object Tracking (RMOT) is a relatively new concept that has rapidly gained traction as a promising research direction at the intersection of computer vision and natural language processing. Unlike traditional multi-object tracking, RMOT identifies and tracks objects and incorporates textual descriptions for object class names, making the approach more intuitive. Various techniques have been proposed to address this challenging problem; however, most require the training of the entire network due to their end-to-end nature. Among these methods, iKUN has emerged as a particularly promising solution. Therefore, we further explore its pipeline and enhance its performance. In this paper, we introduce a practical module dubbed Memory-Efficient Cross-modality -- MEX. This memory-efficient technique can be directly applied to off-the-shelf trackers like iKUN, resulting in significant architectural improvements. Our method proves effective during inference on a single GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI dataset, which offers diverse autonomous driving scenes with relevant language expressions, is particularly useful for studying this problem. Empirically, our method demonstrates effectiveness and efficiency regarding HOTA tracking scores, substantially improving memory allocation and processing speed.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2502.01207.pdf' target='_blank'>https://arxiv.org/pdf/2502.01207.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannes Homburger, Stefan Wirtensohn, Patrick Hoher, Tim Baur, Dennis Griesser, Moritz Diehl, Johannes Reuter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.01207">Solgenia -- A Test Vessel Toward Energy-Efficient Autonomous Water Taxi Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous surface vessels are a promising building block of the future's transport sector and are investigated by research groups worldwide. This paper presents a comprehensive and systematic overview of the autonomous research vessel Solgenia including the latest investigations and recently presented methods that contributed to the fields of autonomous systems, applied numerical optimization, nonlinear model predictive control, multi-extended-object-tracking, computer vision, and collision avoidance. These are considered to be the main components of autonomous water taxi applications. Autonomous water taxis have the potential to transform the traffic in cities close to the water into a more efficient, sustainable, and flexible future state. Regarding this transformation, the test platform Solgenia offers an opportunity to gain new insights by investigating novel methods in real-world experiments. An established test platform will strongly reduce the effort required for real-world experiments in the future.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2412.08313.pdf' target='_blank'>https://arxiv.org/pdf/2412.08313.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gergely SzabÃ³, ZsÃ³fia MolnÃ¡r, AndrÃ¡s HorvÃ¡th
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08313">Post-Hoc MOTS: Exploring the Capabilities of Time-Symmetric Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Temporal forward-tracking has been the dominant approach for multi-object segmentation and tracking (MOTS). However, a novel time-symmetric tracking methodology has recently been introduced for the detection, segmentation, and tracking of budding yeast cells in pre-recorded samples. Although this architecture has demonstrated a unique perspective on stable and consistent tracking, as well as missed instance re-interpolation, its evaluation has so far been largely confined to settings related to videomicroscopic environments. In this work, we aim to reveal the broader capabilities, advantages, and potential challenges of this architecture across various specifically designed scenarios, including a pedestrian tracking dataset. We also conduct an ablation study comparing the model against its restricted variants and the widely used Kalman filter. Furthermore, we present an attention analysis of the tracking architecture for both pretrained and non-pretrained models
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2412.01041.pdf' target='_blank'>https://arxiv.org/pdf/2412.01041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Susu Fang, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01041">LiDAR SLAMMOT based on Confidence-guided Data Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of autonomous driving or robotics, simultaneous localization and mapping (SLAM) and multi-object tracking (MOT) are two fundamental problems and are generally applied separately. Solutions to SLAM and MOT usually rely on certain assumptions, such as the static environment assumption for SLAM and the accurate ego-vehicle pose assumption for MOT. But in complex dynamic environments, it is difficult or even impossible to meet these assumptions. Therefore, the SLAMMOT, i.e., simultaneous localization, mapping, and moving object tracking, integrated system of SLAM and object tracking, has emerged for autonomous vehicles in dynamic environments. However, many conventional SLAMMOT solutions directly perform data association on the predictions and detections for object tracking, but ignore their quality. In practice, inaccurate predictions caused by continuous multi-frame missed detections in temporary occlusion scenarios, may degrade the performance of tracking, thereby affecting SLAMMOT. To address this challenge, this paper presents a LiDAR SLAMMOT based on confidence-guided data association (Conf SLAMMOT) method, which tightly couples the LiDAR SLAM and the confidence-guided data association based multi-object tracking into a graph optimization backend for estimating the state of the ego-vehicle and objects simultaneously. The confidence of prediction and detection are applied in the factor graph-based multi-object tracking for its data association, which not only avoids the performance degradation caused by incorrect initial assignments in some filter-based methods but also handles issues such as continuous missed detection in tracking while also improving the overall performance of SLAMMOT. Various comparative experiments demonstrate the superior advantages of Conf SLAMMOT, especially in scenes with some missed detections.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2411.10072.pdf' target='_blank'>https://arxiv.org/pdf/2411.10072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ishrath Ahamed, Chamith Dilshan Ranathunga, Dinuka Sandun Udayantha, Benny Kai Kiat Ng, Chau Yuen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10072">Real-Time AI-Driven People Tracking and Counting Using Overhead Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate people counting in smart buildings and intelligent transportation systems is crucial for energy management, safety protocols, and resource allocation. This is especially critical during emergencies, where precise occupant counts are vital for safe evacuation. Existing methods struggle with large crowds, often losing accuracy with even a few additional people. To address this limitation, this study proposes a novel approach combining a new object tracking algorithm, a novel counting algorithm, and a fine-tuned object detection model. This method achieves 97% accuracy in real-time people counting with a frame rate of 20-27 FPS on a low-power edge computer.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2411.03702.pdf' target='_blank'>https://arxiv.org/pdf/2411.03702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Depanshu Sani, Saket Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03702">Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing demand for robust scene understanding in mobile robotics and autonomous driving has highlighted the importance of integrating multiple sensing modalities. By combining data from diverse sensors like cameras and LIDARs, fusion techniques can overcome the limitations of individual sensors, enabling a more complete and accurate perception of the environment. We introduce a novel approach to multi-modal sensor fusion, focusing on developing a graph-based state representation that supports critical decision-making processes in autonomous driving. We present a Sensor-Agnostic Graph-Aware Kalman Filter [3], the first online state estimation technique designed to fuse multi-modal graphs derived from noisy multi-sensor data. The estimated graph-based state representations serve as a foundation for advanced applications like Multi-Object Tracking (MOT), offering a comprehensive framework for enhancing the situational awareness and safety of autonomous systems. We validate the effectiveness of our proposed framework through extensive experiments conducted on both synthetic and real-world driving datasets (nuScenes). Our results showcase an improvement in MOTA and a reduction in estimated position errors (MOTP) and identity switches (IDS) for tracked objects using the SAGA-KF. Furthermore, we highlight the capability of such a framework to develop methods that can leverage heterogeneous information (like semantic objects and geometric structures) from various sensing modalities, enabling a more holistic approach to scene understanding and enhancing the safety and effectiveness of autonomous systems.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2410.13240.pdf' target='_blank'>https://arxiv.org/pdf/2410.13240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanpeng Jia, Ting Wang, Xieyuanli Chen, Shiliang Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13240">TRLO: An Efficient LiDAR Odometry with 3D Dynamic Object Tracking and Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous state estimation and mapping is an essential capability for mobile robots working in dynamic urban environment. The majority of existing SLAM solutions heavily rely on a primarily static assumption. However, due to the presence of moving vehicles and pedestrians, this assumption does not always hold, leading to localization accuracy decreased and maps distorted. To address this challenge, we propose TRLO, a dynamic LiDAR odometry that efficiently improves the accuracy of state estimation and generates a cleaner point cloud map. To efficiently detect dynamic objects in the surrounding environment, a deep learning-based method is applied, generating detection bounding boxes. We then design a 3D multi-object tracker based on Unscented Kalman Filter (UKF) and nearest neighbor (NN) strategy to reliably identify and remove dynamic objects. Subsequently, a fast two-stage iterative nearest point solver is employed to solve the state estimation using cleaned static point cloud. Note that a novel hash-based keyframe database management is proposed for fast access to search keyframes. Furthermore, all the detected object bounding boxes are leveraged to impose posture consistency constraint to further refine the final state estimation. Extensive evaluations and ablation studies conducted on the KITTI and UrbanLoco datasets demonstrate that our approach not only achieves more accurate state estimation but also generates cleaner maps, compared with baselines.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2409.03252.pdf' target='_blank'>https://arxiv.org/pdf/2409.03252.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keisuke Toida, Naoki Kato, Osamu Segawa, Takeshi Nakamura, Kazuhiro Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03252">Gr-IoU: Ground-Intersection over Union for Robust Multi-Object Tracking with 3D Geometric Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a Ground IoU (Gr-IoU) to address the data association problem in multi-object tracking. When tracking objects detected by a camera, it often occurs that the same object is assigned different IDs in consecutive frames, especially when objects are close to each other or overlapping. To address this issue, we introduce Gr-IoU, which takes into account the 3D structure of the scene. Gr-IoU transforms traditional bounding boxes from the image space to the ground plane using the vanishing point geometry. The IoU calculated with these transformed bounding boxes is more sensitive to the front-to-back relationships of objects, thereby improving data association accuracy and reducing ID switches. We evaluated our Gr-IoU method on the MOT17 and MOT20 datasets, which contain diverse tracking scenarios including crowded scenes and sequences with frequent occlusions. Experimental results demonstrated that Gr-IoU outperforms conventional real-time methods without appearance features.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2408.13689.pdf' target='_blank'>https://arxiv.org/pdf/2408.13689.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Li, Runze Gan, Simon Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13689">Decentralised Variational Inference Frameworks for Multi-object Tracking on Sensor Networks: Additional Notes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper tackles the challenge of multi-sensor multi-object tracking by proposing various decentralised Variational Inference (VI) schemes that match the tracking performance of centralised sensor fusion with only local message exchanges among neighboring sensors. We first establish a centralised VI sensor fusion scheme as a benchmark and analyse the limitations of its decentralised counterpart, which requires sensors to await consensus at each VI iteration. Therefore, we propose a decentralised gradient-based VI framework that optimises the Locally Maximised Evidence Lower Bound (LM-ELBO) instead of the standard ELBO, which reduces the parameter search space and enables faster convergence, making it particularly beneficial for decentralised tracking. This proposed framework is inherently self-evolving, improving with advancements in decentralised optimisation techniques for convergence guarantees and efficiency. Further, we enhance the convergence speed of proposed decentralised schemes using natural gradients and gradient tracking strategies. Results verify that our decentralised VI schemes are empirically equivalent to centralised fusion in tracking performance. Notably, the decentralised natural gradient VI method is the most communication-efficient, with communication costs comparable to suboptimal decentralised strategies while delivering notably higher tracking accuracy.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2408.09178.pdf' target='_blank'>https://arxiv.org/pdf/2408.09178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Changcheng Xiao, Qiong Cao, Zhigang Luo, Long Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.09178">MambaTrack: A Simple Baseline for Multiple Object Tracking with State Space Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking by detection has been the prevailing paradigm in the field of Multi-object Tracking (MOT). These methods typically rely on the Kalman Filter to estimate the future locations of objects, assuming linear object motion. However, they fall short when tracking objects exhibiting nonlinear and diverse motion in scenarios like dancing and sports. In addition, there has been limited focus on utilizing learning-based motion predictors in MOT. To address these challenges, we resort to exploring data-driven motion prediction methods. Inspired by the great expectation of state space models (SSMs), such as Mamba, in long-term sequence modeling with near-linear complexity, we introduce a Mamba-based motion model named Mamba moTion Predictor (MTP). MTP is designed to model the complex motion patterns of objects like dancers and athletes. Specifically, MTP takes the spatial-temporal location dynamics of objects as input, captures the motion pattern using a bi-Mamba encoding layer, and predicts the next motion. In real-world scenarios, objects may be missed due to occlusion or motion blur, leading to premature termination of their trajectories. To tackle this challenge, we further expand the application of MTP. We employ it in an autoregressive way to compensate for missing observations by utilizing its own predictions as inputs, thereby contributing to more consistent trajectories. Our proposed tracker, MambaTrack, demonstrates advanced performance on benchmarks such as Dancetrack and SportsMOT, which are characterized by complex motion and severe occlusion.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2407.18288.pdf' target='_blank'>https://arxiv.org/pdf/2407.18288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niels G. Faber, Seyed Sahand Mohammadi Ziabari, Fatemeh Karimi Nejadasl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18288">Leveraging Foundation Models via Knowledge Distillation in Multi-Object Tracking: Distilling DINOv2 Features to FairMOT</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) is a computer vision task that has been employed in a variety of sectors. Some common limitations in MOT are varying object appearances, occlusions, or crowded scenes. To address these challenges, machine learning methods have been extensively deployed, leveraging large datasets, sophisticated models, and substantial computational resources. Due to practical limitations, access to the above is not always an option. However, with the recent release of foundation models by prominent AI companies, pretrained models have been trained on vast datasets and resources using state-of-the-art methods. This work tries to leverage one such foundation model, called DINOv2, through using knowledge distillation. The proposed method uses a teacher-student architecture, where DINOv2 is the teacher and the FairMOT backbone HRNetv2 W18 is the student. The results imply that although the proposed method shows improvements in certain scenarios, it does not consistently outperform the original FairMOT model. These findings highlight the potential and limitations of applying foundation models in knowledge
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2407.08817.pdf' target='_blank'>https://arxiv.org/pdf/2407.08817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ish Kumar Jain, Suriyaa MM, Dinesh Bharadia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08817">CommRad: Context-Aware Sensing-Driven Millimeter-Wave Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Millimeter-wave (mmWave) technology is pivotal for next-generation wireless networks, enabling high-data-rate and low-latency applications such as autonomous vehicles and XR streaming. However, maintaining directional mmWave links in dynamic mobile environments is challenging due to mobility-induced disruptions and blockage. While effective, the current 5G NR beam training methods incur significant overhead and scalability issues in multi-user scenarios. To address this, we introduce CommRad, a sensing-driven solution incorporating a radar sensor at the base station to track mobile users and maintain directional beams even under blockages. While radar provides high-resolution object tracking, it suffers from a fundamental challenge of lack of context, i.e., it cannot discern which objects in the environment represent active users, reflectors, or blockers. To obtain this contextual awareness, CommRad unites wireless sensing capabilities of bi-static radio communication with the mono-static radar sensor, allowing radios to provide initial context to radar sensors. Subsequently, the radar aids in user tracking and sustains mobile links even in obstructed scenarios, resulting in robust and high-throughput directional connections for all mobile users at all times. We evaluate this collaborative radar-radio framework using a 28 GHz mmWave testbed integrated with a radar sensor in various indoor and outdoor scenarios, demonstrating a 2.5x improvement in median throughput compared to a non-collaborative baseline.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2407.06337.pdf' target='_blank'>https://arxiv.org/pdf/2407.06337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jon Crall, Connor Greenwell, David Joy, Matthew Leotta, Aashish Chaudhary, Anthony Hoogs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06337">GeoWATCH for Detecting Heavy Construction in Heterogeneous Time Series of Satellite Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning from multiple sensors is challenging due to spatio-temporal misalignment and differences in resolution and captured spectra. To that end, we introduce GeoWATCH, a flexible framework for training models on long sequences of satellite images sourced from multiple sensor platforms, which is designed to handle image classification, activity recognition, object detection, or object tracking tasks. Our system includes a novel partial weight loading mechanism based on sub-graph isomorphism which allows for continually training and modifying a network over many training cycles. This has allowed us to train a lineage of models over a long period of time, which we have observed has improved performance as we adjust configurations while maintaining a core backbone.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2407.05518.pdf' target='_blank'>https://arxiv.org/pdf/2407.05518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Athena Psalta, Vasileios Tsironis, Andreas El Saer, Konstantinos Karantzalos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.05518">Addressing single object tracking in satellite imagery through prompt-engineered solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking in satellite videos remains a complex endeavor in remote sensing due to the intricate and dynamic nature of satellite imagery. Existing state-of-the-art trackers in computer vision integrate sophisticated architectures, attention mechanisms, and multi-modal fusion to enhance tracking accuracy across diverse environments. However, the challenges posed by satellite imagery, such as background variations, atmospheric disturbances, and low-resolution object delineation, significantly impede the precision and reliability of traditional Single Object Tracking (SOT) techniques. Our study delves into these challenges and proposes prompt engineering methodologies, leveraging the Segment Anything Model (SAM) and TAPIR (Tracking Any Point with per-frame Initialization and temporal Refinement), to create a training-free point-based tracking method for small-scale objects on satellite videos. Experiments on the VISO dataset validate our strategy, marking a significant advancement in robust tracking solutions tailored for satellite imagery in remote sensing applications.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2406.14973.pdf' target='_blank'>https://arxiv.org/pdf/2406.14973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Yang, Jisheng Xu, Zhiliang Lin, Jianping He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14973">LU2Net: A Lightweight Network for Real-time Underwater Image Enhancement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer vision techniques have empowered underwater robots to effectively undertake a multitude of tasks, including object tracking and path planning. However, underwater optical factors like light refraction and absorption present challenges to underwater vision, which cause degradation of underwater images. A variety of underwater image enhancement methods have been proposed to improve the effectiveness of underwater vision perception. Nevertheless, for real-time vision tasks on underwater robots, it is necessary to overcome the challenges associated with algorithmic efficiency and real-time capabilities. In this paper, we introduce Lightweight Underwater Unet (LU2Net), a novel U-shape network designed specifically for real-time enhancement of underwater images. The proposed model incorporates axial depthwise convolution and the channel attention module, enabling it to significantly reduce computational demands and model parameters, thereby improving processing speed. The extensive experiments conducted on the dataset and real-world underwater robots demonstrate the exceptional performance and speed of proposed model. It is capable of providing well-enhanced underwater images at a speed 8 times faster than the current state-of-the-art underwater image enhancement method. Moreover, LU2Net is able to handle real-time underwater video enhancement.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2405.15755.pdf' target='_blank'>https://arxiv.org/pdf/2405.15755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Han, Nobuyuki Oishi, Yueying Tian, Elif Ucurum, Rupert Young, Chris Chatwin, Philip Birch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15755">ETTrack: Enhanced Temporal Motion Predictor for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many Multi-Object Tracking (MOT) approaches exploit motion information to associate all the detected objects across frames. However, many methods that rely on filtering-based algorithms, such as the Kalman Filter, often work well in linear motion scenarios but struggle to accurately predict the locations of objects undergoing complex and non-linear movements. To tackle these scenarios, we propose a motion-based MOT approach with an enhanced temporal motion predictor, ETTrack. Specifically, the motion predictor integrates a transformer model and a Temporal Convolutional Network (TCN) to capture short-term and long-term motion patterns, and it predicts the future motion of individual objects based on the historical motion information. Additionally, we propose a novel Momentum Correction Loss function that provides additional information regarding the motion direction of objects during training. This allows the motion predictor rapidly adapt to motion variations and more accurately predict future motion. Our experimental results demonstrate that ETTrack achieves a competitive performance compared with state-of-the-art trackers on DanceTrack and SportsMOT, scoring 56.4% and 74.4% in HOTA metrics, respectively.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2405.13397.pdf' target='_blank'>https://arxiv.org/pdf/2405.13397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harish Prakash, Jia Cheng Shang, Ken M. Nsiempba, Yuhao Chen, David A. Clausi, John S. Zelek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13397">Multi Player Tracking in Ice Hockey with Homographic Projections</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi Object Tracking (MOT) in ice hockey pursues the combined task of localizing and associating players across a given sequence to maintain their identities. Tracking players from monocular broadcast feeds is an important computer vision problem offering various downstream analytics and enhanced viewership experience. However, existing trackers encounter significant difficulties in dealing with occlusions, blurs, and agile player movements prevalent in telecast feeds. In this work, we propose a novel tracking approach by formulating MOT as a bipartite graph matching problem infused with homography. We disentangle the positional representations of occluded and overlapping players in broadcast view, by mapping their foot keypoints to an overhead rink template, and encode these projected positions into the graph network. This ensures reliable spatial context for consistent player tracking and unfragmented tracklet prediction. Our results show considerable improvements in both the IDsw and IDF1 metrics on the two available broadcast ice hockey datasets.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2403.16834.pdf' target='_blank'>https://arxiv.org/pdf/2403.16834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Luo, Xiqing Guo, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16834">From Two-Stream to One-Stream: Efficient RGB-T Tracking via Mutual Prompt Learning and Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the complementary nature of visible light and thermal infrared modalities, object tracking based on the fusion of visible light images and thermal images (referred to as RGB-T tracking) has received increasing attention from researchers in recent years. How to achieve more comprehensive fusion of information from the two modalities at a lower cost has been an issue that researchers have been exploring. Inspired by visual prompt learning, we designed a novel two-stream RGB-T tracking architecture based on cross-modal mutual prompt learning, and used this model as a teacher to guide a one-stream student model for rapid learning through knowledge distillation techniques. Extensive experiments have shown that, compared to similar RGB-T trackers, our designed teacher model achieved the highest precision rate, while the student model, with comparable precision rate to the teacher model, realized an inference speed more than three times faster than the teacher model.(Codes will be available if accepted.)
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2401.15288.pdf' target='_blank'>https://arxiv.org/pdf/2401.15288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ragini Gupta, Lingzhi Zhao, Jiaxi Li, Volodymyr Vakhniuk, Claudiu Danilov, Josh Eckhardt, Keyshla Bernard, Klara Nahrstedt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15288">STAC: Leveraging Spatio-Temporal Data Associations For Efficient Cross-Camera Streaming and Analytics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In IoT based distributed network of cameras, real-time multi-camera video analytics is challenged by high bandwidth demands and redundant visual data, creating a fundamental tension where reducing data saves network overhead but can degrade model performance, and vice versa. We present STAC, a cross-cameras surveillance system that leverages spatio-temporal associations for efficient object tracking under constrained network conditions. STAC integrates multi-resolution feature learning, ensuring robustness under variable networked system level optimizations such as frame filtering, FFmpeg-based compression, and Region-of-Interest (RoI) masking, to eliminate redundant content across distributed video streams while preserving downstream model accuracy for object identification and tracking. Evaluated on NVIDIA's AICity Challenge dataset, STAC achieves a 76\% improvement in tracking accuracy and an 8.6x reduction in inference latency over a standard multi-object multi-camera tracking baseline (using YOLOv4 and DeepSORT). Furthermore, 29\% of redundant frames are filtered, significantly reducing data volume without compromising inference quality.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2401.02960.pdf' target='_blank'>https://arxiv.org/pdf/2401.02960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anton Jeran Ratnarajah, Sahani Goonetilleke, Dumindu Tissera, Kapilan Balagopalan, Ranga Rodrigo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02960">Forensic Video Analytic Software</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Law enforcement officials heavily depend on Forensic Video Analytic (FVA) Software in their evidence extraction process. However present-day FVA software are complex, time consuming, equipment dependent and expensive. Developing countries struggle to gain access to this gateway to a secure haven. The term forensic pertains the application of scientific methods to the investigation of crime through post-processing, whereas surveillance is the close monitoring of real-time feeds.
  The principle objective of this Final Year Project was to develop an efficient and effective FVA Software, addressing the shortcomings through a stringent and systematic review of scholarly research papers, online databases and legal documentation. The scope spans multiple object detection, multiple object tracking, anomaly detection, activity recognition, tampering detection, general and specific image enhancement and video synopsis.
  Methods employed include many machine learning techniques, GPU acceleration and efficient, integrated architecture development both for real-time and postprocessing. For this CNN, GMM, multithreading and OpenCV C++ coding were used. The implications of the proposed methodology would rapidly speed up the FVA process especially through the novel video synopsis research arena. This project has resulted in three research outcomes Moving Object Based Collision Free Video Synopsis, Forensic and Surveillance Analytic Tool Architecture and Tampering Detection Inter-Frame Forgery.
  The results include forensic and surveillance panel outcomes with emphasis on video synopsis and Sri Lankan context. Principal conclusions include the optimization and efficient algorithm integration to overcome limitations in processing power, memory and compromise between real-time performance and accuracy.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2312.11576.pdf' target='_blank'>https://arxiv.org/pdf/2312.11576.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akey Sungheetha, Rajesh Sharma R, Chinnaiyan R
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11576">Emotion Based Prediction in the Context of Optimized Trajectory Planning for Immersive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the virtual elements of immersive learning, the use of Google Expedition and touch-screen-based emotion are examined. The objective is to investigate possible ways to combine these technologies to enhance virtual learning environments and learners emotional engagement. Pedagogical application, affordances, and cognitive load are the corresponding measures that are involved. Students will gain insight into the reason behind their significantly higher post-assessment Prediction Systems scores compared to preassessment scores through this work that leverages technology. This suggests that it is effective to include emotional elements in immersive learning scenarios. The results of this study may help develop new strategies by leveraging the features of immersive learning technology in educational technologies to improve virtual reality and augmented reality experiences. Furthermore, the effectiveness of immersive learning environments can be raised by utilizing magnetic, optical, or hybrid trackers that considerably improve object tracking.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2312.04117.pdf' target='_blank'>https://arxiv.org/pdf/2312.04117.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhan Zhao, Haoyu Ma, Shu Kong, Charless Fowlkes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04117">Instance Tracking in 3D Scenes from Egocentric Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Egocentric sensors such as AR/VR devices capture human-object interactions and offer the potential to provide task-assistance by recalling 3D locations of objects of interest in the surrounding environment. This capability requires instance tracking in real-world 3D scenes from egocentric videos (IT3DEgo). We explore this problem by first introducing a new benchmark dataset, consisting of RGB and depth videos, per-frame camera pose, and instance-level annotations in both 2D camera and 3D world coordinates. We present an evaluation protocol which evaluates tracking performance in 3D coordinates with two settings for enrolling instances to track: (1) single-view online enrollment where an instance is specified on-the-fly based on the human wearer's interactions. and (2) multi-view pre-enrollment where images of an instance to be tracked are stored in memory ahead of time. To address IT3DEgo, we first re-purpose methods from relevant areas, e.g., single object tracking (SOT) -- running SOT methods to track instances in 2D frames and lifting them to 3D using camera pose and depth. We also present a simple method that leverages pretrained segmentation and detection models to generate proposals from RGB frames and match proposals with enrolled instance images. Our experiments show that our method (with no finetuning) significantly outperforms SOT-based approaches in the egocentric setting. We conclude by arguing that the problem of egocentric instance tracking is made easier by leveraging camera pose and using a 3D allocentric (world) coordinate representation.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2311.17656.pdf' target='_blank'>https://arxiv.org/pdf/2311.17656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Somaieh Amraee, Bishoy Galoaa, Matthew Goodwin, Elaheh Hatamimajoumerd, Sarah Ostadabbas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17656">Multiple Toddler Tracking in Indoor Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple toddler tracking (MTT) involves identifying and differentiating toddlers in video footage. While conventional multi-object tracking (MOT) algorithms are adept at tracking diverse objects, toddlers pose unique challenges due to their unpredictable movements, various poses, and similar appearance. Tracking toddlers in indoor environments introduces additional complexities such as occlusions and limited fields of view. In this paper, we address the challenges of MTT and propose MTTSort, a customized method built upon the DeepSort algorithm. MTTSort is designed to track multiple toddlers in indoor videos accurately. Our contributions include discussing the primary challenges in MTT, introducing a genetic algorithm to optimize hyperparameters, proposing an accurate tracking algorithm, and curating the MTTrack dataset using unbiased AI co-labeling techniques. We quantitatively compare MTTSort to state-of-the-art MOT methods on MTTrack, DanceTrack, and MOT15 datasets. In our evaluation, the proposed method outperformed other MOT methods, achieving 0.98, 0.68, and 0.98 in multiple object tracking accuracy (MOTA), higher order tracking accuracy (HOTA), and iterative and discriminative framework 1 (IDF1) metrics, respectively.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2311.10415.pdf' target='_blank'>https://arxiv.org/pdf/2311.10415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RÃ©my Huet, Antoine Lima, Philippe Xu, VÃ©ronique Cherfaoui, Philippe Bonnifait
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.10415">Collaborative Grid Mapping for Moving Object Tracking Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception of other road users is a crucial task for intelligent vehicles. Perception systems can use on-board sensors only or be in cooperation with other vehicles or with roadside units. In any case, the performance of perception systems has to be evaluated against ground-truth data, which is a particularly tedious task and requires numerous manual operations. In this article, we propose a novel semi-automatic method for pseudo ground-truth estimation. The principle consists in carrying out experiments with several vehicles equipped with LiDAR sensors and with fixed perception systems located at the roadside in order to collaboratively build reference dynamic data. The method is based on grid mapping and in particular on the elaboration of a background map that holds relevant information that remains valid during a whole dataset sequence. Data from all agents is converted in time-stamped observations grids. A data fusion method that manages uncertainties combines the background map with observations to produce dynamic reference information at each instant. Several datasets have been acquired with three experimental vehicles and a roadside unit. An evaluation of this method is finally provided in comparison to a handmade ground truth.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2311.07390.pdf' target='_blank'>https://arxiv.org/pdf/2311.07390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuzana ÄernekovÃ¡, Zuzana Berger HaladovÃ¡, JÃ¡n Å pirka, Viktor Kocur
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07390">Evaluating the Significance of Outdoor Advertising from Driver's Perspective Using Computer Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Outdoor advertising, such as roadside billboards, plays a significant role in marketing campaigns but can also be a distraction for drivers, potentially leading to accidents. In this study, we propose a pipeline for evaluating the significance of roadside billboards in videos captured from a driver's perspective. We have collected and annotated a new BillboardLamac dataset, comprising eight videos captured by drivers driving through a predefined path wearing eye-tracking devices. The dataset includes annotations of billboards, including 154 unique IDs and 155 thousand bounding boxes, as well as eye fixation data. We evaluate various object tracking methods in combination with a YOLOv8 detector to identify billboard advertisements with the best approach achieving 38.5 HOTA on BillboardLamac. Additionally, we train a random forest classifier to classify billboards into three classes based on the length of driver fixations achieving 75.8% test accuracy. An analysis of the trained classifier reveals that the duration of billboard visibility, its saliency, and size are the most influential features when assessing billboard significance.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2310.14030.pdf' target='_blank'>https://arxiv.org/pdf/2310.14030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelhakim Amer, Mohit Mehndiratta, Jonas le Fevre Sejersen, Huy Xuan Pham, Erdal Kayacan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14030">Visual Tracking Nonlinear Model Predictive Control Method for Autonomous Wind Turbine Inspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated visual inspection of on-and offshore wind turbines using aerial robots provides several benefits, namely, a safe working environment by circumventing the need for workers to be suspended high above the ground, reduced inspection time, preventive maintenance, and access to hard-to-reach areas. A novel nonlinear model predictive control (NMPC) framework alongside a global wind turbine path planner is proposed to achieve distance-optimal coverage for wind turbine inspection. Unlike traditional MPC formulations, visual tracking NMPC (VT-NMPC) is designed to track an inspection surface, instead of a position and heading trajectory, thereby circumventing the need to provide an accurate predefined trajectory for the drone. An additional capability of the proposed VT-NMPC method is that by incorporating inspection requirements as visual tracking costs to minimize, it naturally achieves the inspection task successfully while respecting the physical constraints of the drone. Multiple simulation runs and real-world tests demonstrate the efficiency and efficacy of the proposed automated inspection framework, which outperforms the traditional MPC designs, by providing full coverage of the target wind turbine blades as well as its robustness to changing wind conditions. The implementation codes are open-sourced.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2310.06109.pdf' target='_blank'>https://arxiv.org/pdf/2310.06109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simeng Qiu, Hadi Amata, Wolfgang Heidrich
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06109">QR-Tag: Angular Measurement and Tracking with a QR-Design Marker</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Directional information measurement has many applications in domains such as robotics, virtual and augmented reality, and industrial computer vision. Conventional methods either require pre-calibration or necessitate controlled environments. The state-of-the-art MoireTag approach exploits the Moire effect and QR-design to continuously track the angular shift precisely. However, it is still not a fully QR code design. To overcome the above challenges, we propose a novel snapshot method for discrete angular measurement and tracking with scannable QR-design patterns that are generated by binary structures printed on both sides of a glass plate. The QR codes, resulting from the parallax effect due to the geometry alignment between two layers, can be readily measured as angular information using a phone camera. The simulation results show that the proposed non-contact object tracking framework is computationally efficient with high accuracy.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2309.17036.pdf' target='_blank'>https://arxiv.org/pdf/2309.17036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linghao Yang, Yanmin Wu, Yu Deng, Rui Tian, Xinggang Hu, Tiefeng Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.17036">UniQuadric: A SLAM Backend for Unknown Rigid Object 3D Tracking and Light-Weight Modeling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking and modeling unknown rigid objects in the environment play a crucial role in autonomous unmanned systems and virtual-real interactive applications. However, many existing Simultaneous Localization, Mapping and Moving Object Tracking (SLAMMOT) methods focus solely on estimating specific object poses and lack estimation of object scales and are unable to effectively track unknown objects. In this paper, we propose a novel SLAM backend that unifies ego-motion tracking, rigid object motion tracking, and modeling within a joint optimization framework. In the perception part, we designed a pixel-level asynchronous object tracker (AOT) based on the Segment Anything Model (SAM) and DeAOT, enabling the tracker to effectively track target unknown objects guided by various predefined tasks and prompts. In the modeling part, we present a novel object-centric quadric parameterization to unify both static and dynamic object initialization and optimization. Subsequently, in the part of object state estimation, we propose a tightly coupled optimization model for object pose and scale estimation, incorporating hybrids constraints into a novel dual sliding window optimization framework for joint estimation. To our knowledge, we are the first to tightly couple object pose tracking with light-weight modeling of dynamic and static objects using quadric. We conduct qualitative and quantitative experiments on simulation datasets and real-world datasets, demonstrating the state-of-the-art robustness and accuracy in motion estimation and modeling. Our system showcases the potential application of object perception in complex dynamic scenes.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2309.00807.pdf' target='_blank'>https://arxiv.org/pdf/2309.00807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qing Li, Runze Gan, Simon Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00807">Consensus-based Distributed Variational Multi-object Tracker in Multi-Sensor Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing need for accurate and reliable tracking systems has driven significant progress in sensor fusion and object tracking techniques. In this paper, we design two variational Bayesian trackers that effectively track multiple targets in cluttered environments within a sensor network. We first present a centralised sensor fusion scheme, which involves transmitting sensor data to a fusion center. Then, we develop a distributed version leveraging the average consensus algorithm, which is theoretically equivalent to the centralised sensor fusion tracker and requires only local message passing with neighbouring sensors. In addition, we empirically verify that our proposed distributed variational tracker performs on par with the centralised version with equal tracking accuracy. Simulation results show that our distributed multi-target tracker outperforms the suboptimal distributed sensor fusion strategy that fuses each sensor's posterior based on arithmetic sensor fusion and an average consensus strategy.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2308.16386.pdf' target='_blank'>https://arxiv.org/pdf/2308.16386.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Luo, Xiqing Guo, Hui Feng, Lei Ao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.16386">RGB-T Tracking via Multi-Modal Mutual Prompt Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking based on the fusion of visible and thermal im-ages, known as RGB-T tracking, has gained increasing atten-tion from researchers in recent years. How to achieve a more comprehensive fusion of information from the two modalities with fewer computational costs has been a problem that re-searchers have been exploring. Recently, with the rise of prompt learning in computer vision, we can better transfer knowledge from visual large models to downstream tasks. Considering the strong complementarity between visible and thermal modalities, we propose a tracking architecture based on mutual prompt learning between the two modalities. We also design a lightweight prompter that incorporates attention mechanisms in two dimensions to transfer information from one modality to the other with lower computational costs, embedding it into each layer of the backbone. Extensive ex-periments have demonstrated that our proposed tracking ar-chitecture is effective and efficient, achieving state-of-the-art performance while maintaining high running speeds.
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2308.04126.pdf' target='_blank'>https://arxiv.org/pdf/2308.04126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyang Yu, Shihao Wang, Yuan Fang, Wangpeng An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04126">OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents OmniDataComposer, an innovative approach for multimodal data fusion and unlimited data generation with an intent to refine and uncomplicate interplay among diverse data modalities. Coming to the core breakthrough, it introduces a cohesive data structure proficient in processing and merging multimodal data inputs, which include video, audio, and text.
  Our crafted algorithm leverages advancements across multiple operations such as video/image caption extraction, dense caption extraction, Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), Recognize Anything Model(RAM), and object tracking. OmniDataComposer is capable of identifying over 6400 categories of objects, substantially broadening the spectrum of visual information. It amalgamates these diverse modalities, promoting reciprocal enhancement among modalities and facilitating cross-modal data correction. \textbf{The final output metamorphoses each video input into an elaborate sequential document}, virtually transmuting videos into thorough narratives, making them easier to be processed by large language models.
  Future prospects include optimizing datasets for each modality to encourage unlimited data generation. This robust base will offer priceless insights to models like ChatGPT, enabling them to create higher quality datasets for video captioning and easing question-answering tasks based on video content. OmniDataComposer inaugurates a new stage in multimodal learning, imparting enormous potential for augmenting AI's understanding and generation of complex, real-world data.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2308.03887.pdf' target='_blank'>https://arxiv.org/pdf/2308.03887.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gergely SzabÃ³, Paolo Bonaiuti, Andrea Ciliberto, AndrÃ¡s HorvÃ¡th
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03887">Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated through biologically motivated validation strategies and compared against multiple state-of-the-art cell tracking methods.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2307.08838.pdf' target='_blank'>https://arxiv.org/pdf/2307.08838.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianlin Zhang, Sikai Guo, Xiaogang Xiong, Wanlei Li, Zezheng Qi, Yunjiang Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.08838">Dynamic Object Tracking for Quadruped Manipulator with Spherical Image-Based Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Exactly estimating and tracking the motion of surrounding dynamic objects is one of important tasks for the autonomy of a quadruped manipulator. However, with only an onboard RGB camera, it is still a challenging work for a quadruped manipulator to track the motion of a dynamic object moving with unknown and changing velocities. To address this problem, this manuscript proposes a novel image-based visual servoing (IBVS) approach consisting of three elements: a spherical projection model, a robust super-twisting observer, and a model predictive controller (MPC). The spherical projection model decouples the visual error of the dynamic target into linear and angular ones. Then, with the presence of the visual error, the robustness of the observer is exploited to estimate the unknown and changing velocities of the dynamic target without depth estimation. Finally, the estimated velocity is fed into the model predictive controller (MPC) to generate joint torques for the quadruped manipulator to track the motion of the dynamical target. The proposed approach is validated through hardware experiments and the experimental results illustrate the approach's effectiveness in improving the autonomy of the quadruped manipulator.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2307.02161.pdf' target='_blank'>https://arxiv.org/pdf/2307.02161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruk Gebregziabher, Hadush Hailu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02161">Multi Object Tracking for Predictive Collision Avoidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The safe and efficient operation of Autonomous Mobile Robots (AMRs) in complex environments, such as manufacturing, logistics, and agriculture, necessitates accurate multi-object tracking and predictive collision avoidance. This paper presents algorithms and techniques for addressing these challenges using Lidar sensor data, emphasizing ensemble Kalman filter. The developed predictive collision avoidance algorithm employs the data provided by lidar sensors to track multiple objects and predict their velocities and future positions, enabling the AMR to navigate safely and effectively. A modification to the dynamic windowing approach is introduced to enhance the performance of the collision avoidance system. The overall system architecture encompasses object detection, multi-object tracking, and predictive collision avoidance control. The experimental results, obtained from both simulation and real-world data, demonstrate the effectiveness of the proposed methods in various scenarios, which lays the foundation for future research on global planners, other controllers, and the integration of additional sensors. This thesis contributes to the ongoing development of safe and efficient autonomous systems in complex and dynamic environments.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2307.02161.pdf' target='_blank'>https://arxiv.org/pdf/2307.02161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bruk Gebregziabher, Hadush Hailu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02161">Multi Object Tracking for Predictive Collision Avoidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The safe and efficient operation of Autonomous Mobile Robots (AMRs) in complex environments, such as manufacturing, logistics, and agriculture, necessitates accurate multi-object tracking and predictive collision avoidance. This paper presents algorithms and techniques for addressing these challenges using Lidar sensor data, emphasizing ensemble Kalman filter. The developed predictive collision avoidance algorithm employs the data provided by lidar sensors to track multiple objects and predict their velocities and future positions, enabling the AMR to navigate safely and effectively. A modification to the dynamic windowing approach is introduced to enhance the performance of the collision avoidance system. The overall system architecture encompasses object detection, multi-object tracking, and predictive collision avoidance control. The experimental results, obtained from both simulation and real-world data, demonstrate the effectiveness of the proposed methods in various scenarios, which lays the foundation for future research on global planners, other controllers, and the integration of additional sensors. This thesis contributes to the ongoing development of safe and efficient autonomous systems in complex and dynamic environments.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2306.17602.pdf' target='_blank'>https://arxiv.org/pdf/2306.17602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Doll, Niklas Hanselmann, Lukas Schneider, Richard Schulz, Markus Enzweiler, Hendrik P. A. Lensch
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17602">S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Following the tracking-by-attention paradigm, this paper introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the same time.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2306.06126.pdf' target='_blank'>https://arxiv.org/pdf/2306.06126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marco Braun, Moritz Luszek, Mirko Meuter, Dominic Spata, Kevin Kollek, Anton Kummert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.06126">Deep Learning Method for Cell-Wise Object Tracking, Velocity Estimation and Projection of Sensor Data over Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current Deep Learning methods for environment segmentation and velocity estimation rely on Convolutional Recurrent Neural Networks to exploit spatio-temporal relationships within obtained sensor data. These approaches derive scene dynamics implicitly by correlating novel input and memorized data utilizing ConvNets. We show how ConvNets suffer from architectural restrictions for this task. Based on these findings, we then provide solutions to various issues on exploiting spatio-temporal correlations in a sequence of sensor recordings by presenting a novel Recurrent Neural Network unit utilizing Transformer mechanisms. Within this unit, object encodings are tracked across consecutive frames by correlating key-query pairs derived from sensor inputs and memory states, respectively. We then use resulting tracking patterns to obtain scene dynamics and regress velocities. In a last step, the memory state of the Recurrent Neural Network is projected based on extracted velocity estimates to resolve aforementioned spatio-temporal misalignment.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2305.09981.pdf' target='_blank'>https://arxiv.org/pdf/2305.09981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fatemeh Azimi, Fahim Mannan, Felix Heide
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09981">S$^3$Track: Self-supervised Tracking with Soft Assignment Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we study self-supervised multiple object tracking without using any video-level association labels. We propose to cast the problem of multiple object tracking as learning the frame-wise associations between detections in consecutive frames. To this end, we propose differentiable soft object assignment for object association, making it possible to learn features tailored to object association with differentiable end-to-end training. With this training approach in hand, we develop an appearance-based model for learning instance-aware object features used to construct a cost matrix based on the pairwise distances between the object features. We train our model using temporal and multi-view data, where we obtain association pseudo-labels using optical flow and disparity information. Unlike most self-supervised tracking methods that rely on pretext tasks for learning the feature correspondences, our method is directly optimized for cross-object association in complex scenarios. As such, the proposed method offers a reidentification-based MOT approach that is robust to training hyperparameters and does not suffer from local minima, which are a challenge in self-supervised methods. We evaluate our proposed model on the KITTI, Waymo, nuScenes, and Argoverse datasets, consistently improving over other unsupervised methods ($7.8\%$ improvement in association accuracy on nuScenes).
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2305.09523.pdf' target='_blank'>https://arxiv.org/pdf/2305.09523.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huan Mao, Yulin Chen, Zongtan Li, Feng Chen, Pingping Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09523">SCTracker: Multi-object tracking with shape and confidence constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detection-based tracking is one of the main methods of multi-object tracking. It can obtain good tracking results when using excellent detectors but it may associate wrong targets when facing overlapping and low-confidence detections. To address this issue, this paper proposes a multi-object tracker based on shape constraint and confidence named SCTracker. In the data association stage, an Intersection of Union distance with shape constraints is applied to calculate the cost matrix between tracks and detections, which can effectively avoid the track tracking to the wrong target with the similar position but inconsistent shape, so as to improve the accuracy of data association. Additionally, the Kalman Filter based on the detection confidence is used to update the motion state to improve the tracking performance when the detection has low confidence. Experimental results on MOT 17 dataset show that the proposed method can effectively improve the tracking performance of multi-object tracking.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2304.13147.pdf' target='_blank'>https://arxiv.org/pdf/2304.13147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Lang, Alexander Braun, Lars Schillingmann, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13147">Self-Supervised Multi-Object Tracking For Autonomous Driving From Consistency Across Timescales</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised multi-object trackers have tremendous potential as they enable learning from raw domain-specific data. However, their re-identification accuracy still falls short compared to their supervised counterparts. We hypothesize that this drawback results from formulating self-supervised objectives that are limited to single frames or frame pairs. Such formulations do not capture sufficient visual appearance variations to facilitate learning consistent re-identification features for autonomous driving when the frame rate is low or object dynamics are high. In this work, we propose a training objective that enables self-supervised learning of re-identification features from multiple sequential frames by enforcing consistent association scores across short and long timescales. We perform extensive evaluations demonstrating that re-identification features trained from longer sequences significantly reduce ID switches on standard autonomous driving datasets compared to existing self-supervised learning methods, which are limited to training on frame pairs. Using our proposed SubCo loss function, we set the new state-of-the-art among self-supervised methods and even perform on par with fully supervised learning methods.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2303.14068.pdf' target='_blank'>https://arxiv.org/pdf/2303.14068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Asif Bin Syed, Imtiaz Ahmed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14068">A CNN-LSTM Architecture for Marine Vessel Track Association Using Automatic Identification System (AIS) Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In marine surveillance, distinguishing between normal and anomalous vessel movement patterns is critical for identifying potential threats in a timely manner. Once detected, it is important to monitor and track these vessels until a necessary intervention occurs. To achieve this, track association algorithms are used, which take sequential observations comprising geological and motion parameters of the vessels and associate them with respective vessels. The spatial and temporal variations inherent in these sequential observations make the association task challenging for traditional multi-object tracking algorithms. Additionally, the presence of overlapping tracks and missing data can further complicate the trajectory tracking process. To address these challenges, in this study, we approach this tracking task as a multivariate time series problem and introduce a 1D CNN-LSTM architecture-based framework for track association. This special neural network architecture can capture the spatial patterns as well as the long-term temporal relations that exist among the sequential observations. During the training process, it learns and builds the trajectory for each of these underlying vessels. Once trained, the proposed framework takes the marine vessel's location and motion data collected through the Automatic Identification System (AIS) as input and returns the most likely vessel track as output in real-time. To evaluate the performance of our approach, we utilize an AIS dataset containing observations from 327 vessels traveling in a specific geographic region. We measure the performance of our proposed framework using standard performance metrics such as accuracy, precision, recall, and F1 score. When compared with other competitive neural network architectures our approach demonstrates a superior tracking performance.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2303.07872.pdf' target='_blank'>https://arxiv.org/pdf/2303.07872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taekbeom Lee, Youngseok Jang, H. Jin Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.07872">Object-based SLAM utilizing unambiguous pose parameters considering general symmetry types</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existence of symmetric objects, whose observation at different viewpoints can be identical, can deteriorate the performance of simultaneous localization and mapping(SLAM). This work proposes a system for robustly optimizing the pose of cameras and objects even in the presence of symmetric objects. We classify objects into three categories depending on their symmetry characteristics, which is efficient and effective in that it allows to deal with general objects and the objects in the same category can be associated with the same type of ambiguity. Then we extract only the unambiguous parameters corresponding to each category and use them in data association and joint optimization of the camera and object pose. The proposed approach provides significant robustness to the SLAM performance by removing the ambiguous parameters and utilizing as much useful geometric information as possible. Comparison with baseline algorithms confirms the superior performance of the proposed system in terms of object tracking and pose estimation, even in challenging scenarios where the baseline fails.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2302.11458.pdf' target='_blank'>https://arxiv.org/pdf/2302.11458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Stoiber, Mariam Elsayed, Anne E. Reichert, Florian Steidle, Dongheui Lee, Rudolph Triebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11458">Fusing Visual Appearance and Geometry for Multi-modality 6DoF Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In many applications of advanced robotic manipulation, six degrees of freedom (6DoF) object pose estimates are continuously required. In this work, we develop a multi-modality tracker that fuses information from visual appearance and geometry to estimate object poses. The algorithm extends our previous method ICG, which uses geometry, to additionally consider surface appearance. In general, object surfaces contain local characteristics from text, graphics, and patterns, as well as global differences from distinct materials and colors. To incorporate this visual information, two modalities are developed. For local characteristics, keypoint features are used to minimize distances between points from keyframes and the current image. For global differences, a novel region approach is developed that considers multiple regions on the object surface. In addition, it allows the modeling of external geometries. Experiments on the YCB-Video and OPT datasets demonstrate that our approach ICG+ performs best on both datasets, outperforming both conventional and deep learning-based methods. At the same time, the algorithm is highly efficient and runs at more than 300 Hz. The source code of our tracker is publicly available.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2302.09043.pdf' target='_blank'>https://arxiv.org/pdf/2302.09043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Lang, Alexander Braun, Lars Schillingmann, Karsten Haug, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09043">Self-Supervised Representation Learning from Temporal Ordering of Automated Driving Sequences</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised feature learning enables perception systems to benefit from the vast raw data recorded by vehicle fleets worldwide. While video-level self-supervised learning approaches have shown strong generalizability on classification tasks, the potential to learn dense representations from sequential data has been relatively unexplored. In this work, we propose TempO, a temporal ordering pretext task for pre-training region-level feature representations for perception tasks. We embed each frame by an unordered set of proposal feature vectors, a representation that is natural for object detection or tracking systems, and formulate the sequential ordering by predicting frame transition probabilities in a transformer-based multi-frame architecture whose complexity scales less than quadratic with respect to the sequence length. Extensive evaluations on the BDD100K, nuImages, and MOT17 datasets show that our TempO pre-training approach outperforms single-frame self-supervised learning methods as well as supervised transfer learning initialization strategies, achieving an improvement of +0.7% in mAP for object detection and +2.0% in the HOTA score for multi-object tracking.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2302.02444.pdf' target='_blank'>https://arxiv.org/pdf/2302.02444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Wang, Kean Chen, Weiyao Lin, John See, Zenghui Zhang, Qian Xu, Xia Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02444">Spatio-Temporal Point Process for Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) focuses on modeling the relationship of detected objects among consecutive frames and merge them into different trajectories. MOT remains a challenging task as noisy and confusing detection results often hinder the final performance. Furthermore, most existing research are focusing on improving detection algorithms and association strategies. As such, we propose a novel framework that can effectively predict and mask-out the noisy and confusing detection results before associating the objects into trajectories. In particular, we formulate such "bad" detection results as a sequence of events and adopt the spatio-temporal point process}to model such events. Traditionally, the occurrence rate in a point process is characterized by an explicitly defined intensity function, which depends on the prior knowledge of some specific tasks. Thus, designing a proper model is expensive and time-consuming, with also limited ability to generalize well. To tackle this problem, we adopt the convolutional recurrent neural network (conv-RNN) to instantiate the point process, where its intensity function is automatically modeled by the training data. Furthermore, we show that our method captures both temporal and spatial evolution, which is essential in modeling events for MOT. Experimental results demonstrate notable improvements in addressing noisy and confusing detection results in MOT datasets. An improved state-of-the-art performance is achieved by incorporating our baseline MOT algorithm with the spatio-temporal point process model.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2211.14317.pdf' target='_blank'>https://arxiv.org/pdf/2211.14317.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yang, Shigeyuki Odashima, Shoichi Masui, Shan Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.14317">Hard to Track Objects with Irregular Motions and Similar Appearances? Make It Easier by Buffering the Matching Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a Cascaded Buffered IoU (C-BIoU) tracker to track multiple objects that have irregular motions and indistinguishable appearances. When appearance features are unreliable and geometric features are confused by irregular motions, applying conventional Multiple Object Tracking (MOT) methods may generate unsatisfactory results. To address this issue, our C-BIoU tracker adds buffers to expand the matching space of detections and tracks, which mitigates the effect of irregular motions in two aspects: one is to directly match identical but non-overlapping detections and tracks in adjacent frames, and the other is to compensate for the motion estimation bias in the matching space. In addition, to reduce the risk of overexpansion of the matching space, cascaded matching is employed: first matching alive tracks and detections with a small buffer, and then matching unmatched tracks and detections with a large buffer. Despite its simplicity, our C-BIoU tracker works surprisingly well and achieves state-of-the-art results on MOT datasets that focus on irregular motions and indistinguishable appearances. Moreover, the C-BIoU tracker is the dominant component for our 2-nd place solution in the CVPR'22 SoccerNet MOT and ECCV'22 MOTComplex DanceTrack challenges. Finally, we analyze the limitation of our C-BIoU tracker in ablation studies and discuss its application scope.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2210.11441.pdf' target='_blank'>https://arxiv.org/pdf/2210.11441.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karina Ruzaeva, Jan-Christopher Cohrs, Keitaro Kasahara, Dietrich Kohlheyer, Katharina NÃ¶h, Benjamin Berkels
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.11441">Cell tracking for live-cell microscopy using an activity-prioritized assignment strategy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cell tracking is an essential tool in live-cell imaging to determine single-cell features, such as division patterns or elongation rates. Unlike in common multiple object tracking, in microbial live-cell experiments cells are growing, moving, and dividing over time, to form cell colonies that are densely packed in mono-layer structures. With increasing cell numbers, following the precise cell-cell associations correctly over many generations becomes more and more challenging, due to the massively increasing number of possible associations.
  To tackle this challenge, we propose a fast parameter-free cell tracking approach, which consists of activity-prioritized nearest neighbor assignment of growing cells and a combinatorial solver that assigns splitting mother cells to their daughters. As input for the tracking, Omnipose is utilized for instance segmentation. Unlike conventional nearest-neighbor-based tracking approaches, the assignment steps of our proposed method are based on a Gaussian activity-based metric, predicting the cell-specific migration probability, thereby limiting the number of erroneous assignments. In addition to being a building block for cell tracking, the proposed activity map is a standalone tracking-free metric for indicating cell activity. Finally, we perform a quantitative analysis of the tracking accuracy for different frame rates, to inform life scientists about a suitable (in terms of tracking performance) choice of the frame rate for their cultivation experiments, when cell tracks are the desired key outcome.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2210.06984.pdf' target='_blank'>https://arxiv.org/pdf/2210.06984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Fischer, Thomas E. Huang, Jiangmiao Pang, Linlu Qiu, Haofeng Chen, Trevor Darrell, Fisher Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.06984">QDTrack: Quasi-Dense Similarity Learning for Appearance-Only Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Similarity learning has been recognized as a crucial step for object tracking. However, existing multiple object tracking methods only use sparse ground truth matching as the training objective, while ignoring the majority of the informative regions in images. In this paper, we present Quasi-Dense Similarity Learning, which densely samples hundreds of object regions on a pair of images for contrastive learning. We combine this similarity learning with multiple existing object detectors to build Quasi-Dense Tracking (QDTrack), which does not require displacement regression or motion priors. We find that the resulting distinctive feature space admits a simple nearest neighbor search at inference time for object association. In addition, we show that our similarity learning scheme is not limited to video data, but can learn effective instance similarity even from static input, enabling a competitive tracking performance without training on videos or using tracking supervision. We conduct extensive experiments on a wide variety of popular MOT benchmarks. We find that, despite its simplicity, QDTrack rivals the performance of state-of-the-art tracking methods on all benchmarks and sets a new state-of-the-art on the large-scale BDD100K MOT benchmark, while introducing negligible computational overhead to the detector.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2210.03296.pdf' target='_blank'>https://arxiv.org/pdf/2210.03296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyang Lu, Ming Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.03296">GMA3D: Local-Global Attention Learning to Estimate Occluded Motions of Scene Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Scene flow represents the motion information of each point in the 3D point clouds. It is a vital downstream method applied to many tasks, such as motion segmentation and object tracking. However, there are always occlusion points between two consecutive point clouds, whether from the sparsity data sampling or real-world occlusion. In this paper, we focus on addressing occlusion issues in scene flow by the semantic self-similarity and motion consistency of the moving objects. We propose a GMA3D module based on the transformer framework, which utilizes local and global semantic similarity to infer the motion information of occluded points from the motion information of local and global non-occluded points respectively, and then uses an offset aggregator to aggregate them. Our module is the first to apply the transformer-based architecture to gauge the scene flow occlusion problem on point clouds. Experiments show that our GMA3D can solve the occlusion problem in the scene flow, especially in the real scene. We evaluated the proposed method on the occluded version of point cloud datasets and get state-of-the-art results on the real scene KITTI dataset. To testify that GMA3D is still beneficial to non-occluded scene flow, we also conducted experiments on non-occluded version datasets and achieved promising performance on FlyThings3D and KITTI. The code is available at https://anonymous.4open.science/r/GMA3D-E100.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2208.01502.pdf' target='_blank'>https://arxiv.org/pdf/2208.01502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manuel Stoiber, Martin Sundermeyer, Wout Boerdijk, Rudolph Triebel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.01502">A Multi-body Tracking Framework - From Rigid Objects to Kinematic Structures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Kinematic structures are very common in the real world. They range from simple articulated objects to complex mechanical systems. However, despite their relevance, most model-based 3D tracking methods only consider rigid objects. To overcome this limitation, we propose a flexible framework that allows the extension of existing 6DoF algorithms to kinematic structures. Our approach focuses on methods that employ Newton-like optimization techniques, which are widely used in object tracking. The framework considers both tree-like and closed kinematic structures and allows a flexible configuration of joints and constraints. To project equations from individual rigid bodies to a multi-body system, Jacobians are used. For closed kinematic chains, a novel formulation that features Lagrange multipliers is developed. In a detailed mathematical proof, we show that our constraint formulation leads to an exact kinematic solution and converges in a single iteration. Based on the proposed framework, we extend ICG, which is a state-of-the-art rigid object tracking algorithm, to multi-body tracking. For the evaluation, we create a highly-realistic synthetic dataset that features a large number of sequences and various robots. Based on this dataset, we conduct a wide variety of experiments that demonstrate the excellent performance of the developed framework and our multi-body tracker.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2206.10981.pdf' target='_blank'>https://arxiv.org/pdf/2206.10981.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyang Kang, Ariel Herrera, Henry Lema, Esteban Valencia, Patrick Vandewalle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.10981">Adaptive Sampling-based Particle Filter for Visual-inertial Gimbal in the Wild</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present a Computer Vision (CV) based tracking and fusion algorithm, dedicated to a 3D printed gimbal system on drones operating in nature. The whole gimbal system can stabilize the camera orientation robustly in a challenging nature scenario by using skyline and ground plane as references. Our main contributions are the following: a) a light-weight Resnet-18 backbone network model was trained from scratch, and deployed onto the Jetson Nano platform to segment the image into binary parts (ground and sky); b) our geometry assumption from nature cues delivers the potential for robust visual tracking by using the skyline and ground plane as a reference; c) a spherical surface-based adaptive particle sampling, can fuse orientation from multiple sensor sources flexibly. The whole algorithm pipeline is tested on our customized gimbal module including Jetson and other hardware components. The experiments were performed on top of a building in the real landscape.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2203.10729.pdf' target='_blank'>https://arxiv.org/pdf/2203.10729.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JiaXu Wan, Hong Zhang, Jin Zhang, Yuan Ding, Yifan Yang, Yan Li, Xuliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.10729">DSRRTracker: Dynamic Search Region Refinement for Attention-based Siamese Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many multi-object tracking (MOT) methods follow the framework of "tracking by detection", which associates the target objects-of-interest based on the detection results. However, due to the separate models for detection and association, the tracking results are not optimal.Moreover, the speed is limited by some cumbersome association methods to achieve high tracking performance. In this work, we propose an end-to-end MOT method, with a Gaussian filter-inspired dynamic search region refinement module to dynamically filter and refine the search region by considering both the template information from the past frames and the detection results from the current frame with little computational burden, and a lightweight attention-based tracking head to achieve the effective fine-grained instance association. Extensive experiments and ablation study on MOT17 and MOT20 datasets demonstrate that our method can achieve the state-of-the-art performance with reasonable speed.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2510.09878.pdf' target='_blank'>https://arxiv.org/pdf/2510.09878.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Khanchi, Maria Amer, Charalambos Poullis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09878">Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) methods often rely on Intersection-over-Union (IoU) for association. However, this becomes unreliable when objects are similar or occluded. Also, computing IoU for segmentation masks is computationally expensive. In this work, we use segmentation masks to capture object shapes, but we do not compute segmentation IoU. Instead, we fuse depth and mask features and pass them through a compact encoder trained self-supervised. This encoder produces stable object representations, which we use as an additional similarity cue alongside bounding box IoU and re-identification features for matching. We obtain depth maps from a zero-shot depth estimator and object masks from a promptable visual segmentation model to obtain fine-grained spatial cues. Our MOT method is the first to use the self-supervised encoder to refine segmentation masks without computing masks IoU. MOT can be divided into joint detection-ReID (JDR) and tracking-by-detection (TBD) models. The latter are computationally more efficient. Experiments of our TBD method on challenging benchmarks with non-linear motion, occlusion, and crowded scenes, such as SportsMOT and DanceTrack, show that our method outperforms the TBD state-of-the-art on most metrics, while achieving competitive performance on simpler benchmarks with linear motion, such as MOT17.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2510.09092.pdf' target='_blank'>https://arxiv.org/pdf/2510.09092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juanqin Liu, Leonardo Plotegher, Eloy Roura, Shaoming He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09092">GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2509.21715.pdf' target='_blank'>https://arxiv.org/pdf/2509.21715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Yang, Gady Agam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21715">Motion-Aware Transformer for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in videos remains challenging due to complex object motions and crowded scenes. Recent DETR-based frameworks offer end-to-end solutions but typically process detection and tracking queries jointly within a single Transformer Decoder layer, leading to conflicts and degraded association accuracy. We introduce the Motion-Aware Transformer (MATR), which explicitly predicts object movements across frames to update track queries in advance. By reducing query collisions, MATR enables more consistent training and improves both detection and association. Extensive experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers significant gains across standard metrics. On DanceTrack, MATR improves HOTA by more than 9 points over MOTR without additional data and reaches a new state-of-the-art score of 71.3 with supplementary data. MATR also achieves state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6 mHOTA) without relying on external datasets. These results demonstrate that explicitly modeling motion within end-to-end Transformers offers a simple yet highly effective approach to advancing multi-object tracking.
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2509.21715.pdf' target='_blank'>https://arxiv.org/pdf/2509.21715.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Yang, Gady Agam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21715">Motion-Aware Transformer for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in videos remains challenging due to complex object motions and crowded scenes. Recent DETR-based frameworks offer end-to-end solutions but typically process detection and tracking queries jointly within a single Transformer Decoder layer, leading to conflicts and degraded association accuracy. We introduce the Motion-Aware Transformer (MATR), which explicitly predicts object movements across frames to update track queries in advance. By reducing query collisions, MATR enables more consistent training and improves both detection and association. Extensive experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers significant gains across standard metrics. On DanceTrack, MATR improves HOTA by more than 9 points over MOTR without additional data and reaches a new state-of-the-art score of 71.3 with supplementary data. MATR also achieves state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6 mHOTA) without relying on external datasets. These results demonstrate that explicitly modeling motion within end-to-end Transformers offers a simple yet highly effective approach to advancing multi-object tracking.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2509.09349.pdf' target='_blank'>https://arxiv.org/pdf/2509.09349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Nell, Shane Gilroy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09349">Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behavior classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviors such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioral analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2509.09349.pdf' target='_blank'>https://arxiv.org/pdf/2509.09349.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ian Nell, Shane Gilroy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09349">Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behavior classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviors such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioral analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2509.06536.pdf' target='_blank'>https://arxiv.org/pdf/2509.06536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06536">Benchmarking EfficientTAM on FMO datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast and tiny object tracking remains a challenge in computer vision and in this paper we first introduce a JSON metadata file associated with four open source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we extend the description of the FMOs datasets with additional ground truth information in JSON format (called FMOX) with object size information. Finally we use our FMOX file to test a recently proposed foundational model for tracking (called EfficientTAM) showing that its performance compares well with the pipelines originally taylored for these FMO datasets. Our comparison of these state-of-the-art techniques on FMOX is provided with Trajectory Intersection of Union (TIoU) scores. The code and JSON is shared open source allowing FMOX to be accessible and usable for other machine learning pipelines aiming to process FMO datasets.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2509.06536.pdf' target='_blank'>https://arxiv.org/pdf/2509.06536.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06536">Benchmarking EfficientTAM on FMO datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fast and tiny object tracking remains a challenge in computer vision and in this paper we first introduce a JSON metadata file associated with four open source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we extend the description of the FMOs datasets with additional ground truth information in JSON format (called FMOX) with object size information. Finally we use our FMOX file to test a recently proposed foundational model for tracking (called EfficientTAM) showing that its performance compares well with the pipelines originally taylored for these FMO datasets. Our comparison of these state-of-the-art techniques on FMOX is provided with Trajectory Intersection of Union (TIoU) scores. The code and JSON is shared open source allowing FMOX to be accessible and usable for other machine learning pipelines aiming to process FMO datasets.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2508.12982.pdf' target='_blank'>https://arxiv.org/pdf/2508.12982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Krejčí, Ondřej Straka, Petr Girg, Jiří Benedikt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12982">Revisiting Functional Derivatives in Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probability generating functionals (PGFLs) are efficient and powerful tools for tracking independent objects in clutter. It was shown that PGFLs could be used for the elegant derivation of practical multi-object tracking algorithms, e.g., the probability hypothesis density (PHD) filter. However, derivations using PGFLs use the so-called functional derivatives whose definitions usually appear too complicated or heuristic, involving Dirac delta ``functions''. This paper begins by comparing different definitions of functional derivatives and exploring their relationships and implications for practical applications. It then proposes a rigorous definition of the functional derivative, utilizing straightforward yet precise mathematics for clarity. Key properties of the functional derivative are revealed and discussed.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2508.12982.pdf' target='_blank'>https://arxiv.org/pdf/2508.12982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Krejčí, Ondřej Straka, Petr Girg, Jiří Benedikt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12982">Revisiting Functional Derivatives in Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Probability generating functionals (PGFLs) are efficient and powerful tools for tracking independent objects in clutter. It was shown that PGFLs could be used for the elegant derivation of practical multi-object tracking algorithms, e.g., the probability hypothesis density (PHD) filter. However, derivations using PGFLs use the so-called functional derivatives whose definitions usually appear too complicated or heuristic, involving Dirac delta ``functions''. This paper begins by comparing different definitions of functional derivatives and exploring their relationships and implications for practical applications. It then proposes a rigorous definition of the functional derivative, utilizing straightforward yet precise mathematics for clarity. Key properties of the functional derivative are revealed and discussed.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2508.12777.pdf' target='_blank'>https://arxiv.org/pdf/2508.12777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenguang Tao, Xiaotian Wang, Tian Yan, Jie Yan, Guodong Li, Kun Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12777">SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a key research direction in the field of multi-object tracking (MOT), UAV-based multi-object tracking has significant application value in the analysis and understanding of urban intelligent transportation systems. However, in complex UAV perspectives, challenges such as small target scale variations, occlusions, nonlinear crossing motions, and motion blur severely hinder the stability of multi-object tracking. To address these challenges, this paper proposes a novel multi-object tracking framework, SocialTrack, aimed at enhancing the tracking accuracy and robustness of small targets in complex urban traffic environments. The specialized small-target detector enhances the detection performance by employing a multi-scale feature enhancement mechanism. The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of trajectory prediction by incorporating a velocity dynamic modeling mechanism. The Group Motion Compensation Strategy (GMCS) models social group motion priors to provide stable state update references for low-quality tracks, significantly improving the target association accuracy in complex dynamic environments. Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical trajectory information to predict the future state of low-quality tracks, effectively mitigating identity switching issues. Extensive experiments on the UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing state-of-the-art (SOTA) methods across several key metrics. Significant improvements in MOTA and IDF1, among other core performance indicators, highlight its superior robustness and adaptability. Additionally, SocialTrack is highly modular and compatible, allowing for seamless integration with existing trackers to further enhance performance.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2508.12777.pdf' target='_blank'>https://arxiv.org/pdf/2508.12777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenguang Tao, Xiaotian Wang, Tian Yan, Jie Yan, Guodong Li, Kun Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12777">SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a key research direction in the field of multi-object tracking (MOT), UAV-based multi-object tracking has significant application value in the analysis and understanding of urban intelligent transportation systems. However, in complex UAV perspectives, challenges such as small target scale variations, occlusions, nonlinear crossing motions, and motion blur severely hinder the stability of multi-object tracking. To address these challenges, this paper proposes a novel multi-object tracking framework, SocialTrack, aimed at enhancing the tracking accuracy and robustness of small targets in complex urban traffic environments. The specialized small-target detector enhances the detection performance by employing a multi-scale feature enhancement mechanism. The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of trajectory prediction by incorporating a velocity dynamic modeling mechanism. The Group Motion Compensation Strategy (GMCS) models social group motion priors to provide stable state update references for low-quality tracks, significantly improving the target association accuracy in complex dynamic environments. Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical trajectory information to predict the future state of low-quality tracks, effectively mitigating identity switching issues. Extensive experiments on the UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing state-of-the-art (SOTA) methods across several key metrics. Significant improvements in MOTA and IDF1, among other core performance indicators, highlight its superior robustness and adaptability. Additionally, SocialTrack is highly modular and compatible, allowing for seamless integration with existing trackers to further enhance performance.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2508.09585.pdf' target='_blank'>https://arxiv.org/pdf/2508.09585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Haag, Bharanidhar Duraisamy, Felix Govaers, Wolfgang Koch, Martin Fritzsche, Juergen Dickmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09585">Offline Auto Labeling: BAAS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces BAAS, a new Extended Object Tracking (EOT) and fusion-based label annotation framework for radar detections in autonomous driving. Our framework utilizes Bayesian-based tracking, smoothing and eventually fusion methods to provide veritable and precise object trajectories along with shape estimation to provide annotation labels on the detection level under various supervision levels. Simultaneously, the framework provides evaluation of tracking performance and label annotation. If manually labeled data is available, each processing module can be analyzed independently or combined with other modules to enable closed-loop continuous improvements. The framework performance is evaluated in a challenging urban real-world scenario in terms of tracking performance and the label annotation errors. We demonstrate the functionality of the proposed approach for varying dynamic objects and class types
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2508.05514.pdf' target='_blank'>https://arxiv.org/pdf/2508.05514.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Wu, CÃ©sar Teixeira, Wei Ke, Zhang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05514">Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual pedestrian tracking represents a promising research field, with extensive applications in intelligent surveillance, behavior analysis, and human-computer interaction. However, real-world applications face significant occlusion challenges. When multiple pedestrians interact or overlap, the loss of target features severely compromises the tracker's ability to maintain stable trajectories. Traditional tracking methods, which typically rely on full-body bounding box features extracted from {Re-ID} models and linear constant-velocity motion assumptions, often struggle in severe occlusion scenarios. To address these limitations, this work proposes an enhanced tracking framework that leverages richer feature representations and a more robust motion model. Specifically, the proposed method incorporates detection features from both the regression and classification branches of an object detector, embedding spatial and positional information directly into the feature representations. To further mitigate occlusion challenges, a head keypoint detection model is introduced, as the head is less prone to occlusion compared to the full body. In terms of motion modeling, we propose an iterative Kalman filtering approach designed to align with modern detector assumptions, integrating 3D priors to better complete motion trajectories in complex scenes. By combining these advancements in appearance and motion modeling, the proposed method offers a more robust solution for multi-object tracking in crowded environments where occlusions are prevalent.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2508.05172.pdf' target='_blank'>https://arxiv.org/pdf/2508.05172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Wu, Longhao Wang, Cui Wang, CÃ©sar Teixeira, Wei Ke, Zhang Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05172">Multi-tracklet Tracking for Generic Targets with Adaptive Detection Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking specific targets, such as pedestrians and vehicles, has been the focus of recent vision-based multitarget tracking studies. However, in some real-world scenarios, unseen categories often challenge existing methods due to low-confidence detections, weak motion and appearance constraints, and long-term occlusions. To address these issues, this article proposes a tracklet-enhanced tracker called Multi-Tracklet Tracking (MTT) that integrates flexible tracklet generation into a multi-tracklet association framework. This framework first adaptively clusters the detection results according to their short-term spatio-temporal correlation into robust tracklets and then estimates the best tracklet partitions using multiple clues, such as location and appearance over time to mitigate error propagation in long-term association. Finally, extensive experiments on the benchmark for generic multiple object tracking demonstrate the competitiveness of the proposed framework.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2508.02067.pdf' target='_blank'>https://arxiv.org/pdf/2508.02067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Manikanta Kotthapalli, Deepika Ravipati, Reshma Bhatia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02067">YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Over the past decade, object detection has advanced significantly, with the YOLO (You Only Look Once) family of models transforming the landscape of real-time vision applications through unified, end-to-end detection frameworks. From YOLOv1's pioneering regression-based detection to the latest YOLOv9, each version has systematically enhanced the balance between speed, accuracy, and deployment efficiency through continuous architectural and algorithmic advancements.. Beyond core object detection, modern YOLO architectures have expanded to support tasks such as instance segmentation, pose estimation, object tracking, and domain-specific applications including medical imaging and industrial automation. This paper offers a comprehensive review of the YOLO family, highlighting architectural innovations, performance benchmarks, extended capabilities, and real-world use cases. We critically analyze the evolution of YOLO models and discuss emerging research directions that extend their impact across diverse computer vision domains.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2507.16639.pdf' target='_blank'>https://arxiv.org/pdf/2507.16639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Henrich, Christian Post, Maximilian Zilke, Parth Shiroya, Emma Chanut, Amir Mollazadeh Yamchi, Ramin Yahyapour, Thomas Kneib, Imke Traulsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16639">Benchmarking pig detection and tracking under diverse and challenging conditions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To ensure animal welfare and effective management in pig farming, monitoring individual behavior is a crucial prerequisite. While monitoring tasks have traditionally been carried out manually, advances in machine learning have made it possible to collect individualized information in an increasingly automated way. Central to these methods is the localization of animals across space (object detection) and time (multi-object tracking). Despite extensive research of these two tasks in pig farming, a systematic benchmarking study has not yet been conducted. In this work, we address this gap by curating two datasets: PigDetect for object detection and PigTrack for multi-object tracking. The datasets are based on diverse image and video material from realistic barn conditions, and include challenging scenarios such as occlusions or bad visibility. For object detection, we show that challenging training images improve detection performance beyond what is achievable with randomly sampled images alone. Comparing different approaches, we found that state-of-the-art models offer substantial improvements in detection quality over real-time alternatives. For multi-object tracking, we observed that SORT-based methods achieve superior detection performance compared to end-to-end trainable models. However, end-to-end models show better association performance, suggesting they could become strong alternatives in the future. We also investigate characteristic failure cases of end-to-end models, providing guidance for future improvements. The detection and tracking models trained on our datasets perform well in unseen pens, suggesting good generalization capabilities. This highlights the importance of high-quality training data. The datasets and research code are made publicly available to facilitate reproducibility, re-use and further development.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2507.08548.pdf' target='_blank'>https://arxiv.org/pdf/2507.08548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alen Adamyan, TomÃ¡Å¡ ÄÃ­Å¾ek, Matej Straka, Klara Janouskova, Martin Schmid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08548">SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Segment Anything Model 2 (SAM 2) has demonstrated strong performance in object segmentation tasks and has become the state-of-the-art for visual object tracking. The model stores information from previous frames in a memory bank, enabling temporal consistency across video sequences. Recent methods augment SAM 2 with hand-crafted update rules to better handle distractors, occlusions, and object motion. We propose a fundamentally different approach using reinforcement learning for optimizing memory updates in SAM 2 by framing memory control as a sequential decision-making problem. In an overfitting setup with a separate agent per video, our method achieves a relative improvement over SAM 2 that exceeds by more than three times the gains of existing heuristics. These results reveal the untapped potential of the memory bank and highlight reinforcement learning as a powerful alternative to hand-crafted update rules for memory control in visual object tracking.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2507.04116.pdf' target='_blank'>https://arxiv.org/pdf/2507.04116.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fred Lydeard, Bashar I. Ahmad, Simon Godsill
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04116">Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a computationally efficient multi-object tracking approach that can minimise track breaks (e.g., in challenging environments and against agile targets), learn the measurement model parameters on-line (e.g., in dynamically changing scenes) and infer the class of the tracked objects, if joint tracking and kinematic behaviour classification is sought. It capitalises on the flexibilities offered by the integrated Gaussian process as a motion model and the convenient statistical properties of non-homogeneous Poisson processes as a suitable observation model. This can be combined with the proposed effective track revival / stitching mechanism. We accordingly introduce the two robust and adaptive trackers, Gaussian and Poisson Process with Classification (GaPP-Class) and GaPP with Revival and Classification (GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme that efficiently integrates track management and hyperparameter learning (including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class with the addition of a Markov Chain Monte Carlo kernel applied to each particle permitting track revival and stitching (e.g., within a few time steps after deleting a trajectory). Performance evaluation and benchmarking using synthetic and real data show that GaPP-Class and GaPP-ReaCtion outperform other state-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly reduces track breaks (e.g., by around 30% from real radar data and markedly more from simulated data).
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2507.02408.pdf' target='_blank'>https://arxiv.org/pdf/2507.02408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duong Nguyen-Ngoc Tran, Long Hoang Pham, Chi Dai Tran, Quoc Pham-Nam Ho, Huy-Hung Nguyen, Jae Wook Jeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02408">A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking in thermal images is essential for surveillance systems, particularly in challenging environments where RGB cameras struggle due to low visibility or poor lighting conditions. Thermal sensors enhance recognition tasks by capturing infrared signatures, but a major challenge is their low-level feature representation, which makes it difficult to accurately detect and track pedestrians. To address this, the paper introduces a novel tuning method for pedestrian tracking, specifically designed to handle the complex motion patterns in thermal imagery. The proposed framework optimizes two-stages, ensuring that each stage is tuned with the most suitable hyperparameters to maximize tracking performance. By fine-tuning hyperparameters for real-time tracking, the method achieves high accuracy without relying on complex reidentification or motion models. Extensive experiments on PBVS Thermal MOT dataset demonstrate that the approach is highly effective across various thermal camera conditions, making it a robust solution for real-world surveillance applications.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2506.22562.pdf' target='_blank'>https://arxiv.org/pdf/2506.22562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhineet Singh, Nilanjan Ray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22562">Improving Token-based Object Detection with Video</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper improves upon the Pix2Seq object detector by extending it for videos. In the process, it introduces a new way to perform end-to-end video object detection that improves upon existing video detectors in two key ways. First, by representing objects as variable-length sequences of discrete tokens, we can succinctly represent widely varying numbers of video objects, with diverse shapes and locations, without having to inject any localization cues in the training process. This eliminates the need to sample the space of all possible boxes that constrains conventional detectors and thus solves the dual problems of loss sparsity during training and heuristics-based postprocessing during inference. Second, it conceptualizes and outputs the video objects as fully integrated and indivisible 3D boxes or tracklets instead of generating image-specific 2D boxes and linking these boxes together to construct the video object, as done in most conventional detectors. This allows it to scale effortlessly with available computational resources by simply increasing the length of the video subsequence that the network takes as input, even generalizing to multi-object tracking if the subsequence can span the entire video. We compare our video detector with the baseline Pix2Seq static detector on several datasets and demonstrate consistent improvement, although with strong signs of being bottlenecked by our limited computational resources. We also compare it with several video detectors on UA-DETRAC to show that it is competitive with the current state of the art even with the computational bottleneck. We make our code and models publicly available.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2506.04122.pdf' target='_blank'>https://arxiv.org/pdf/2506.04122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharang Kaul, Mario Berk, Thiemo Gerbich, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04122">Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding reliable matches is essential in multi-object tracking to ensure the accuracy and reliability of perception systems in safety-critical applications such as autonomous vehicles. Effective matching mitigates perception errors, enhancing object identification and tracking for improved performance and safety. However, traditional metrics such as Intersection over Union (IoU) and Center Point Distances (CPDs), which are effective in 2D image planes, often fail to find critical matches in complex 3D scenes. To address this limitation, we introduce Contour Errors (CEs), an ego or object-centric metric for identifying matches of interest in tracking scenarios from a functional perspective. By comparing bounding boxes in the ego vehicle's frame, contour errors provide a more functionally relevant assessment of object matches. Extensive experiments on the nuScenes dataset demonstrate that contour errors improve the reliability of matches over the state-of-the-art 2D IoU and CPD metrics in tracking-by-detection methods. In 3D car tracking, our results show that Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges and 60% at far ranges compared to IoU in the evaluation stage.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2506.04122.pdf' target='_blank'>https://arxiv.org/pdf/2506.04122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sharang Kaul, Mario Berk, Thiemo Gerbich, Abhinav Valada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04122">Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Finding reliable matches is essential in multi-object tracking to ensure the accuracy and reliability of perception systems in safety-critical applications such as autonomous vehicles. Effective matching mitigates perception errors, enhancing object identification and tracking for improved performance and safety. However, traditional metrics such as Intersection over Union (IoU) and Center Point Distances (CPDs), which are effective in 2D image planes, often fail to find critical matches in complex 3D scenes. To address this limitation, we introduce Contour Errors (CEs), an ego or object-centric metric for identifying matches of interest in tracking scenarios from a functional perspective. By comparing bounding boxes in the ego vehicle's frame, contour errors provide a more functionally relevant assessment of object matches. Extensive experiments on the nuScenes dataset demonstrate that contour errors improve the reliability of matches over the state-of-the-art 2D IoU and CPD metrics in tracking-by-detection methods. In 3D car tracking, our results show that Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges and 60% at far ranges compared to IoU in the evaluation stage.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2505.12753.pdf' target='_blank'>https://arxiv.org/pdf/2505.12753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martha Teiko Teye, Ori Maoz, Matthias Rottmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12753">LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking from LiDAR point clouds presents unique challenges due to the sparse and irregular nature of the data, compounded by the need for temporal coherence across frames. Traditional tracking systems often rely on hand-crafted features and motion models, which can struggle to maintain consistent object identities in crowded or fast-moving scenes. We present a lidar-based two-staged DETR inspired transformer; a smoother and tracker. The smoother stage refines lidar object detections, from any off-the-shelf detector, across a moving temporal window. The tracker stage uses a DETR-based attention block to maintain tracks across time by associating tracked objects with the refined detections using the point cloud as context. The model is trained on the datasets nuScenes and KITTI in both online and offline (forward peeking) modes demonstrating strong performance across metrics such as ID-switch and multiple object tracking accuracy (MOTA). The numerical results indicate that the online mode outperforms the lidar-only baseline and SOTA models on the nuScenes dataset, with an aMOTA of 0.724 and an aMOTP of 0.475, while the offline mode provides an additional 3 pp aMOTP.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2505.12753.pdf' target='_blank'>https://arxiv.org/pdf/2505.12753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martha Teiko Teye, Ori Maoz, Matthias Rottmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12753">LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking from LiDAR point clouds presents unique challenges due to the sparse and irregular nature of the data, compounded by the need for temporal coherence across frames. Traditional tracking systems often rely on hand-crafted features and motion models, which can struggle to maintain consistent object identities in crowded or fast-moving scenes. We present a lidar-based two-staged DETR inspired transformer; a smoother and tracker. The smoother stage refines lidar object detections, from any off-the-shelf detector, across a moving temporal window. The tracker stage uses a DETR-based attention block to maintain tracks across time by associating tracked objects with the refined detections using the point cloud as context. The model is trained on the datasets nuScenes and KITTI in both online and offline (forward peeking) modes demonstrating strong performance across metrics such as ID-switch and multiple object tracking accuracy (MOTA). The numerical results indicate that the online mode outperforms the lidar-only baseline and SOTA models on the nuScenes dataset, with an aMOTA of 0.724 and an aMOTP of 0.475, while the offline mode provides an additional 3 pp aMOTP.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2505.04392.pdf' target='_blank'>https://arxiv.org/pdf/2505.04392.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Petr Jahoda, Jan Cech
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04392">Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. The method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. The method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. Anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. A challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. Therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. Our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. The method is effective and performs in real time on standard consumer hardware.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2505.00534.pdf' target='_blank'>https://arxiv.org/pdf/2505.00534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Imran Zaman, Usama Ijaz Bajwa, Gulshan Saleem, Rana Hammad Raza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.00534">A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2504.21692.pdf' target='_blank'>https://arxiv.org/pdf/2504.21692.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Zhou, Changrui Dai, Aibo Song, Xiaolin Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21692">Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Successful video analysis relies on accurate recognition of pixels across frames, and frame reconstruction methods based on video correspondence learning are popular due to their efficiency. Existing frame reconstruction methods, while efficient, neglect the value of direct involvement of multiple reference frames for reconstruction and decision-making aspects, especially in complex situations such as occlusion or fast movement. In this paper, we introduce a Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple reference frames to concisely and directly enhance frame reconstruction. Its core component is a Reference Frame Memory Engine that dynamically selects frames based on object pixel features to improve tracking accuracy. In addition, a Bidirectional Target Prediction Network is built to utilize multiple reference frames to improve the robustness of the model. Through experiments, our algorithm outperforms the state-of-the-art self-supervised techniques on two fine-grained video object tracking tasks: object segmentation and keypoint tracking.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2504.20234.pdf' target='_blank'>https://arxiv.org/pdf/2504.20234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bartosz Ptak, Marek Kraft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20234">Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone-based crowd monitoring is the key technology for applications in surveillance, public safety, and event management. However, maintaining tracking continuity and consistency remains a significant challenge. Traditional detection-assignment tracking methods struggle with false positives, false negatives, and frequent identity switches, leading to degraded counting accuracy and making in-depth analysis impossible. This paper introduces a point-oriented online tracking algorithm that improves trajectory continuity and counting reliability in drone-based crowd monitoring. Our method builds on the Simple Online and Real-time Tracking (SORT) framework, replacing the original bounding-box assignment with a point-distance metric. The algorithm is enhanced with three cost-effective techniques: camera motion compensation, altitude-aware assignment, and classification-based trajectory validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use spatial feature maps from localisation algorithms for increased computational efficiency through neural network resource sharing are integrated to refine object tracking by reducing noise and handling missed detections. The proposed method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets, demonstrating substantial improvements in tracking metrics, reducing counting errors to 23% and 15%, respectively. The results also indicate a significant reduction of identity switches while maintaining high tracking accuracy, outperforming baseline online trackers and even an offline greedy optimisation method.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2503.16768.pdf' target='_blank'>https://arxiv.org/pdf/2503.16768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Zhou, Jiadong Xie, Mingsheng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16768">Dynamic Attention Mechanism in Spatiotemporal Memory Networks for Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mainstream visual object tracking frameworks predominantly rely on template matching paradigms. Their performance heavily depends on the quality of template features, which becomes increasingly challenging to maintain in complex scenarios involving target deformation, occlusion, and background clutter. While existing spatiotemporal memory-based trackers emphasize memory capacity expansion, they lack effective mechanisms for dynamic feature selection and adaptive fusion. To address this gap, we propose a Dynamic Attention Mechanism in Spatiotemporal Memory Network (DASTM) with two key innovations: 1) A differentiable dynamic attention mechanism that adaptively adjusts channel-spatial attention weights by analyzing spatiotemporal correlations between the templates and memory features; 2) A lightweight gating network that autonomously allocates computational resources based on target motion states, prioritizing high-discriminability features in challenging scenarios. Extensive evaluations on OTB-2015, VOT 2018, LaSOT, and GOT-10K benchmarks demonstrate our DASTM's superiority, achieving state-of-the-art performance in success rate, robustness, and real-time efficiency, thereby offering a novel solution for real-time tracking in complex environments.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2503.16318.pdf' target='_blank'>https://arxiv.org/pdf/2503.16318.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edgar Sucar, Zihang Lai, Eldar Insafutdinov, Andrea Vedaldi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16318">Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DUSt3R has recently shown that one can reduce many tasks in multi-view geometry, including estimating camera intrinsics and extrinsics, reconstructing the scene in 3D, and establishing image correspondences, to the prediction of a pair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds defined in a common reference frame. This formulation is elegant and powerful, but unable to tackle dynamic scenes. To address this challenge, we introduce the concept of Dynamic Point Maps (DPM), extending standard point maps to support 4D tasks such as motion segmentation, scene flow estimation, 3D object tracking, and 2D correspondence. Our key intuition is that, when time is introduced, there are several possible spatial and time references that can be used to define the point maps. We identify a minimal subset of such combinations that can be regressed by a network to solve the sub tasks mentioned above. We train a DPM predictor on a mixture of synthetic and real data and evaluate it across diverse benchmarks for video depth prediction, dynamic point cloud reconstruction, 3D scene flow and object pose tracking, achieving state-of-the-art performance. Code, models and additional results are available at https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2503.09951.pdf' target='_blank'>https://arxiv.org/pdf/2503.09951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinglong Sun, Haijiang Sun, Shan Jiang, Jiacheng Wang, Jiasong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.09951">Target-aware Bidirectional Fusion Transformer for Aerial Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The trackers based on lightweight neural networks have achieved great success in the field of aerial remote sensing, most of which aggregate multi-stage deep features to lift the tracking quality. However, existing algorithms usually only generate single-stage fusion features for state decision, which ignore that diverse kinds of features are required for identifying and locating the object, limiting the robustness and precision of tracking. In this paper, we propose a novel target-aware Bidirectional Fusion transformer (BFTrans) for UAV tracking. Specifically, we first present a two-stream fusion network based on linear self and cross attentions, which can combine the shallow and the deep features from both forward and backward directions, providing the adjusted local details for location and global semantics for recognition. Besides, a target-aware positional encoding strategy is designed for the above fusion model, which is helpful to perceive the object-related attributes during the fusion phase. Finally, the proposed method is evaluated on several popular UAV benchmarks, including UAV-123, UAV20L and UAVTrack112. Massive experimental results demonstrate that our approach can exceed other state-of-the-art trackers and run with an average speed of 30.5 FPS on embedded platform, which is appropriate for practical drone deployments.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2502.03760.pdf' target='_blank'>https://arxiv.org/pdf/2502.03760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nhat-Tan Do, Nhi Ngoc-Yen Nguyen, Dieu-Phuong Nguyen, Trong-Hop Do
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.03760">RAMOTS: A Real-Time System for Aerial Multi-Object Tracking based on Deep Learning and Big Data Technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in UAV-based video is challenging due to variations in viewpoint, low resolution, and the presence of small objects. While other research on MOT dedicated to aerial videos primarily focuses on the academic aspect by developing sophisticated algorithms, there is a lack of attention to the practical aspect of these systems. In this paper, we propose a novel real-time MOT framework that integrates Apache Kafka and Apache Spark for efficient and fault-tolerant video stream processing, along with state-of-the-art deep learning models YOLOv8/YOLOv10 and BYTETRACK/BoTSORT for accurate object detection and tracking. Our work highlights the importance of not only the advanced algorithms but also the integration of these methods with scalable and distributed systems. By leveraging these technologies, our system achieves a HOTA of 48.14 and a MOTA of 43.51 on the Visdrone2019-MOT test set while maintaining a real-time processing speed of 28 FPS on a single GPU. Our work demonstrates the potential of big data technologies and deep learning for addressing the challenges of MOT in UAV applications.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2501.14587.pdf' target='_blank'>https://arxiv.org/pdf/2501.14587.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Viktor KozÃ¡k, Karel KoÅ¡nar, Jan Chudoba, Miroslav Kulich, Libor PÅeuÄil
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14587">Visual Localization via Semantic Structures in Autonomous Photovoltaic Power Plant Inspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Inspection systems utilizing unmanned aerial vehicles (UAVs) equipped with thermal cameras are increasingly popular for the maintenance of photovoltaic (PV) power plants. However, automation of the inspection task is a challenging problem as it requires precise navigation to capture images from optimal distances and viewing angles.
  This paper presents a novel localization pipeline that directly integrates PV module detection with UAV navigation, allowing precise positioning during inspection. Detections are used to identify the power plant structures in the image and associate these with the power plant model. We define visually recognizable anchor points for the initial association and use object tracking to discern global associations. We present three distinct methods for visual segmentation of PV modules based on traditional computer vision, deep learning, and their fusion, and we evaluate their performance in relation to the proposed localization pipeline.
  The presented methods were verified and evaluated using custom aerial inspection data sets, demonstrating their robustness and applicability for real-time navigation. Additionally, we evaluate the influence of the power plant model's precision on the localization methods.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2501.03874.pdf' target='_blank'>https://arxiv.org/pdf/2501.03874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ning Zhang, Timothy Shea, Arto Nurmikko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03874">Neuromorphic Optical Tracking and Imaging of Randomly Moving Targets through Strongly Scattering Media</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking and acquiring simultaneous optical images of randomly moving targets obscured by scattering media remains a challenging problem of importance to many applications that require precise object localization and identification. In this work we develop an end-to-end neuromorphic optical engineering and computational approach to demonstrate how to track and image normally invisible objects by combining an event detecting camera with a multistage neuromorphic deep learning strategy. Photons emerging from dense scattering media are detected by the event camera and converted to pixel-wise asynchronized spike trains - a first step in isolating object-specific information from the dominant uninformative background. Spiking data is fed into a deep spiking neural network (SNN) engine where object tracking and image reconstruction are performed by two separate yet interconnected modules running in parallel in discrete time steps over the event duration. Through benchtop experiments we demonstrate tracking and imaging randomly moving objects in dense turbid media as well as image reconstruction of spatially stationary but optically dynamic objects. Standardized character sets serve as representative proxies for geometrically complex objects, underscoring the method's generality. The results highlight the advantages of a fully neuromorphic approach in meeting a major imaging technology with high computational efficiency and low power consumption.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2412.14088.pdf' target='_blank'>https://arxiv.org/pdf/2412.14088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Dal'Col, Miguel Oliveira, VÃ­tor Santos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14088">Joint Perception and Prediction for Autonomous Driving: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Perception and prediction modules are critical components of autonomous driving systems, enabling vehicles to navigate safely through complex environments. The perception module is responsible for perceiving the environment, including static and dynamic objects, while the prediction module is responsible for predicting the future behavior of these objects. These modules are typically divided into three tasks: object detection, object tracking, and motion prediction. Traditionally, these tasks are developed and optimized independently, with outputs passed sequentially from one to the next. However, this approach has significant limitations: computational resources are not shared across tasks, the lack of joint optimization can amplify errors as they propagate throughout the pipeline, and uncertainty is rarely propagated between modules, resulting in significant information loss. To address these challenges, the joint perception and prediction paradigm has emerged, integrating perception and prediction into a unified model through multi-task learning. This strategy not only overcomes the limitations of previous methods, but also enables the three tasks to have direct access to raw sensor data, allowing richer and more nuanced environmental interpretations. This paper presents the first comprehensive survey of joint perception and prediction for autonomous driving. We propose a taxonomy that categorizes approaches based on input representation, scene context modeling, and output representation, highlighting their contributions and limitations. Additionally, we present a qualitative analysis and quantitative comparison of existing methods. Finally, we discuss future research directions based on identified gaps in the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2412.08121.pdf' target='_blank'>https://arxiv.org/pdf/2412.08121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel NordstrÃ¶m, BjÃ¶rn Lindquist, George Nikolakopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08121">DTAA: A Detect, Track and Avoid Architecture for navigation in spaces with Multiple Velocity Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proactive collision avoidance measures are imperative in environments where humans and robots coexist. Moreover, the introduction of high quality legged robots into workplaces highlighted the crucial role of a robust, fully autonomous safety solution for robots to be viable in shared spaces or in co-existence with humans. This article establishes for the first time ever an innovative Detect-Track-and-Avoid Architecture (DTAA) to enhance safety and overall mission performance. The proposed novel architectyre has the merit ot integrating object detection using YOLOv8, utilizing Ultralytics embedded object tracking, and state estimation of tracked objects through Kalman filters. Moreover, a novel heuristic clustering is employed to facilitate active avoidance of multiple closely positioned objects with similar velocities, creating sets of unsafe spaces for the Nonlinear Model Predictive Controller (NMPC) to navigate around. The NMPC identifies the most hazardous unsafe space, considering not only their current positions but also their predicted future locations. In the sequel, the NMPC calculates maneuvers to guide the robot along a path planned by D$^{*}_{+}$ towards its intended destination, while maintaining a safe distance to all identified obstacles. The efficacy of the novelly suggested DTAA framework is being validated by Real-life experiments featuring a Boston Dynamics Spot robot that demonstrates the robot's capability to consistently maintain a safe distance from humans in dynamic subterranean, urban indoor, and outdoor environments.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2411.06896.pdf' target='_blank'>https://arxiv.org/pdf/2411.06896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hemal Naik, Junran Yang, Dipin Das, Margaret C Crofoot, Akanksha Rathore, Vivek Hari Sridhar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.06896">BuckTales : A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Understanding animal behaviour is central to predicting, understanding, and mitigating impacts of natural and anthropogenic changes on animal populations and ecosystems. However, the challenges of acquiring and processing long-term, ecologically relevant data in wild settings have constrained the scope of behavioural research. The increasing availability of Unmanned Aerial Vehicles (UAVs), coupled with advances in machine learning, has opened new opportunities for wildlife monitoring using aerial tracking. However, limited availability of datasets with wild animals in natural habitats has hindered progress in automated computer vision solutions for long-term animal tracking. Here we introduce BuckTales, the first large-scale UAV dataset designed to solve multi-object tracking (MOT) and re-identification (Re-ID) problem in wild animals, specifically the mating behaviour (or lekking) of blackbuck antelopes. Collected in collaboration with biologists, the MOT dataset includes over 1.2 million annotations including 680 tracks across 12 high-resolution (5.4K) videos, each averaging 66 seconds and featuring 30 to 130 individuals. The Re-ID dataset includes 730 individuals captured with two UAVs simultaneously. The dataset is designed to drive scalable, long-term animal behaviour tracking using multiple camera sensors. By providing baseline performance with two detectors, and benchmarking several state-of-the-art tracking methods, our dataset reflects the real-world challenges of tracking wild animals in socially and ecologically relevant contexts. In making these data widely available, we hope to catalyze progress in MOT and Re-ID for wild animals, fostering insights into animal behaviour, conservation efforts, and ecosystem dynamics through automated, long-term monitoring.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2410.15428.pdf' target='_blank'>https://arxiv.org/pdf/2410.15428.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chung Shue Chen, Wing Shing Wong, Yuan-Hsun Lo, Tsai-Lien Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15428">Multiset Combinatorial Gray Codes with Application to Proximity Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate coding schemes that map source symbols into multisets of an alphabet set. Such a formulation of source coding is an alternative approach to the traditional framework and is inspired by an object tracking problem over proximity sensor networks. We define a \textit{multiset combinatorial Gray code} as a mulitset code with fixed multiset cardinality that possesses combinatorial Gray code characteristic. For source codes that are organized as a grid, namely an integer lattice, we propose a solution by first constructing a mapping from the grid to the alphabet set, the codes are then defined as the images of rectangular blocks in the grid of fixed dimensions. We refer to the mapping as a \textit{color mapping} and the code as a \textit{color multiset code}. We propose the idea of product multiset code that enables us to construct codes for high dimensional grids based on 1-dimensional (1D) grids. We provide a detailed analysis of color multiset codes on 1D grids, focusing on codes that require the minimal number of colors. To illustrate the application of such a coding scheme, we consider an object tracking problem on 2D grids and show its efficiency, which comes from exploiting transmission parallelism. Some numerical results are presented to conclude the paper.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2409.19891.pdf' target='_blank'>https://arxiv.org/pdf/2409.19891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Ishige, Yasuhiro Yoshimura, Ryo Yonetani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19891">Opt-in Camera: Person Identification in Video via UWB Localization and Its Application to Opt-in Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents opt-in camera, a concept of privacy-preserving camera systems capable of recording only specific individuals in a crowd who explicitly consent to be recorded. Our system utilizes a mobile wireless communication tag attached to personal belongings as proof of opt-in and as a means of localizing tag carriers in video footage. Specifically, the on-ground positions of the wireless tag are first tracked over time using the unscented Kalman filter (UKF). The tag trajectory is then matched against visual tracking results for pedestrians found in videos to identify the tag carrier. Technically, we devise a dedicated trajectory matching technique based on constrained linear optimization, as well as a novel calibration technique that handles wireless tag-camera calibration and hyperparameter tuning for the UKF, which mitigates the non-line-of-sight (NLoS) issue in wireless localization. We implemented the proposed opt-in camera system using ultra-wideband (UWB) devices and an off-the-shelf webcam. Experimental results demonstrate that our system can perform opt-in recording of individuals in real-time at 10 fps, with reliable identification accuracy in crowds of 8-23 people in a confined space.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2409.18901.pdf' target='_blank'>https://arxiv.org/pdf/2409.18901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18901">Improving Visual Object Tracking through Visual Prompting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learning a discriminative model to distinguish a target from its surrounding distractors is essential to generic visual object tracking. Dynamic target representation adaptation against distractors is challenging due to the limited discriminative capabilities of prevailing trackers. We present a new visual Prompting mechanism for generic Visual Object Tracking (PiVOT) to address this issue. PiVOT proposes a prompt generation network with the pre-trained foundation model CLIP to automatically generate and refine visual prompts, enabling the transfer of foundation model knowledge for tracking. While CLIP offers broad category-level knowledge, the tracker, trained on instance-specific data, excels at recognizing unique object instances. Thus, PiVOT first compiles a visual prompt highlighting potential target locations. To transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP to refine the visual prompt based on the similarities between candidate objects and the reference templates across potential targets. Once the visual prompt is refined, it can better highlight potential target locations, thereby reducing irrelevant prompt information. With the proposed prompting mechanism, the tracker can generate improved instance-aware feature maps through the guidance of the visual prompt, thus effectively reducing distractors. The proposed method does not involve CLIP during training, thereby keeping the same training complexity and preserving the generalization capability of the pretrained foundation model. Extensive experiments across multiple benchmarks indicate that PiVOT, using the proposed prompting method can suppress distracting objects and enhance the tracker.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2409.14220.pdf' target='_blank'>https://arxiv.org/pdf/2409.14220.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomasz Stanczyk, Francois Bremond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14220">Temporally Propagated Masks and Bounding Boxes: Combining the Best of Both Worlds for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) involves identifying and consistently tracking objects across video sequences. Traditional tracking-by-detection methods, while effective, often require extensive tuning and lack generalizability. On the other hand, segmentation mask-based methods are more generic but struggle with tracking management, making them unsuitable for MOT. We propose a novel approach, McByte, which incorporates a temporally propagated segmentation mask as a strong association cue within a tracking-by-detection framework. By combining bounding box and propagated mask information, McByte enhances robustness and generalizability without per-sequence tuning. Evaluated on four benchmark datasets - DanceTrack, MOT17, SoccerNet-tracking 2022, and KITTI-tracking - McByte demonstrates performance gain in all cases examined. At the same time, it outperforms existing mask-based methods. Implementation code will be provided upon acceptance.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2408.16190.pdf' target='_blank'>https://arxiv.org/pdf/2408.16190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tanner D. Harms, Steven L. Brunton, Beverley J. McKeon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16190">Estimating Dynamic Flow Features in Groups of Tracked Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Interpreting motion captured in image sequences is crucial for a wide range of computer vision applications. Typical estimation approaches include optical flow (OF), which approximates the apparent motion instantaneously in a scene, and multiple object tracking (MOT), which tracks the motion of subjects over time. Often, the motion of objects in a scene is governed by some underlying dynamical system which could be inferred by analyzing the motion of groups of objects. Standard motion analyses, however, are not designed to intuit flow dynamics from trajectory data, making such measurements difficult in practice. The goal of this work is to extend gradient-based dynamical systems analyses to real-world applications characterized by complex, feature-rich image sequences with imperfect tracers. The tracer trajectories are tracked using deep vision networks and gradients are approximated using Lagrangian gradient regression (LGR), a tool designed to estimate spatial gradients from sparse data. From gradients, dynamical features such as regions of coherent rotation and transport barriers are identified. The proposed approach is affordably implemented and enables advanced studies including the motion analysis of two distinct object classes in a single image sequence. Two examples of the method are presented on data sets for which standard gradient-based analyses do not apply.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2407.04327.pdf' target='_blank'>https://arxiv.org/pdf/2407.04327.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thuc Nguyen-Quang, Minh-Triet Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04327">TF-SASM: Training-free Spatial-aware Sparse Memory for Multi-object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in computer vision remains a significant challenge, requiring precise localization and continuous tracking of multiple objects in video sequences. The emergence of data sets that emphasize robust reidentification, such as DanceTrack, has highlighted the need for effective solutions. While memory-based approaches have shown promise, they often suffer from high computational complexity and memory usage due to storing feature at every single frame. In this paper, we propose a novel memory-based approach that selectively stores critical features based on object motion and overlapping awareness, aiming to enhance efficiency while minimizing redundancy. As a result, our method not only store longer temporal information with limited number of stored features in the memory, but also diversify states of a particular object to enhance the association performance. Our approach significantly improves over MOTRv2 in the DanceTrack test set, demonstrating a gain of 2.0% AssA score and 2.1% in IDF1 score.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2407.00830.pdf' target='_blank'>https://arxiv.org/pdf/2407.00830.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ogulcan Eryuksel, Kamil Anil Ozfuttu, Fatih Cagatay Akyon, Kadir Sahin, Efe Buyukborekci, Devrim Cavusoglu, Sinan Altinuc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00830">DroBoost: An Intelligent Score and Model Boosting Method for Drone Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drone detection is a challenging object detection task where visibility conditions and quality of the images may be unfavorable, and detections might become difficult due to complex backgrounds, small visible objects, and hard to distinguish objects. Both provide high confidence for drone detections, and eliminating false detections requires efficient algorithms and approaches. Our previous work, which uses YOLOv5, uses both real and synthetic data and a Kalman-based tracker to track the detections and increase their confidence using temporal information. Our current work improves on the previous approach by combining several improvements. We used a more diverse dataset combining multiple sources and combined with synthetic samples chosen from a large synthetic dataset based on the error analysis of the base model. Also, to obtain more resilient confidence scores for objects, we introduced a classification component that discriminates whether the object is a drone or not. Finally, we developed a more advanced scoring algorithm for object tracking that we use to adjust localization confidence. Furthermore, the proposed technique won 1st Place in the Drone vs. Bird Challenge (Workshop on Small-Drone Surveillance, Detection and Counteraction Techniques at ICIAP 2021).
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2407.00738.pdf' target='_blank'>https://arxiv.org/pdf/2407.00738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir AdÅ¾emoviÄ, Predrag TadiÄ, Andrija PetroviÄ, Mladen NikoliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.00738">Engineering an Efficient Object Tracker for Non-Linear Motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of multi-object tracking is to detect and track all objects in a scene while maintaining unique identifiers for each, by associating their bounding boxes across video frames. This association relies on matching motion and appearance patterns of detected objects. This task is especially hard in case of scenarios involving dynamic and non-linear motion patterns. In this paper, we introduce DeepMoveSORT, a novel, carefully engineered multi-object tracker designed specifically for such scenarios. In addition to standard methods of appearance-based association, we improve motion-based association by employing deep learnable filters (instead of the most commonly used Kalman filter) and a rich set of newly proposed heuristics. Our improvements to motion-based association methods are severalfold. First, we propose a new transformer-based filter architecture, TransFilter, which uses an object's motion history for both motion prediction and noise filtering. We further enhance the filter's performance by careful handling of its motion history and accounting for camera motion. Second, we propose a set of heuristics that exploit cues from the position, shape, and confidence of detected bounding boxes to improve association performance. Our experimental evaluation demonstrates that DeepMoveSORT outperforms existing trackers in scenarios featuring non-linear motion, surpassing state-of-the-art results on three such datasets. We also perform a thorough ablation study to evaluate the contributions of different tracker components which we proposed. Based on our study, we conclude that using a learnable filter instead of the Kalman filter, along with appearance-based association is key to achieving strong general tracking performance.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2406.17183.pdf' target='_blank'>https://arxiv.org/pdf/2406.17183.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Srebrnjak Yang, Dheeraj Khanna, John S. Zelek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.17183">POPCat: Propagation of particles for complex annotation tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Novel dataset creation for all multi-object tracking, crowd-counting, and industrial-based videos is arduous and time-consuming when faced with a unique class that densely populates a video sequence. We propose a time efficient method called POPCat that exploits the multi-target and temporal features of video data to produce a semi-supervised pipeline for segmentation or box-based video annotation. The method retains the accuracy level associated with human level annotation while generating a large volume of semi-supervised annotations for greater generalization. The method capitalizes on temporal features through the use of a particle tracker to expand the domain of human-provided target points. This is done through the use of a particle tracker to reassociate the initial points to a set of images that follow the labeled frame. A YOLO model is then trained with this generated data, and then rapidly infers on the target video. Evaluations are conducted on GMOT-40, AnimalTrack, and Visdrone-2019 benchmarks. These multi-target video tracking/detection sets contain multiple similar-looking targets, camera movements, and other features that would commonly be seen in "wild" situations. We specifically choose these difficult datasets to demonstrate the efficacy of the pipeline and for comparison purposes. The method applied on GMOT-40, AnimalTrack, and Visdrone shows a margin of improvement on recall/mAP50/mAP over the best results by a value of 24.5%/9.6%/4.8%, -/43.1%/27.8%, and 7.5%/9.4%/7.5% where metrics were collected.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2405.04290.pdf' target='_blank'>https://arxiv.org/pdf/2405.04290.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Xia, Erik Stenborg, Junsheng Fu, Gustaf Hendeby
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04290">Bayesian Simultaneous Localization and Multi-Lane Tracking Using Onboard Sensors and a SD Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-definition map with accurate lane-level information is crucial for autonomous driving, but the creation of these maps is a resource-intensive process. To this end, we present a cost-effective solution to create lane-level roadmaps using only the global navigation satellite system (GNSS) and a camera on customer vehicles. Our proposed solution utilizes a prior standard-definition (SD) map, GNSS measurements, visual odometry, and lane marking edge detection points, to simultaneously estimate the vehicle's 6D pose, its position within a SD map, and also the 3D geometry of traffic lines. This is achieved using a Bayesian simultaneous localization and multi-object tracking filter, where the estimation of traffic lines is formulated as a multiple extended object tracking problem, solved using a trajectory Poisson multi-Bernoulli mixture (TPMBM) filter. In TPMBM filtering, traffic lines are modeled using B-spline trajectories, and each trajectory is parameterized by a sequence of control points. The proposed solution has been evaluated using experimental data collected by a test vehicle driving on highway. Preliminary results show that the traffic line estimates, overlaid on the satellite image, generally align with the lane markings up to some lateral offsets.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2404.18720.pdf' target='_blank'>https://arxiv.org/pdf/2404.18720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shimian Zhang, Qiuhong Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18720">Innovative Integration of Visual Foundation Model with a Robotic Arm on a Mobile Platform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly advancing field of robotics, the fusion of state-of-the-art visual technologies with mobile robotic arms has emerged as a critical integration. This paper introduces a novel system that combines the Segment Anything model (SAM) -- a transformer-based visual foundation model -- with a robotic arm on a mobile platform. The design of integrating a depth camera on the robotic arm's end-effector ensures continuous object tracking, significantly mitigating environmental uncertainties. By deploying on a mobile platform, our grasping system has an enhanced mobility, playing a key role in dynamic environments where adaptability are critical. This synthesis enables dynamic object segmentation, tracking, and grasping. It also elevates user interaction, allowing the robot to intuitively respond to various modalities such as clicks, drawings, or voice commands, beyond traditional robotic systems. Empirical assessments in both simulated and real-world demonstrate the system's capabilities. This configuration opens avenues for wide-ranging applications, from industrial settings, agriculture, and household tasks, to specialized assignments and beyond.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2403.16395.pdf' target='_blank'>https://arxiv.org/pdf/2403.16395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinglong Sun, Haijiang Sun, Shan Jiang, Jiacheng Wang, Xilai Wei, Zhonghe Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.16395">Multi-attention Associate Prediction Network for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Classification-regression prediction networks have realized impressive success in several modern deep trackers. However, there is an inherent difference between classification and regression tasks, so they have diverse even opposite demands for feature matching. Existed models always ignore the key issue and only employ a unified matching block in two task branches, decaying the decision quality. Besides, these models also struggle with decision misalignment situation. In this paper, we propose a multi-attention associate prediction network (MAPNet) to tackle the above problems. Concretely, two novel matchers, i.e., category-aware matcher and spatial-aware matcher, are first designed for feature comparison by integrating self, cross, channel or spatial attentions organically. They are capable of fully capturing the category-related semantics for classification and the local spatial contexts for regression, respectively. Then, we present a dual alignment module to enhance the correspondences between two branches, which is useful to find the optimal tracking solution. Finally, we describe a Siamese tracker built upon the proposed prediction network, which achieves the leading performance on five tracking benchmarks, consisting of LaSOT, TrackingNet, GOT-10k, TNL2k and UAV123, and surpasses other state-of-the-art approaches.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2403.15831.pdf' target='_blank'>https://arxiv.org/pdf/2403.15831.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoyu Sun, Chunyang Wang, Xuelian Liu, Chunhao Shi, Yueyang Ding, Guan Xi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15831">Spatio-Temporal Bi-directional Cross-frame Memory for Distractor Filtering Point Cloud Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D single object tracking within LIDAR point clouds is a pivotal task in computer vision, with profound implications for autonomous driving and robotics. However, existing methods, which depend solely on appearance matching via Siamese networks or utilize motion information from successive frames, encounter significant challenges. Issues such as similar objects nearby or occlusions can result in tracker drift. To mitigate these challenges, we design an innovative spatio-temporal bi-directional cross-frame distractor filtering tracker, named STMD-Tracker. Our first step involves the creation of a 4D multi-frame spatio-temporal graph convolution backbone. This design separates KNN graph spatial embedding and incorporates 1D temporal convolution, effectively capturing temporal fluctuations and spatio-temporal information. Subsequently, we devise a novel bi-directional cross-frame memory procedure. This integrates future and synthetic past frame memory to enhance the current memory, thereby improving the accuracy of iteration-based tracking. This iterative memory update mechanism allows our tracker to dynamically compensate for information in the current frame, effectively reducing tracker drift. Lastly, we construct spatially reliable Gaussian masks on the fused features to eliminate distractor points. This is further supplemented by an object-aware sampling strategy, which bolsters the efficiency and precision of object localization, thereby reducing tracking errors caused by distractors. Our extensive experiments on KITTI, NuScenes and Waymo datasets demonstrate that our approach significantly surpasses the current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2402.17976.pdf' target='_blank'>https://arxiv.org/pdf/2402.17976.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhewei Wu, Ruilong Yu, Qihe Liu, Shuying Cheng, Shilin Qiu, Shijie Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17976">Enhancing Tracking Robustness with Auxiliary Adversarial Defense Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial attacks in visual object tracking have significantly degraded the performance of advanced trackers by introducing imperceptible perturbations into images. However, there is still a lack of research on designing adversarial defense methods for object tracking. To address these issues, we propose an effective auxiliary pre-processing defense network, AADN, which performs defensive transformations on the input images before feeding them into the tracker. Moreover, it can be seamlessly integrated with other visual trackers as a plug-and-play module without parameter adjustments. We train AADN using adversarial training, specifically employing Dua-Loss to generate adversarial samples that simultaneously attack the classification and regression branches of the tracker. Extensive experiments conducted on the OTB100, LaSOT, and VOT2018 benchmarks demonstrate that AADN maintains excellent defense robustness against adversarial attack methods in both adaptive and non-adaptive attack scenarios. Moreover, when transferring the defense network to heterogeneous trackers, it exhibits reliable transferability. Finally, AADN achieves a processing time of up to 5ms/frame, allowing seamless integration with existing high-speed trackers without introducing significant computational overhead.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2402.17892.pdf' target='_blank'>https://arxiv.org/pdf/2402.17892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandro Papais, Robert Ren, Steven Waslander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17892">SWTrack: Multiple Hypothesis Sliding Window 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern robotic systems are required to operate in dense dynamic environments, requiring highly accurate real-time track identification and estimation. For 3D multi-object tracking, recent approaches process a single measurement frame recursively with greedy association and are prone to errors in ambiguous association decisions. Our method, Sliding Window Tracker (SWTrack), yields more accurate association and state estimation by batch processing many frames of sensor data while being capable of running online in real-time. The most probable track associations are identified by evaluating all possible track hypotheses across the temporal sliding window. A novel graph optimization approach is formulated to solve the multidimensional assignment problem with lifted graph edges introduced to account for missed detections and graph sparsity enforced to retain real-time efficiency. We evaluate our SWTrack implementation$^{2}$ on the NuScenes autonomous driving dataset to demonstrate improved tracking performance.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2402.16246.pdf' target='_blank'>https://arxiv.org/pdf/2402.16246.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Zhu, Yanqiang Wang, Yadong An, Hong Yang, Yiming Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16246">Real-Time Vehicle Detection and Urban Traffic Behavior Analysis Based on UAV Traffic Videos on Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on a real-time vehicle detection and urban traffic behavior analysis system based on Unmanned Aerial Vehicle (UAV) traffic video. By using UAV to collect traffic data and combining the YOLOv8 model and SORT tracking algorithm, the object detection and tracking functions are implemented on the iOS mobile platform. For the problem of traffic data acquisition and analysis, the dynamic computing method is used to process the performance in real time and calculate the micro and macro traffic parameters of the vehicles, and real-time traffic behavior analysis is conducted and visualized. The experiment results reveals that the vehicle object detection can reach 98.27% precision rate and 87.93% recall rate, and the real-time processing capacity is stable at 30 frames per seconds. This work integrates drone technology, iOS development, and deep learning techniques to integrate traffic video acquisition, object detection, object tracking, and traffic behavior analysis functions on mobile devices. It provides new possibilities for lightweight traffic information collection and data analysis, and offers innovative solutions to improve the efficiency of analyzing road traffic conditions and addressing transportation issues for transportation authorities.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2402.09865.pdf' target='_blank'>https://arxiv.org/pdf/2402.09865.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir AdÅ¾emoviÄ, Predrag TadiÄ, Andrija PetroviÄ, Mladen NikoliÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09865">Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2312.17641.pdf' target='_blank'>https://arxiv.org/pdf/2312.17641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Feng, Liao Pan, Wu Di, Liu Bo, Zhang Xingle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17641">Motion State: A New Benchmark Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of video analysis, the field of multiple object tracking (MOT) assumes paramount importance, with the motion state of objects-whether static or dynamic relative to the ground-holding practical significance across diverse scenarios. However, the extant literature exhibits a notable dearth in the exploration of this aspect. Deep learning methodologies encounter challenges in accurately discerning object motion states, while conventional approaches reliant on comprehensive mathematical modeling may yield suboptimal tracking accuracy. To address these challenges, we introduce a Model-Data-Driven Motion State Judgment Object Tracking Method (MoD2T). This innovative architecture adeptly amalgamates traditional mathematical modeling with deep learning-based multi-object tracking frameworks. The integration of mathematical modeling and deep learning within MoD2T enhances the precision of object motion state determination, thereby elevating tracking accuracy. Our empirical investigations comprehensively validate the efficacy of MoD2T across varied scenarios, encompassing unmanned aerial vehicle surveillance and street-level tracking. Furthermore, to gauge the method's adeptness in discerning object motion states, we introduce the Motion State Validation F1 (MVF1) metric. This novel performance metric aims to quantitatively assess the accuracy of motion state classification, furnishing a comprehensive evaluation of MoD2T's performance. Elaborate experimental validations corroborate the rationality of MVF1. In order to holistically appraise MoD2T's performance, we meticulously annotate several renowned datasets and subject MoD2T to stringent testing. Remarkably, under conditions characterized by minimal or moderate camera motion, the achieved MVF1 values are particularly noteworthy, with exemplars including 0.774 for the KITTI dataset, 0.521 for MOT17, and 0.827 for UAVDT.
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2312.08650.pdf' target='_blank'>https://arxiv.org/pdf/2312.08650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kawisorn Kamtue, Jose M. F. Moura, Orathai Sangpetch, Paulo Garcia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08650">PhyOT: Physics-informed object tracking in surveillance cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep learning has been very successful in computer vision, real world operating conditions such as lighting variation, background clutter, or occlusion hinder its accuracy across several tasks. Prior work has shown that hybrid models -- combining neural networks and heuristics/algorithms -- can outperform vanilla deep learning for several computer vision tasks, such as classification or tracking. We consider the case of object tracking, and evaluate a hybrid model (PhyOT) that conceptualizes deep neural networks as ``sensors'' in a Kalman filter setup, where prior knowledge, in the form of Newtonian laws of motion, is used to fuse sensor observations and to perform improved estimations. Our experiments combine three neural networks, performing position, indirect velocity and acceleration estimation, respectively, and evaluate such a formulation on two benchmark datasets: a warehouse security camera dataset that we collected and annotated and a traffic camera open dataset. Results suggest that our PhyOT can track objects in extreme conditions that the state-of-the-art deep neural networks fail while its performance in general cases does not degrade significantly from that of existing deep learning approaches. Results also suggest that our PhyOT components are generalizable and transferable.
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2311.07616.pdf' target='_blank'>https://arxiv.org/pdf/2311.07616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaer Huang, Weitu Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07616">ReIDTracker Sea: the technical report of BoaTrack and SeaDronesSee-MOT challenge at MaCVi of WACV24</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking is one of the most important technologies in maritime computer vision. Our solution tries to explore Multi-Object Tracking in maritime Unmanned Aerial vehicles (UAVs) and Unmanned Surface Vehicles (USVs) usage scenarios. Most of the current Multi-Object Tracking algorithms require complex association strategies and association information (2D location and motion, 3D motion, 3D depth, 2D appearance) to achieve better performance, which makes the entire tracking system extremely complex and heavy. At the same time, most of the current Multi-Object Tracking algorithms still require video annotation data which is costly to obtain for training. Our solution tries to explore Multi-Object Tracking in a completely unsupervised way. The scheme accomplishes instance representation learning by using self-supervision on ImageNet. Then, by cooperating with high-quality detectors, the multi-target tracking task can be completed simply and efficiently. The scheme achieved top 3 performance on both UAV-based Multi-Object Tracking with Reidentification and USV-based Multi-Object Tracking benchmarks and the solution won the championship in many multiple Multi-Object Tracking competitions. such as BDD100K MOT,MOTS, Waymo 2D MOT
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2310.12393.pdf' target='_blank'>https://arxiv.org/pdf/2310.12393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Xu, Chang-Tsun Li, Yongjian Hu, Chee Peng Lim, Douglas Creighton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12393">Deep Learning Techniques for Video Instance Segmentation: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video instance segmentation, also known as multi-object tracking and segmentation, is an emerging computer vision research area introduced in 2019, aiming at detecting, segmenting, and tracking instances in videos simultaneously. By tackling the video instance segmentation tasks through effective analysis and utilization of visual information in videos, a range of computer vision-enabled applications (e.g., human action recognition, medical image processing, autonomous vehicle navigation, surveillance, etc) can be implemented. As deep-learning techniques take a dominant role in various computer vision areas, a plethora of deep-learning-based video instance segmentation schemes have been proposed. This survey offers a multifaceted view of deep-learning schemes for video instance segmentation, covering various architectural paradigms, along with comparisons of functional performance, model complexity, and computational overheads. In addition to the common architectural designs, auxiliary techniques for improving the performance of deep-learning models for video instance segmentation are compiled and discussed. Finally, we discuss a range of major challenges and directions for further investigations to help advance this promising research field.
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2310.09747.pdf' target='_blank'>https://arxiv.org/pdf/2310.09747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dianbo Ma, Jianqiang Xiao, Ziyan Gao, Satoshi Yamane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09747">Staged Depthwise Correlation and Feature Fusion for Siamese Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a novel staged depthwise correlation and feature fusion network, named DCFFNet, to further optimize the feature extraction for visual tracking. We build our deep tracker upon a siamese network architecture, which is offline trained from scratch on multiple large-scale datasets in an end-to-end manner. The model contains a core component, that is, depthwise correlation and feature fusion module (correlation-fusion module), which facilitates model to learn a set of optimal weights for a specific object by utilizing ensembles of multi-level features from lower and higher layers and multi-channel semantics on the same layer. We combine the modified ResNet-50 with the proposed correlation-fusion layer to constitute the feature extractor of our model. In training process, we find the training of model become more stable, that benifits from the correlation-fusion module. For comprehensive evaluations of performance, we implement our tracker on the popular benchmarks, including OTB100, VOT2018 and LaSOT. Extensive experiment results demonstrate that our proposed method achieves favorably competitive performance against many leading trackers in terms of accuracy and precision, while satisfying the real-time requirements of applications.
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2310.02532.pdf' target='_blank'>https://arxiv.org/pdf/2310.02532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tara Sadjadpour, Rares Ambrus, Jeannette Bohg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02532">ShaSTA-Fuse: Camera-LiDAR Sensor Fusion to Model Shape and Spatio-Temporal Affinities for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking (MOT) is essential for an autonomous mobile agent to safely navigate a scene. In order to maximize the perception capabilities of the autonomous agent, we aim to develop a 3D MOT framework that fuses camera and LiDAR sensor information. Building on our prior LiDAR-only work, ShaSTA, which models shape and spatio-temporal affinities for 3D MOT, we propose a novel camera-LiDAR fusion approach for learning affinities. At its core, this work proposes a fusion technique that generates a rich sensory signal incorporating information about depth and distant objects to enhance affinity estimation for improved data association, track lifecycle management, false-positive elimination, false-negative propagation, and track confidence score refinement. Our main contributions include a novel fusion approach for combining camera and LiDAR sensory signals to learn affinities, and a first-of-its-kind multimodal sequential track confidence refinement technique that fuses 2D and 3D detections. Additionally, we perform an ablative analysis on each fusion step to demonstrate the added benefits of incorporating the camera sensor, particular for small, distant objects that tend to suffer from the depth-sensing limits and sparsity of LiDAR sensors. In sum, our technique achieves state-of-the-art performance on the nuScenes benchmark amongst multimodal 3D MOT algorithms using CenterPoint detections.
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2309.16987.pdf' target='_blank'>https://arxiv.org/pdf/2309.16987.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Wang, Zhu Wang, Can Li, Xiaojuan Qi, Hayden Kwok-Hay So
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16987">SpikeMOT: Event-based Multi-Object Tracking with Sparse Motion Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In comparison to conventional RGB cameras, the superior temporal resolution of event cameras allows them to capture rich information between frames, making them prime candidates for object tracking. Yet in practice, despite their theoretical advantages, the body of work on event-based multi-object tracking (MOT) remains in its infancy, especially in real-world settings where events from complex background and camera motion can easily obscure the true target motion. In this work, an event-based multi-object tracker, called SpikeMOT, is presented to address these challenges. SpikeMOT leverages spiking neural networks to extract sparse spatiotemporal features from event streams associated with objects. The resulting spike train representations are used to track the object movement at high frequency, while a simultaneous object detector provides updated spatial information of these objects at an equivalent frame rate. To evaluate the effectiveness of SpikeMOT, we introduce DSEC-MOT, the first large-scale event-based MOT benchmark incorporating fine-grained annotations for objects experiencing severe occlusions, frequent trajectory intersections, and long-term re-identification in real-world contexts. Extensive experiments employing DSEC-MOT and another event-based dataset, named FE240hz, demonstrate SpikeMOT's capability to achieve high tracking accuracy amidst challenging real-world scenarios, advancing the state-of-the-art in event-based multi-object tracking.
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2309.06819.pdf' target='_blank'>https://arxiv.org/pdf/2309.06819.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>LoÃ¯c J. Azzalini, Dario Izzo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06819">Tracking Particles Ejected From Active Asteroid Bennu With Event-Based Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Early detection and tracking of ejecta in the vicinity of small solar system bodies is crucial to guarantee spacecraft safety and support scientific observation. During the visit of active asteroid Bennu, the OSIRIS-REx spacecraft relied on the analysis of images captured by onboard navigation cameras to detect particle ejection events, which ultimately became one of the mission's scientific highlights. To increase the scientific return of similar time-constrained missions, this work proposes an event-based solution that is dedicated to the detection and tracking of centimetre-sized particles. Unlike a standard frame-based camera, the pixels of an event-based camera independently trigger events indicating whether the scene brightness has increased or decreased at that time and location in the sensor plane. As a result of the sparse and asynchronous spatiotemporal output, event cameras combine very high dynamic range and temporal resolution with low-power consumption, which could complement existing onboard imaging techniques. This paper motivates the use of a scientific event camera by reconstructing the particle ejection episodes reported by the OSIRIS-REx mission in a photorealistic scene generator and in turn, simulating event-based observations. The resulting streams of spatiotemporal data support future work on event-based multi-object tracking.
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2309.00942.pdf' target='_blank'>https://arxiv.org/pdf/2309.00942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sha Meng, Dian Shao, Jiacheng Guo, Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00942">Tracking without Label: Unsupervised Multiple Object Tracking via Contrastive Similarity Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised learning is a challenging task due to the lack of labels. Multiple Object Tracking (MOT), which inevitably suffers from mutual object interference, occlusion, etc., is even more difficult without label supervision. In this paper, we explore the latent consistency of sample features across video frames and propose an Unsupervised Contrastive Similarity Learning method, named UCSL, including three contrast modules: self-contrast, cross-contrast, and ambiguity contrast. Specifically, i) self-contrast uses intra-frame direct and inter-frame indirect contrast to obtain discriminative representations by maximizing self-similarity. ii) Cross-contrast aligns cross- and continuous-frame matching results, mitigating the persistent negative effect caused by object occlusion. And iii) ambiguity contrast matches ambiguous objects with each other to further increase the certainty of subsequent object association through an implicit manner. On existing benchmarks, our method outperforms the existing unsupervised methods using only limited help from ReID head, and even provides higher accuracy than lots of fully supervised methods.
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2308.11870.pdf' target='_blank'>https://arxiv.org/pdf/2308.11870.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shixing Huang, Zhihao Wang, Junyuan Ouyang, Haoyao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11870">Multi-object Detection, Tracking and Prediction in Rugged Dynamic Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) has important applications in monitoring, logistics, and other fields. This paper develops a real-time multi-object tracking and prediction system in rugged environments. A 3D object detection algorithm based on Lidar-camera fusion is designed to detect the target objects. Based on the Hungarian algorithm, this paper designs a 3D multi-object tracking algorithm with an adaptive threshold to realize the stable matching and tracking of the objects. We combine Memory Augmented Neural Networks (MANN) and Kalman filter to achieve 3D trajectory prediction on rugged terrains. Besides, we realize a new dynamic SLAM by using the results of multi-object tracking to remove dynamic points for better SLAM performance and static map. To verify the effectiveness of the proposed multi-object tracking and prediction system, several simulations and physical experiments are conducted. The results show that the proposed system can track dynamic objects and provide future trajectory and a more clean static map in real-time.
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2308.05846.pdf' target='_blank'>https://arxiv.org/pdf/2308.05846.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Venkat Margapuri, Prapti Thapaliya, Mitchell Neilsen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.05846">Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping, is the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of parameters that form more complex traits. One of the key aspects of seed phenotyping is cereal yield estimation that the seed production industry relies upon to conduct their business. While mechanized seed kernel counters are available in the market currently, they are often priced high and sometimes outside the range of small scale seed production firms' affordability. The development of object tracking neural network models such as You Only Look Once (YOLO) enables computer scientists to design algorithms that can estimate cereal yield inexpensively. The key bottleneck with neural network models is that they require a plethora of labelled training data before they can be put to task. We demonstrate that the use of synthetic imagery serves as a feasible substitute to train neural networks for object tracking that includes the tasks of object classification and detection. Furthermore, we propose a seed kernel counter that uses a low-cost mechanical hopper, trained YOLOv8 neural network model, and object tracking algorithms on StrongSORT and ByteTrack to estimate cereal yield from videos. The experiment yields a seed kernel count with an accuracy of 95.2\% and 93.2\% for Soy and Wheat respectively using the StrongSORT algorithm, and an accuray of 96.8\% and 92.4\% for Soy and Wheat respectively using the ByteTrack algorithm.
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2308.04872.pdf' target='_blank'>https://arxiv.org/pdf/2308.04872.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Young-Ching Chou, Shen-Ru Zhang, Bo-Wei Chen, Hong-Qi Chen, Cheng-Kuan Lin, Yu-Chee Tseng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04872">Tracking Players in a Badminton Court by Two Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a simple method for multi-object tracking (MOT) of players in a badminton court. We leverage two off-the-shelf cameras, one on the top of the court and the other on the side of the court. The one on the top is to track players' trajectories, while the one on the side is to analyze the pixel features of players. By computing the correlations between adjacent frames and engaging the information of the two cameras, MOT of badminton players is obtained. This two-camera approach addresses the challenge of player occlusion and overlapping in a badminton court, providing player trajectory tracking and multi-angle analysis. The presented system offers insights into the positions and movements of badminton players, thus serving as a coaching or self-training tool for badminton players to improve their gaming strategies.
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2308.01256.pdf' target='_blank'>https://arxiv.org/pdf/2308.01256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincenzo Mariano Scarrica, Antonino Staiano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01256">Learning Spatial Distribution of Long-Term Trackers Scores</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Long-Term tracking is a hot topic in Computer Vision. In this context, competitive models are presented every year, showing a constant growth rate in performances, mainly measured in standardized protocols as Visual Object Tracking (VOT) and Object Tracking Benchmark (OTB). Fusion-trackers strategy has been applied over last few years for overcoming the known re-detection problem, turning out to be an important breakthrough. Following this approach, this work aims to generalize the fusion concept to an arbitrary number of trackers used as baseline trackers in the pipeline, leveraging a learning phase to better understand how outcomes correlate with each other, even when no target is present. A model and data independence conjecture will be evidenced in the manuscript, yielding a recall of 0.738 on LTB-50 dataset when learning from VOT-LT2022, and 0.619 by reversing the two datasets. In both cases, results are strongly competitive with state-of-the-art and recall turns out to be the first on the podium.
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2308.01248.pdf' target='_blank'>https://arxiv.org/pdf/2308.01248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincenzo Mariano Scarrica, Ciro Panariello, Alessio Ferone, Antonino Staiano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.01248">A Hybrid Approach To Real-Time Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking, also known as Multi-Target Tracking, is a significant area of computer vision that has many uses in a variety of settings. The development of deep learning, which has encouraged researchers to propose more and more work in this direction, has significantly impacted the scientific advancement around the study of tracking as well as many other domains related to computer vision. In fact, all of the solutions that are currently state-of-the-art in the literature and in the tracking industry, are built on top of deep learning methodologies that produce exceptionally good results. Deep learning is enabled thanks to the ever more powerful technology researchers can use to handle the significant computational resources demanded by these models. However, when real-time is a main requirement, developing a tracking system without being constrained by expensive hardware support with enormous computational resources is necessary to widen tracking applications in real-world contexts. To this end, a compromise is to combine powerful deep strategies with more traditional approaches to favor considerably lower processing solutions at the cost of less accurate tracking results even though suitable for real-time domains. Indeed, the present work goes in that direction, proposing a hybrid strategy for real-time multi-target tracking that combines effectively a classical optical flow algorithm with a deep learning architecture, targeted to a human-crowd tracking system exhibiting a desirable trade-off between performance in tracking precision and computational costs. The developed architecture was experimented with different settings, and yielded a MOTA of 0.608 out of the compared state-of-the-art 0.549 results, and about half the running time when introducing the optical flow phase, achieving almost the same performance in terms of accuracy.
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2308.00596.pdf' target='_blank'>https://arxiv.org/pdf/2308.00596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Marcelo Eduardo Pederiva, JosÃ© Mario De Martino, Alessandro Zimmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00596">MonoNext: A 3D Monocular Object Detection with ConvNext</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autonomous driving perception tasks rely heavily on cameras as the primary sensor for Object Detection, Semantic Segmentation, Instance Segmentation, and Object Tracking. However, RGB images captured by cameras lack depth information, which poses a significant challenge in 3D detection tasks. To supplement this missing data, mapping sensors such as LIDAR and RADAR are used for accurate 3D Object Detection. Despite their significant accuracy, the multi-sensor models are expensive and require a high computational demand. In contrast, Monocular 3D Object Detection models are becoming increasingly popular, offering a faster, cheaper, and easier-to-implement solution for 3D detections. This paper introduces a different Multi-Tasking Learning approach called MonoNext that utilizes a spatial grid to map objects in the scene. MonoNext employs a straightforward approach based on the ConvNext network and requires only 3D bounding box annotated data. In our experiments with the KITTI dataset, MonoNext achieved high precision and competitive performance comparable with state-of-the-art approaches. Furthermore, by adding more training data, MonoNext surpassed itself and achieved higher accuracies.
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2307.15671.pdf' target='_blank'>https://arxiv.org/pdf/2307.15671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantin RÃ¶hrl, Dominik Bauer, Timothy Patten, Markus Vincze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15671">TrackAgent: 6D Object Tracking via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking an object's 6D pose, while either the object itself or the observing camera is moving, is important for many robotics and augmented reality applications. While exploiting temporal priors eases this problem, object-specific knowledge is required to recover when tracking is lost. Under the tight time constraints of the tracking task, RGB(D)-based methods are often conceptionally complex or rely on heuristic motion models. In comparison, we propose to simplify object tracking to a reinforced point cloud (depth only) alignment task. This allows us to train a streamlined approach from scratch with limited amounts of sparse 3D point clouds, compared to the large datasets of diverse RGBD sequences required in previous works. We incorporate temporal frame-to-frame registration with object-based recovery by frame-to-model refinement using a reinforcement learning (RL) agent that jointly solves for both objectives. We also show that the RL agent's uncertainty and a rendering-based mask propagation are effective reinitialization triggers.
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2307.14294.pdf' target='_blank'>https://arxiv.org/pdf/2307.14294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Diego Botache, Kristina Dingel, Rico Huhnstock, Arno Ehresmann, Bernhard Sick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14294">Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection. However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses. This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies. We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2306.15135.pdf' target='_blank'>https://arxiv.org/pdf/2306.15135.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony Trezza, Donald J. Bucci, Pramod K. Varshney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15135">On Gibbs Sampling Architecture for Labeled Random Finite Sets Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gibbs sampling is one of the most popular Markov chain Monte Carlo algorithms because of its simplicity, scalability, and wide applicability within many fields of statistics, science, and engineering. In the labeled random finite sets literature, Gibbs sampling procedures have recently been applied to efficiently truncate the single-sensor and multi-sensor $Î´$-generalized labeled multi-Bernoulli posterior density as well as the multi-sensor adaptive labeled multi-Bernoulli birth distribution. However, only a limited discussion has been provided regarding key Gibbs sampler architecture details including the Markov chain Monte Carlo sample generation technique and early termination criteria. This paper begins with a brief background on Markov chain Monte Carlo methods and a review of the Gibbs sampler implementations proposed for labeled random finite sets filters. Next, we propose a short chain, multi-simulation sample generation technique that is well suited for these applications and enables a parallel processing implementation. Additionally, we present two heuristic early termination criteria that achieve similar sampling performance with substantially fewer Markov chain observations. Finally, the benefits of the proposed Gibbs samplers are demonstrated via two Monte Carlo simulations.
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2306.15010.pdf' target='_blank'>https://arxiv.org/pdf/2306.15010.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ankit Gupta, Ida-Maria Sintorn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15010">Efficient High-Resolution Template Matching with Vector Quantized Nearest Neighbour Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Template matching is a fundamental problem in computer vision with applications in fields including object detection, image registration, and object tracking. Current methods rely on nearest-neighbour (NN) matching, where the query feature space is converted to NN space by representing each query pixel with its NN in the template. NN-based methods have been shown to perform better in occlusions, appearance changes, and non-rigid transformations; however, they scale poorly with high-resolution data and high feature dimensions. We present an NN-based method which efficiently reduces the NN computations and introduces filtering in the NN fields (NNFs). A vector quantization step is introduced before the NN calculation to represent the template with $k$ features, and the filter response over the NNFs is used to compare the template and query distributions over the features. We show that state-of-the-art performance is achieved in low-resolution data, and our method outperforms previous methods at higher resolution.
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2305.16968.pdf' target='_blank'>https://arxiv.org/pdf/2305.16968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philippe Bernet, Joseph Chazalon, Edwin Carlinet, Alexandre Bourquelot, Elodie Puybareau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16968">Linear Object Detection in Document Images using Multiple Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Linear objects convey substantial information about document structure, but are challenging to detect accurately because of degradation (curved, erased) or decoration (doubled, dashed). Many approaches can recover some vector representation, but only one closed-source technique introduced in 1994, based on Kalman filters (a particular case of Multiple Object Tracking algorithm), can perform a pixel-accurate instance segmentation of linear objects and enable to selectively remove them from the original image. We aim at re-popularizing this approach and propose: 1. a framework for accurate instance segmentation of linear objects in document images using Multiple Object Tracking (MOT); 2. document image datasets and metrics which enable both vector- and pixel-based evaluation of linear object detection; 3. performance measures of MOT approaches against modern segment detectors; 4. performance measures of various tracking strategies, exhibiting alternatives to the original Kalman filters approach; and 5. an open-source implementation of a detector which can discriminate instances of curved, erased, dashed, intersecting and/or overlapping linear objects.
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2303.14653.pdf' target='_blank'>https://arxiv.org/pdf/2303.14653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingda Guan, Zhengyang Feng, Huiying Chang, Kuo Du, Tingting Li, Min Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14653">SDTracker: Synthetic Data Based Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present SDTracker, a method that harnesses the potential of synthetic data for multi-object tracking of real-world scenes in a domain generalization and semi-supervised fashion. First, we use the ImageNet dataset as an auxiliary to randomize the style of synthetic data. With out-of-domain data, we further enforce pyramid consistency loss across different "stylized" images from the same sample to learn domain invariant features. Second, we adopt the pseudo-labeling method to effectively utilize the unlabeled MOT17 training data. To obtain high-quality pseudo-labels, we apply proximal policy optimization (PPO2) algorithm to search confidence thresholds for each sequence. When using the unlabeled MOT17 training set, combined with the pure-motion tracking strategy upgraded via developed post-processing, we finally reach 61.4 HOTA.
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2302.11867.pdf' target='_blank'>https://arxiv.org/pdf/2302.11867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janani Thangavel, Thanikasalam Kokul, Amirthalingam Ramanan, Subha Fernando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11867">Transformers in Single Object Tracking: An Experimental Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-object tracking is a well-known and challenging research topic in computer vision. Over the last two decades, numerous researchers have proposed various algorithms to solve this problem and achieved promising results. Recently, Transformer-based tracking approaches have ushered in a new era in single-object tracking by introducing new perspectives and achieving superior tracking robustness. In this paper, we conduct an in-depth literature analysis of Transformer tracking approaches by categorizing them into CNN-Transformer based trackers, Two-stream Two-stage fully-Transformer based trackers, and One-stream One-stage fully-Transformer based trackers. In addition, we conduct experimental evaluations to assess their tracking robustness and computational efficiency using publicly available benchmark datasets. Furthermore, we measure their performances on different tracking scenarios to identify their strengths and weaknesses in particular situations. Our survey provides insights into the underlying principles of Transformer tracking approaches, the challenges they encounter, and the future directions they may take.
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2301.10057.pdf' target='_blank'>https://arxiv.org/pdf/2301.10057.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Serych, Jiri Matas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10057">Planar Object Tracking via Weighted Optical Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose WOFT -- a novel method for planar object tracking that estimates a full 8 degrees-of-freedom pose, i.e. the homography w.r.t. a reference view. The method uses a novel module that leverages dense optical flow and assigns a weight to each optical flow correspondence, estimating a homography by weighted least squares in a fully differentiable manner. The trained module assigns zero weights to incorrect correspondences (outliers) in most cases, making the method robust and eliminating the need of the typically used non-differentiable robust estimators like RANSAC. The proposed weighted optical flow tracker (WOFT) achieves state-of-the-art performance on two benchmarks, POT-210 and POIC, tracking consistently well across a wide range of scenarios.
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2301.01917.pdf' target='_blank'>https://arxiv.org/pdf/2301.01917.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwei Sun, Zexi Hua, Hengcao Li, Haiyan Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.01917">Flying Bird Object Detection Algorithm in Surveillance Video Based on Motion Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A Flying Bird Object Detection algorithm Based on Motion Information (FBOD-BMI) is proposed to solve the problem that the features of the object are not obvious in a single frame, and the size of the object is small (low Signal-to-Noise Ratio (SNR)) in surveillance video. Firstly, a ConvLSTM-PAN model structure is designed to capture suspicious flying bird objects, in which the Convolutional Long and Short Time Memory (ConvLSTM) network aggregated the Spatio-temporal features of the flying bird object on adjacent multi-frame before the input of the model and the Path Aggregation Network (PAN) located the suspicious flying bird objects. Then, an object tracking algorithm is used to track suspicious flying bird objects and calculate their Motion Range (MR). At the same time, the size of the MR of the suspicious flying bird object is adjusted adaptively according to its speed of movement (specifically, if the bird moves slowly, its MR will be expanded according to the speed of the bird to ensure the environmental information needed to detect the flying bird object). Adaptive Spatio-temporal Cubes (ASt-Cubes) of the flying bird objects are generated to ensure that the SNR of the flying bird objects is improved, and the necessary environmental information is retained adaptively. Finally, a LightWeight U-Shape Net (LW-USN) based on ASt-Cubes is designed to detect flying bird objects, which rejects the false detections of the suspicious flying bird objects and returns the position of the real flying bird objects. The monitoring video including the flying birds is collected in the unattended traction substation as the experimental dataset to verify the performance of the algorithm. The experimental results show that the flying bird object detection method based on motion information proposed in this paper can effectively detect the flying bird object in surveillance video.
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2301.00190.pdf' target='_blank'>https://arxiv.org/pdf/2301.00190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abubakar Siddique, Henry Medeiros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.00190">Tracking Passengers and Baggage Items using Multiple Overhead Cameras at Security Checkpoints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a novel framework to track multiple objects in overhead camera videos for airport checkpoint security scenarios where targets correspond to passengers and their baggage items. We propose a Self-Supervised Learning (SSL) technique to provide the model information about instance segmentation uncertainty from overhead images. Our SSL approach improves object detection by employing a test-time data augmentation and a regression-based, rotation-invariant pseudo-label refinement technique. Our pseudo-label generation method provides multiple geometrically-transformed images as inputs to a Convolutional Neural Network (CNN), regresses the augmented detections generated by the network to reduce localization errors, and then clusters them using the mean-shift algorithm. The self-supervised detector model is used in a single-camera tracking algorithm to generate temporal identifiers for the targets. Our method also incorporates a multi-view trajectory association mechanism to maintain consistent temporal identifiers as passengers travel across camera views. An evaluation of detection, tracking, and association performances on videos obtained from multiple overhead cameras in a realistic airport checkpoint environment demonstrates the effectiveness of the proposed approach. Our results show that self-supervision improves object detection accuracy by up to $42\%$ without increasing the inference time of the model. Our multi-camera association method achieves up to $89\%$ multi-object tracking accuracy with an average computation time of less than $15$ ms.
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2211.03919.pdf' target='_blank'>https://arxiv.org/pdf/2211.03919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tara Sadjadpour, Jie Li, Rares Ambrus, Jeannette Bohg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.03919">ShaSTA: Modeling Shape and Spatio-Temporal Affinities for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking is a cornerstone capability of any robotic system. The quality of tracking is largely dependent on the quality of the detector used. In many applications, such as autonomous vehicles, it is preferable to over-detect objects to avoid catastrophic outcomes due to missed detections. As a result, current state-of-the-art 3D detectors produce high rates of false-positives to ensure a low number of false-negatives. This can negatively affect tracking by making data association and track lifecycle management more challenging. Additionally, occasional false-negative detections due to difficult scenarios like occlusions can harm tracking performance. To address these issues in a unified framework, we propose to learn shape and spatio-temporal affinities between tracks and detections in consecutive frames. Our affinity provides a probabilistic matching that leads to robust data association, track lifecycle management, false-positive elimination, false-negative propagation, and sequential track confidence refinement. Though past 3D MOT approaches address a subset of components in this problem domain, we offer the first self-contained framework that addresses all these aspects of the 3D MOT problem. We quantitatively evaluate our method on the nuScenes tracking benchmark where we achieve 1st place amongst LiDAR-only trackers using CenterPoint detections. Our method estimates accurate and precise tracks, while decreasing the overall number of false-positive and false-negative tracks and increasing the number of true-positive tracks. We analyze our performance with 5 metrics, giving a comprehensive overview of our approach to indicate how our tracking framework may impact the ultimate goal of an autonomous mobile agent. We also present ablative experiments and qualitative results that demonstrate our framework's capabilities in complex scenarios.
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2207.11356.pdf' target='_blank'>https://arxiv.org/pdf/2207.11356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keith A. LeGrand, Silvia Ferrari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.11356">Split Happens! Imprecise and Negative Information in Gaussian Mixture Random Finite Set Filtering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In object tracking and state estimation problems, ambiguous evidence such as imprecise measurements and the absence of detections can contain valuable information and thus be leveraged to further refine the probabilistic belief state. In particular, knowledge of a sensor's bounded field-of-view can be exploited to incorporate evidence of where an object was not observed. This paper presents a systematic approach for incorporating knowledge of the field-of-view geometry and position and object inclusion/exclusion evidence into object state densities and random finite set multi-object cardinality distributions. The resulting state estimation problem is nonlinear and solved using a new Gaussian mixture approximation based on recursive component splitting. Based on this approximation, a novel Gaussian mixture Bernoulli filter for imprecise measurements is derived and demonstrated in a tracking problem using only natural language statements as inputs. This paper also considers the relationship between bounded fields-of-view and cardinality distributions for a representative selection of multi-object distributions, which can be used for sensor planning, as is demonstrated through a problem involving a multi-Bernoulli process with up to one-hundred potential objects.
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2206.01942.pdf' target='_blank'>https://arxiv.org/pdf/2206.01942.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Endai Huang, Axiu Mao, Junhui Hou, Yongjian Wu, Weitao Xu, Maria Camila Ceballos, Thomas D. Parsons, Kai Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.01942">Occlusion-Resistant Instance Segmentation of Piglets in Farrowing Pens Using Center Clustering Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer vision enables the development of new approaches to monitor the behavior, health, and welfare of animals. Instance segmentation is a high-precision method in computer vision for detecting individual animals of interest. This method can be used for in-depth analysis of animals, such as examining their subtle interactive behaviors, from videos and images. However, existing deep-learning-based instance segmentation methods have been mostly developed based on public datasets, which largely omit heavy occlusion problems; therefore, these methods have limitations in real-world applications involving object occlusions, such as farrowing pen systems used on pig farms in which the farrowing crates often impede the sow and piglets. In this paper, we adapt a Center Clustering Network originally designed for counting to achieve instance segmentation, dubbed as CClusnet-Inseg. Specifically, CClusnet-Inseg uses each pixel to predict object centers and trace these centers to form masks based on clustering results, which consists of a network for segmentation and center offset vector map, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, Centers-to-Mask (C2M), and Remain-Centers-to-Mask (RC2M) algorithms. In all, 4,600 images were extracted from six videos collected from three closed and three half-open farrowing crates to train and validate our method. CClusnet-Inseg achieves a mean average precision (mAP) of 84.1 and outperforms all other methods compared in this study. We conduct comprehensive ablation studies to demonstrate the advantages and effectiveness of core modules of our method. In addition, we apply CClusnet-Inseg to multi-object tracking for animal monitoring, and the predicted object center that is a conjunct output could serve as an occlusion-resistant representation of the location of an object.
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2205.15477.pdf' target='_blank'>https://arxiv.org/pdf/2205.15477.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ala-Eddine Benrazek, Zineddine Kouahla, Brahim Farou, Hamid Seridi, Imane Allele
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.15477">Introduction of a tree-based technique for efficient and real-time label retrieval in the object tracking system</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the issue of the real-time tracking quality of moving objects in large-scale video surveillance systems. During the tracking process, the system assigns an identifier or label to each tracked object to distinguish it from other objects. In such a mission, it is essential to keep this identifier for the same objects, whatever the area, the time of their appearance, or the detecting camera. This is to conserve as much information about the tracking object as possible, decrease the number of ID switching (ID-Sw), and increase the quality of object tracking. To accomplish object labeling, a massive amount of data collected by the cameras must be searched to retrieve the most similar (nearest neighbor) object identifier. Although this task is simple, it becomes very complex in large-scale video surveillance networks, where the data becomes very large. In this case, the label retrieval time increases significantly with this increase, which negatively affects the performance of the real-time tracking system. To avoid such problems, we propose a new solution to automatically label multiple objects for efficient real-time tracking using the indexing mechanism. This mechanism organizes the metadata of the objects extracted during the detection and tracking phase in an Adaptive BCCF-tree. The main advantage of this structure is: its ability to index massive metadata generated by multi-cameras, its logarithmic search complexity, which implicitly reduces the search response time, and its quality of research results, which ensure coherent labeling of the tracked objects. The system load is distributed through a new Internet of Video Things infrastructure-based architecture to improve data processing and real-time object tracking performance. The experimental evaluation was conducted on a publicly available dataset generated by multi-camera containing different crowd activities.
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2204.11621.pdf' target='_blank'>https://arxiv.org/pdf/2204.11621.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingchen Ma, Yongsheng Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.11621">Semantic Geometric Fusion Multi-object Tracking and Lidar Odometry in Dynamic Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The SLAM system based on static scene assumption will introduce huge estimation errors when moving objects appear in the field of view. This paper proposes a novel multi-object dynamic lidar odometry (MLO) based on semantic object detection technology to solve this problem. The MLO system can provide reliable localization of robot and semantic objects and build long-term static maps in complex dynamic scenes. For ego-motion estimation, we use the environment features that take semantic and geometric consistency constraints into account in the extraction process. The filtering features are robust to semantic movable and unknown dynamic objects. At the same time, a least square estimator using the semantic bounding box and object point cloud is proposed to achieve accurate and stable multi-object tracking between frames. In the mapping module, we further realize dynamic semantic object detection based on the absolute trajectory tracking list (ATTL). Then, static semantic objects and environmental features can be used to eliminate accumulated localization errors and build pure static maps. Experiments on public KITTI data sets show that the proposed system can achieve more accurate and robust tracking of the object and better real-time localization accuracy in complex scenes compared with existing technologies.
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2112.06809.pdf' target='_blank'>https://arxiv.org/pdf/2112.06809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael P. J. Camilleri, Li Zhang, Rasneer S. Bains, Andrew Zisserman, Christopher K. I. Williams
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2112.06809">Persistent Animal Identification Leveraging Non-Visual Markers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Our objective is to locate and provide a unique identifier for each mouse in a cluttered home-cage environment through time, as a precursor to automated behaviour recognition for biological research. This is a very challenging problem due to (i) the lack of distinguishing visual features for each mouse, and (ii) the close confines of the scene with constant occlusion, making standard visual tracking approaches unusable. However, a coarse estimate of each mouse's location is available from a unique RFID implant, so there is the potential to optimally combine information from (weak) tracking with coarse information on identity. To achieve our objective, we make the following key contributions: (a) the formulation of the object identification problem as an assignment problem (solved using Integer Linear Programming), and (b) a novel probabilistic model of the affinity between tracklets and RFID data. The latter is a crucial part of the model, as it provides a principled probabilistic treatment of object detections given coarse localisation. Our approach achieves 77% accuracy on this animal identification problem, and is able to reject spurious detections when the animals are hidden.
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2108.11236.pdf' target='_blank'>https://arxiv.org/pdf/2108.11236.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keith A. LeGrand, Pingping Zhu, Silvia Ferrari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2108.11236">Cell Multi-Bernoulli (Cell-MB) Sensor Control for Multi-object Search-While-Tracking (SWT)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Information-driven control can be used to develop intelligent sensors that can optimize their measurement value based on environmental feedback. In object tracking applications, sensor actions are chosen based on the expected reduction in uncertainty also known as information gain. Random finite set (RFS) theory provides a formalism for quantifying and estimating information gain in multi-object tracking problems. However, estimating information gain in these applications remains computationally challenging. This paper presents a new tractable approximation of the RFS expected information gain applicable to sensor control for multi-object search and tracking. Unlike existing RFS approaches, the information gain approximation presented in this paper considers the contributions of non-ideal noisy measurements, missed detections, false alarms, and object appearance/disappearance. The effectiveness of the information-driven sensor control is demonstrated through two multi-vehicle search-while-tracking experiments using real video data from remote terrestrial and satellite sensors.
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2101.04281.pdf' target='_blank'>https://arxiv.org/pdf/2101.04281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathan Louis, Luowei Zhou, Steven J. Yule, Roger D. Dias, Milisa Manojlovich, Francis D. Pagani, Donald S. Likosky, Jason J. Corso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2101.04281">Temporally Guided Articulated Hand Pose Tracking in Surgical Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Articulated hand pose tracking is an under-explored problem that carries the potential for use in an extensive number of applications, especially in the medical domain. With a robust and accurate tracking system on surgical videos, the motion dynamics and movement patterns of the hands can be captured and analyzed for many rich tasks. In this work, we propose a novel hand pose estimation model, CondPose, which improves detection and tracking accuracy by incorporating a pose prior into its prediction. We show improvements over state-of-the-art methods which provide frame-wise independent predictions, by following a temporally guided approach that effectively leverages past predictions. We collect Surgical Hands, the first dataset that provides multi-instance articulated hand pose annotations for videos. Our dataset provides over 8.1k annotated hand poses from publicly available surgical videos and bounding boxes, pose annotations, and tracking IDs to enable multi-instance tracking. When evaluated on Surgical Hands, we show our method outperforms the state-of-the-art approach using mean Average Precision (mAP), to measure pose estimation accuracy, and Multiple Object Tracking Accuracy (MOTA), to assess pose tracking performance. In comparison to a frame-wise independent strategy, we show greater performance in detecting and tracking hand poses and more substantial impact on localization accuracy. This has positive implications in generating more accurate representations of hands in the scene to be used for targeted downstream tasks.
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/1912.00826.pdf' target='_blank'>https://arxiv.org/pdf/1912.00826.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiujie Dong, Xuedong He, Haiyan Ge, Qin Liu, Aifu Han, Shengzong Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1912.00826">Improving Model Drift for Robust Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discriminative correlation filters show excellent performance in object tracking. However, in complex scenes, the apparent characteristics of the tracked target are variable, which makes it easy to pollute the model and cause the model drift. In this paper, considering that the secondary peak has a greater impact on the model update, we propose a method for detecting the primary and secondary peaks of the response map. Secondly, a novel confidence function which uses the adaptive update discriminant mechanism is proposed, which yield good robustness. Thirdly, we propose a robust tracker with correlation filters, which uses hand-crafted features and can improve model drift in complex scenes. Finally, in order to cope with the current trackers' multi-feature response merge, we propose a simple exponential adaptive merge approach. Extensive experiments are performed on OTB2013, OTB100 and TC128 datasets. Our approach performs superiorly against several state-of-the-art trackers while runs at speed in real time.
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/1508.04124.pdf' target='_blank'>https://arxiv.org/pdf/1508.04124.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard Altendorfer, Sebastian Wirkert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1508.04124">A Complete Derivation Of The Association Log-Likelihood Distance For Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Mahalanobis distance is commonly used in multi-object trackers for measurement-to-track association. Starting with the original definition of the Mahalanobis distance we review its use in association. Given that there is no principle in multi-object tracking that sets the Mahalanobis distance apart as a distinguished statistical distance we revisit the global association hypotheses of multiple hypothesis tracking as the most general association setting. Those association hypotheses induce a distance-like quantity for assignment which we refer to as association log-likelihood distance. We compare the ability of the Mahalanobis distance to the association log-likelihood distance to yield correct association relations in Monte-Carlo simulations. It turns out that on average the distance based on association log-likelihood performs better than the Mahalanobis distance, confirming that the maximization of global association hypotheses is a more fundamental approach to association than the minimization of a certain statistical distance measure.
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2508.13507.pdf' target='_blank'>https://arxiv.org/pdf/2508.13507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungheon Baek, Jinhyuk Yun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13507">Bridging the Gap: Doubles Badminton Analysis with Singles-Trained Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Badminton is known as one of the fastest racket sports in the world. Despite doubles matches being more prevalent in international tournaments than singles, previous research has mainly focused on singles due to the challenges in data availability and multi-person tracking. To address this gap, we designed an approach that transfers singles-trained models to doubles analysis. We extracted keypoints from the ShuttleSet single matches dataset using ViT-Pose and embedded them through a contrastive learning framework based on ST-GCN. To improve tracking stability, we incorporated a custom multi-object tracking algorithm that resolves ID switching issues from fast and overlapping player movements. A Transformer-based classifier then determines shot occurrences based on the learned embeddings. Our findings demonstrate the feasibility of extending pose-based shot recognition to doubles badminton, broadening analytics capabilities. This work establishes a foundation for doubles-specific datasets to enhance understanding of this predominant yet understudied format of the fast racket sport.
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2508.09796.pdf' target='_blank'>https://arxiv.org/pdf/2508.09796.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjie Wang, Zhixing Wang, Le Zheng, Tianxiao Liu, Roujing Li, Xueyao Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09796">MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) in human-dominant scenarios, which involves continuously tracking multiple people within video sequences, remains a significant challenge in computer vision due to targets' complex motion and severe occlusions. Conventional tracking-by-detection methods are fundamentally limited by their reliance on Kalman filter (KF) and rigid Intersection over Union (IoU)-based association. The motion model in KF often mismatches real-world object dynamics, causing filtering errors, while rigid association struggles under occlusions, leading to identity switches or target loss. To address these issues, we propose MeMoSORT, a simple, online, and real-time MOT tracker with two key innovations. First, the Memory-assisted Kalman filter (MeKF) uses memory-augmented neural networks to compensate for mismatches between assumed and actual object motion. Second, the Motion-adaptive IoU (Mo-IoU) adaptively expands the matching space and incorporates height similarity to reduce the influence of detection errors and association failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of 67.9\% and 82.1\%, respectively.
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2508.01752.pdf' target='_blank'>https://arxiv.org/pdf/2508.01752.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumail Abbas, Zeeshan Afzal, Aqeel Raza, Taha Mansouri, Andrew W. Dowsey, Chaidate Inchaisri, Ali Alameer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01752">Vision transformer-based multi-camera multi-object tracking framework for dairy cow monitoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Activity and behaviour correlate with dairy cow health and welfare, making continual and accurate monitoring crucial for disease identification and farm productivity. Manual observation and frequent assessments are laborious and inconsistent for activity monitoring. In this study, we developed a unique multi-camera, real-time tracking system for indoor-housed Holstein Friesian dairy cows. This technology uses cutting-edge computer vision techniques, including instance segmentation and tracking algorithms to monitor cow activity seamlessly and accurately. An integrated top-down barn panorama was created by geometrically aligning six camera feeds using homographic transformations. The detection phase used a refined YOLO11-m model trained on an overhead cow dataset, obtaining high accuracy (mAP\@0.50 = 0.97, F1 = 0.95). SAMURAI, an upgraded Segment Anything Model 2.1, generated pixel-precise cow masks for instance segmentation utilizing zero-shot learning and motion-aware memory. Even with occlusion and fluctuating posture, a motion-aware Linear Kalman filter and IoU-based data association reliably identified cows over time for object tracking. The proposed system significantly outperformed Deep SORT Realtime. Multi-Object Tracking Accuracy (MOTA) was 98.7% and 99.3% in two benchmark video sequences, with IDF1 scores above 99% and near-zero identity switches. This unified multi-camera system can track dairy cows in complex interior surroundings in real time, according to our data. The system reduces redundant detections across overlapping cameras, maintains continuity as cows move between viewpoints, with the aim of improving early sickness prediction through activity quantification and behavioural classification.
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2507.05229.pdf' target='_blank'>https://arxiv.org/pdf/2507.05229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Markiyan Kostiv, Anatolii Adamovskyi, Yevhen Cherniavskyi, Mykyta Varenyk, Ostap Viniavskyi, Igor Krashenyi, Oles Dobosevych
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05229">Self-Supervised Real-Time Tracking of Military Vehicles in Low-FPS UAV Footage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims to maintain consistent identities of objects across video frames. Associating objects in low-frame-rate videos captured by moving unmanned aerial vehicles (UAVs) in actual combat scenarios is complex due to rapid changes in object appearance and position within the frame. The task becomes even more challenging due to image degradation caused by cloud video streaming and compression algorithms. We present how instance association learning from single-frame annotations can overcome these challenges. We show that global features of the scene provide crucial context for low-FPS instance association, allowing our solution to be robust to distractors and gaps in detections. We also demonstrate that such a tracking approach maintains high association quality even when reducing the input image resolution and latent representation size for faster inference. Finally, we present a benchmark dataset of annotated military vehicles collected from publicly available data sources. This paper was initially presented at the NATO Science and Technology Organization Symposium (ICMCIS) organized by the Information Systems Technology (IST)Scientific and Technical Committee, IST-209-RSY - the ICMCIS, held in Oeiras, Portugal, 13-14 May 2025.
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2506.21980.pdf' target='_blank'>https://arxiv.org/pdf/2506.21980.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biao Wang, Wenwen Li, Jiawei Ge
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21980">R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual single object tracking aims to continuously localize and estimate the scale of a target in subsequent video frames, given only its initial state in the first frame. This task has traditionally been framed as a template matching problem, evolving through major phases including correlation filters, two-stream networks, and one-stream networks with significant progress achieved. However, these methods typically require explicit classification and regression modeling, depend on supervised training with large-scale datasets, and are limited to the single task of tracking, lacking flexibility. In recent years, multi-modal large language models (MLLMs) have advanced rapidly. Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational capabilities, demonstrate excellent performance in grounding tasks. This has spurred interest in applying such models directly to visual tracking. However, experiments reveal that Qwen2.5-VL struggles with template matching between image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement learning method on a small-scale dataset with a rule-based reward function. The resulting model, R1-Track, achieved notable performance on the GOT-10k benchmark. R1-Track supports flexible initialization via bounding boxes or text descriptions while retaining most of the original model's general capabilities. And we further discuss potential improvements for R1-Track. This rough technical report summarizes our findings as of May 2025.
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2506.19341.pdf' target='_blank'>https://arxiv.org/pdf/2506.19341.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongping Dong, Liming Chen, Mohand Tahar Kechadi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19341">Trajectory Prediction in Dynamic Object Tracking: A Critical Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study provides a detailed analysis of current advancements in dynamic object tracking (DOT) and trajectory prediction (TP) methodologies, including their applications and challenges. It covers various approaches, such as feature-based, segmentation-based, estimation-based, and learning-based methods, evaluating their effectiveness, deployment, and limitations in real-world scenarios. The study highlights the significant impact of these technologies in automotive and autonomous vehicles, surveillance and security, healthcare, and industrial automation, contributing to safety and efficiency. Despite the progress, challenges such as improved generalization, computational efficiency, reduced data dependency, and ethical considerations still exist. The study suggests future research directions to address these challenges, emphasizing the importance of multimodal data integration, semantic information fusion, and developing context-aware systems, along with ethical and privacy-preserving frameworks.
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2506.19154.pdf' target='_blank'>https://arxiv.org/pdf/2506.19154.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Falaki, Maria A. Amer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.19154">Lightweight RGB-T Tracking with Mobile Vision Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-modality object tracking (e.g., RGB-only) encounters difficulties in challenging imaging conditions, such as low illumination and adverse weather conditions. To solve this, multimodal tracking (e.g., RGB-T models) aims to leverage complementary data such as thermal infrared features. While recent Vision Transformer-based multimodal trackers achieve strong performance, they are often computationally expensive due to large model sizes. In this work, we propose a novel lightweight RGB-T tracking algorithm based on Mobile Vision Transformers (MobileViT). Our tracker introduces a progressive fusion framework that jointly learns intra-modal and inter-modal interactions between the template and search regions using separable attention. This design produces effective feature representations that support more accurate target localization while achieving a small model size and fast inference speed. Compared to state-of-the-art efficient multimodal trackers, our model achieves comparable accuracy while offering significantly lower parameter counts (less than 4 million) and the fastest GPU inference speed of 122 frames per second. This paper is the first to propose a tracker using Mobile Vision Transformers for RGB-T tracking and multimodal tracking at large. Tracker code and model weights will be made publicly available upon acceptance.
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2506.14256.pdf' target='_blank'>https://arxiv.org/pdf/2506.14256.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Deepak Ghimire, Joonwhoan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14256">Comparison of Two Methods for Stationary Incident Detection Based on Background Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2506.07850.pdf' target='_blank'>https://arxiv.org/pdf/2506.07850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Arash Rocky, Q. M. Jonathan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07850">SAM2Auto: Auto Annotation Using FLASH</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-Language Models (VLMs) lag behind Large Language Models due to the scarcity of annotated datasets, as creating paired visual-textual annotations is labor-intensive and expensive. To address this bottleneck, we introduce SAM2Auto, the first fully automated annotation pipeline for video datasets requiring no human intervention or dataset-specific training. Our approach consists of two key components: SMART-OD, a robust object detection system that combines automatic mask generation with open-world object detection capabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a multi-object real-time video instance segmentation (VIS) that maintains consistent object identification across video frames even with intermittent detection gaps. Unlike existing open-world detection methods that require frame-specific hyperparameter tuning and suffer from numerous false positives, our system employs statistical approaches to minimize detection errors while ensuring consistent object tracking throughout entire video sequences. Extensive experimental validation demonstrates that SAM2Auto achieves comparable accuracy to manual annotation while dramatically reducing annotation time and eliminating labor costs. The system successfully handles diverse datasets without requiring retraining or extensive parameter adjustments, making it a practical solution for large-scale dataset creation. Our work establishes a new baseline for automated video annotation and provides a pathway for accelerating VLM development by addressing the fundamental dataset bottleneck that has constrained progress in vision-language understanding.
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2505.22677.pdf' target='_blank'>https://arxiv.org/pdf/2505.22677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jisu Kim, Alex Mattingly, Eung-Joo Lee, Benjamin S. Riggan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.22677">Using Cross-Domain Detection Loss to Infer Multi-Scale Information for Improved Tiny Head Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Head detection and tracking are essential for downstream tasks, but current methods often require large computational budgets, which increase latencies and ties up resources (e.g., processors, memory, and bandwidth). To address this, we propose a framework to enhance tiny head detection and tracking by optimizing the balance between performance and efficiency. Our framework integrates (1) a cross-domain detection loss, (2) a multi-scale module, and (3) a small receptive field detection mechanism. These innovations enhance detection by bridging the gap between large and small detectors, capturing high-frequency details at multiple scales during training, and using filters with small receptive fields to detect tiny heads. Evaluations on the CroHD and CrowdHuman datasets show improved Multiple Object Tracking Accuracy (MOTA) and mean Average Precision (mAP), demonstrating the effectiveness of our approach in crowded scenes.
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2505.07336.pdf' target='_blank'>https://arxiv.org/pdf/2505.07336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixuan Zhang, Xiaopeng Li, Qi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.07336">SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2504.19719.pdf' target='_blank'>https://arxiv.org/pdf/2504.19719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lukas Folkman, Quynh LK Vo, Colin Johnston, Bela Stantic, Kylie A Pitt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19719">A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing demand for aquaculture production necessitates the development of innovative, intelligent tools to effectively monitor and manage fish health and welfare. While non-invasive video monitoring has become a common practice in finfish aquaculture, existing intelligent monitoring methods predominantly focus on assessing body condition or fish swimming patterns and are often developed and evaluated in controlled tank environments, without demonstrating their applicability to real-world aquaculture settings in open sea farms. This underscores the necessity for methods that can monitor physiological traits directly within the production environment of sea fish farms. To this end, we have developed a computer vision method for monitoring ventilation rates of Atlantic salmon (Salmo salar), which was specifically designed for videos recorded in the production environment of commercial sea fish farms using the existing infrastructure. Our approach uses a fish head detection model, which classifies the mouth state as either open or closed using a convolutional neural network. This is followed with multiple object tracking to create temporal sequences of fish swimming across the field of view of the underwater video camera to estimate ventilation rates. The method demonstrated high efficiency, achieving a Pearson correlation coefficient of 0.82 between ground truth and predicted ventilation rates in a test set of 100 fish collected independently of the training data. By accurately identifying pens where fish exhibit signs of respiratory distress, our method offers broad applicability and the potential to transform fish health and welfare monitoring in finfish aquaculture.
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2504.18165.pdf' target='_blank'>https://arxiv.org/pdf/2504.18165.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michel Gokan Khan, Renan Guarese, Fabian Johnson, Xi Vincent Wang, Anders Bergman, Benjamin Edvinsson, Mario Romero, JÃ©rÃ©my Vachier, Jan Kronqvist
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18165">PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2504.11310.pdf' target='_blank'>https://arxiv.org/pdf/2504.11310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayong Liu, Qingrui Zhang, Zeyang Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11310">Intelligent driving vehicle front multi-target tracking and detection based on YOLOv5 and point cloud 3D projection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In multi-target tracking and detection tasks, it is necessary to continuously track multiple targets, such as vehicles, pedestrians, etc. To achieve this goal, the system must be able to continuously acquire and process image frames containing these targets. These consecutive frame images enable the algorithm to update the position and state of the target in real-time in each frame of the image. How to accurately associate the detected target with the target in the previous or next frame to form a stable trajectory is a complex problem. Therefore, a multi object tracking and detection method for intelligent driving vehicles based on YOLOv5 and point cloud 3D projection is proposed. Using Retinex algorithm to enhance the image of the environment in front of the vehicle, remove light interference in the image, and build an intelligent detection model based on YOLOv5 network structure. The enhanced image is input into the model, and multiple targets in front of the vehicle are identified through feature extraction and target localization. By combining point cloud 3D projection technology, the correlation between the position changes of adjacent frame images in the projection coordinate system can be inferred. By sequentially projecting the multi-target recognition results of multiple consecutive frame images into the 3D laser point cloud environment, effective tracking of the motion trajectories of all targets in front of the vehicle can be achieved. The experimental results show that the application of this method for intelligent driving vehicle front multi-target tracking and detection yields a MOTA (Tracking Accuracy) value greater than 30, demonstrating its superior tracking and detection performance.
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2504.02519.pdf' target='_blank'>https://arxiv.org/pdf/2504.02519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Alexander Holz, Christian Bader, Markus Enzweiler, Matthias DrÃ¼ppel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02519">Data-Driven Object Tracking: Integrating Modular Neural Networks into a Kalman Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents novel Machine Learning (ML) methodologies for Multi-Object Tracking (MOT), specifically designed to meet the increasing complexity and precision demands of Advanced Driver Assistance Systems (ADAS). We introduce three Neural Network (NN) models that address key challenges in MOT: (i) the Single-Prediction Network (SPENT) for trajectory prediction, (ii) the Single-Association Network (SANT) for mapping individual Sensor Object (SO) to existing tracks, and (iii) the Multi-Association Network (MANTa) for associating multiple SOs to multiple tracks. These models are seamlessly integrated into a traditional Kalman Filter (KF) framework, maintaining the system's modularity by replacing relevant components without disrupting the overall architecture. Importantly, all three networks are designed to be run in a realtime, embedded environment. Each network contains less than 50k trainable parameters. Our evaluation, conducted on the public KITTI tracking dataset, demonstrates significant improvements in tracking performance. SPENT reduces the Root Mean Square Error (RMSE) by 50% compared to a standard KF, while SANT and MANTa achieve up to 95% accuracy in sensor object-to-track assignments. These results underscore the effectiveness of incorporating task-specific NNs into traditional tracking systems, boosting performance and robustness while preserving modularity, maintainability, and interpretability.
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2503.04500.pdf' target='_blank'>https://arxiv.org/pdf/2503.04500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Hsi Chen, Chin-Tien Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04500">ReynoldsFlow: Exquisite Flow Estimation via Reynolds Transport Theorem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Optical flow is a fundamental technique for motion estimation, widely applied in video stabilization, interpolation, and object tracking. Traditional optical flow estimation methods rely on restrictive assumptions like brightness constancy and slow motion constraints. Recent deep learning-based flow estimations require extensive training on large domain-specific datasets, making them computationally demanding. Also, artificial intelligence (AI) advances have enabled deep learning models to take advantage of optical flow as an important feature for object tracking and motion analysis. Since optical flow is commonly encoded in HSV for visualization, its conversion to RGB for neural network processing is nonlinear and may introduce perceptual distortions. These transformations amplify the sensitivity to estimation errors, potentially affecting the predictive accuracy of the networks. To address these challenges that are influential to the performance of downstream network models, we propose Reynolds flow, a novel training-free flow estimation inspired by the Reynolds transport theorem, offering a principled approach to modeling complex motion dynamics. In addition to conventional HSV-based visualization of Reynolds flow, we also introduce an RGB-encoded representation of Reynolds flow designed to improve flow visualization and feature enhancement for neural networks. We evaluated the effectiveness of Reynolds flow in video-based tasks. Experimental results on three benchmarks, tiny object detection on UAVDB, infrared object detection on Anti-UAV, and pose estimation on GolfDB, demonstrate that networks trained with RGB-encoded Reynolds flow achieve SOTA performance, exhibiting improved robustness and efficiency across all tasks.
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2502.19705.pdf' target='_blank'>https://arxiv.org/pdf/2502.19705.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juntao Liang, Jun Hou, Weijun Zhang, Yong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19705">CFTrack: Enhancing Lightweight Visual Tracking through Contrastive Learning and Feature Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Achieving both efficiency and strong discriminative ability in lightweight visual tracking is a challenge, especially on mobile and edge devices with limited computational resources. Conventional lightweight trackers often struggle with robustness under occlusion and interference, while deep trackers, when compressed to meet resource constraints, suffer from performance degradation. To address these issues, we introduce CFTrack, a lightweight tracker that integrates contrastive learning and feature matching to enhance discriminative feature representations. CFTrack dynamically assesses target similarity during prediction through a novel contrastive feature matching module optimized with an adaptive contrastive loss, thereby improving tracking accuracy. Extensive experiments on LaSOT, OTB100, and UAV123 show that CFTrack surpasses many state-of-the-art lightweight trackers, operating at 136 frames per second on the NVIDIA Jetson NX platform. Results on the HOOT dataset further demonstrate CFTrack's strong discriminative ability under heavy occlusion.
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2502.18867.pdf' target='_blank'>https://arxiv.org/pdf/2502.18867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Akhil Penta, Vaibhav Adwani, Ankush Chopra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18867">Enhanced Transformer-Based Tracking for Skiing Events: Overcoming Multi-Camera Challenges, Scale Variations and Rapid Motion -- SkiTB Visual Tracking Challenge 2025</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate skier tracking is essential for performance analysis, injury prevention, and optimizing training strategies in alpine sports. Traditional tracking methods often struggle with occlusions, dynamic movements, and varying environmental conditions, limiting their effectiveness. In this work, we used STARK (Spatio-Temporal Transformer Network for Visual Tracking), a transformer-based model, to track skiers. We adapted STARK to address domain-specific challenges such as camera movements, camera changes, occlusions, etc. by optimizing the model's architecture and hyperparameters to better suit the dataset.
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2502.04478.pdf' target='_blank'>https://arxiv.org/pdf/2502.04478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luiz C. S. de Araujo, Carlos M. S. Figueiredo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04478">OneTrack-M: A multitask approach to transformer-based MOT models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) is a critical problem in computer vision, essential for understanding how objects move and interact in videos. This field faces significant challenges such as occlusions and complex environmental dynamics, impacting model accuracy and efficiency. While traditional approaches have relied on Convolutional Neural Networks (CNNs), introducing transformers has brought substantial advancements. This work introduces OneTrack-M, a transformer-based MOT model designed to enhance tracking computational efficiency and accuracy. Our approach simplifies the typical transformer-based architecture by eliminating the need for a decoder model for object detection and tracking. Instead, the encoder alone serves as the backbone for temporal data interpretation, significantly reducing processing time and increasing inference speed. Additionally, we employ innovative data pre-processing and multitask training techniques to address occlusion and diverse objective challenges within a single set of weights. Experimental results demonstrate that OneTrack-M achieves at least 25% faster inference times compared to state-of-the-art models in the literature while maintaining or improving tracking accuracy metrics. These improvements highlight the potential of the proposed solution for real-time applications such as autonomous vehicles, surveillance systems, and robotics, where rapid responses are crucial for system effectiveness.
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2412.10453.pdf' target='_blank'>https://arxiv.org/pdf/2412.10453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kailas PS, Selvakumaran R, Palani Murugan, Ramesh Kumar, Malaya Kumar Biswal M
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10453">Analysis of Object Detection Models for Tiny Object in Satellite Imagery: A Dataset-Centric Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, significant advancements have been made in deep learning-based object detection algorithms, revolutionizing basic computer vision tasks, notably in object detection, tracking, and segmentation. This paper delves into the intricate domain of Small-Object-Detection (SOD) within satellite imagery, highlighting the unique challenges stemming from wide imaging ranges, object distribution, and their varying appearances in bird's-eye-view satellite images. Traditional object detection models face difficulties in detecting small objects due to limited contextual information and class imbalances. To address this, our research presents a meticulously curated dataset comprising 3000 images showcasing cars, ships, and airplanes in satellite imagery. Our study aims to provide valuable insights into small object detection in satellite imagery by empirically evaluating state-of-the-art models. Furthermore, we tackle the challenges of satellite video-based object tracking, employing the Byte Track algorithm on the SAT-MTB dataset. Through rigorous experimentation, we aim to offer a comprehensive understanding of the efficacy of state-of-the-art models in Small-Object-Detection for satellite applications. Our findings shed light on the effectiveness of these models and pave the way for future advancements in satellite imagery analysis.
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2412.01119.pdf' target='_blank'>https://arxiv.org/pdf/2412.01119.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mojtaba S. Fazli, Shannon Quinn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01119">Object Tracking in a $360^o$ View: A Novel Perspective on Bridging the Gap to Biomedical Advancements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a fundamental tool in modern innovation, with applications in defense systems, autonomous vehicles, and biomedical research. It enables precise identification, monitoring, and spatiotemporal analysis of objects across sequential frames, providing insights into dynamic behaviors. In cell biology, object tracking is vital for uncovering cellular mechanisms, such as migration, interactions, and responses to drugs or pathogens. These insights drive breakthroughs in understanding disease progression and therapeutic interventions.
  Over time, object tracking methods have evolved from traditional feature-based approaches to advanced machine learning and deep learning frameworks. While classical methods are reliable in controlled settings, they struggle in complex environments with occlusions, variable lighting, and high object density. Deep learning models address these challenges by delivering greater accuracy, adaptability, and robustness.
  This review categorizes object tracking techniques into traditional, statistical, feature-based, and machine learning paradigms, with a focus on biomedical applications. These methods are essential for tracking cells and subcellular structures, advancing our understanding of health and disease. Key performance metrics, including accuracy, efficiency, and adaptability, are discussed. The paper explores limitations of current methods and highlights emerging trends to guide the development of next-generation tracking systems for biomedical research and broader scientific domains.
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2411.19134.pdf' target='_blank'>https://arxiv.org/pdf/2411.19134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peilin Tian, Hao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19134">Visual SLAMMOT Considering Multiple Motion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Simultaneous Localization and Mapping (SLAM) and Multi-Object Tracking (MOT) are pivotal tasks in the realm of autonomous driving, attracting considerable research attention. While SLAM endeavors to generate real-time maps and determine the vehicle's pose in unfamiliar settings, MOT focuses on the real-time identification and tracking of multiple dynamic objects. Despite their importance, the prevalent approach treats SLAM and MOT as independent modules within an autonomous vehicle system, leading to inherent limitations. Classical SLAM methodologies often rely on a static environment assumption, suitable for indoor rather than dynamic outdoor scenarios. Conversely, conventional MOT techniques typically rely on the vehicle's known state, constraining the accuracy of object state estimations based on this prior. To address these challenges, previous efforts introduced the unified SLAMMOT paradigm, yet primarily focused on simplistic motion patterns. In our team's previous work IMM-SLAMMOT\cite{IMM-SLAMMOT}, we present a novel methodology incorporating consideration of multiple motion models into SLAMMOT i.e. tightly coupled SLAM and MOT, demonstrating its efficacy in LiDAR-based systems. This paper studies feasibility and advantages of instantiating this methodology as visual SLAMMOT, bridging the gap between LiDAR and vision-based sensing mechanisms. Specifically, we propose a solution of visual SLAMMOT considering multiple motion models and validate the inherent advantages of IMM-SLAMMOT in the visual domain.
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2411.18443.pdf' target='_blank'>https://arxiv.org/pdf/2411.18443.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonathan Lichtenfeld, Kevin Daun, Oskar von Stryk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18443">Efficient Dynamic LiDAR Odometry for Mobile Robots with Structured Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a real-time dynamic LiDAR odometry pipeline for mobile robots in Urban Search and Rescue (USAR) scenarios. Existing approaches to dynamic object detection often rely on pretrained learned networks or computationally expensive volumetric maps. To enhance efficiency on computationally limited robots, we reuse data between the odometry and detection module. Utilizing a range image segmentation technique and a novel residual-based heuristic, our method distinguishes dynamic from static objects before integrating them into the point cloud map. The approach demonstrates robust object tracking and improved map accuracy in environments with numerous dynamic objects. Even highly non-rigid objects, such as running humans, are accurately detected at point level without prior downsampling of the point cloud and hence, without loss of information. Evaluation on simulated and real-world data validates its computational efficiency. Compared to a state-of-the-art volumetric method, our approach shows comparable detection performance at a fraction of the processing time, adding only 14 ms to the odometry module for dynamic object detection and tracking. The implementation and a new real-world dataset are available as open-source for further research.
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2411.08335.pdf' target='_blank'>https://arxiv.org/pdf/2411.08335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muttahirul Islam, Nazmul Haque, Md. Hadiuzzaman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.08335">DEEGITS: Deep Learning based Framework for Measuring Heterogenous Traffic State in Challenging Traffic Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents DEEGITS (Deep Learning Based Heterogeneous Traffic State Measurement), a comprehensive framework that leverages state-of-the-art convolutional neural network (CNN) techniques to accurately and rapidly detect vehicles and pedestrians, as well as to measure traffic states in challenging scenarios (i.e., congestion, occlusion). In this study, we enhance the training dataset through data fusion, enabling simultaneous detection of vehicles and pedestrians. Image preprocessing and augmentation are subsequently performed to improve the quality and quantity of the dataset. Transfer learning is applied on the YOLOv8 pretrained model to increase the model's capability to identify a diverse array of vehicles. Optimal hyperparameters are obtained using the Grid Search algorithm, with the Stochastic Gradient Descent (SGD) optimizer outperforming other optimizers under these settings. Extensive experimentation and evaluation demonstrate substantial accuracy within the detection framework, with the model achieving 0.794 mAP@0.5 on the validation set and 0.786 mAP@0.5 on the test set, surpassing previous benchmarks on similar datasets. The DeepSORT multi-object tracking algorithm is incorporated to track detected vehicles and pedestrians in this study. Finally, the framework is tested to measure heterogeneous traffic states in mixed traffic conditions. Two locations with differing traffic compositions and congestion levels are selected: one motorized-dominant location with moderate density and one non-motorized-dominant location with higher density. Errors are statistically insignificant for both cases, showing correlations from 0.99 to 0.88 and 0.91 to 0.97 for heterogeneous traffic flow and speed measurements, respectively.
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2410.20079.pdf' target='_blank'>https://arxiv.org/pdf/2410.20079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>InPyo Song, Jangwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20079">SFTrack: A Robust Scale and Motion Adaptive Algorithm for Tracking Small and Fast Moving Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper addresses the problem of multi-object tracking in Unmanned Aerial Vehicle (UAV) footage. It plays a critical role in various UAV applications, including traffic monitoring systems and real-time suspect tracking by the police. However, this task is highly challenging due to the fast motion of UAVs, as well as the small size of target objects in the videos caused by the high-altitude and wide angle views of drones. In this study, we thus introduce a simple yet more effective method compared to previous work to overcome these challenges. Our approach involves a new tracking strategy, which initiates the tracking of target objects from low-confidence detections commonly encountered in UAV application scenarios. Additionally, we propose revisiting traditional appearance-based matching algorithms to improve the association of low-confidence detections. To evaluate the effectiveness of our method, we conducted benchmark evaluations on two UAV-specific datasets (VisDrone2019, UAVDT) and one general object tracking dataset (MOT17). The results demonstrate that our approach surpasses current state-of-the art methodologies, highlighting its robustness and adaptability in diverse tracking environments. Furthermore, we have improved the annotation of the UAVDT dataset by rectifying several errors and addressing omissions found in the original annotations. We will provide this refined version of the dataset to facilitate better benchmarking in the field.
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2410.08769.pdf' target='_blank'>https://arxiv.org/pdf/2410.08769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan MÃ¼ller, Adrian Pigors
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08769">Efficient Multi-Object Tracking on Edge Devices via Reconstruction-Based Channel Pruning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of multi-object tracking (MOT) technologies presents the dual challenge of maintaining high performance while addressing critical security and privacy concerns. In applications such as pedestrian tracking, where sensitive personal data is involved, the potential for privacy violations and data misuse becomes a significant issue if data is transmitted to external servers. To mitigate these risks, processing data directly on an edge device, such as a smart camera, has emerged as a viable solution. Edge computing ensures that sensitive information remains local, thereby aligning with stringent privacy principles and significantly reducing network latency. However, the implementation of MOT on edge devices is not without its challenges. Edge devices typically possess limited computational resources, necessitating the development of highly optimized algorithms capable of delivering real-time performance under these constraints. The disparity between the computational requirements of state-of-the-art MOT algorithms and the capabilities of edge devices emphasizes a significant obstacle. To address these challenges, we propose a neural network pruning method specifically tailored to compress complex networks, such as those used in modern MOT systems. This approach optimizes MOT performance by ensuring high accuracy and efficiency within the constraints of limited edge devices, such as NVIDIA's Jetson Orin Nano. By applying our pruning method, we achieve model size reductions of up to 70% while maintaining a high level of accuracy and further improving performance on the Jetson Orin Nano, demonstrating the effectiveness of our approach for edge computing applications.
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2409.17533.pdf' target='_blank'>https://arxiv.org/pdf/2409.17533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Limanta, Kuniaki Uto, Koichi Shinoda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17533">CAMOT: Camera Angle-aware Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes CAMOT, a simple camera angle estimator for multi-object tracking to tackle two problems: 1) occlusion and 2) inaccurate distance estimation in the depth direction. Under the assumption that multiple objects are located on a flat plane in each video frame, CAMOT estimates the camera angle using object detection. In addition, it gives the depth of each object, enabling pseudo-3D MOT. We evaluated its performance by adding it to various 2D MOT methods on the MOT17 and MOT20 datasets and confirmed its effectiveness. Applying CAMOT to ByteTrack, we obtained 63.8% HOTA, 80.6% MOTA, and 78.5% IDF1 in MOT17, which are state-of-the-art results. Its computational cost is significantly lower than the existing deep-learning-based depth estimators for tracking.
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2409.09841.pdf' target='_blank'>https://arxiv.org/pdf/2409.09841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oriel Perl, Ido Leshem, Uria Franko, Yuval Goldman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09841">Tracking Virtual Meetings in the Wild: Re-identification in Multi-Participant Virtual Meetings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, workplaces and educational institutes have widely adopted virtual meeting platforms. This has led to a growing interest in analyzing and extracting insights from these meetings, which requires effective detection and tracking of unique individuals. In practice, there is no standardization in video meetings recording layout, and how they are captured across the different platforms and services. This, in turn, creates a challenge in acquiring this data stream and analyzing it in a uniform fashion. Our approach provides a solution to the most general form of video recording, usually consisting of a grid of participants (\cref{fig:videomeeting}) from a single video source with no metadata on participant locations, while using the least amount of constraints and assumptions as to how the data was acquired. Conventional approaches often use YOLO models coupled with tracking algorithms, assuming linear motion trajectories akin to that observed in CCTV footage. However, such assumptions fall short in virtual meetings, where participant video feed window can abruptly change location across the grid. In an organic video meeting setting, participants frequently join and leave, leading to sudden, non-linear movements on the video grid. This disrupts optical flow-based tracking methods that depend on linear motion. Consequently, standard object detection and tracking methods might mistakenly assign multiple participants to the same tracker. In this paper, we introduce a novel approach to track and re-identify participants in remote video meetings, by utilizing the spatio-temporal priors arising from the data in our domain. This, in turn, increases tracking capabilities compared to the use of general object tracking. Our approach reduces the error rate by 95% on average compared to YOLO-based tracking methods as a baseline.
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2408.13243.pdf' target='_blank'>https://arxiv.org/pdf/2408.13243.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandru Niculescu-Mizil, Deep Patel, Iain Melvin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13243">MCTR: Multi Camera Tracking Transformer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-camera tracking plays a pivotal role in various real-world applications. While end-to-end methods have gained significant interest in single-camera tracking, multi-camera tracking remains predominantly reliant on heuristic techniques. In response to this gap, this paper introduces Multi-Camera Tracking tRansformer (MCTR), a novel end-to-end approach tailored for multi-object detection and tracking across multiple cameras with overlapping fields of view. MCTR leverages end-to-end detectors like DEtector TRansformer (DETR) to produce detections and detection embeddings independently for each camera view. The framework maintains set of track embeddings that encaplusate global information about the tracked objects, and updates them at every frame by integrating the local information from the view-specific detection embeddings. The track embeddings are probabilistically associated with detections in every camera view and frame to generate consistent object tracks. The soft probabilistic association facilitates the design of differentiable losses that enable end-to-end training of the entire system. To validate our approach, we conduct experiments on MMPTrack and AI City Challenge, two recently introduced large-scale multi-camera multi-object tracking datasets.
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2408.07344.pdf' target='_blank'>https://arxiv.org/pdf/2408.07344.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Guo, Rujie Liu, Narishige Abe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.07344">RTAT: A Robust Two-stage Association Tracker for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data association is an essential part in the tracking-by-detection based Multi-Object Tracking (MOT). Most trackers focus on how to design a better data association strategy to improve the tracking performance. The rule-based handcrafted association methods are simple and highly efficient but lack generalization capability to deal with complex scenes. While the learnt association methods can learn high-order contextual information to deal with various complex scenes, but they have the limitations of higher complexity and cost. To address these limitations, we propose a Robust Two-stage Association Tracker, named RTAT. The first-stage association is performed between tracklets and detections to generate tracklets with high purity, and the second-stage association is performed between tracklets to form complete trajectories. For the first-stage association, we use a simple data association strategy to generate tracklets with high purity by setting a low threshold for the matching cost in the assignment process. We conduct the tracklet association in the second-stage based on the framework of message-passing GNN. Our method models the tracklet association as a series of edge classification problem in hierarchical graphs, which can recursively merge short tracklets into longer ones. Our tracker RTAT ranks first on the test set of MOT17 and MOT20 benchmarks in most of the main MOT metrics: HOTA, IDF1, and AssA. We achieve 67.2 HOTA, 84.7 IDF1, and 69.7 AssA on MOT17, and 66.2 HOTA, 82.5 IDF1, and 68.1 AssA on MOT20.
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2408.00417.pdf' target='_blank'>https://arxiv.org/pdf/2408.00417.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christian Gramsch, Shishan Yang, Hosam Alqaderi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.00417">A Batch Update Using Multiplicative Noise Modelling for Extended Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While the tracking of multiple extended targets demands for sophisticated algorithms to handle the high complexity inherent to the task, it also requires low runtime for online execution in real-world scenarios. In this work, we derive a batch update for the recently introduced elliptical-target tracker called MEM-EKF*. The MEM-EKF* is based on the same likelihood as the well-established random matrix approach but is derived from the multiplicative error model (MEM) and uses an extended Kalman filter (EKF) to update the target state sequentially, i.e., measurement-by-measurement. Our batch variant updates the target state in a single step based on straightforward sums over all measurements and the MEM-specific pseudo-measurements. This drastically reduces the scaling constant for typical implementations and indeed we find a speedup of roughly 100x in our numerical experiments. At the same time, the estimation error which we measure using the Gaussian Wasserstein distance stays significantly below that of the random matrix approach in coordinated turn scenarios while being comparable otherwise.
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2407.20623.pdf' target='_blank'>https://arxiv.org/pdf/2407.20623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Varini, Joel H. Gayford, Jeremy Jenrette, Matthew J. Witt, Francesco Garzon, Francesco Ferretti, Sophie Wilday, Mark E. Bond, Michael R. Heithaus, Danielle Robinson, Devon Carter, Najee Gumbs, Vincent Webster, Ben Glocker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20623">SharkTrack: an accurate, generalisable software for streamlining shark and ray underwater video analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Elasmobranchs (shark sand rays) represent a critical component of marine ecosystems. Yet, they are experiencing global population declines and effective monitoring of populations is essential to their protection. Underwater stationary videos, such as those from Baited Remote Underwater Video Stations (BRUVS), are critical for understanding elasmobranch spatial ecology and abundance. However, processing these videos requires time-consuming manual analysis that can delay conservation. To address this challenge, we developed SharkTrack, a semi-automatic underwater video analysis software. SharkTrack uses Convolutional Neural Networks (CNN) and Multi-Object Tracking to automatically detect and track elasmobranchs and provides an annotation pipeline to manually classify elasmobranch species and compute species-specific MaxN (ssMaxN), the standard metric of relative abundance. When tested on BRUVS footage from locations unseen by the CNN model during training, SharkTrack computed ssMaxN with 89% accuracy over 207 hours of footage. The semi-automatic SharkTrack pipeline required two minutes of manual classification per hour of video, an estimated 95% reduction of manual analysis time compared to traditional methods. Furthermore, we demonstrate SharkTrack accuracy across diverse marine ecosystems and elasmobranch species, an advancement compared to previous models, which were limited to specific species or locations. SharkTrack applications extend beyond BRUVS, facilitating the analysis of any underwater stationary video. By making video analysis faster and more accessible, SharkTrack enables research and conservation organisations to monitor elasmobranch populations more efficiently, thereby improving conservation efforts. To further support these goals, we provide public access to the SharkTrack software.
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2407.20446.pdf' target='_blank'>https://arxiv.org/pdf/2407.20446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaid A. El Shair, Samir A. Rawashdeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20446">MEVDT: Multi-Modal Event-Based Vehicle Detection and Tracking Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this data article, we introduce the Multi-Modal Event-based Vehicle Detection and Tracking (MEVDT) dataset. This dataset provides a synchronized stream of event data and grayscale images of traffic scenes, captured using the Dynamic and Active-Pixel Vision Sensor (DAVIS) 240c hybrid event-based camera. MEVDT comprises 63 multi-modal sequences with approximately 13k images, 5M events, 10k object labels, and 85 unique object tracking trajectories. Additionally, MEVDT includes manually annotated ground truth labels $\unicode{x2014}$ consisting of object classifications, pixel-precise bounding boxes, and unique object IDs $\unicode{x2014}$ which are provided at a labeling frequency of 24 Hz. Designed to advance the research in the domain of event-based vision, MEVDT aims to address the critical need for high-quality, real-world annotated datasets that enable the development and evaluation of object detection and tracking algorithms in automotive environments.
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2407.19430.pdf' target='_blank'>https://arxiv.org/pdf/2407.19430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Li, Kanlun Tan, Qiao Liu, Di Yuan, Xin Li, Yunpeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.19430">Progressive Domain Adaptation for Thermal Infrared Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the lack of large-scale labeled Thermal InfraRed (TIR) training datasets, most existing TIR trackers are trained directly on RGB datasets. However, tracking methods trained on RGB datasets suffer a significant drop-off in TIR data due to the domain shift issue. To this end, in this work, we propose a Progressive Domain Adaptation framework for TIR Tracking (PDAT), which transfers useful knowledge learned from RGB tracking to TIR tracking. The framework makes full use of large-scale labeled RGB datasets without requiring time-consuming and labor-intensive labeling of large-scale TIR data. Specifically, we first propose an adversarial-based global domain adaptation module to reduce domain gap on the feature level coarsely. Second, we design a clustering-based subdomain adaptation method to further align the feature distributions of the RGB and TIR datasets finely. These two domain adaptation modules gradually eliminate the discrepancy between the two domains, and thus learn domain-invariant fine-grained features through progressive training. Additionally, we collect a largescale TIR dataset with over 1.48 million unlabeled TIR images for training the proposed domain adaptation framework. Experimental results on five TIR tracking benchmarks show that the proposed method gains a nearly 6% success rate, demonstrating its effectiveness.
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2407.17521.pdf' target='_blank'>https://arxiv.org/pdf/2407.17521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edoardo Cittadini, Alessandro De Siena, Giorgio Buttazzo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17521">CORT: Class-Oriented Real-time Tracking for Embedded Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ever-increasing use of artificial intelligence in autonomous systems has significantly contributed to advance the research on multi-object tracking, adopted in several real-time applications (e.g., autonomous driving, surveillance drones, robotics) to localize and follow the trajectory of multiple objects moving in front of a camera. Current tracking algorithms can be divided into two main categories: some approaches introduce complex heuristics and re-identification models to improve the tracking accuracy and reduce the number of identification switches, without particular attention to the timing performance, whereas other approaches are aimed at reducing response times by removing the re-identification phase, thus penalizing the tracking accuracy. This work proposes a new approach to multi-class object tracking that allows achieving smaller and more predictable execution times, without penalizing the tracking performance. The idea is to reduce the problem of matching predictions with detections into smaller sub-problems by splitting the Hungarian matrix by class and invoking the second re-identification stage only when strictly necessary for a smaller number of elements. The proposed solution was evaluated in complex urban scenarios with several objects of different types (as cars, trucks, bikes, and pedestrians), showing the effectiveness of the multi-class approach with respect to state of the art trackers.
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2407.12614.pdf' target='_blank'>https://arxiv.org/pdf/2407.12614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiyu Liu, Congliang Zhou, Won Suk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.12614">Strawberry detection and counting based on YOLOv7 pruning and information based tracking algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The strawberry industry yields significant economic benefits for Florida, yet the process of monitoring strawberry growth and yield is labor-intensive and costly. The development of machine learning-based detection and tracking methodologies has been used for helping automated monitoring and prediction of strawberry yield, still, enhancement has been limited as previous studies only applied the deep learning method for flower and fruit detection, which did not consider the unique characteristics of image datasets collected by the machine vision system. This study proposed an optimal pruning of detection heads of the deep learning model (YOLOv7 and its variants) that could achieve fast and precise strawberry flower, immature fruit, and mature fruit detection. Thereafter, an enhanced object tracking algorithm, which is called the Information Based Tracking Algorithm (IBTA) utilized the best detection result, removed the Kalman Filter, and integrated moving direction, velocity, and spatial information to improve the precision in strawberry flower and fruit tracking. The proposed pruning of detection heads across YOLOv7 variants, notably Pruning-YOLOv7-tiny with detection head 3 and Pruning-YOLOv7-tiny with heads 2 and 3 achieved the best inference speed (163.9 frames per second) and detection accuracy (89.1%), respectively. On the other hand, the effect of IBTA was proved by comparing it with the centroid tracking algorithm (CTA), the Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP) of IBTA were 12.3% and 6.0% higher than that of CTA, accordingly. In addition, other object-tracking evaluation metrics, including IDF1, IDR, IDP, MT, and IDs, show that IBTA performed better than CTA in strawberry flower and fruit tracking.
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2407.04308.pdf' target='_blank'>https://arxiv.org/pdf/2407.04308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Griffin Golias, Masa Nakura-Fan, Vitaly Ablavsky
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04308">SSP-GNN: Learning to Track via Bilevel Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a graph-based tracking formulation for multi-object tracking (MOT) where target detections contain kinematic information and re-identification features (attributes). Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant. The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization guided by our novel loss function. We evaluate our algorithm on simulated scenarios to understand its sensitivity to scenario aspects and model hyperparameters. Across varied scenario complexities, our method compares favorably to a strong baseline.
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2406.08294.pdf' target='_blank'>https://arxiv.org/pdf/2406.08294.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasod Ginige, Ransika Gunasekara, Darsha Hewavitharana, Manjula Ariyarathne, Ranga Rodrigo, Peshala Jayasekara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08294">Vessel Re-identification and Activity Detection in Thermal Domain for Maritime Surveillance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Maritime surveillance is vital to mitigate illegal activities such as drug smuggling, illegal fishing, and human trafficking. Vision-based maritime surveillance is challenging mainly due to visibility issues at night, which results in failures in re-identifying vessels and detecting suspicious activities. In this paper, we introduce a thermal, vision-based approach for maritime surveillance with object tracking, vessel re-identification, and suspicious activity detection capabilities. For vessel re-identification, we propose a novel viewpoint-independent algorithm which compares features of the sides of the vessel separately (separate side-spaces) leveraging shape information in the absence of color features. We propose techniques to adapt tracking and activity detection algorithms for the thermal domain and train them using a thermal dataset we created. This dataset will be the first publicly available benchmark dataset for thermal maritime surveillance. Our system is capable of re-identifying vessels with an 81.8% Top1 score and identifying suspicious activities with a 72.4\% frame mAP score; a new benchmark for each task in the thermal domain.
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2406.07680.pdf' target='_blank'>https://arxiv.org/pdf/2406.07680.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duc Pham, Matthew Hansen, FÃ©licie Dhellemmes, Jens Krause, Pia Bideau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07680">Watching Swarm Dynamics from Above: A Framework for Advanced Object Tracking in Drone Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Easily accessible sensors, like drones with diverse onboard sensors, have greatly expanded studying animal behavior in natural environments. Yet, analyzing vast, unlabeled video data, often spanning hours, remains a challenge for machine learning, especially in computer vision. Existing approaches often analyze only a few frames. Our focus is on long-term animal behavior analysis. To address this challenge, we utilize classical probabilistic methods for state estimation, such as particle filtering. By incorporating recent advancements in semantic object segmentation, we enable continuous tracking of rapidly evolving object formations, even in scenarios with limited data availability. Particle filters offer a provably optimal algorithmic structure for recursively adding new incoming information. We propose a novel approach for tracking schools of fish in the open ocean from drone videos. Our framework not only performs classical object tracking in 2D, instead it tracks the position and spatial expansion of the fish school in world coordinates by fusing video data and the drone's on board sensor information (GPS and IMU). The presented framework for the first time allows researchers to study collective behavior of fish schools in its natural social and environmental context in a non-invasive and scalable way.
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2405.14195.pdf' target='_blank'>https://arxiv.org/pdf/2405.14195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wei, Yujie He, Zhanchuan Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14195">Enhanced Object Tracking by Self-Supervised Auxiliary Depth Estimation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-D tracking significantly improves the accuracy of object tracking. However, its dependency on real depth inputs and the complexity involved in multi-modal fusion limit its applicability across various scenarios. The utilization of depth information in RGB-D tracking inspired us to propose a new method, named MDETrack, which trains a tracking network with an additional capability to understand the depth of scenes, through supervised or self-supervised auxiliary Monocular Depth Estimation learning. The outputs of MDETrack's unified feature extractor are fed to the side-by-side tracking head and auxiliary depth estimation head, respectively. The auxiliary module will be discarded in inference, thus keeping the same inference speed. We evaluated our models with various training strategies on multiple datasets, and the results show an improved tracking accuracy even without real depth. Through these findings we highlight the potential of depth estimation in enhancing object tracking performance.
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2405.11655.pdf' target='_blank'>https://arxiv.org/pdf/2405.11655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tharun V. Puthanveettil, Fnu Obaid ur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11655">Track Anything Rapter(TAR)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is a fundamental task in computer vision with broad practical applications across various domains, including traffic monitoring, robotics, and autonomous vehicle tracking. In this project, we aim to develop a sophisticated aerial vehicle system known as Track Anything Rapter (TAR), designed to detect, segment, and track objects of interest based on user-provided multimodal queries, such as text, images, and clicks. TAR utilizes cutting-edge pre-trained models like DINO, CLIP, and SAM to estimate the relative pose of the queried object. The tracking problem is approached as a Visual Servoing task, enabling the UAV to consistently focus on the object through advanced motion planning and control algorithms. We showcase how the integration of these foundational models with a custom high-level control algorithm results in a highly stable and precise tracking system deployed on a custom-built PX4 Autopilot-enabled Voxl2 M500 drone. To validate the tracking algorithm's performance, we compare it against Vicon-based ground truth. Additionally, we evaluate the reliability of the foundational models in aiding tracking in scenarios involving occlusions. Finally, we test and validate the model's ability to work seamlessly with multiple modalities, such as click, bounding box, and image templates.
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2405.10868.pdf' target='_blank'>https://arxiv.org/pdf/2405.10868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>P. Sarveswarasarma, T. Sathulakjan, V. J. V. Godfrey, Thanuja D. Ambegoda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10868">Air Signing and Privacy-Preserving Signature Verification for Digital Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to the digital signing of electronic documents through the use of a camera-based interaction system, single-finger tracking for sign recognition, and multi commands executing hand gestures. The proposed solution, referred to as "Air Signature," involves writing the signature in front of the camera, rather than relying on traditional methods such as mouse drawing or physically signing on paper and showing it to a web camera. The goal is to develop a state-of-the-art method for detecting and tracking gestures and objects in real-time. The proposed methods include applying existing gesture recognition and object tracking systems, improving accuracy through smoothing and line drawing, and maintaining continuity during fast finger movements. An evaluation of the fingertip detection, sketching, and overall signing process is performed to assess the effectiveness of the proposed solution. The secondary objective of this research is to develop a model that can effectively recognize the unique signature of a user. This type of signature can be verified by neural cores that analyze the movement, speed, and stroke pixels of the signing in real time. The neural cores use machine learning algorithms to match air signatures to the individual's stored signatures, providing a secure and efficient method of verification. Our proposed System does not require sensors or any hardware other than the camera.
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2405.10444.pdf' target='_blank'>https://arxiv.org/pdf/2405.10444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Abdelaziz, Mohamed Sami Shehata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10444">A Novel Bounding Box Regression Method for Single Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Locating an object in a sequence of frames, given its appearance in the first frame of the sequence, is a hard problem that involves many stages. Usually, state-of-the-art methods focus on bringing novel ideas in the visual encoding or relational modelling phases. However, in this work, we show that bounding box regression from learned joint search and template features is of high importance as well. While previous methods relied heavily on well-learned features representing interactions between search and template, we hypothesize that the receptive field of the input convolutional bounding box network plays an important role in accurately determining the object location. To this end, we introduce two novel bounding box regression networks: inception and deformable. Experiments and ablation studies show that our inception module installed on the recent ODTrack outperforms the latter on three benchmarks: the GOT-10k, the UAV123 and the OTB2015.
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2405.10439.pdf' target='_blank'>https://arxiv.org/pdf/2405.10439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Abdelaziz, Mohamed Shehata, Mohamed Mohamed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10439">Beyond Traditional Single Object Tracking: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single object tracking is a vital task of many applications in critical fields. However, it is still considered one of the most challenging vision tasks. In recent years, computer vision, especially object tracking, witnessed the introduction or adoption of many novel techniques, setting new fronts for performance. In this survey, we visit some of the cutting-edge techniques in vision, such as Sequence Models, Generative Models, Self-supervised Learning, Unsupervised Learning, Reinforcement Learning, Meta-Learning, Continual Learning, and Domain Adaptation, focusing on their application in single object tracking. We propose a novel categorization of single object tracking methods based on novel techniques and trends. Also, we conduct a comparative analysis of the performance reported by the methods presented on popular tracking benchmarks. Moreover, we analyze the pros and cons of the presented approaches and present a guide for non-traditional techniques in single object tracking. Finally, we suggest potential avenues for future research in single-object tracking.
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2405.08996.pdf' target='_blank'>https://arxiv.org/pdf/2405.08996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Priya Sundaresan, Aditya Ganapathi, Harry Zhang, Shivin Devgon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08996">Learning Correspondence for Deformable Objects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the problem of pixelwise correspondence for deformable objects, namely cloth and rope, by comparing both classical and learning-based methods. We choose cloth and rope because they are traditionally some of the most difficult deformable objects to analytically model with their large configuration space, and they are meaningful in the context of robotic tasks like cloth folding, rope knot-tying, T-shirt folding, curtain closing, etc. The correspondence problem is heavily motivated in robotics, with wide-ranging applications including semantic grasping, object tracking, and manipulation policies built on top of correspondences. We present an exhaustive survey of existing classical methods for doing correspondence via feature-matching, including SIFT, SURF, and ORB, and two recently published learning-based methods including TimeCycle and Dense Object Nets. We make three main contributions: (1) a framework for simulating and rendering synthetic images of deformable objects, with qualitative results demonstrating transfer between our simulated and real domains (2) a new learning-based correspondence method extending Dense Object Nets, and (3) a standardized comparison across state-of-the-art correspondence methods. Our proposed method provides a flexible, general formulation for learning temporally and spatially continuous correspondences for nonrigid (and rigid) objects. We report root mean squared error statistics for all methods and find that Dense Object Nets outperforms baseline classical methods for correspondence, and our proposed extension of Dense Object Nets performs similarly.
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2405.07272.pdf' target='_blank'>https://arxiv.org/pdf/2405.07272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Chen, Chunhua Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07272">MAML MOT: Multiple Object Tracking based on Meta-Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of video analysis technology, the multi-object tracking (MOT) problem in complex scenes involving pedestrians is gaining increasing importance. This challenge primarily involves two key tasks: pedestrian detection and re-identification. While significant progress has been achieved in pedestrian detection tasks in recent years, enhancing the effectiveness of re-identification tasks remains a persistent challenge. This difficulty arises from the large total number of pedestrian samples in multi-object tracking datasets and the scarcity of individual instance samples. Motivated by recent rapid advancements in meta-learning techniques, we introduce MAML MOT, a meta-learning-based training approach for multi-object tracking. This approach leverages the rapid learning capability of meta-learning to tackle the issue of sample scarcity in pedestrian re-identification tasks, aiming to improve the model's generalization performance and robustness. Experimental results demonstrate that the proposed method achieves high accuracy on mainstream datasets in the MOT Challenge. This offers new perspectives and solutions for research in the field of pedestrian multi-object tracking.
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2405.00023.pdf' target='_blank'>https://arxiv.org/pdf/2405.00023.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>A. Hossam, A. Ramadan, M. Magdy, R. Abdelwahab, S. Ashraf, Z. Mohamed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.00023">Revolutionizing Retail Analytics: Advancing Inventory and Customer Insight with AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In response to the significant challenges facing the retail sector, including inefficient queue management, poor demand forecasting, and ineffective marketing, this paper introduces an innovative approach utilizing cutting-edge machine learning technologies. We aim to create an advanced smart retail analytics system (SRAS), leveraging these technologies to enhance retail efficiency and customer engagement. To enhance customer tracking capabilities, a new hybrid architecture is proposed integrating several predictive models. In the first stage of the proposed hybrid architecture for customer tracking, we fine-tuned the YOLOV8 algorithm using a diverse set of parameters, achieving exceptional results across various performance metrics. This fine-tuning process utilized actual surveillance footage from retail environments, ensuring its practical applicability. In the second stage, we explored integrating two sophisticated object-tracking models, BOT-SORT and ByteTrack, with the labels detected by YOLOV8. This integration is crucial for tracing customer paths within stores, which facilitates the creation of accurate visitor counts and heat maps. These insights are invaluable for understanding consumer behavior and improving store operations. To optimize inventory management, we delved into various predictive models, optimizing and contrasting their performance against complex retail data patterns. The GRU model, with its ability to interpret time-series data with long-range temporal dependencies, consistently surpassed other models like Linear Regression, showing 2.873% and 29.31% improvements in R2-score and mAPE, respectively.
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2404.12133.pdf' target='_blank'>https://arxiv.org/pdf/2404.12133.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Julia Vinogradova, Gabor Fodor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.12133">On Target Detection in the Presence of Clutter in Joint Communication and Sensing Cellular Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent works on joint communication and sensing (JCAS) cellular networks have proposed to use time division mode (TDM) and concurrent mode (CM), as alternative methods for sharing the resources between communication and sensing signals. While the performance of these JCAS schemes for object tracking and parameter estimation has been studied in previous works, their performance on target detection in the presence of clutter has not been analyzed. In this paper, we propose a detection scheme for estimating the number of targets in JCAS cellular networks that employ TDM or CM resource sharing. The proposed detection method allows for the presence of clutter and/or temporally correlated noise. This scheme is studied with respect to the JCAS trade-off parameters that allow to control the time slots in TDM and the power resources in CM allocated to sensing and communications. The performance of two fundamental transmit beamforming schemes, typical for JCAS, is compared in terms of the receiver operating characteristics curves. Our results indicate that in general the TDM scheme gives a somewhat better detection performance compared to the CM scheme, although both schemes outperform existing approaches provided that their respective trade-off parameters are tuned properly.
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2404.10992.pdf' target='_blank'>https://arxiv.org/pdf/2404.10992.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Z. Alam, Zeeshan Kaleem, Sousso Kelouwani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.10992">How to deal with glare for improved perception of Autonomous Vehicles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision sensors are versatile and can capture a wide range of visual cues, such as color, texture, shape, and depth. This versatility, along with the relatively inexpensive availability of machine vision cameras, played an important role in adopting vision-based environment perception systems in autonomous vehicles (AVs). However, vision-based perception systems can be easily affected by glare in the presence of a bright source of light, such as the sun or the headlights of the oncoming vehicle at night or simply by light reflecting off snow or ice-covered surfaces; scenarios encountered frequently during driving. In this paper, we investigate various glare reduction techniques, including the proposed saturated pixel-aware glare reduction technique for improved performance of the computer vision (CV) tasks employed by the perception layer of AVs. We evaluate these glare reduction methods based on various performance metrics of the CV algorithms used by the perception layer. Specifically, we considered object detection, object recognition, object tracking, depth estimation, and lane detection which are crucial for autonomous driving. The experimental findings validate the efficacy of the proposed glare reduction approach, showcasing enhanced performance across diverse perception tasks and remarkable resilience against varying levels of glare.
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2404.07467.pdf' target='_blank'>https://arxiv.org/pdf/2404.07467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kashish Jain, Manthan Juthani, Jash Jain, Anant V. Nimkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07467">Trashbusters: Deep Learning Approach for Litter Detection and Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The illegal disposal of trash is a major public health and environmental concern. Disposing of trash in unplanned places poses serious health and environmental risks. We should try to restrict public trash cans as much as possible. This research focuses on automating the penalization of litterbugs, addressing the persistent problem of littering in public places. Traditional approaches relying on manual intervention and witness reporting suffer from delays, inaccuracies, and anonymity issues. To overcome these challenges, this paper proposes a fully automated system that utilizes surveillance cameras and advanced computer vision algorithms for litter detection, object tracking, and face recognition. The system accurately identifies and tracks individuals engaged in littering activities, attaches their identities through face recognition, and enables efficient enforcement of anti-littering policies. By reducing reliance on manual intervention, minimizing human error, and providing prompt identification, the proposed system offers significant advantages in addressing littering incidents. The primary contribution of this research lies in the implementation of the proposed system, leveraging advanced technologies to enhance surveillance operations and automate the penalization of litterbugs.
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2403.18193.pdf' target='_blank'>https://arxiv.org/pdf/2403.18193.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiming Wang, Yongqiang Bai, Hongxing Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18193">Middle Fusion and Multi-Stage, Multi-Form Prompts for Robust RGB-T Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>RGB-T tracking, a vital downstream task of object tracking, has made remarkable progress in recent years. Yet, it remains hindered by two major challenges: 1) the trade-off between performance and efficiency; 2) the scarcity of training data. To address the latter challenge, some recent methods employ prompts to fine-tune pre-trained RGB tracking models and leverage upstream knowledge in a parameter-efficient manner. However, these methods inadequately explore modality-independent patterns and disregard the dynamic reliability of different modalities in open scenarios. We propose M3PT, a novel RGB-T prompt tracking method that leverages middle fusion and multi-modal and multi-stage visual prompts to overcome these challenges. We pioneer the use of the adjustable middle fusion meta-framework for RGB-T tracking, which could help the tracker balance the performance with efficiency, to meet various demands of application. Furthermore, based on the meta-framework, we utilize multiple flexible prompt strategies to adapt the pre-trained model to comprehensive exploration of uni-modal patterns and improved modeling of fusion-modal features in diverse modality-priority scenarios, harnessing the potential of prompt learning in RGB-T tracking. Evaluating on 6 existing challenging benchmarks, our method surpasses previous state-of-the-art prompt fine-tuning methods while maintaining great competitiveness against excellent full-parameter fine-tuning methods, with only 0.34M fine-tuned parameters.
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2403.07914.pdf' target='_blank'>https://arxiv.org/pdf/2403.07914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yushan Han, Kaer Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07914">ACTrack: Adding Spatio-Temporal Condition for Visual Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficiently modeling spatio-temporal relations of objects is a key challenge in visual object tracking (VOT). Existing methods track by appearance-based similarity or long-term relation modeling, resulting in rich temporal contexts between consecutive frames being easily overlooked. Moreover, training trackers from scratch or fine-tuning large pre-trained models needs more time and memory consumption. In this paper, we present ACTrack, a new tracking framework with additive spatio-temporal conditions. It preserves the quality and capabilities of the pre-trained Transformer backbone by freezing its parameters, and makes a trainable lightweight additive net to model spatio-temporal relations in tracking. We design an additive siamese convolutional network to ensure the integrity of spatial features and perform temporal sequence modeling to simplify the tracking pipeline. Experimental results on several benchmarks prove that ACTrack could balance training efficiency and tracking performance.
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2403.07635.pdf' target='_blank'>https://arxiv.org/pdf/2403.07635.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Angelos Dimakos, Daniel Woodhall, Seemal Asif
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.07635">A Study on Centralised and Decentralised Swarm Robotics Architecture for Part Delivery System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Drones are also known as UAVs are originally designed for military purposes. With the technological advances, they can be seen in most of the aspects of life from filming to logistics. The increased use of drones made it sometimes essential to form a collaboration between them to perform the task efficiently in a defined process. This paper investigates the use of a combined centralised and decentralised architecture for the collaborative operation of drones in a parts delivery scenario to enable and expedite the operation of the factories of the future. The centralised and decentralised approaches were extensively researched, with experimentation being undertaken to determine the appropriateness of each approach for this use-case. Decentralised control was utilised to remove the need for excessive communication during the operation of the drones, resulting in smoother operations. Initial results suggested that the decentralised approach is more appropriate for this use-case. The individual functionalities necessary for the implementation of a decentralised architecture were proven and assessed, determining that a combination of multiple individual functionalities, namely VSLAM, dynamic collision avoidance and object tracking, would give an appropriate solution for use in an industrial setting. A final architecture for the parts delivery system was proposed for future work, using a combined centralised and decentralised approach to combat the limitations inherent in each architecture.
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2403.02767.pdf' target='_blank'>https://arxiv.org/pdf/2403.02767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Huang, Shoudong Han, Mengyu He, Wenbo Zheng, Yuhao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02767">DeconfuseTrack:Dealing with Confusion for Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate data association is crucial in reducing confusion, such as ID switches and assignment errors, in multi-object tracking (MOT). However, existing advanced methods often overlook the diversity among trajectories and the ambiguity and conflicts present in motion and appearance cues, leading to confusion among detections, trajectories, and associations when performing simple global data association. To address this issue, we propose a simple, versatile, and highly interpretable data association approach called Decomposed Data Association (DDA). DDA decomposes the traditional association problem into multiple sub-problems using a series of non-learning-based modules and selectively addresses the confusion in each sub-problem by incorporating targeted exploitation of new cues. Additionally, we introduce Occlusion-aware Non-Maximum Suppression (ONMS) to retain more occluded detections, thereby increasing opportunities for association with trajectories and indirectly reducing the confusion caused by missed detections. Finally, based on DDA and ONMS, we design a powerful multi-object tracker named DeconfuseTrack, specifically focused on resolving confusion in MOT. Extensive experiments conducted on the MOT17 and MOT20 datasets demonstrate that our proposed DDA and ONMS significantly enhance the performance of several popular trackers. Moreover, DeconfuseTrack achieves state-of-the-art performance on the MOT17 and MOT20 test sets, significantly outperforms the baseline tracker ByteTrack in metrics such as HOTA, IDF1, AssA. This validates that our tracking design effectively reduces confusion caused by simple global association.
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2402.12968.pdf' target='_blank'>https://arxiv.org/pdf/2402.12968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Wang, Ruohui Zhang, Chenglin Chen, Min Yang, Yun Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12968">MapTrack: Tracking in the Map</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target. Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities. Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations. In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes. Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter. The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons. Trajectories of undetected targets that are still within the probability map are extended by state estimations directly. The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter. The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20. Despite its superior performance, our method remains simple, online, and real-time. The code will be open-sourced later.
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2401.17874.pdf' target='_blank'>https://arxiv.org/pdf/2401.17874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyan Zhang, Rahul Chaudhari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17874">VR-based generation of photorealistic synthetic data for training hand-object tracking models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images. To address these issues, we present "blender-hoisynth", an interactive synthetic data generator based on the Blender software. Blender-hoisynth can scalably generate and automatically annotate visual HOI training data. Other competing approaches usually generate synthetic HOI data compeletely without human input. While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent. With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware. The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it. We show that there is no significant degradation in the model performance despite the data replacement.
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2401.12182.pdf' target='_blank'>https://arxiv.org/pdf/2401.12182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Robinson, Michael Stein, Henry S. Owen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.12182">Tracking before detection using partial orders and optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article addresses the problem of multi-object tracking by using a non-deterministic model of target behaviors with hard constraints. To capture the evolution of target features as well as their locations, we permit objects to lie in a general topological target configuration space, rather than a Euclidean space. We obtain tracker performance bounds based on sample rates, and derive a flexible, agnostic tracking algorithm. We demonstrate our algorithm on two scenarios involving laboratory and field data.
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2401.07929.pdf' target='_blank'>https://arxiv.org/pdf/2401.07929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rakibul Karim Akanda, Joshua Reynolds, Treylin Jackson, Milijah Gray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.07929">Machine Learning Based Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning based object detection as well as tracking that object have been performed in this paper. The authors were able to set a range of interest (ROI) around an object using Open Computer Vision, better known as OpenCV. Next a tracking algorithm has been used to maintain tracking on an object while simultaneously operating two servo motors to keep the object centered in the frame. Detailed procedure and code are included in this paper.
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2312.16250.pdf' target='_blank'>https://arxiv.org/pdf/2312.16250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anqi Yi, Nantheera Anantrasirichai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16250">A Comprehensive Study of Object Tracking in Low-Light Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Accurate object tracking in low-light environments is crucial, particularly in surveillance and ethology applications. However, achieving this is significantly challenging due to the poor quality of captured sequences. Factors such as noise, color imbalance, and low contrast contribute to these challenges. This paper presents a comprehensive study examining the impact of these distortions on automatic object trackers. Additionally, we propose a solution to enhance tracking performance by integrating denoising and low-light enhancement methods into the transformer-based object tracking system. Experimental results show that the proposed tracker, trained with low-light synthetic datasets, outperforms both the vanilla MixFormer and Siam R-CNN.
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2312.11929.pdf' target='_blank'>https://arxiv.org/pdf/2312.11929.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamza Mukhtar, Muhammad Usman Ghani Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.11929">Transformer Network for Multi-Person Tracking and Re-Identification in Unconstrained Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) has profound applications in a variety of fields, including surveillance, sports analytics, self-driving, and cooperative robotics. Despite considerable advancements, existing MOT methodologies tend to falter when faced with non-uniform movements, occlusions, and appearance-reappearance scenarios of the objects. Recognizing this inadequacy, we put forward an integrated MOT method that not only marries object detection and identity linkage within a singular, end-to-end trainable framework but also equips the model with the ability to maintain object identity links over long periods of time. Our proposed model, named STMMOT, is built around four key modules: 1) candidate proposal generation, which generates object proposals via a vision-transformer encoder-decoder architecture that detects the object from each frame in the video; 2) scale variant pyramid, a progressive pyramid structure to learn the self-scale and cross-scale similarities in multi-scale feature maps; 3) spatio-temporal memory encoder, extracting the essential information from the memory associated with each object under tracking; and 4) spatio-temporal memory decoder, simultaneously resolving the tasks of object detection and identity association for MOT. Our system leverages a robust spatio-temporal memory module that retains extensive historical observations and effectively encodes them using an attention-based aggregator. The uniqueness of STMMOT lies in representing objects as dynamic query embeddings that are updated continuously, which enables the prediction of object states with attention mechanisms and eradicates the need for post-processing.
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2312.10922.pdf' target='_blank'>https://arxiv.org/pdf/2312.10922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Ahmed Al Muzaddid, William J. Beksi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.10922">NTrack: A Multiple-Object Tracker and Dataset for Infield Cotton Boll Counting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In agriculture, automating the accurate tracking of fruits, vegetables, and fiber is a very tough problem. The issue becomes extremely challenging in dynamic field environments. Yet, this information is critical for making day-to-day agricultural decisions, assisting breeding programs, and much more. To tackle this dilemma, we introduce NTrack, a novel multiple object tracking framework based on the linear relationship between the locations of neighboring tracks. NTrack computes dense optical flow and utilizes particle filtering to guide each tracker. Correspondences between detections and tracks are found through data association via direct observations and indirect cues, which are then combined to obtain an updated observation. Our modular multiple object tracking system is independent of the underlying detection method, thus allowing for the interchangeable use of any off-the-shelf object detector. We show the efficacy of our approach on the task of tracking and counting infield cotton bolls. Experimental results show that our system exceeds contemporary tracking and cotton boll-based counting methods by a large margin. Furthermore, we publicly release the first annotated cotton boll video dataset to the research community.
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2311.03029.pdf' target='_blank'>https://arxiv.org/pdf/2311.03029.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mincheul Kang, Junhyoung Ha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.03029">Obstacle- and Occlusion-Responsive Visual Tracking Control for Redundant Manipulators using Reachability Measure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A vision system attached to a manipulator excels at tracing a moving target object while effectively handling obstacles, overcoming limitations arising from the camera's confined field of view and occluded line of sight. Meanwhile, the manipulator may encounter certain challenges, including restricted motion due to kinematic constraints and the risk of colliding with external obstacles. These challenges are typically addressed by assigning multiple task objectives to the manipulator. However, doing so can cause an increased risk of driving the manipulator to its kinematic limits, leading to failures in object tracking or obstacle avoidance. To address this issue, we propose a novel visual tracking control method for a redundant manipulator that takes the kinematic constraints into account via a reachability measure. Our method employs an optimization-based controller that considers object tracking, occlusion avoidance, collision avoidance, and the kinematic constraints represented by the reachability measure. Subsequently, it determines a suitable joint configuration through real-time inverse kinematics, accounting for dynamic obstacle avoidance and the continuity of joint configurations. To validate our approach, we conducted simulations and hardware experiments involving a moving target and dynamic obstacles. The results of our evaluations highlight the significance of incorporating the reachability measure.
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2311.02642.pdf' target='_blank'>https://arxiv.org/pdf/2311.02642.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huining Li, Yalong Jiang, Xianlin Zeng, Feng Li, Zhipeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02642">An Approach for Multi-Object Tracking with Two-Stage Min-Cost Flow</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The minimum network flow algorithm is widely used in multi-target tracking. However, the majority of the present methods concentrate exclusively on minimizing cost functions whose values may not indicate accurate solutions under occlusions. In this paper, by exploiting the properties of tracklets intersections and low-confidence detections, we develop a two-stage tracking pipeline with an intersection mask that can accurately locate inaccurate tracklets which are corrected in the second stage. Specifically, we employ the minimum network flow algorithm with high-confidence detections as input in the first stage to obtain the candidate tracklets that need correction. Then we leverage the intersection mask to accurately locate the inaccurate parts of candidate tracklets. The second stage utilizes low-confidence detections that may be attributed to occlusions for correcting inaccurate tracklets. This process constructs a graph of nodes in inaccurate tracklets and low-confidence nodes and uses it for the second round of minimum network flow calculation. We perform sufficient experiments on popular MOT benchmark datasets and achieve 78.4 MOTA on the test set of MOT16, 79.2 on MOT17, and 76.4 on MOT20, which shows that the proposed method is effective.
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2310.14345.pdf' target='_blank'>https://arxiv.org/pdf/2310.14345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Himanshu Sahu, Kallol Sen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14345">Quantum-walk search in motion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In quantum computing, the quantum walk search algorithm is designed for locating fixed marked nodes within a graph. However, when multiple marked nodes exist, the conventional search algorithm lacks the capacity to simultaneously amplify the marked nodes as well as identify the correct chronological ordering between the marked nodes, if any. To address this limitation, we explore a potential extension of the algorithm by introducing additional quantum states to label the marked nodes. The labels resolve the ambiguity of simultaneous amplification of the marked nodes. Additionally, by associating the label states with a chronological ordering, we can extend the algorithm to track a moving particle on a two-dimensional surface. Our algorithm efficiently searches for the trajectory of the particle and is supported by a proposed quantum circuit. This concept holds promise for a range of applications, from real-time object tracking to network management and routing.
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2310.05171.pdf' target='_blank'>https://arxiv.org/pdf/2310.05171.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Zhao, Gongming Wei, Yang Xiao, Xianglei Xing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05171">Multi-Ship Tracking by Robust Similarity metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-ship tracking (MST) as a core technology has been proven to be applied to situational awareness at sea and the development of a navigational system for autonomous ships. Despite impressive tracking outcomes achieved by multi-object tracking (MOT) algorithms for pedestrian and vehicle datasets, these models and techniques exhibit poor performance when applied to ship datasets. Intersection of Union (IoU) is the most popular metric for computing similarity used in object tracking. The low frame rates and severe image shake caused by wave turbulence in ship datasets often result in minimal, or even zero, Intersection of Union (IoU) between the predicted and detected bounding boxes. This issue contributes to frequent identity switches of tracked objects, undermining the tracking performance. In this paper, we address the weaknesses of IoU by incorporating the smallest convex shapes that enclose both the predicted and detected bounding boxes. The calculation of the tracking version of IoU (TIoU) metric considers not only the size of the overlapping area between the detection bounding box and the prediction box, but also the similarity of their shapes. Through the integration of the TIoU into state-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, we consistently achieve improvements in the tracking performance of these frameworks.
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2310.01288.pdf' target='_blank'>https://arxiv.org/pdf/2310.01288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianzhong Liu, Holger Caesar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.01288">Offline Tracking with Object Permanence</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To reduce the expensive labor cost for manual labeling autonomous driving datasets, an alternative is to automatically label the datasets using an offline perception system. However, objects might be temporally occluded. Such occlusion scenarios in the datasets are common yet underexplored in offline auto labeling. In this work, we propose an offline tracking model that focuses on occluded object tracks. It leverages the concept of object permanence which means objects continue to exist even if they are not observed anymore. The model contains three parts: a standard online tracker, a re-identification (Re-ID) module that associates tracklets before and after occlusion, and a track completion module that completes the fragmented tracks. The Re-ID module and the track completion module use the vectorized map as one of the inputs to refine the tracking results with occlusion. The model can effectively recover the occluded object trajectories. It achieves state-of-the-art performance in 3D multi-object tracking by significantly improving the original online tracking result, showing its potential to be applied in offline auto labeling as a useful plugin to improve tracking by recovering occlusions.
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2309.15703.pdf' target='_blank'>https://arxiv.org/pdf/2309.15703.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rama Krishna Kandukuri, Michael Strecke, Joerg Stueckler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15703">Physics-Based Rigid Body Object Tracking and Friction Filtering From RGB-D Videos</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physics-based understanding of object interactions from sensory observations is an essential capability in augmented reality and robotics. It enables to capture the properties of a scene for simulation and control. In this paper, we propose a novel approach for real-to-sim which tracks rigid objects in 3D from RGB-D images and infers physical properties of the objects. We use a differentiable physics simulation as state-transition model in an Extended Kalman Filter which can model contact and friction for arbitrary mesh-based shapes and in this way estimate physically plausible trajectories. We demonstrate that our approach can filter position, orientation, velocities, and concurrently can estimate the coefficient of friction of the objects. We analyze our approach on various sliding scenarios in synthetic image sequences of single objects and colliding objects. We also demonstrate and evaluate our approach on a real-world dataset. We make our novel benchmark datasets publicly available to foster future research in this novel problem setting and comparison with our method.
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2309.03247.pdf' target='_blank'>https://arxiv.org/pdf/2309.03247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Leo, Kurban Ubul, ShengJie Cheng, Michael Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03247">Robust Visual Tracking by Motion Analyzing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, Video Object Segmentation (VOS) has emerged as a complementary method to Video Object Tracking (VOT). VOS focuses on classifying all the pixels around the target, allowing for precise shape labeling, while VOT primarily focuses on the approximate region where the target might be. However, traditional segmentation modules usually classify pixels frame by frame, disregarding information between adjacent frames.
  In this paper, we propose a new algorithm that addresses this limitation by analyzing the motion pattern using the inherent tensor structure. The tensor structure, obtained through Tucker2 tensor decomposition, proves to be effective in describing the target's motion. By incorporating this information, we achieved competitive results on Four benchmarks LaSOT\cite{fan2019lasot}, AVisT\cite{noman2022avist}, OTB100\cite{7001050}, and GOT-10k\cite{huang2019got} LaSOT\cite{fan2019lasot} with SOTA. Furthermore, the proposed tracker is capable of real-time operation, adding value to its practical application.
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2308.13229.pdf' target='_blank'>https://arxiv.org/pdf/2308.13229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng-Che Cheng, Min-Xuan Qiu, Chen-Kuo Chiang, Shang-Hong Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13229">ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Camera Multi-Object Tracking (MC-MOT) utilizes information from multiple views to better handle problems with occlusion and crowded scenes. Recently, the use of graph-based approaches to solve tracking problems has become very popular. However, many current graph-based methods do not effectively utilize information regarding spatial and temporal consistency. Instead, they rely on single-camera trackers as input, which are prone to fragmentation and ID switch errors. In this paper, we propose a novel reconfigurable graph model that first associates all detected objects across cameras spatially before reconfiguring it into a temporal graph for Temporal Association. This two-stage association approach enables us to extract robust spatial and temporal-aware features and address the problem with fragmented tracklets. Furthermore, our model is designed for online tracking, making it suitable for real-world applications. Experimental results show that the proposed graph model is able to extract more discriminating features for object tracking, and our model achieves state-of-the-art performance on several public datasets.
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2308.12723.pdf' target='_blank'>https://arxiv.org/pdf/2308.12723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhifei Li, Yan Liang, Linfeng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12723">Distributed Extended Object Tracking Using Coupled Velocity Model from WLS Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This study proposes a coupled velocity model (CVM) that establishes the relation between the orientation and velocity using their correlation, avoiding that the existing extended object tracking (EOT) models treat them as two independent quantities. As a result, CVM detects the mismatch between the prior dynamic model and actual motion pattern to correct the filtering gain, and simultaneously becomes a nonlinear and state-coupled model with multiplicative noise. The study considers CVM to design a feasible distributed weighted least squares (WLS) filter. The WLS criterion requires a linear state-space model containing only additive noise about the estimated state. To meet the requirement, we derive such two separate pseudo-linearized models by using the first-order Taylor series expansion. The separation is merely in form, and the estimates of interested states are embedded as parameters into each other's model, which implies that their interdependency is still preserved in the iterative operation of two linear filters. With the two models, we first propose a centralized WLS filter by converting the measurements from all nodes into a summation form. Then, a distributed consensus scheme, which directly performs an inner iteration on the priors across different nodes, is proposed to incorporate the cross-covariances between nodes. Under the consensus scheme, a distributed WLS filter over a realistic network with ``naive'' node is developed by proper weighting of the priors and measurements. Finally, the performance of proposed filters in terms of accuracy, robustness, and consistency is testified under different prior situations.
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2308.10671.pdf' target='_blank'>https://arxiv.org/pdf/2308.10671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Juan Sandino, Peter A. Caccetta, Conrad Sanderson, Frederic Maire, Felipe Gonzalez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10671">Reducing Object Detection Uncertainty from RGB and Thermal Data for UAV Outdoor Surveillance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Unmanned Aerial Vehicles (UAVs) have resulted in their quick adoption for wide a range of civilian applications, including precision agriculture, biosecurity, disaster monitoring and surveillance. UAVs offer low-cost platforms with flexible hardware configurations, as well as an increasing number of autonomous capabilities, including take-off, landing, object tracking and obstacle avoidance. However, little attention has been paid to how UAVs deal with object detection uncertainties caused by false readings from vision-based detectors, data noise, vibrations, and occlusion. In most situations, the relevance and understanding of these detections are delegated to human operators, as many UAVs have limited cognition power to interact autonomously with the environment. This paper presents a framework for autonomous navigation under uncertainty in outdoor scenarios for small UAVs using a probabilistic-based motion planner. The framework is evaluated with real flight tests using a sub 2 kg quadrotor UAV and illustrated in victim finding Search and Rescue (SAR) case study in a forest/bushland. The navigation problem is modelled using a Partially Observable Markov Decision Process (POMDP), and solved in real time onboard the small UAV using Augmented Belief Trees (ABT) and the TAPIR toolkit. Results from experiments using colour and thermal imagery show that the proposed motion planner provides accurate victim localisation coordinates, as the UAV has the flexibility to interact with the environment and obtain clearer visualisations of any potential victims compared to the baseline motion planner. Incorporating this system allows optimised UAV surveillance operations by diminishing false positive readings from vision-based object detectors.
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2308.10604.pdf' target='_blank'>https://arxiv.org/pdf/2308.10604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwook Lee, Wonjun Choi, Seohyung Lee, ByungIn Yoo, Eunho Yang, Seongju Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10604">BackTrack: Robust template update via Backward Tracking of candidate template</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variations of target appearance such as deformations, illumination variance, occlusion, etc., are the major challenges of visual object tracking that negatively impact the performance of a tracker. An effective method to tackle these challenges is template update, which updates the template to reflect the change of appearance in the target object during tracking. However, with template updates, inadequate quality of new templates or inappropriate timing of updates may induce a model drift problem, which severely degrades the tracking performance. Here, we propose BackTrack, a robust and reliable method to quantify the confidence of the candidate template by backward tracking it on the past frames. Based on the confidence score of candidates from BackTrack, we can update the template with a reliable candidate at the right time while rejecting unreliable candidates. BackTrack is a generic template update scheme and is applicable to any template-based trackers. Extensive experiments on various tracking benchmarks verify the effectiveness of BackTrack over existing template update algorithms, as it achieves SOTA performance on various tracking benchmarks.
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2308.08753.pdf' target='_blank'>https://arxiv.org/pdf/2308.08753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lubing Zhou, Xiaoli Meng, Yiluan Guo, Jiong Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.08753">BOTT: Box Only Transformer Tracker for 3D Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking 3D objects is an important task in autonomous driving. Classical Kalman Filtering based methods are still the most popular solutions. However, these methods require handcrafted designs in motion modeling and can not benefit from the growing data amounts. In this paper, Box Only Transformer Tracker (BOTT) is proposed to learn to link 3D boxes of the same object from the different frames, by taking all the 3D boxes in a time window as input. Specifically, transformer self-attention is applied to exchange information between all the boxes to learn global-informative box embeddings. The similarity between these learned embeddings can be used to link the boxes of the same object. BOTT can be used for both online and offline tracking modes seamlessly. Its simplicity enables us to significantly reduce engineering efforts required by traditional Kalman Filtering based methods. Experiments show BOTT achieves competitive performance on two largest 3D MOT benchmarks: 69.9 and 66.7 AMOTA on nuScenes validation and test splits, respectively, 56.45 and 59.57 MOTA L2 on Waymo Open Dataset validation and test splits, respectively. This work suggests that tracking 3D objects by learning features directly from 3D boxes using transformers is a simple yet effective way.
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2308.07723.pdf' target='_blank'>https://arxiv.org/pdf/2308.07723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruican Xia, Hailong Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07723">Extended Preintegration for Relative State Estimation of Leader-Follower Platform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Relative state estimation using exteroceptive sensors suffers from limitations of the field of view (FOV) and false detection, that the proprioceptive sensor (IMU) data are usually engaged to compensate. Recently ego-motion constraint obtained by Inertial measurement unit (IMU) preintegration has been extensively used in simultaneous localization and mapping (SLAM) to alleviate the computation burden. This paper introduces an extended preintegration incorporating the IMU preintegration of two platforms to formulate the motion constraint of relative state. One merit of this analytic constraint is that it can be seamlessly integrated into the unified graph optimization framework to implement the relative state estimation in a high-performance real-time tracking thread, another point is a full smoother design with this precise constraint to optimize the 3D coordinate and refine the state for the refinement thread. We compare extensively in simulations the proposed algorithms with two existing approaches to confirm our outperformance. In the real virtual reality (VR) application design with the proposed estimator, we properly realize the visual tracking of the six degrees of freedom (6DoF) controller suitable for almost all scenarios, including the challenging environment with missing features, light mutation, dynamic scenes, etc. The demo video is at https://www.youtube.com/watch?v=0idb9Ls2iAM. For the benefit of the community, we make the source code public.
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2307.14591.pdf' target='_blank'>https://arxiv.org/pdf/2307.14591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchao Huang, Xiaoqi He Yebo Wu, Sheng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.14591">The detection and rectification for identity-switch based on unfalsified control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The purpose of multi-object tracking (MOT) is to continuously track and identify objects detected in videos. Currently, most methods for multi-object tracking model the motion information and combine it with appearance information to determine and track objects. In this paper, unfalsified control is employed to address the ID-switch problem in multi-object tracking. We establish sequences of appearance information variations for the trajectories during the tracking process and design a detection and rectification module specifically for ID-switch detection and recovery. We also propose a simple and effective strategy to address the issue of ambiguous matching of appearance information during the data association process. Experimental results on publicly available MOT datasets demonstrate that the tracker exhibits excellent effectiveness and robustness in handling tracking errors caused by occlusions and rapid movements.
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2307.05874.pdf' target='_blank'>https://arxiv.org/pdf/2307.05874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiroshi Fukui, Taiki Miyagawa, Yusuke Morishita
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.05874">Multi-Object Tracking as Attention Mechanism</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a conceptually simple and thus fast multi-object tracking (MOT) model that does not require any attached modules, such as the Kalman filter, Hungarian algorithm, transformer blocks, or graph networks. Conventional MOT models are built upon the multi-step modules listed above, and thus the computational cost is high. Our proposed end-to-end MOT model, \textit{TicrossNet}, is composed of a base detector and a cross-attention module only. As a result, the overhead of tracking does not increase significantly even when the number of instances ($N_t$) increases. We show that TicrossNet runs \textit{in real-time}; specifically, it achieves 32.6 FPS on MOT17 and 31.0 FPS on MOT20 (Tesla V100), which includes as many as $>$100 instances per frame. We also demonstrate that TicrossNet is robust to $N_t$; thus, it does not have to change the size of the base detector, depending on $N_t$, as is often done by other models for real-time processing.
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2307.03602.pdf' target='_blank'>https://arxiv.org/pdf/2307.03602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Matvei Panteleev, Houari Bettahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03602">Depth Estimation Analysis of Orthogonally Divergent Fisheye Cameras with Distortion Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stereo vision systems have become popular in computer vision applications, such as 3D reconstruction, object tracking, and autonomous navigation. However, traditional stereo vision systems that use rectilinear lenses may not be suitable for certain scenarios due to their limited field of view. This has led to the popularity of vision systems based on one or multiple fisheye cameras in different orientations, which can provide a field of view of 180x180 degrees or more. However, fisheye cameras introduce significant distortion at the edges that affects the accuracy of stereo matching and depth estimation. To overcome these limitations, this paper proposes a method for distortion-removal and depth estimation analysis for stereovision system using orthogonally divergent fisheye cameras (ODFC). The proposed method uses two virtual pinhole cameras (VPC), each VPC captures a small portion of the original view and presents it without any lens distortions, emulating the behavior of a pinhole camera. By carefully selecting the captured regions, it is possible to create a stereo pair using two VPCs. The performance of the proposed method is evaluated in both simulation using virtual environment and experiments using real cameras and their results compared to stereo cameras with parallel optical axes. The results demonstrate the effectiveness of the proposed method in terms of distortion removal and depth estimation accuracy.
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2307.01893.pdf' target='_blank'>https://arxiv.org/pdf/2307.01893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abbas TÃ¼rkoÄlu, Erdem AkagÃ¼ndÃ¼z
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.01893">EANet: Enhanced Attribute-based RGBT Tracker Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tracking objects can be a difficult task in computer vision, especially when faced with challenges such as occlusion, changes in lighting, and motion blur. Recent advances in deep learning have shown promise in challenging these conditions. However, most deep learning-based object trackers only use visible band (RGB) images. Thermal infrared electromagnetic waves (TIR) can provide additional information about an object, including its temperature, when faced with challenging conditions. We propose a deep learning-based image tracking approach that fuses RGB and thermal images (RGBT). The proposed model consists of two main components: a feature extractor and a tracker. The feature extractor encodes deep features from both the RGB and the TIR images. The tracker then uses these features to track the object using an enhanced attribute-based architecture. We propose a fusion of attribute-specific feature selection with an aggregation module. The proposed methods are evaluated on the RGBT234 \cite{LiCLiang2018} and LasHeR \cite{LiLasher2021} datasets, which are the most widely used RGBT object-tracking datasets in the literature. The results show that the proposed system outperforms state-of-the-art RGBT object trackers on these datasets, with a relatively smaller number of parameters.
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2306.17000.pdf' target='_blank'>https://arxiv.org/pdf/2306.17000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ce Zhang, Chengjie Zhang, Yiluan Guo, Lingji Chen, Michael Happold
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17000">MotionTrack: End-to-End Transformer-based Multi-Object Tracing with LiDAR-Camera Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple Object Tracking (MOT) is crucial to autonomous vehicle perception. End-to-end transformer-based algorithms, which detect and track objects simultaneously, show great potential for the MOT task. However, most existing methods focus on image-based tracking with a single object category. In this paper, we propose an end-to-end transformer-based MOT algorithm (MotionTrack) with multi-modality sensor inputs to track objects with multiple classes. Our objective is to establish a transformer baseline for the MOT in an autonomous driving environment. The proposed algorithm consists of a transformer-based data association (DA) module and a transformer-based query enhancement module to achieve MOT and Multiple Object Detection (MOD) simultaneously. The MotionTrack and its variations achieve better results (AMOTA score at 0.55) on the nuScenes dataset compared with other classical baseline models, such as the AB3DMOT, the CenterTrack, and the probabilistic 3D Kalman filter. In addition, we prove that a modified attention mechanism can be utilized for DA to accomplish the MOT, and aggregate history features to enhance the MOD performance.
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2306.16890.pdf' target='_blank'>https://arxiv.org/pdf/2306.16890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ãngel F. GarcÃ­a-FernÃ¡ndez, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.16890">Trajectory Poisson multi-Bernoulli mixture filter for traffic monitoring using a drone</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a multi-object tracking (MOT) algorithm for traffic monitoring using a drone equipped with optical and thermal cameras. Object detections on the images are obtained using a neural network for each type of camera. The cameras are modelled as direction-of-arrival (DOA) sensors. Each DOA detection follows a von-Mises Fisher distribution, whose mean direction is obtain by projecting a vehicle position on the ground to the camera. We then use the trajectory Poisson multi-Bernoulli mixture filter (TPMBM), which is a Bayesian MOT algorithm, to optimally estimate the set of vehicle trajectories. We have also developed a parameter estimation algorithm for the measurement model. We have tested the accuracy of the resulting TPMBM filter in synthetic and experimental data sets.
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2306.08560.pdf' target='_blank'>https://arxiv.org/pdf/2306.08560.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John Lloyd, Nathan Lepora
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.08560">A pose and shear-based tactile robotic system for object tracking, surface following and object pushing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tactile perception is a crucial sensing modality in robotics, particularly in scenarios that require precise manipulation and safe interaction with other objects. Previous research in this area has focused extensively on tactile perception of contact poses as this is an important capability needed for tasks such as traversing an object's surface or edge, manipulating an object, or pushing an object along a predetermined path. Another important capability needed for tasks such as object tracking and manipulation is estimation of post-contact shear but this has received much less attention. Indeed, post-contact shear has often been considered a "nuisance variable" and is removed if possible because it can have an adverse effect on other types of tactile perception such as contact pose estimation. This paper proposes a tactile robotic system that can simultaneously estimate both the contact pose and post-contact shear, and use this information to control its interaction with other objects. Moreover, our new system is capable of interacting with other objects in a smooth and continuous manner, unlike the stepwise, position-controlled systems we have used in the past. We demonstrate the capabilities of our new system using several different controller configurations, on tasks including object tracking, surface following, single-arm object pushing, and dual-arm object pushing.
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2305.19557.pdf' target='_blank'>https://arxiv.org/pdf/2305.19557.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Subhroshekhar Ghosh, Aaron Y. R. Low, Yong Sheng Soh, Zhuohang Feng, Brendan K. Y. Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.19557">Dictionary Learning under Symmetries via Group Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The dictionary learning problem can be viewed as a data-driven process to learn a suitable transformation so that data is sparsely represented directly from example data. In this paper, we examine the problem of learning a dictionary that is invariant under a pre-specified group of transformations. Natural settings include Cryo-EM, multi-object tracking, synchronization, pose estimation, etc. We specifically study this problem under the lens of mathematical representation theory. Leveraging the power of non-abelian Fourier analysis for functions over compact groups, we prescribe an algorithmic recipe for learning dictionaries that obey such invariances. We relate the dictionary learning problem in the physical domain, which is naturally modelled as being infinite dimensional, with the associated computational problem, which is necessarily finite dimensional. We establish that the dictionary learning problem can be effectively understood as an optimization instance over certain matrix orbitopes having a particular block-diagonal structure governed by the irreducible representations of the group of symmetries. This perspective enables us to introduce a band-limiting procedure which obtains dimensionality reduction in applications. We provide guarantees for our computational ansatz to provide a desirable dictionary learning outcome. We apply our paradigm to investigate the dictionary learning problem for the groups SO(2) and SO(3). While the SO(2)-orbitope admits an exact spectrahedral description, substantially less is understood about the SO(3)-orbitope. We describe a tractable spectrahedral outer approximation of the SO(3)-orbitope, and contribute an alternating minimization paradigm to perform optimization in this setting. We provide numerical experiments to highlight the efficacy of our approach in learning SO(3)-invariant dictionaries, both on synthetic and on real world data.
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2305.00597.pdf' target='_blank'>https://arxiv.org/pdf/2305.00597.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonardo de Lellis Rossi, Leticia Mara Berto, Eric Rohmer, Paula Paro Costa, Ricardo Ribeiro Gudwin, Esther Luna Colombini, Alexandre da Silva Simoes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00597">Incremental procedural and sensorimotor learning in cognitive humanoid robots</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to automatically learn movements and behaviors of increasing complexity is a long-term goal in autonomous systems. Indeed, this is a very complex problem that involves understanding how knowledge is acquired and reused by humans as well as proposing mechanisms that allow artificial agents to reuse previous knowledge. Inspired by Jean Piaget's theory's first three sensorimotor substages, this work presents a cognitive agent based on CONAIM (Conscious Attention-Based Integrated Model) that can learn procedures incrementally. Throughout the paper, we show the cognitive functions required in each substage and how adding new functions helps address tasks previously unsolved by the agent. Experiments were conducted with a humanoid robot in a simulated environment modeled with the Cognitive Systems Toolkit (CST) performing an object tracking task. The system is modeled using a single procedural learning mechanism based on Reinforcement Learning. The increasing agent's cognitive complexity is managed by adding new terms to the reward function for each learning phase. Results show that this approach is capable of solving complex tasks incrementally.
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2304.06114.pdf' target='_blank'>https://arxiv.org/pdf/2304.06114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob Meilleur, Guillaume-Alexandre Bilodeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06114">TopTrack: Tracking Objects By Their Top</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, the joint detection-and-tracking paradigm has been a very popular way of tackling the multi-object tracking (MOT) task. Many of the methods following this paradigm use the object center keypoint for detection. However, we argue that the center point is not optimal since it is often not visible in crowded scenarios, which results in many missed detections when the objects are partially occluded. We propose TopTrack, a joint detection-and-tracking method that uses the top of the object as a keypoint for detection instead of the center because it is more often visible. Furthermore, TopTrack processes consecutive frames in separate streams in order to facilitate training. We performed experiments to show that using the object top as a keypoint for detection can reduce the amount of missed detections, which in turn leads to more complete trajectories and less lost trajectories. TopTrack manages to achieve competitive results with other state-of-the-art trackers on two MOT benchmarks.
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2303.12760.pdf' target='_blank'>https://arxiv.org/pdf/2303.12760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Na, Varuna De-Silva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.12760">Uncertainty Aware Active Learning for Reconfiguration of Pre-trained Deep Object-Detection Networks for New Target Domains</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection is one of the most important and fundamental aspects of computer vision tasks, which has been broadly utilized in pose estimation, object tracking and instance segmentation models. To obtain training data for object detection model efficiently, many datasets opt to obtain their unannotated data in video format and the annotator needs to draw a bounding box around each object in the images. Annotating every frame from a video is costly and inefficient since many frames contain very similar information for the model to learn from. How to select the most informative frames from a video to annotate has become a highly practical task to solve but attracted little attention in research. In this paper, we proposed a novel active learning algorithm for object detection models to tackle this problem. In the proposed active learning algorithm, both classification and localization informativeness of unlabelled data are measured and aggregated. Utilizing the temporal information from video frames, two novel localization informativeness measurements are proposed. Furthermore, a weight curve is proposed to avoid querying adjacent frames. Proposed active learning algorithm with multiple configurations was evaluated on the MuPoTS dataset and FootballPD dataset.
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2303.09668.pdf' target='_blank'>https://arxiv.org/pdf/2303.09668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukuan Zhang, Yunhua Jia, Housheng Xie, Mengzhen Li, Limin Zhao, Yang Yang, Shan Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09668">Rt-Track: Robust Tricks for Multi-Pedestrian Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object tracking is divided into single-object tracking (SOT) and multi-object tracking (MOT). MOT aims to maintain the identities of multiple objects across a series of continuous video sequences. In recent years, MOT has made rapid progress. However, modeling the motion and appearance models of objects in complex scenes still faces various challenging issues. In this paper, we design a novel direction consistency method for smooth trajectory prediction (STP-DC) to increase the modeling of motion information and overcome the lack of robustness in previous methods in complex scenes. Existing methods use pedestrian re-identification (Re-ID) to model appearance, however, they extract more background information which lacks discriminability in occlusion and crowded scenes. We propose a hyper-grain feature embedding network (HG-FEN) to enhance the modeling of appearance models, thus generating robust appearance descriptors. We also proposed other robustness techniques, including CF-ECM for storing robust appearance information and SK-AS for improving association accuracy. To achieve state-of-the-art performance in MOT, we propose a robust tracker named Rt-track, incorporating various tricks and techniques. It achieves 79.5 MOTA, 76.0 IDF1 and 62.1 HOTA on the test set of MOT17.Rt-track also achieves 77.9 MOTA, 78.4 IDF1 and 63.3 HOTA on MOT20, surpassing all published methods.
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2303.08444.pdf' target='_blank'>https://arxiv.org/pdf/2303.08444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huilan Luo, Zehua Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.08444">Real-time Multi-Object Tracking Based on Bi-directional Matching</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, anchor-free object detection models combined with matching algorithms are used to achieve real-time muti-object tracking and also ensure high tracking accuracy. However, there are still great challenges in multi-object tracking. For example, when most part of a target is occluded or the target just disappears from images temporarily, it often leads to tracking interruptions for most of the existing tracking algorithms. Therefore, this study offers a bi-directional matching algorithm for multi-object tracking that makes advantage of bi-directional motion prediction information to improve occlusion handling. A stranded area is used in the matching algorithm to temporarily store the objects that fail to be tracked. When objects recover from occlusions, our method will first try to match them with objects in the stranded area to avoid erroneously generating new identities, thus forming a more continuous trajectory. Experiments show that our approach can improve the multi-object tracking performance in the presence of occlusions. In addition, this study provides an attentional up-sampling module that not only assures tracking accuracy but also accelerates training speed. In the MOT17 challenge, the proposed algorithm achieves 63.4% MOTA, 55.3% IDF1, and 20.1 FPS tracking speed.
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2303.02857.pdf' target='_blank'>https://arxiv.org/pdf/2303.02857.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fateme Bahri, Nilanjan Ray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.02857">Weakly Supervised Realtime Dynamic Background Subtraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Background subtraction is a fundamental task in computer vision with numerous real-world applications, ranging from object tracking to video surveillance. Dynamic backgrounds poses a significant challenge here. Supervised deep learning-based techniques are currently considered state-of-the-art for this task. However, these methods require pixel-wise ground-truth labels, which can be time-consuming and expensive. In this work, we propose a weakly supervised framework that can perform background subtraction without requiring per-pixel ground-truth labels. Our framework is trained on a moving object-free sequence of images and comprises two networks. The first network is an autoencoder that generates background images and prepares dynamic background images for training the second network. The dynamic background images are obtained by thresholding the background-subtracted images. The second network is a U-Net that uses the same object-free video for training and the dynamic background images as pixel-wise ground-truth labels. During the test phase, the input images are processed by the autoencoder and U-Net, which generate background and dynamic background images, respectively. The dynamic background image helps remove dynamic motion from the background-subtracted image, enabling us to obtain a foreground image that is free of dynamic artifacts. To demonstrate the effectiveness of our method, we conducted experiments on selected categories of the CDnet 2014 dataset and the I2R dataset. Our method outperformed all top-ranked unsupervised methods. We also achieved better results than one of the two existing weakly supervised methods, and our performance was similar to the other. Our proposed method is online, real-time, efficient, and requires minimal frame-level annotation, making it suitable for a wide range of real-world applications.
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2302.14589.pdf' target='_blank'>https://arxiv.org/pdf/2302.14589.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ren, Shoudong Han, Huilin Ding, Ziwen Zhang, Hongwei Wang, Faquan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14589">Focus On Details: Online Multi-object Tracking with Diverse Fine-grained Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Discriminative representation is essential to keep a unique identifier for each target in Multiple object tracking (MOT). Some recent MOT methods extract features of the bounding box region or the center point as identity embeddings. However, when targets are occluded, these coarse-grained global representations become unreliable. To this end, we propose exploring diverse fine-grained representation, which describes appearance comprehensively from global and local perspectives. This fine-grained representation requires high feature resolution and precise semantic information. To effectively alleviate the semantic misalignment caused by indiscriminate contextual information aggregation, Flow Alignment FPN (FAFPN) is proposed for multi-scale feature alignment aggregation. It generates semantic flow among feature maps from different resolutions to transform their pixel positions. Furthermore, we present a Multi-head Part Mask Generator (MPMG) to extract fine-grained representation based on the aligned feature maps. Multiple parallel branches of MPMG allow it to focus on different parts of targets to generate local masks without label supervision. The diverse details in target masks facilitate fine-grained representation. Eventually, benefiting from a Shuffle-Group Sampling (SGS) training strategy with positive and negative samples balanced, we achieve state-of-the-art performance on MOT17 and MOT20 test sets. Even on DanceTrack, where the appearance of targets is extremely similar, our method significantly outperforms ByteTrack by 5.0% on HOTA and 5.6% on IDF1. Extensive experiments have proved that diverse fine-grained representation makes Re-ID great again in MOT.
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2302.08943.pdf' target='_blank'>https://arxiv.org/pdf/2302.08943.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Silva, Nicolas Jourdan, Nils GÃ¤hlert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08943">Long Range Object-Level Monocular Depth Estimation for UAVs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Computer vision-based object detection is a key modality for advanced Detect-And-Avoid systems that allow for autonomous flight missions of UAVs. While standard object detection frameworks do not predict the actual depth of an object, this information is crucial to avoid collisions. In this paper, we propose several novel extensions to state-of-the-art methods for monocular object detection from images at long range. Firstly, we propose Sigmoid and ReLU-like encodings when modeling depth estimation as a regression task. Secondly, we frame the depth estimation as a classification problem and introduce a Soft-Argmax function in the calculation of the training loss. The extensions are exemplarily applied to the YOLOX object detection framework. We evaluate the performance using the Amazon Airborne Object Tracking dataset. In addition, we introduce the Fitness score as a new metric that jointly assesses both object detection and depth estimation performance. Our results show that the proposed methods outperform state-of-the-art approaches w.r.t. existing, as well as the proposed metrics.
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2302.05425.pdf' target='_blank'>https://arxiv.org/pdf/2302.05425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erdi Kara, George Zhang, Joseph J. Williams, Gonzalo Ferrandez-Quinto, Leviticus J. Rhoden, Maximilian Kim, J. Nathan Kutz, Aminur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.05425">Deep Learning Based Object Tracking in Walking Droplet and Granular Intruder Experiments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a deep-learning based tracking objects of interest in walking droplet and granular intruder experiments. In a typical walking droplet experiment, a liquid droplet, known as \textit{walker}, propels itself laterally on the free surface of a vibrating bath of the same liquid. This motion is the result of the interaction between the droplets and the surface waves generated by the droplet itself after each successive bounce. A walker can exhibit a highly irregular trajectory over the course of its motion, including rapid acceleration and complex interactions with the other walkers present in the same bath. In analogy with the hydrodynamic experiments, the granular matter experiments consist of a vibrating bath of very small solid particles and a larger solid \textit{intruder}. Like the fluid droplets, the intruder interacts with and travels the domain due to the waves of the bath but tends to move much slower and much less smoothly than the droplets. When multiple intruders are introduced, they also exhibit complex interactions with each other. We leverage the state-of-art object detection model YOLO and the Hungarian Algorithm to accurately extract the trajectory of a walker or intruder in real-time. Our proposed methodology is capable of tracking individual walker(s) or intruder(s) in digital images acquired from a broad spectrum of experimental settings and does not suffer from any identity-switch issues. Thus, the deep learning approach developed in this work could be used to automatize the efficient, fast and accurate extraction of observables of interests in walking droplet and granular flow experiments. Such extraction capabilities are critically enabling for downstream tasks such as building data-driven dynamical models for the coarse-grained dynamics and interactions of the objects of interest.
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2212.14289.pdf' target='_blank'>https://arxiv.org/pdf/2212.14289.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaid El-Shair, Samir Rawashdeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.14289">High-temporal-resolution event-based vehicle detection and tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event-based vision has been rapidly growing in recent years justified by the unique characteristics it presents such as its high temporal resolutions (~1us), high dynamic range (>120dB), and output latency of only a few microseconds. This work further explores a hybrid, multi-modal, approach for object detection and tracking that leverages state-of-the-art frame-based detectors complemented by hand-crafted event-based methods to improve the overall tracking performance with minimal computational overhead. The methods presented include event-based bounding box (BB) refinement that improves the precision of the resulting BBs, as well as a continuous event-based object detection method, to recover missed detections and generate inter-frame detections that enable a high-temporal-resolution tracking output. The advantages of these methods are quantitatively verified by an ablation study using the higher order tracking accuracy (HOTA) metric. Results show significant performance gains resembled by an improvement in the HOTA from 56.6%, using only frames, to 64.1% and 64.9%, for the event and edge-based mask configurations combined with the two methods proposed, at the baseline framerate of 24Hz. Likewise, incorporating these methods with the same configurations has improved HOTA from 52.5% to 63.1%, and from 51.3% to 60.2% at the high-temporal-resolution tracking rate of 384Hz. Finally, a validation experiment is conducted to analyze the real-world single-object tracking performance using high-speed LiDAR. Empirical evidence shows that our approaches provide significant advantages compared to using frame-based object detectors at the baseline framerate of 24Hz and higher tracking rates of up to 500Hz.
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2211.07173.pdf' target='_blank'>https://arxiv.org/pdf/2211.07173.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Wang, Yuzhou Peng, Xiaodong Yang, Ting Wang, Yanming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.07173">SportsTrack: An Innovative Method for Tracking Athletes in Sports Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The SportsMOT dataset aims to solve multiple object tracking of athletes in different sports scenes such as basketball or soccer. The dataset is challenging because of the unstable camera view, athletes' complex trajectory, and complicated background. Previous MOT methods can not match enough high-quality tracks of athletes. To pursue higher performance of MOT in sports scenes, we introduce an innovative tracker named SportsTrack, we utilize tracking by detection as our detection paradigm. Then we will introduce a three-stage matching process to solve the motion blur and body overlapping in sports scenes. Meanwhile, we present another innovation point: one-to-many correspondence between detection bboxes and crowded tracks to handle the overlap of athletes' bodies during sports competitions. Compared to other trackers such as BOT-SORT and ByteTrack, We carefully restored edge-lost tracks that were ignored by other trackers. Finally, we reached the SOTA result in the SportsMOT dataset.
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2210.12476.pdf' target='_blank'>https://arxiv.org/pdf/2210.12476.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yo-Chung Lau, Kuan-Wei Tseng, I-Ju Hsieh, Hsiao-Ching Tseng, Yi-Ping Hung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.12476">A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time object pose estimation and tracking is challenging but essential for emerging augmented reality (AR) applications. In general, state-of-the-art methods address this problem using deep neural networks which indeed yield satisfactory results. Nevertheless, the high computational cost of these methods makes them unsuitable for mobile devices where real-world applications usually take place. In addition, head-mounted displays such as AR glasses require at least 90~FPS to avoid motion sickness, which further complicates the problem. We propose a flexible-frame-rate object pose estimation and tracking system for mobile devices. It is a monocular visual-inertial-based system with a client-server architecture. Inertial measurement unit (IMU) pose propagation is performed on the client side for high speed tracking, and RGB image-based 3D pose estimation is performed on the server side to obtain accurate poses, after which the pose is sent to the client side for visual-inertial fusion, where we propose a bias self-correction mechanism to reduce drift. We also propose a pose inspection algorithm to detect tracking failures and incorrect pose estimation. Connected by high-speed networking, our system supports flexible frame rates up to 120 FPS and guarantees high precision and real-time tracking on low-end devices. Both simulations and real world experiments show that our method achieves accurate and robust object tracking.
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2209.14122.pdf' target='_blank'>https://arxiv.org/pdf/2209.14122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shule Li, Vincent Albert Wolff
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.14122">Tracking Accuracy Based Generation Rules of Collective Perception Messages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Collective Perception Service (CPS) enables the enhancement of environmental awareness of Intelligent Transport System Stations (ITS-S) through the exchange of tracking information between stations. As the market penetration of CPS is growing, the intelligent distribution of the limited communication resources becomes more and more challenging. To address this problem, the ETSI CPS proposes dynamic-based object selection to generate Collective Perception Messages. However, this approach has limits and barely considers detection accuracy and the recent information available at other ITS-Ss in the transmission range. We show a proposal considering the current object tracking accuracy in the local environment and the object tracking from messages received by other stations in order to intelligently decide whether to include an object in a CPM. The algorithm decides based on the relative entropy between the local and V2X tracking accuracy if the object information is valuable for the nearby stations. Our simulation according to the ITS-G5 standard shows that the Channel Busy Ratio (CBR) can be reduced with accuracy-based generation of CPM while improving Object Tracking Accuracy (OTA) compared to generation based on ETSI rules.
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2209.07888.pdf' target='_blank'>https://arxiv.org/pdf/2209.07888.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathieu Gonzalez, Eric Marchand, Amine Kacete, JÃ©rÃ´me Royan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.07888">TwistSLAM++: Fusing multiple modalities for accurate dynamic semantic SLAM</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most classical SLAM systems rely on the static scene assumption, which limits their applicability in real world scenarios. Recent SLAM frameworks have been proposed to simultaneously track the camera and moving objects. However they are often unable to estimate the canonical pose of the objects and exhibit a low object tracking accuracy. To solve this problem we propose TwistSLAM++, a semantic, dynamic, SLAM system that fuses stereo images and LiDAR information. Using semantic information, we track potentially moving objects and associate them to 3D object detections in LiDAR scans to obtain their pose and size. Then, we perform registration on consecutive object scans to refine object pose estimation. Finally, object scans are used to estimate the shape of the object and constrain map points to lie on the estimated surface within the BA. We show on classical benchmarks that this fusion approach based on multimodal information improves the accuracy of object tracking.
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2208.08041.pdf' target='_blank'>https://arxiv.org/pdf/2208.08041.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John Willes, Cody Reading, Steven L. Waslander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.08041">InterTrack: Interaction Transformer for 3D Multi-Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D multi-object tracking (MOT) is a key problem for autonomous vehicles, required to perform well-informed motion planning in dynamic environments. Particularly for densely occupied scenes, associating existing tracks to new detections remains challenging as existing systems tend to omit critical contextual information. Our proposed solution, InterTrack, introduces the Interaction Transformer for 3D MOT to generate discriminative object representations for data association. We extract state and shape features for each track and detection, and efficiently aggregate global information via attention. We then perform a learned regression on each track/detection feature pair to estimate affinities, and use a robust two-stage data association and track management approach to produce the final tracks. We validate our approach on the nuScenes 3D MOT benchmark, where we observe significant improvements, particularly on classes with small physical sizes and clustered objects. As of submission, InterTrack ranks 1st in overall AMOTA among methods using CenterPoint detections.
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2203.06424.pdf' target='_blank'>https://arxiv.org/pdf/2203.06424.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Run Luo, JinLin Wei, Qiao Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2203.06424">VariabilityTrack:Multi-Object Tracking with Variable Speed Object Movement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects in videos. Most methods can be roughly classified as tracking-by-detection and joint-detection-association paradigms. Although the latter has elicited more attention and demonstrates comparable performance relative than the former, we claim that the tracking-by-detection paradigm is still the optimal solution in terms of tracking accuracy,such as ByteTrack,which achieves 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of MOT17 with 30 FPS running speed on a single V100 GPU.However, under complex perspectives such as vehicle and UAV acceleration, the performance of such a tracker using uniform Kalman filter will be greatly affected, resulting in tracking loss.In this paper, we propose a variable speed Kalman filter algorithm based on environmental feedback and improve the matching process, which can greatly improve the tracking effect in complex variable speed scenes while maintaining high tracking accuracy in relatively static scenes. Eventually, higher MOTA and IDF1 results can be achieved on MOT17 test set than ByteTrack
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2110.12962.pdf' target='_blank'>https://arxiv.org/pdf/2110.12962.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haosheng Chen, Yue Wu, Yidong Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.12962">Event Data Association via Robust Model Fitting for Event-based Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Event-based approaches, which are based on bio-inspired asynchronous event cameras, have achieved promising performance on various computer vision tasks. However, the study of the fundamental event data association problem is still in its infancy. In this paper, we propose a novel Event Data Association (called EDA) approach to explicitly address the event association and fusion problem. The proposed EDA seeks for event trajectories that best fit the event data, in order to perform unifying data association and information fusion. In EDA, we first asynchronously fuse the event data based on its information entropy. Then, we introduce a deterministic model hypothesis generation strategy, which effectively generates model hypotheses from the fused events, to represent the corresponding event trajectories. After that, we present a two-stage weighting algorithm, which robustly weighs and selects true models from the generated model hypotheses, through multi-structural geometric model fitting. Meanwhile, we also propose an adaptive model selection strategy to automatically determine the number of the true models. Finally, we use the selected true models to associate and fuse the event data, without being affected by sensor noise and irrelevant structures. We evaluate the performance of the proposed EDA on the object tracking task. The experimental results show the effectiveness of EDA under challenging scenarios, such as high speed, motion blur, and high dynamic range conditions.
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2107.02984.pdf' target='_blank'>https://arxiv.org/pdf/2107.02984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reza Jalil Mozhdehi, Henry Medeiros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.02984">Deep Convolutional Correlation Iterative Particle Filter for Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work proposes a novel framework for visual tracking based on the integration of an iterative particle filter, a deep convolutional neural network, and a correlation filter. The iterative particle filter enables the particles to correct themselves and converge to the correct target position. We employ a novel strategy to assess the likelihood of the particles after the iterations by applying K-means clustering. Our approach ensures a consistent support for the posterior distribution. Thus, we do not need to perform resampling at every video frame, improving the utilization of prior distribution information. Experimental results on two different benchmark datasets show that our tracker performs favorably against state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2011.10471.pdf' target='_blank'>https://arxiv.org/pdf/2011.10471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yorai Shaoul, Katherine Liu, Kyel Ok, Nicholas Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2011.10471">Online Descriptor Enhancement via Self-Labelling Triplets for Visual Data Association</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object-level data association is central to robotic applications such as tracking-by-detection and object-level simultaneous localization and mapping. While current learned visual data association methods outperform hand-crafted algorithms, many rely on large collections of domain-specific training examples that can be difficult to obtain without prior knowledge. Additionally, such methods often remain fixed during inference-time and do not harness observed information to better their performance. We propose a self-supervised method for incrementally refining visual descriptors to improve performance in the task of object-level visual data association. Our method optimizes deep descriptor generators online, by continuously training a widely available image classification network pre-trained with domain-independent data. We show that earlier layers in the network outperform later-stage layers for the data association task while also allowing for a 94% reduction in the number of parameters, enabling the online optimization. We show that self-labelling challenging triplets--choosing positive examples separated by large temporal distances and negative examples close in the descriptor space--improves the quality of the learned descriptors for the multi-object tracking task. Finally, we demonstrate that our approach surpasses other visual data-association methods applied to a tracking-by-detection task, and show that it provides better performance-gains when compared to other methods that attempt to adapt to observed information.
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/1809.01701.pdf' target='_blank'>https://arxiv.org/pdf/1809.01701.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Athindran Ramesh Kumar, Balaraman Ravindran, Anand Raghunathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1809.01701">Pack and Detect: Fast Object Detection in Videos Using Region-of-Interest Packing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection in videos is an important task in computer vision for various applications such as object tracking, video summarization and video search. Although great progress has been made in improving the accuracy of object detection in recent years due to the rise of deep neural networks, the state-of-the-art algorithms are highly computationally intensive. In order to address this challenge, we make two important observations in the context of videos: (i) Objects often occupy only a small fraction of the area in each video frame, and (ii) There is a high likelihood of strong temporal correlation between consecutive frames. Based on these observations, we propose Pack and Detect (PaD), an approach to reduce the computational requirements of object detection in videos. In PaD, only selected video frames called anchor frames are processed at full size. In the frames that lie between anchor frames (inter-anchor frames), regions of interest (ROIs) are identified based on the detections in the previous frame. We propose an algorithm to pack the ROIs of each inter-anchor frame together into a reduced-size frame. The computational requirements of the detector are reduced due to the lower size of the input. In order to maintain the accuracy of object detection, the proposed algorithm expands the ROIs greedily to provide additional background around each object to the detector. PaD can use any underlying neural network architecture to process the full-size and reduced-size frames. Experiments using the ImageNet video object detection dataset indicate that PaD can potentially reduce the number of FLOPS required for a frame by $4\times$. This leads to an overall increase in throughput of $1.25\times$ on a 2.1 GHz Intel Xeon server with a NVIDIA Titan X GPU at the cost of $1.1\%$ drop in accuracy.
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/1808.01356.pdf' target='_blank'>https://arxiv.org/pdf/1808.01356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Beatriz Blanco-Filgueira, Daniel GarcÃ­a-Lesta, Mauro FernÃ¡ndez-Sanjurjo, VÃ­ctor M. Brea, Paula LÃ³pez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1808.01356">Deep Learning-Based Multiple Object Visual Tracking on Embedded System for IoT and Mobile Edge Computing Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Compute and memory demands of state-of-the-art deep learning methods are still a shortcoming that must be addressed to make them useful at IoT end-nodes. In particular, recent results depict a hopeful prospect for image processing using Convolutional Neural Netwoks, CNNs, but the gap between software and hardware implementations is already considerable for IoT and mobile edge computing applications due to their high power consumption. This proposal performs low-power and real time deep learning-based multiple object visual tracking implemented on an NVIDIA Jetson TX2 development kit. It includes a camera and wireless connection capability and it is battery powered for mobile and outdoor applications. A collection of representative sequences captured with the on-board camera, dETRUSC video dataset, is used to exemplify the performance of the proposed algorithm and to facilitate benchmarking. The results in terms of power consumption and frame rate demonstrate the feasibility of deep learning algorithms on embedded platforms although more effort to joint algorithm and hardware design of CNNs is needed.
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2509.01095.pdf' target='_blank'>https://arxiv.org/pdf/2509.01095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihong Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01095">An End-to-End Framework for Video Multi-Person Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based human pose estimation models aim to address scenarios that cannot be effectively solved by static image models such as motion blur, out-of-focus and occlusion. Most existing approaches consist of two stages: detecting human instances in each image frame and then using a temporal model for single-person pose estimation. This approach separates the spatial and temporal dimensions and cannot capture the global spatio-temporal context between spatial instances for end-to-end optimization. In addition, it relies on separate detectors and complex post-processing such as RoI cropping and NMS, which reduces the inference efficiency of the video scene. To address the above problems, we propose VEPE (Video End-to-End Pose Estimation), a simple and flexible framework for end-to-end pose estimation in video. The framework utilizes three crucial spatio-temporal Transformer components: the Spatio-Temporal Pose Encoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and the Spatio-Temporal Pose Decoder (STPD). These components are designed to effectively utilize temporal context for optimizing human body pose estimation. Furthermore, to reduce the mismatch problem during the cross-frame pose query matching process, we propose an instance consistency mechanism, which aims to enhance the consistency and discrepancy of the cross-frame instance query and realize the instance tracking function, which in turn accurately guides the pose query to perform cross-frame matching. Extensive experiments on the Posetrack dataset show that our approach outperforms most two-stage models and improves inference efficiency by 300%.
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2509.01095.pdf' target='_blank'>https://arxiv.org/pdf/2509.01095.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihong Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01095">An End-to-End Framework for Video Multi-Person Pose Estimation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video-based human pose estimation models aim to address scenarios that cannot be effectively solved by static image models such as motion blur, out-of-focus and occlusion. Most existing approaches consist of two stages: detecting human instances in each image frame and then using a temporal model for single-person pose estimation. This approach separates the spatial and temporal dimensions and cannot capture the global spatio-temporal context between spatial instances for end-to-end optimization. In addition, it relies on separate detectors and complex post-processing such as RoI cropping and NMS, which reduces the inference efficiency of the video scene. To address the above problems, we propose VEPE (Video End-to-End Pose Estimation), a simple and flexible framework for end-to-end pose estimation in video. The framework utilizes three crucial spatio-temporal Transformer components: the Spatio-Temporal Pose Encoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and the Spatio-Temporal Pose Decoder (STPD). These components are designed to effectively utilize temporal context for optimizing human body pose estimation. Furthermore, to reduce the mismatch problem during the cross-frame pose query matching process, we propose an instance consistency mechanism, which aims to enhance the consistency and discrepancy of the cross-frame instance query and realize the instance tracking function, which in turn accurately guides the pose query to perform cross-frame matching. Extensive experiments on the Posetrack dataset show that our approach outperforms most two-stage models and improves inference efficiency by 300%.
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2508.08269.pdf' target='_blank'>https://arxiv.org/pdf/2508.08269.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sagar Verma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08269">emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tendon-driven robotic hands offer unparalleled dexterity for manipulation tasks, but learning control policies for such systems presents unique challenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a direct one-to-one mapping between motion capture (mocap) data and tendon controls, making the learning process complex and expensive. Additionally, visual tracking methods for real-world applications are prone to occlusions and inaccuracies, further complicating joint tracking. Wrist-wearable surface electromyography (sEMG) sensors present an inexpensive, robust alternative to capture hand motion. However, mapping sEMG signals to tendon control remains a significant challenge despite the availability of EMG-to-pose data sets and regression-based models in the existing literature.
  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic hands, extending the emg2pose dataset, which includes recordings from 193 subjects, spanning 370 hours and 29 stages with diverse gestures. This dataset incorporates tendon control signals derived using the MyoSuite MyoHand model, addressing limitations such as invalid poses in prior methods. We provide three baseline regression models to demonstrate emg2tendon utility and propose a novel diffusion-based regression model for predicting tendon control from sEMG recordings. This dataset and modeling framework marks a significant step forward for tendon-driven dexterous robotic manipulation, laying the groundwork for scalable and accurate tendon control in robotic hands. https://emg2tendon.github.io/
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2508.00272.pdf' target='_blank'>https://arxiv.org/pdf/2508.00272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyue Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00272">Towards Robust Semantic Correspondence: A Benchmark and Insights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic correspondence aims to identify semantically meaningful relationships between different images and is a fundamental challenge in computer vision. It forms the foundation for numerous tasks such as 3D reconstruction, object tracking, and image editing. With the progress of large-scale vision models, semantic correspondence has achieved remarkable performance in controlled and high-quality conditions. However, the robustness of semantic correspondence in challenging scenarios is much less investigated. In this work, we establish a novel benchmark for evaluating semantic correspondence in adverse conditions. The benchmark dataset comprises 14 distinct challenging scenarios that reflect commonly encountered imaging issues, including geometric distortion, image blurring, digital artifacts, and environmental occlusion. Through extensive evaluations, we provide several key insights into the robustness of semantic correspondence approaches: (1) All existing methods suffer from noticeable performance drops under adverse conditions; (2) Using large-scale vision models can enhance overall robustness, but fine-tuning on these models leads to a decline in relative robustness; (3) The DINO model outperforms the Stable Diffusion in relative robustness, and their fusion achieves better absolute robustness; Moreover, We evaluate common robustness enhancement strategies for semantic correspondence and find that general data augmentations are ineffective, highlighting the need for task-specific designs. These results are consistent across both our dataset and real-world benchmarks.
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2507.22421.pdf' target='_blank'>https://arxiv.org/pdf/2507.22421.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahla John
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22421">Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Real-time video analysis remains a challenging problem in computer vision, requiring efficient processing of both spatial and temporal information while maintaining computational efficiency. Existing approaches often struggle to balance accuracy and speed, particularly in resource-constrained environments. In this work, we present a unified framework that leverages advanced spatial-temporal modeling techniques for simultaneous action recognition and object tracking. Our approach builds upon recent advances in parallel sequence modeling and introduces a novel hierarchical attention mechanism that adaptively focuses on relevant spatial regions across temporal sequences. We demonstrate that our method achieves state-of-the-art performance on standard benchmarks while maintaining real-time inference speeds. Extensive experiments on UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action recognition accuracy and 2.8% in tracking precision compared to existing methods, with 40% faster inference time.
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2506.13769.pdf' target='_blank'>https://arxiv.org/pdf/2506.13769.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Leveni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13769">Non-planar Object Detection and Identification by Features Matching and Triangulation Growth</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2506.13457.pdf' target='_blank'>https://arxiv.org/pdf/2506.13457.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Momir AdÅ¾emoviÄ
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13457">Deep Learning-Based Multi-Object Tracking: A Comprehensive Survey from Foundations to State-of-the-Art</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-object tracking (MOT) is a core task in computer vision that involves detecting objects in video frames and associating them across time. The rise of deep learning has significantly advanced MOT, particularly within the tracking-by-detection paradigm, which remains the dominant approach. Advancements in modern deep learning-based methods accelerated in 2022 with the introduction of ByteTrack for tracking-by-detection and MOTR for end-to-end tracking. Our survey provides an in-depth analysis of deep learning-based MOT methods, systematically categorizing tracking-by-detection approaches into five groups: joint detection and embedding, heuristic-based, motion-based, affinity learning, and offline methods. In addition, we examine end-to-end tracking methods and compare them with existing alternative approaches. We evaluate the performance of recent trackers across multiple benchmarks and specifically assess their generality by comparing results across different domains. Our findings indicate that heuristic-based methods achieve state-of-the-art results on densely populated datasets with linear object motion, while deep learning-based association methods, in both tracking-by-detection and end-to-end approaches, excel in scenarios with complex motion patterns.
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2501.01206.pdf' target='_blank'>https://arxiv.org/pdf/2501.01206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karolina Prawda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01206">Sensitivity of Room Impulse Responses in Changing Acoustic Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Changes in room acoustics, such as modifications to surface absorption or the insertion of a scattering object, significantly impact measured room impulse responses (RIRs). These changes can affect the performance of systems used in echo cancellation and active acoustics and support tasks such as navigation and object tracking. Recognizing and quantifying such changes is, therefore, critical for advancing technologies based on room acoustics. This study introduces a method for analyzing acoustic environment changes by evaluating the similarity of consecutively recorded RIRs. Short-time coherence is employed to characterize modifications, including changes in wall absorption or the presence of a moving person in the room. A sensitivity rating is further used to quantify the magnitude of these changes. The results clearly differentiate between types of modifications -- atmospheric variation, changes in absorption, and human presence. The methods described provide a novel approach to analyzing and interpreting room acoustics, emphasizing RIR similarity and extracting information from temporal and spectral signal properties.
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2501.00843.pdf' target='_blank'>https://arxiv.org/pdf/2501.00843.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathanael L. Baisa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00843">FusionSORT: Fusion Methods for Online Multi-object Visual Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2412.07966.pdf' target='_blank'>https://arxiv.org/pdf/2412.07966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kurt H. W. Stolle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07966">Balancing Shared and Task-Specific Representations: A Hybrid Approach to Depth-Aware Video Panoptic Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we present Multiformer, a novel approach to depth-aware video panoptic segmentation (DVPS) based on the mask transformer paradigm. Our method learns object representations that are shared across segmentation, monocular depth estimation, and object tracking subtasks. In contrast to recent unified approaches that progressively refine a common object representation, we propose a hybrid method using task-specific branches within each decoder block, ultimately fusing them into a shared representation at the block interfaces. Extensive experiments on the Cityscapes-DVPS and SemKITTI-DVPS datasets demonstrate that Multiformer achieves state-of-the-art performance across all DVPS metrics, outperforming previous methods by substantial margins. With a ResNet-50 backbone, Multiformer surpasses the previous best result by 3.0 DVPQ points while also improving depth estimation accuracy. Using a Swin-B backbone, Multiformer further improves performance by 4.0 DVPQ points. Multiformer also provides valuable insights into the design of multi-task decoder architectures.
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2411.10028.pdf' target='_blank'>https://arxiv.org/pdf/2411.10028.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanzhao Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10028">MOT FCG++: Enhanced Representation of Spatio-temporal Motion and Appearance Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of multi-object tracking (MOT) is to detect and track all objects in a scene across frames, while maintaining a unique identity for each object. Most existing methods rely on the spatial-temporal motion features and appearance embedding features of the detected objects in consecutive frames. Effectively and robustly representing the spatial and appearance features of long trajectories has become a critical factor affecting the performance of MOT. We propose a novel approach for appearance and spatial-temporal motion feature representation, improving upon the hierarchical clustering association method MOT FCG. For spatialtemporal motion features, we first propose Diagonal Modulated GIoU, which more accurately represents the relationship between the position and shape of the objects. Second, Mean Constant Velocity Modeling is proposed to reduce the effect of observation noise on target motion state estimation. For appearance features, we utilize a dynamic appearance representation that incorporates confidence information, enabling the trajectory appearance features to be more robust and global. Based on the baseline model MOT FCG, we have realized further improvements in the performance of all. we achieved 63.1 HOTA, 76.9 MOTA and 78.2 IDF1 on the MOT17 test set, and also achieved competitive performance on the MOT20 and DanceTrack sets.
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2407.08885.pdf' target='_blank'>https://arxiv.org/pdf/2407.08885.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mihir Godbole
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.08885">Manipulating a Tetris-Inspired 3D Video Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Video Synopsis is a technique that performs video compression in a way that preserves the activity in the video. This technique is particularly useful in surveillance and monitoring applications. Although it is still a nascent field of research, there have been several approaches proposed over the last two decades varying with the application, optimization type, nature of data feed, etc. The primary data required for these algorithms arises from some sort of object tracking method. In this paper, we discuss different spatio-temporal data representations suitable for different applications. We also present a formal definition for the video synopsis algorithm. We further discuss the assumptions and modifications to this definition required for a simpler version of the problem. We explore the application of a packing algorithm to solve the problem of video synopsis. Since the nature of the data is three dimensional, we consider 3D packing problems in the discussion. This paper also provides an extensive literature review of different video synopsis methods and packing problems. Lastly, we examine the different applications of this algorithm and how the different data representations discussed earlier can make the problem simpler. We also discuss the future directions of research that can be explored following this discussion.
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2406.16784.pdf' target='_blank'>https://arxiv.org/pdf/2406.16784.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhi Kamboj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.16784">The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The transformer neural network architecture allows for autoregressive sequence-to-sequence modeling through the use of attention layers. It was originally created with the application of machine translation but has revolutionized natural language processing. Recently, transformers have also been applied across a wide variety of pattern recognition tasks, particularly in computer vision. In this literature review, we describe major advances in computer vision utilizing transformers. We then focus specifically on Multi-Object Tracking (MOT) and discuss how transformers are increasingly becoming competitive in state-of-the-art MOT works, yet still lag behind traditional deep learning methods.
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2406.09914.pdf' target='_blank'>https://arxiv.org/pdf/2406.09914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sandeep Singh Sengar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09914">Robust compressive tracking via online weighted multiple instance learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing a robust object tracker is a challenging task due to factors such as occlusion, motion blur, fast motion, illumination variations, rotation, background clutter, low resolution and deformation across the frames. In the literature, lots of good approaches based on sparse representation have already been presented to tackle the above problems. However, most of the algorithms do not focus on the learning of sparse representation. They only consider the modeling of target appearance and therefore drift away from the target with the imprecise training samples. By considering all the above factors in mind, we have proposed a visual object tracking algorithm by integrating a coarse-to-fine search strategy based on sparse representation and the weighted multiple instance learning (WMIL) algorithm. Compared with the other trackers, our approach has more information of the original signal with less complexity due to the coarse-to-fine search method, and also has weights for important samples. Thus, it can easily discriminate the background features from the foreground. Furthermore, we have also selected the samples from the un-occluded sub-regions to efficiently develop the strong classifier. As a consequence, a stable and robust object tracker is achieved to tackle all the aforementioned problems. Experimental results with quantitative as well as qualitative analysis on challenging benchmark datasets show the accuracy and efficiency of our method.
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2406.07228.pdf' target='_blank'>https://arxiv.org/pdf/2406.07228.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07228">Haptic Repurposing with GenAI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mixed Reality aims to merge the digital and physical worlds to create immersive human-computer interactions. Despite notable advancements, the absence of realistic haptic feedback often breaks the immersive experience by creating a disconnect between visual and tactile perceptions. This paper introduces Haptic Repurposing with GenAI, an innovative approach to enhance MR interactions by transforming any physical objects into adaptive haptic interfaces for AI-generated virtual assets. Utilizing state-of-the-art generative AI models, this system captures both 2D and 3D features of physical objects and, through user-directed prompts, generates corresponding virtual objects that maintain the physical form of the original objects. Through model-based object tracking, the system dynamically anchors virtual assets to physical props in real time, allowing objects to visually morph into any user-specified virtual object. This paper details the system's development, presents findings from usability studies that validate its effectiveness, and explores its potential to significantly enhance interactive MR environments. The hope is this work can lay a foundation for further research into AI-driven spatial transformation in immersive and haptic technologies.
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2404.00085.pdf' target='_blank'>https://arxiv.org/pdf/2404.00085.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bahman Moraffah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00085">Bayesian Nonparametrics: An Alternative to Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Bayesian nonparametric models offer a flexible and powerful framework for statistical model selection, enabling the adaptation of model complexity to the intricacies of diverse datasets. This survey intends to delve into the significance of Bayesian nonparametrics, particularly in addressing complex challenges across various domains such as statistics, computer science, and electrical engineering. By elucidating the basic properties and theoretical foundations of these nonparametric models, this survey aims to provide a comprehensive understanding of Bayesian nonparametrics and their relevance in addressing complex problems, particularly in the domain of multi-object tracking. Through this exploration, we uncover the versatility and efficacy of Bayesian nonparametric methodologies, paving the way for innovative solutions to intricate challenges across diverse disciplines.
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2403.18908.pdf' target='_blank'>https://arxiv.org/pdf/2403.18908.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yasuyuki Ihara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18908">Enhancing Multiple Object Tracking Accuracy via Quantum Annealing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multiple object tracking (MOT), a key task in image recognition, presents a persistent challenge in balancing processing speed and tracking accuracy. This study introduces a novel approach that leverages quantum annealing (QA) to expedite computation speed, while enhancing tracking accuracy through the ensembling of object tracking processes. A method to improve the matching integration process is also proposed. By utilizing the sequential nature of MOT, this study further augments the tracking method via reverse annealing (RA). Experimental validation confirms the maintenance of high accuracy with an annealing time of a mere 3 $Î¼$s per tracking process. The proposed method holds significant potential for real-time MOT applications, including traffic flow measurement for urban traffic light control, collision prediction for autonomous robots and vehicles, and management of products mass-produced in factories.
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2402.04275.pdf' target='_blank'>https://arxiv.org/pdf/2402.04275.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenping Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04275">Motion Mapping Cognition: A Nondecomposable Primary Process in Human Vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Human intelligence seems so mysterious that we have not successfully understood its foundation until now. Here, I want to present a basic cognitive process, motion mapping cognition (MMC), which should be a nondecomposable primary function in human vision. Wherein, I point out that, MMC process can be used to explain most of human visual functions in fundamental, but can not be effectively modelled by traditional visual processing ways including image segmentation, object recognition, object tracking etc. Furthermore, I state that MMC may be looked as an extension of Chen's theory of topological perception on human vision, and seems to be unsolvable using existing intelligent algorithm skills. Finally, along with the requirements of MMC problem, an interesting computational model, quantized topological matching principle can be derived by developing the idea of optimal transport theory. Above results may give us huge inspiration to develop more robust and interpretable machine vision models.
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2311.18636.pdf' target='_blank'>https://arxiv.org/pdf/2311.18636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Apoorv Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18636">End-to-end Autonomous Driving using Deep Learning: A Systematic Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>End-to-end autonomous driving is a fully differentiable machine learning system that takes raw sensor input data and other metadata as prior information and directly outputs the ego vehicle's control signals or planned trajectories. This paper attempts to systematically review all recent Machine Learning-based techniques to perform this end-to-end task, including, but not limited to, object detection, semantic scene understanding, object tracking, trajectory predictions, trajectory planning, vehicle control, social behavior, and communications. This paper focuses on recent fully differentiable end-to-end reinforcement learning and deep learning-based techniques. Our paper also builds taxonomies of the significant approaches by sub-grouping them and showcasing their research trends. Finally, this survey highlights the open challenges and points out possible future directions to enlighten further research on the topic.
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2308.07016.pdf' target='_blank'>https://arxiv.org/pdf/2308.07016.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuedong Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.07016">HHTrack: Hyperspectral Object Tracking Using Hybrid Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hyperspectral imagery provides abundant spectral information beyond the visible RGB bands, offering rich discriminative details about objects in a scene. Leveraging such data has the potential to enhance visual tracking performance. In this paper, we propose a hyperspectral object tracker based on hybrid attention (HHTrack). The core of HHTrack is a hyperspectral hybrid attention (HHA) module that unifies feature extraction and fusion within one component through token interactions. A hyperspectral bands fusion (HBF) module is also introduced to selectively aggregate spatial and spectral signatures from the full hyperspectral input. Extensive experiments demonstrate the state-of-the-art performance of HHTrack on benchmark Near Infrared (NIR), Red Near Infrared (Red-NIR), and Visible (VIS) hyperspectral tracking datasets. Our work provides new insights into harnessing the strengths of transformers and hyperspectral fusion to advance robust object tracking.
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2308.04000.pdf' target='_blank'>https://arxiv.org/pdf/2308.04000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinggang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04000">Multi-level Map Construction for Dynamic Scenes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In dynamic scenes, both localization and mapping in visual SLAM face significant challenges. In recent years, numerous outstanding research works have proposed effective solutions for the localization problem. However, there has been a scarcity of excellent works focusing on constructing long-term consistent maps in dynamic scenes, which severely hampers map applications. To address this issue, we have designed a multi-level map construction system tailored for dynamic scenes. In this system, we employ multi-object tracking algorithms, DBSCAN clustering algorithm, and depth information to rectify the results of object detection, accurately extract static point clouds, and construct dense point cloud maps and octree maps. We propose a plane map construction algorithm specialized for dynamic scenes, involving the extraction, filtering, data association, and fusion optimization of planes in dynamic environments, thus creating a plane map. Additionally, we introduce an object map construction algorithm targeted at dynamic scenes, which includes object parameterization, data association, and update optimization. Extensive experiments on public datasets and real-world scenarios validate the accuracy of the multi-level maps constructed in this study and the robustness of the proposed algorithms. Furthermore, we demonstrate the practical application prospects of our algorithms by utilizing the constructed object maps for dynamic object tracking.
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2306.07559.pdf' target='_blank'>https://arxiv.org/pdf/2306.07559.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangchun Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07559">Marking anything: application of point cloud in extracting video target features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Extracting retrievable features from video is of great significance for structured video database construction, video copyright protection and fake video rumor refutation. Inspired by point cloud data processing, this paper proposes a method for marking anything (MA) in the video, which can extract the contour features of any target in the video and convert it into a feature vector with a length of 256 that can be retrieved. The algorithm uses YOLO-v8 algorithm, multi-object tracking algorithm and PointNet++ to extract contour of the video detection target to form spatial point cloud data. Then extract the point cloud feature vector and use it as the retrievable feature of the video detection target. In order to verify the effectiveness and robustness of contour feature, some datasets are crawled from Dou Yin and Kinetics-700 dataset as experimental data. For Dou Yin's homogenized videos, the proposed contour features achieve retrieval accuracy higher than 97% in Top1 return mode. For videos from Kinetics 700, the contour feature also showed good robustness for partial clip mode video tracing.
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2303.09840.pdf' target='_blank'>https://arxiv.org/pdf/2303.09840.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix HÃ¤rer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09840">Executable Models and Instance Tracking for Decentralized Applications on Blockchains and Cloud Platforms -- Metamodel and Implementation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Decentralized applications rely on non-centralized technical infrastructures and coordination principles. Without trusted third parties, their execution is not controlled by entities exercising centralized coordination but is instead realized through technologies supporting distribution such as blockchains and serverless computing. Executing decentralized applications with these technologies, however, is challenging due to the limited transparency and insight in the execution, especially when involving centralized cloud platforms. This paper extends an approach for execution and instance tracking on blockchains and cloud platforms permitting distributed parties to observe the instances and states of executable models. The approach is extended with (1.) a metamodel describing the concepts for instance tracking on cloud platforms independent of concrete models or implementation, (2.) a multidimensional data model realizing the concepts accordingly, permitting the verifiable storage, tracking, and analysis of execution states for distributed parties, and (3.) an implementation on the Ethereum blockchain and Amazon Web Services (AWS) using state machine models. Towards supporting decentralized applications with high scalability and distribution requirements, the approach establishes a consistent view on instances for distributed parties to track and analyze the execution along multiple dimensions such as specific clients and execution engines.
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2302.14415.pdf' target='_blank'>https://arxiv.org/pdf/2302.14415.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ZongTan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.14415">Mesh-SORT: Simple and effective location-wise tracker with lost management strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-Object Tracking (MOT) has gained extensive attention in recent years due to its potential applications in traffic and pedestrian detection. We note that tracking by detection may suffer from errors generated by noise detectors, such as an imprecise bounding box before the occlusions, and observed that in most tracking scenarios, objects tend to move and lost within specific locations. To counter this, we present a novel tracker to deal with the bad detector and occlusions. Firstly, we proposed a location-wise sub-region recognition method which equally divided the frame, which we called mesh. Then we proposed corresponding location-wise loss management strategies and different matching strategies. The resulting Mesh-SORT, ablation studies demonstrate its effectiveness and made 3% fragmentation 7.2% ID switches drop and 0.4% MOTA improvement compared to the baseline on MOT17 datasets. Finally, we analyze its limitation on the specific scene and discussed what future works can be extended.
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2302.02622.pdf' target='_blank'>https://arxiv.org/pdf/2302.02622.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabian KÃ¼ppers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.02622">Uncertainty Calibration and its Application to Object Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based environment perception is an important component especially for driver assistance systems or autonomous driving. In this scope, modern neuronal networks are used to identify multiple objects as well as the according position and size information within a single frame. The performance of such an object detection model is important for the overall performance of the whole system. However, a detection model might also predict these objects under a certain degree of uncertainty. [...]
  In this work, we examine the semantic uncertainty (which object type?) as well as the spatial uncertainty (where is the object and how large is it?). We evaluate if the predicted uncertainties of an object detection model match with the observed error that is achieved on real-world data. In the first part of this work, we introduce the definition for confidence calibration of the semantic uncertainty in the context of object detection, instance segmentation, and semantic segmentation. We integrate additional position information in our examinations to evaluate the effect of the object's position on the semantic calibration properties. Besides measuring calibration, it is also possible to perform a post-hoc recalibration of semantic uncertainty that might have turned out to be miscalibrated. [...]
  The second part of this work deals with the spatial uncertainty obtained by a probabilistic detection model. [...] We review and extend common calibration methods so that it is possible to obtain parametric uncertainty distributions for the position information in a more flexible way.
  In the last part, we demonstrate a possible use-case for our derived calibration methods in the context of object tracking. [...] We integrate our previously proposed calibration techniques and demonstrate the usefulness of semantic and spatial uncertainty calibration in a subsequent process. [...]
